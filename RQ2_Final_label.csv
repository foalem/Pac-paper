full_name,Label_Patrick,Label_Leuson,Last_commit_date,First_commit_date,HashiCorp Sentinel,Open Policy Agent (OPA),Pulumi,Cedar Policy Language (CPL),Kyverno OSS,Cloud Custodian,AWS Config,OpagateKeeper,created_at,updated_at,size,stargazers_count,language,has_issues,forks_count,archived,open_issues_count,topics,open_issues,description,fork,has_rego,has_sentinel,has_pulumi,has_cedar,has_kyverno,has_custodian,has_awsconfigcloudgaurd,has_opagatekeeper,contributors_count,readme_content,Disagreement
a2-4am/4cade,Application System,Documentations,2025-05-10T04:14:23Z,2025-04-22T02:29:57Z,0,0,0,0,0,0,8,0,2018-05-04T21:35:21Z,2025-04-06T05:29:18Z,168173,162,Python,VRAI,25,FAUX,11,hacktoberfest,11,"100s of games at your fingertips, as long as your fingertips are on an Apple ][",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,17,"# Is this page for you?

[Download the latest Total Replay disk image](https://archive.org/details/TotalReplay) at the archive.org home page if you just want to play hundreds of Apple II games. The rest of this page is for developers who want to work with the source code and assemble it themselves.

# Building the code

## Mac OS X

You will need
 - [Xcode command line tools](https://developer.apple.com/library/archive/technotes/tn2339/_index.html)
 - [ACME](https://sourceforge.net/projects/acme-crossass/)
 - [Parallel](https://www.gnu.org/software/parallel/)
 - [Cadius](https://github.com/mach-kernel/cadius)
 - [Exomizer](https://bitbucket.org/magli143/exomizer/wiki/Home)

As of this writing, all of the non-Xcode programs are installable via [Homebrew](https://brew.sh/).

``` shell
$ brew tap lifepillar/appleii
$ brew install acme parallel mach-kernel-cadius exomizer
```

Then open a terminal window and type

``` shell
$ cd 4cade/
$ make
```

If all goes well, the `build/` subdirectory will contain a `4cade.hdv` image which can be mounted in emulators like [OpenEmulator](https://archive.org/details/OpenEmulatorSnapshots), [Ample](https://github.com/ksherlock/ample), or [Virtual II](http://virtualii.com/).

If all does not go well, try doing a clean build (`makeg clean && make`)

If that fails, perhaps you have out-of-date versions of one of the required tools? The [Makefile](https://github.com/a2-4am/4cade/blob/main/Makefile) lists, but does not enforce, the minimum version requirements of each third-party tool.

If that fails, please [file a bug](https://github.com/a2-4am/4cade/issues/new).

## Windows

You will need
 - [ACME](https://sourceforge.net/projects/acme-crossass/)
 - [Cadius for Windows](https://github.com/mach-kernel/cadius)
 - [Exomizer](https://bitbucket.org/magli143/exomizer/wiki/Home)

(Those tools will need to be added to your command-line PATH.)

Then open a `CMD.EXE` window and type

```
C:\> cd 4cade
C:\4cade> winmake dsk
```
If all goes well, the `build\` subdirectory will contain a `4cade.hdv` image which can be mounted in emulators like [AppleWin](https://github.com/AppleWin/AppleWin) or [MAME](http://www.mamedev.org).

If all does not go well, try doing a clean build (`winmake clean`, `winmake dsk`)

If that fails, perhaps you have out-of-date versions of one of the required tools? The [winmake](https://github.com/a2-4am/4cade/blob/main/winmake.bat) lists, but does not enforce, the minimum version requirements of each third-party tool.

If that fails, please [file a bug](https://github.com/a2-4am/4cade/issues/new).

## Linux

You will need
 - [Cadius](https://github.com/mach-kernel/cadius)
 - [Exomizer](https://bitbucket.org/magli143/exomizer/wiki/Home)

Most of the packages are already available pre-compiled and can be installed with the following

``` shell
$ sudo apt-get install git parallel acme
```

To compile Cadius enter the following

``` shell
$ git clone https://github.com/mach-kernel/cadius.git
$ cd cadius
$ make
$ cd bin/release
$ sudo cp cadius /usr/bin
```

Then open a terminal window and type

``` shell
$ git clone https://github.com/a2-4am/4cade.git
$ cd 4cade/
$ make
```

If all goes well, the `build/` subdirectory will contain a `4cade.hdv` image which can be mounted in emulators like [MAME](http://www.mamedev.org).

If all does not go well, try doing a clean build (`make clean && make`)

If that fails, perhaps you have out-of-date versions of one of the required tools? The [Makefile](https://github.com/a2-4am/4cade/blob/main/Makefile) lists, but does not enforce, the minimum version requirements of each third-party tool.

If that fails, please [file a bug](https://github.com/a2-4am/4cade/issues/new).

# Navigating the code

## Initialization

[`4cade.a`](https://github.com/a2-4am/4cade/blob/main/src/4cade.a) is the main assembler target. It builds the launcher itself. Launcher code is split into code that can be run once from main memory then discarded, and code which is relocated to the language card and persists throughout the lifetime of the launcher. As the language card is only 16KB and will also need to store some persistent data structures, memory is precious and tightly managed.

[`4cade.init.a`](https://github.com/a2-4am/4cade/blob/main/src/4cade.init.a) contains the code that is run once at program startup. First, we do some hardware detection, like how much memory you have, whether you have a joystick, and whether you have a IIgs. Then we relocate selected code to the language card and load the appropriate search index. (For example, if you do not have a joystick, games that require a joystick will not appear in search results.) [`constants.a`](https://github.com/a2-4am/4cade/blob/main/src/constants.a) has a rough map of what ends up where, within the language card and its overlapping memory regions. Then we load and parse the global preferences file ([`PREFS.CONF`](https://github.com/a2-4am/4cade/blob/main/res/prefs.conf)) and store the results in the language card. Finally, we jump to the main entry point (`Reenter`). The launcher is initialized; anything left in main memory is discarded.

## Search mode

There are three major modes in the launcher: search mode, browse mode, and mega-attract mode. Search mode is the default, and it is always the first mode you enter when launching the program. [`ui.search.mode.a`](https://github.com/a2-4am/4cade/blob/main/src/ui.search.mode.a) tracks your keystrokes to determine the best match within the game list for the keys you have typed, then loads the game's title screenshot and displays the game name and other information at the bottom of the screen. If you have not yet typed any keys, it displays the title page and welcome message instead. The `InputKeys` table documents all other recognized keys.

The text ranking algorithm is in [`textrank.a`](https://github.com/a2-4am/4cade/blob/main/src/textrank.a). It was inspired by [Quicksilver](https://github.com/quicksilver/Quicksilver) but is an independent implementation.

## Browse mode

The user enters browse mode by pressing the right or down arrow key. [`ui.browse.mode.a`](https://github.com/a2-4am/4cade/blob/main/src/ui.browse.mode.a) then watches for other arrow keys and displays the next or previous game in the game list. The `BrowseKeys` table documents all other recognized keys.

## Mega-Attract mode

If the user presses `Esc` from any other mode, or does not type anything for 30 seconds, the launcher goes into Mega-Attract mode, a.k.a. screensaver mode. [`ui.attract.mode.a`](https://github.com/a2-4am/4cade/blob/main/src/ui.attract.mode.a) manages loading and executing attract mode modules. An attract mode module can be a short slideshow, a self-running demo, or just a single screenshot. Modules are listed in [`ATTRACT.CONF`](https://github.com/a2-4am/4cade/blob/main/res/attract.conf) and are run in order until end-of-file, then it starts over from the beginning. The entire cycle is quite long (several hours), and some screenshots appear in multiple slideshows, but there is no actual randomness in selecting the next attract mode module.

# Navigating the configuration files

## `GAMES.CONF`

[`GAMES.CONF`](https://github.com/a2-4am/4cade/blob/main/res/GAMES.CONF) is the master games list. It contains 1 record for every game in Total Replay. However, not every game is playable on every device, so each record also contains metadata, e.g. ""this game requires a joystick,"" or ""this game requires 128K,"" or ""this game has a double hi-res title screen"" (which is not identical to ""this game requires 128K"").

The format of the `GAMES.CONF` file has changed as new requirements have appeared, and it may change again in the future. There is up-to-date format information in comments in the file itself, which I will not duplicate here. However, in general, each record is 1 line and contains the name and flags for 1 game. The file is parsed during build and used to create the search indexes and other files which are stored on the final disk image.

Each game's filename is used as a ""foreign key"" (in database terms) to build directory paths, to locate files in subdirectories, and to reference the game in other configuration files.

- A game's HGR title screenshot is always `TITLE.HGR/FILENAME`
- A game's super hi-res box art is always `ARTWORK.SHR/FILENAME` (not all games have artwork)
- A games's help page is always `GAMEHELP/FILENAME` (not all games have help)
- A game's mini-attract mode configuration file is always `ATTRACT/FILENAME`
- Games are included in other attract mode configuration files by `FILENAME`
- The source disk image of a game (in [`res/dsk`](https://github.com/a2-4am/4cade/tree/main/res/dsk)) must have a volume name of `FILENAME`, and there must be a file in the disk image's root directory also named `FILENAME` which is the game's main executable

## `ATTRACT.CONF`

[`ATTRACT.CONF`](https://github.com/a2-4am/4cade/blob/main/res/ATTRACT.CONF) is the master configuration file for Mega-Attract mode. There is up-to-date format information in comments in the file itself, which I will not duplicate here. In general, each record is the name of an attract module, which can be a slideshow, self-running demo, or even a single screenshot. Each attract module corresponds to a file in a separate directory; see format information for details. So the record `FAVORITES2.CONF=1` corresponds to [`a real file`](https://github.com/a2-4am/4cade/blob/main/res/SS/FAVORITES2.CONF) that contains details about that particular hi-res slideshow. `ATTRACT.CONF` and the linked slideshow configuration files are parsed at build time and stored in a custom format on the final disk image.

Attract modules are loosely divided into sets that have a loosely similar mix of hi-res, double hi-res, super hi-res, and self-running demos. The `ATTRACT.CONF` file is maintained by hand and changes frequently as we add games, split up slideshows, or reorder things on a whim.

Since everything is so loosely associated by filename, it is easy to end up with attract modules that aren't listed in `ATTRACT.CONF` (will use disk space but never run), duplicate modules (will loop incorrectly), or modules that refer to non-existent files (will crash and burn). If you make any changes to `ATTRACT.CONF` or any of the files that it references, or any of the files that those files reference, or even sneeze in their general direction, you should run the attract mode consistency check:

``` shell
$ cd 4cade/
$ make attract
```

## `FX.CONF`, `DFX.CONF`

[`FX.CONF`](https://github.com/a2-4am/4cade/blob/main/res/FX.CONF) and its sister [`DFX.CONF`](https://github.com/a2-4am/4cade/blob/main/res/DFX.CONF) list the HGR and DHGR transition effects used in hi-res and double hi-res slideshows. Each record is a filename of a transition effect file, which is an executable file [assembled at build time](https://github.com/a2-4am/4cade/tree/main/src/fx) and stored on final disk image in a custom format. At the beginning of each slideshow, we query the global preferences to find the filename of the FX or DFX file, then update the global preferences with the next filename (wrapping around to the beginning of the list). If you watch the Mega-Attract mode long enough, you will eventually see all the transition effects, and since the cycle of transition effects is separate from the cycle of slideshows, you will eventually see the same slideshow with different transition effects.

These files are parsed at build time and stored on the final disk image in a binary format, then read from disk every time they are needed. Due to memory restrictions, the parsed data is not persisted.

## `PREFS.CONF`

[`PREFS.CONF`](https://github.com/a2-4am/4cade/blob/main/res/PREFS.CONF) contains persistent global state, including Mega-Attract mode state and whether cheats are enabled. There is up-to-date format information in comments in the file itself.

This file is read and parsed once at program startup, and the parsed data is stored persistently in the language card. It is written to disk every time global state changes, which is often during Mega-Attract mode, or if the user toggles cheat mode.

# Compression

Many graphic files in Total Replay are stored in a compressed format, then decompressed at run-time. The compression and decompression is handled by [Exomizer](https://bitbucket.org/magli143/exomizer/wiki/Home), which targets 8-bit platforms. Compressed files include

- [hi-res action screenshots](https://github.com/a2-4am/4cade/tree/main/res/ACTION.HGR)
- [double hi-res action screenshots](https://github.com/a2-4am/4cade/tree/main/res/ACTION.DHGR)
- [super hi-res box art](https://github.com/a2-4am/4cade/tree/main/res/ARTWORK.SHR)

To add a new compressed graphic file, add the uncompressed original file to the appropriate directory ([`ACTION.HGR.UNCOMPRESSED`](https://github.com/a2-4am/4cade/tree/main/res/ACTION.HGR.UNCOMPRESSED), [`ACTION.DHGR.UNCOMPRESSED`](https://github.com/a2-4am/4cade/tree/main/res/ACTION.DHGR.UNCOMPRESSED), or [`ARTWORK.SHR.UNCOMPRESSED`](https://github.com/a2-4am/4cade/tree/main/res/ARTWORK.SHR.UNCOMPRESSED) respectively), then run

``` shell
$ cd 4cade/
$ make compress
```

Then add and commit the compressed files to the repository.",VRAI
AdminTurnedDevOps/kubernetes-real-world-course,Documentations,Documentations,2024-03-27T11:49:06Z,2023-06-03T14:10:35Z,0,0,0,0,0,0,0,2,2023-05-25T13:35:24Z,2025-01-27T20:27:20Z,49,51,HCL,VRAI,56,FAUX,0,,0,,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,2,,FAUX
alcideio/rbac-tool,Toolkit,DevOPs,2024-10-29T19:18:31Z,2022-06-16T05:14:36Z,0,0,0,0,0,0,0,0,2020-03-22T17:22:01Z,2025-04-07T12:36:02Z,851,1009,Go,VRAI,72,FAUX,10,"access-control,acl,authorization,cluster,k8s-cluster,krew-plugin,kubectl,kubectl-plugin,kubernetes,kubernetes-api,kubernetes-rbac,least-privilege,permissions,podsecuritypolicies,rapid7,rbac,security,who-can,whoami",10,"Rapid7 | insightCloudSec | Kubernetes RBAC Power Toys - Visualize, Analyze, Generate & Query",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,14,"![release](https://img.shields.io/github/v/release/alcideio/rbac-tool?sort=semver)
![Go Version](https://img.shields.io/github/go-mod/go-version/alcideio/rbac-tool)
[![Build](https://github.com/alcideio/rbac-tool/actions/workflows/build.yml/badge.svg?branch=master)](https://github.com/alcideio/rbac-tool/actions/workflows/build.yml)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
![Tweet](https://img.shields.io/twitter/url?style=social&url=https%3A%2F%2Fgithub.com%2Falcideio%2Frbac-tool)

## <img src=""https://www.rapid7.com/Areas/Docs/includes/img/r7-nav/Rapid7_logo-short.svg"" alt=""insightCloudSec"" width=""28""/> | insightCloudSec | RBAC Tool For Kubernetes

## Kubernetes RBAC

Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization.
RBAC authorization uses the `rbac.authorization.k8s.io` API group to drive authorization decisions, allowing you to dynamically configure policies through the Kubernetes API.

Permissions are purely **additive** (there are **no “deny”** rules).

A Role always sets permissions within a particular namespace ; when you create a Role, you have to specify the namespace it belongs in.
ClusterRole, by contrast, is a non-namespaced resource.
ClusterRoles have several uses. You can use a ClusterRole to:

- define permissions on namespaced resources and be granted within individual namespace(s)
- define permissions on namespaced resources and be granted across all namespaces
- define permissions on cluster-scoped resources

If you want to define a role within a namespace, use a Role; if you want to define a role cluster-wide, use a ClusterRole.

**rbac-tool** simplifies querying and creation RBAC policies.

## Install

#### Standalone
Download the latest from the [release](https://github.com/alcideio/rbac-tool/releases) page

```shell script
curl https://raw.githubusercontent.com/alcideio/rbac-tool/master/download.sh | bash
```

#### kubectl plugin // <img src=""https://raw.githubusercontent.com/kubernetes-sigs/krew/master/assets/logo/horizontal/color/krew-horizontal-color.png"" alt=""krew"" width=""48""/>  

```shell script
$ kubectl krew install rbac-tool
```

# rbac-tool

A collection of Kubernetes RBAC tools to sugar coat Kubernetes RBAC complexity

```shell script
rbac-tool

Usage:
  rbac-tool [command]

Available Commands:
  analysis        Analyze RBAC permissions and highlight overly permissive principals, risky permissions, etc.
  auditgen        Generate RBAC policy from Kubernetes audit events
  bash-completion Generate bash completion. source <(rbac-tool bash-completion)
  generate        Generate Role or ClusterRole and reduce the use of wildcards
  help            Help about any command
  lookup          RBAC Lookup by subject (user/group/serviceaccount) name
  policy-rules    RBAC List Policy Rules For subject (user/group/serviceaccount) name
  show            Generate ClusterRole with all available permissions from the target cluster
  version         Print rbac-tool version
  visualize       A RBAC visualizer
  who-can         Shows which subjects have RBAC permissions to perform an action
  whoami          Shows the subject for the current context with which one authenticates with the cluster
  
Flags:
  -h, --help      help for rbac-tool
  -v, --v Level   number for the log level verbosity

Use ""rbac-tool [command] --help"" for more information about a command.
```

- [The `rbac-tool viz` command](#rbac-tool-viz)
- [The `rbac-tool analysis` command](#rbac-tool-analysis)
- [The `rbac-tool lookup` command](#rbac-tool-lookup)
- [The `rbac-tool who-can` command](#rbac-tool-who-can)
- [The `rbac-tool policy-rules` command](#rbac-tool-policy-rules)
- [The `rbac-tool auditgen` command](#rbac-tool-auditgen)
- [The `rbac-tool gen` command](#rbac-tool-gen)
- [The `rbac-tool show` command](#rbac-tool-show)
- [The `rbac-tool whoami` command](#rbac-tool-whoami)
- [Command Line Reference](#command-line-reference)
- [Contributing](#contributing)


# `rbac-tool viz`

A Kubernetes RBAC visualizer that generate a graph as dot file format or in HTML format.

<img src=""img/rbac-viz-html-example.png"" alt=""rbac-tool"" width=""600""/>

By default 'rbac-tool viz' will connect to the local cluster (pointed by kubeconfig)
Create a RBAC graph of the actively running workload on all namespaces except kube-system

See run options on how to render specific namespaces, other clusters, etc.

```shell script
#Render Locally
rbac-tool viz --outformat dot && cat rbac.dot | dot -Tpng > rbac.png  && open rbac.png

# Render Online
https://dreampuf.github.io/GraphvizOnline
```

Examples:

```shell script
# Scan the cluster pointed by the kubeconfig context 'myctx'
rbac-tool viz --cluster-context myctx
```

```shell script
# Scan and create a PNG image from the graph
rbac-tool viz --outformat dot --exclude-namespaces=soemns && cat rbac.dot | dot -Tpng > rbac.png && google-chrome rbac.png
```


# `rbac-tool show`

Generate sample ClusterRole with all available permissions from the target cluster.

rbac-tool read from the Kubernetes discovery API the available API Groups and resources,
and based on the command line options, generate an explicit ClusterRole with available resource permissions.
Examples:

```shell script
# Generate a ClusterRole with all the available permissions for core and apps api groups
rbac-tool show  --for-groups=,apps
```


# `rbac-tool analysis`

Analyze RBAC permissions and highlight overly permissive principals, risky permissions.
The command allows to use a custom analysis rule set, as well as the ability to define custom exceptions (global and per-rule).

The default rule set can be found [here](pkg/analysis/default-rules.yaml)

Examples:

```shell script
# Analyze the cluster pointed by the kubeconfig context 'myctx' with the internal analysis rule set
rbac-tool analysis --cluster-context myctx
```

```shell script
# Analyze the cluster pointed by kubeconfig with the the provided analysis rule set
rbac-tool analysis --config myruleset.yaml
```


# `rbac-tool lookup`
Lookup of the Roles/ClusterRoles used attached to User/ServiceAccount/Group with or without [regex](https://regex101.com/)


Examples:

```shell script
# Search All Service Accounts
rbac-tool lookup
```

```shell script
# Search Service Accounts that match myname exactly
rbac-tool lookup myname
```

```shell script
# Search All Service Accounts that contain myname
rbac-tool lookup -e '.*myname.*'
```

```shell script
# Lookup System Accounts (all accounts that start with system: )
rbac-tool lookup -e '^system:'
  SUBJECT                                         | SUBJECT TYPE | SCOPE       | NAMESPACE   | ROLE                                                                 | BINDING
+-------------------------------------------------+--------------+-------------+-------------+----------------------------------------------------------------------+---------------------------------------------------+
  system:anonymous                                | User         | Role        | kube-public | kubeadm:bootstrap-signer-clusterinfo                                 | kubeadm:bootstrap-signer-clusterinfo
  system:authenticated                            | Group        | ClusterRole |             | system:basic-user                                                    | system:basic-user
  system:authenticated                            | Group        | ClusterRole |             | system:public-info-viewer                                            | system:public-info-viewer
  system:authenticated                            | Group        | ClusterRole |             | system:discovery                                                     | system:discovery
  system:bootstrappers:kubeadm:default-node-token | Group        | ClusterRole |             | kubeadm:get-nodes                                                    | kubeadm:get-nodes
  system:bootstrappers:kubeadm:default-node-token | Group        | ClusterRole |             | system:node-bootstrapper                                             | kubeadm:kubelet-bootstrap
  system:bootstrappers:kubeadm:default-node-token | Group        | ClusterRole |             | system:certificates.k8s.io:certificatesigningrequests:nodeclient     | kubeadm:node-autoapprove-bootstrap
  system:bootstrappers:kubeadm:default-node-token | Group        | Role        | kube-system | kube-proxy                                                           | kube-proxy
  system:bootstrappers:kubeadm:default-node-token | Group        | Role        | kube-system | kubeadm:nodes-kubeadm-config                                         | kubeadm:nodes-kubeadm-config
  system:bootstrappers:kubeadm:default-node-token | Group        | Role        | kube-system | kubeadm:kubelet-config                                               | kubeadm:kubelet-config
  system:kube-controller-manager                  | User         | ClusterRole |             | system:kube-controller-manager                                       | system:kube-controller-manager
...
```

# `rbac-tool who-can`

Shows which subjects have RBAC permissions to perform an action denoted by VERB on an object denoted as ( KIND | KIND/NAME | NON-RESOURCE-URL)

* VERB is a logical Kubernetes API verb like 'get', 'list', 'watch', 'delete', etc.
* KIND is a Kubernetes resource kind. Shortcuts and API groups will be resolved, e.g. 'po' or 'deploy'.
* NAME is the name of a particular Kubernetes resource.
* NON-RESOURCE-URL is a partial URL that starts with ""/"".

Examples:

```shell script
# Who can read ConfigMap resources
rbac-tool who-can get cm

# Who can watch Deployments
rbac-tool who-can watch deployments.apps

# Who can read the Kubernetes API endpoint /apis
rbac-tool who-can get /apis

# Who can read a secret resource by the name some-secret
rbac-tool who-can get secret/some-secret
```

# `rbac-tool policy-rules`
List Kubernetes RBAC policy rules for a given User/ServiceAccount/Group with or without [regex](https://regex101.com/)


Examples:

```shell script
# List policy rules for system unauthenicated group
rbac-tool policy-rules -e '^system:unauth'
```

Output:

```shell script
  TYPE  | SUBJECT                | VERBS | NAMESPACE | API GROUP | KIND | NAMES | NONRESOURCEURI                              
+-------+------------------------+-------+-----------+-----------+------+-------+--------------------------------------------+
  Group | system:unauthenticated | get   | *         | -         | -    | -     | /healthz,/livez,/readyz,/version,/version/  

```

> Leveraging JMESPath to filter and transform RBAC Policy rules.
>
>  For example: *Who Can Read Secrets*
>```shell script
> rbac-tool policy-rules -o json  | jp ""[? @.allowedTo[? (verb=='get' || verb=='*') && (apiGroup=='core' || apiGroup=='*') && (resource=='secrets' || resource == '*')  ]].{name: name, namespace: namespace, kind: kind}""
>```
>
> See [https://jmespath.org/](https://jmespath.org/)
>

# `rbac-tool auditgen`

Generate RBAC policy from Kubernetes audit events.
Audit source format can be:
- Kubernetes List Object that contains Audit Events
- Newline seperated Audit Event objects
Audit source can be file, directory or http URL.

```shell script
rbac-tool auditgen -f audit.log
```

> This command is based on [this](https://github.com/liggitt/audit2rbac) prior work.

# `rbac-tool gen`

Examples would be simplest way to describe how `rbac-tool gen` can help:
*  Generate a `ClusterRole` policy that allows to read everything **except** *secrets* and *services*
*  Generate a `Role` policy that allows create,update,get,list  (read/write) everything **except** *secrets*, *services*, *ingresses*, *networkpolicies*
*  Generate a `Role` policy that allows create,update,get,list  (read/write) everything **except** *statefulsets*

`rbac-tool` generate RBAC `Role` or RBAC `ClusterRole` resource while reducing the use of wildcards, and support **deny** semantics for specific Kubernetes clusters.

# `rbac-tool whoami`

Shows the subject for the current context with which one authenticates with the cluster.

Examples:

```shell script
rbac-tool whoami --cluster-context myctx
```

### How `rbac-tool gen` works?

`rbac-tool` reads from the Kubernetes discovery API the available API Groups and resources, which represents the ""world"" of resources.
Based on the command line options, generate an explicit Role/ClusterRole that avoid wildcards by expanding wildcards to the available ""world"" resources.

###  Command Line Examples

Examples generated against Kubernetes cluster v1.16 deployed using KIND. 

> Generate a `ClusterRole` policy that allows to read everything **except** *secrets* and *services*
```bash
rbac-tool  gen  --deny-resources=secrets.,services. --allowed-verbs=get,list
```

>  Generate a `Role` policy that allows create,update,get,list (read/write) everything **except** *secrets*, *services*, *networkpolicies* in *core*,*apps* & *networking.k8s.io* API groups
```bash
rbac-tool  gen --generated-type=Role --deny-resources=secrets.,services.,networkpolicies.networking.k8s.io --allowed-verbs=* --allowed-groups=,extensions,apps,networking.k8s.io
```

> Generate a `Role` policy that allows create,update,get,list  (read/write) everything **except** *statefulsets*
```bash
rbac-tool  gen --generated-type=Role --deny-resources=apps.statefulsets --allowed-verbs=* 
```


### Example Output

>  Generate a `Role` policy that allows create,update,get,list (read/write) everything **except** *secrets*, *services*, *networkpolicies* in *core*,*apps* & *networking.k8s.io* API groups
```bash
rbac-tool  gen --generated-type=Role --deny-resources=secrets.,services.,networkpolicies.networking.k8s.io --allowed-verbs=* --allowed-groups=,extensions,apps,networking.k8s.io
```

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: null
  name: custom-role
  namespace: mynamespace
rules:
- apiGroups:
  - """"
  resources:
  - events
  - componentstatuses
  - podtemplates
  - namespaces
  - replicationcontrollers
  - persistentvolumes
  - configmaps
  - persistentvolumeclaims
  - resourcequotas
  - limitranges
  - nodes
  - bindings
  - serviceaccounts
  - pods
  - endpoints
  verbs:
  - '*'
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs:
  - '*'
- apiGroups:
  - apps
  resources:
  - replicasets
  - daemonsets
  - deployments
  - controllerrevisions
  - statefulsets
  verbs:
  - '*'
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - '*'
```

## Command Line Reference

```bash
Generate Role or ClusterRole resource while reducing the use of wildcards.

rbac-tool read from the Kubernetes discovery API the available API Groups and resources, 
and based on the command line options, generate an explicit Role/ClusterRole that avoid wildcards

Examples:

# Generate a Role with read-only (get,list) excluding secrets (core group) and ingresses (extensions group) 
rbac-tool gen --generated-type=Role --deny-resources=secrets.,ingresses.extensions --allowed-verbs=get,list

# Generate a Role with read-only (get,list) excluding secrets (core group) from core group, admissionregistration.k8s.io,storage.k8s.io,networking.k8s.io
rbac-tool gen --generated-type=ClusterRole --deny-resources=secrets., --allowed-verbs=get,list  --allowed-groups=,admissionregistration.k8s.io,storage.k8s.io,networking.k8s.io

Usage:
  rbac-tool generate [flags]

Aliases:
  generate, gen

Flags:
      --allowed-groups strings   Comma separated list of API groups we would like to allow '*' (default [*])
      --allowed-verbs strings    Comma separated list of verbs to include. To include all use '* (default [*])
  -c, --cluster-context string   Cluster.use 'kubectl config get-contexts' to list available contexts
      --deny-resources strings   Comma separated list of resource.group
  -t, --generated-type string    Role or ClusteRole (default ""ClusterRole"")
  -h, --help                     help for generate
```

## Contributing

### Bugs

If you think you have found a bug please follow the instructions below.

- Please spend a small amount of time giving due diligence to the issue tracker. Your issue might be a duplicate.
- Open a [new issue](https://github.com/alcideio/rbac-tool/issues/new) if a duplicate doesn't already exist.

### Features

If you have an idea to enhance rbac-tool follow the steps below.

- Open a [new issue](https://github.com/alcideio/rbac-tool/issues/new).
- Remember users might be searching for your issue in the future, so please give it a meaningful title to helps others.
- Clearly define the use case, using concrete examples.
- Feel free to include any technical design for your feature.

### Pull Requests

- Your PR is more likely to be accepted if it focuses on just one change.
- Please include a comment with the results before and after your change. 
- Your PR is more likely to be accepted if it includes tests. 
- You're welcome to submit a draft PR if you would like early feedback on an idea or an approach.


[![Stargazers over time](https://starchart.cc/alcideio/rbac-tool.svg)](https://starchart.cc/alcideio/rbac-tool)",VRAI
amigavision/AmigaVision,Application System,Documentations,2025-05-10T20:19:25Z,2025-04-09T11:11:47Z,0,0,0,0,0,0,0,0,2020-02-12T13:55:40Z,2025-04-06T20:55:23Z,571824,138,Python,VRAI,5,FAUX,68,"amiga,emulation,fpga,preservation",68,"The ultimate Amiga games & demo scene setup for MiSTer & Pocket FPGAs, emulators, and real hardware. Open source, community driven. This is an Amiga HDF image builder that uses WHDLoad and custom installs, based on the Arcade Game Selector launcher.",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,4,"# AmigaVision

(The latest version of this documentation can also always be found on our web site at [https://amiga.vision/docs])

AmigaVision is a carefully curated collection of game and demo configurations for the [Amiga computer] platform, as well as a minimal [Workbench] setup with useful utilities and apps, wrapped in a user-friendly launcher.

It aims to balance preservation of the historical and current output of the [Amiga games] and [demo scene] as accurately as possible, while still being easy to use for people new to the Amiga computer — making it as easy to use as any game console.

It has many features specifically for use with [MiSTer] and [Analogue Pocket] FPGA devices, but also aims to work with emulators like UAE, handheld emulation devices, and original hardware like the Amiga 1200, Amiga 4000 and Amiga CD32.

## Contents

1. [Features](#features)
2. [Save Files](#save-files)
3. [Upgrading](#upgrading)
4. [Setup for Amiga hardware](#setup-for-amiga-hardware)
5. [Setup for emulators](#setup-for-emulators)
6. [Setup for Pocket](#setup-for-pocket)
7. [Setup for MiSTer](#setup-for-mister)
8. [Optional Setups](#optional-setups)
9. [MiSTer: Gamepad & Joystick Mapping](#mister-gamepad--joystick-mapping)
10. [MiSTer: Video Modes](#mister-video-modes)
11. [MiSTer: CPU Performance Notes](#mister-cpu-performance-notes)
12. [Workbench](#workbench)
13. [CD³² Games Support](#cd-games-support)
14. [Non-working Games](#non-working-games)
15. [Custom Scripts](#custom-scripts)
16. [Bug Reports & Feature Requests](#bug-reports--feature-requests)
17. [Credits](#credits)
18. [Troubleshooting](#troubleshooting)
19. [Frequently Asked Questions](#frequently-asked-questions)

## Features

* Supports original Amiga 1200, 4000 *&* CD32 hardware, MiSTer, MiST and Analogue Pocket FPGA hardware recreations, as well as Amiga emulators.

* Sophisticated and performant Amiga games and demo launcher with screenshots included, can be entirely controlled using gamepads, joysticks or via keyboard. This lets you quickly and easily experience the best of what the system has to offer.

* Close to 2000 hand-tuned, per-game 5× integer scaling with Dynamic Crop settings on MiSTer, ensuring that games and demos make the best use of modern 1080p and 4K 16:9 displays without leaving large parts of the screen blank.

* Carefully curated and well-tested settings for games and demos, no duplication of AGA and ECS versions, with lots of genre and top lists to help you navigate the massive amount of Amiga games available. We take special care to run every game at the correct aspect ratio and CPU speed.

* Games are configured to run in their correct modes, games created in Europe use PAL with 5× Dynamic Crop where appropriate, whereas US-made games run in NTSC for the correct aspect ratio and CPU speed. You can optionally override this in the settings.

* Includes key productions from the legendary Amiga demo scene, including disk magazines sorted chronologically, making it a great companion to explore the demo scene's [UNESCO-nominated] cultural heritage artifacts.

* Hand-tuned scanline and shadow mask settings for MiSTer to get you close to that original CRT look if you are using it on a modern flat panel display. Of course, the setup also works with analog output to real CRT displays.

* Shared file system support for the `MiSTer:` volume, making it trivial to transfer files to and from the Amiga over WiFi or wired networks, or directly using the SD card.

* Internet support is included, you can go online with your Amiga and download apps, games and graphics/music files directly to your setup.

* Minimalist Workbench setup with support for including your own custom set of configurations, games, applications and files using the `Saves:` HD image that will survive upgrades of the main HD image.

* RTG resolution support for running Workbench in modern resolutions like 1920×1080 and in 16:9 aspect ratios on MiSTer.

* Uses PFS as its file system to avoid accidental corruption on write operations, which the standard FFS file system is very prone to.

* Includes a dedicated MiSTer setup to closely mimic a stock Amiga 500 with memory expansion (for use with ADF files only) for maximum compatibility with demo scene productions and any troublesome games that rely on cycle accuracy and exact hardware timings.

* Includes an optional, dedicated Amiga 500HD setup that gives you a representative feel for how it was to use Workbench 1.3 with a hard disk and productivity apps around 1989.

* Includes an optional, dedicated Amiga 600HD setup that gives you a representative feel for how it was to use Workbench 2.x with an Amiga 600 or 3000 and productivity apps around 1991-1992.

* Includes a dedicated Amiga CD32 setup for MiSTer that is compatible with the majority of the released games for it.

## Save Files

Before we start, a quick note on save files:

**IMPORTANT:** For games with save functionality you **MUST** quit the game using the `DEL` key to ensure the save data will be written to ""disk"", and thus the SD card. You *will* lose your save games if you don't exit the game *after* also saving in-game! 

In the `[Options]` menu of the launcher you can choose between a few alternative quit key options if the `DEL` key doesn't work for you. If set, it will override the preconfigured default Quit key. The active Quit key is displayed on the splash screen shown when a game is loading.

The reason for this is that most games run in a WHDLoad container (think ""virtualization""), and cannot write the save data to disk outside the container until you explicitly exit the game.

## Upgrading

When upgrading AmigaVision from a previous release (on any platform), we always recommend following the instructions for doing a clean install and **only** keep your `Saves.hdf` hard drive image, which is where your game saves are stored.

We often make important changes to the configuration files, `MiSTer.ini` and other included files, so the only way to ensure that you can use the new version properly and get the new capabilities (examples from previous releases: 5×PAL Dynamic Scaling, CD32 support on MiSTer, etc.) is to do a clean install of everything, every time. 

Follow the instructions for your platform below.

## Setup for Amiga hardware

AmigaVision supports any AGA-capable Amiga: Amiga 1200, Amiga 4000, and Amiga CD32 — as long as it has a mass storage device like an SD card or CF card connected via an adapter to the IDE bus.

To set up AmigaVision, choose your favorite disk imaging tool — e.g. [Balena Etcher] for Mac, Linux and Windows, [Win32 Disk Imager] for Windows, or whatever tool you prefer for writing to SD/CF cards.

Simply locate the `games/Amiga/AmigaVision.hdf` file, and load that in your disk imaging tool of choice, and write it to the SD/CF card. If the file requester in the disk imaging tool does not allow you to select `.hdf` files, you may need to rename it to have a different extension, e.g. `.img`, `.bin` or similar.

If your HDF image contains every game in the database, you will need a 16GB CF/SD card.

### A note about save files & upgrading on Amiga hardware

Note that save files in games will only be written to disk when you quit the game as described in the [Save Files](#save-files) section.

When installed on a single partition like this, you obviously will overwrite any save files if you upgrade AmigaVision to its newest release. The easiest way is to mount an ADF (or a real floppy!) and copy out the save files before upgrading, and put them back after the new image has been written to your SD/CF card.

Save files are located in `DH0:WHDSaves` — and are usually small enough that all of them fit comfortably on an ADF or a real floppy. So don't let that stop you from upgrading to new and improved versions of AmigaVision, it's really quite easy!

### Joystick and Gamepad support on Amiga hardware

We support single-button Amiga/C64 joysticks, as well as four-button CD32 gamepads, and probably Sega Mega Drive (aka. Sega Genesis) gamepads as well — although we haven't personally tested this.

Many WHDLoad games have been patched to support multiple buttons, so check for those options when starting a game.

## Setup for Emulators

We recommend — and include a setup for — the [FS-UAE] Amiga emulator, which supports Mac, Windows and Linux.

1. Download and install [FS-UAE].
2. Copy the `games/Amiga` directory to your preferred location.
3. Double-click the `AmigaVision.fs-uae` file to run the setup with the preferred settings. You can of course also add a shortcut to this file to your Windows start menu, or as an alias in the Mac's Applications folder.

For any additional configuration or customizations, consult the FS-UAE documentation.

## Setup for Pocket

AmigaVision also works great with the handheld [Analogue Pocket] FPGA device. Do note that the Amiga core on Analogue Pocket does not support mounting/loading disc images for CD³² yet, but the 3000+ games in the library work great.

* Use any of the Pocket updaters to get the OpenFPGA cores, or manually download the latest version of the [Amiga Pocket Core] and put it on your device.
* Copy the files in the AmigaVision directory `games/Amiga` (HDF and ROM files) to the `/Assets/amiga/common` directory on your Pocket SD card.
* Start the Amiga core, enjoy!

### Pocket Controls

* `Select` button brings up the on-screen keyboard, hit `DEL` to quit a game.
* `Start` button toggles mouse emulation mode, left/right triggers are the left/right mouse buttons.
* `A` button selects an entry in the launcher.
* `B` button goes back to the parent category in the launcher.

Many games are updated to support two-button controls, but some require remapping if you prefer button-to-jump instead of up-to-jump (which makes sense on a joystick, but is pretty terrible on a gamepad). Button remapping is done just like in any other Pocket core.

## Setup for MiSTer

Copy the contents of the following directories to the corresponding directories in the top level on MiSTer's file system:

```
_Computer
_Console
config
Filters
games
Presets
Shadow_Masks
```

Paste the following recommended core settings to the bottom of your `MiSTer.ini` file in the root of your MiSTer file system — these settings are further explained in the [Video Modes](#mister-video-modes) section. It's especially important to explicity define resolutions for both PAL and NTSC, and *not* rely on the automatic fallback that MiSTer has available:

```
[Amiga
+Amiga500
+Amiga500HD
+Amiga600HD
+AmigaCD32]
video_mode_ntsc=8 ; These two use the recommended setting of 1080p60 and
video_mode_pal=9  ; 1080p50, adjust if you want a different resolution
vscale_mode=0
vsync_adjust=1 ; You can set this to 2 if your display can handle it
custom_aspect_ratio_1=40:27
bootscreen=0
```

Note that *even if* your 16:9 4K TV *can* handle and scale 1440p, we *still recommend* using 1080p output, since that will do proper integer scaling to 4K and make use of the per-game 5× Zoom and Dynamic Crop modes, to reduce the amount of pixels wasted on black bars along the edges.

### Other monitor sizes

The above default settings assume that you are using it with a 16:9 format 1080p or 4K TV or computer monitor. 

If you are using a 1440p computer monitor (great for scaling ~240p retro gaming output, since 240×6 = 1440) — use these settings for the video modes instead, no matter how many pixels wide it is:

```
video_mode_ntsc=1920,1440,60
video_mode_pal=1920,1440,50
```

Finally, 1920×1200 (16:10) computer monitors are also especially good for scaling ~240p retro gaming output with minimal black borders (since 240×5 = 1200). In that case, use these settings for the video modes:

```
video_mode_ntsc=1920,1200,60
video_mode_pal=1920,1200,50
```

### Using the Amiga Cores

Reboot your MiSTer, you should now have two entries in the `Computer` section, and one in `Console`: 

  * `Amiga` for the main AmigaVision setup — you'll be using this one 99% of the time.
  * `Amiga 500` for a stock Amiga 500 hardware setup with no hard drive to use with ADF floppy disk images for any troublesome demos or games that don't work with the main setup. Some demo ADFs are included and can be mounted as floppy disks in MiSTer's OSD menu, invoked with the `F12` key.
  * `Amiga CD32` for the Amiga CD³² games console — see the instructions for how to load disc images in the dedicated [CD³² section](#cd-games-support)

Launch the `Amiga` entry — and enjoy! Don't forget to check out the other sections of the documentation — especially on save files, controller mappings and video modes — once the basic setup is up and running.

### Potential issues

* If you are using an external hard drive or a network drive, do note that you will have to manually specify the location of the Kickstart file as specified in the [Troubleshooting](#troubleshooting) section.

* If you use `names.txt` to rename cores (or pull it down via the `update_all` script), you may end up with *two* entries that *both* say `Amiga` after this. The entry without a date listed is AmigaVision. To fix this duplication, edit `names.txt` and give the `Minimig` core a different name — e.g. use `Commodore Amiga` for that entry instead, and you can use that for any setups unrelated to AmigaVision, if you so desire.

## Optional Setups

(You can probably skip this section if you were not an Amiga user back in the day or unless you have a special interest in computing history :)

If you used the Amiga back in the day, you may have memories of using an Amiga 500 with a hard disk and Workbench 1.3, or maybe an Amiga 600 or 3000 with Workbench 2.x. We have included dedicated and separate setups for these in the included `Extras` archive.

* Copy the contents of `Amiga 500 HD Setup` and/or `Amiga 600 HD Setup` to their respective directories on the MiSTer or emulators — or, you can of course also use a disk imager to write this to an SD/CF card.
* You will now have separate `Amiga 500HD` and/or `Amiga600HD` launch items in the `Computer` section. These are fully configured to support shared drives, PFS file systems (even on 1.3!), RTC clock, etc.

There are `ReadMe` files that go into more detail about these setups.

These are *not* meant to be used for games or demos, but instead for giving you a basic setup that lets you run productivity apps like you did back in the day. For games and demos, we recommend the `Amiga` (main AmigaVision setup) and `Amiga 500` (for use with ADF files) instead.

## MiSTer: Gamepad & Joystick Mapping

While many games supports two or more buttons, Amiga games were generally designed for one button joysticks. Consequently ""up to jump"" (or accelerate) control scheme is very common. 

If you are using a gamepad, you might want to use MiSTer's controller mapping to bind the up direction to the D-pad and/or an dedicated jump/accelerate button, typically the `X` button. Here's how:

* First, make sure to have CD32 controller mode enabled (this is the default).
* Enter ""Define joystick buttons"" mode
* Map directions as usual
* Map the first three buttons (red, blue and yellow) to `A`, `B` and `Y`.
* The fourth button (green) is practically never used, and can be mapped to `Select`, `R2/ZL` or similar — or skipped altogether.
* Go ahead and map right/left triggers and play/pause.
* When asked to if you want to ""setup alternative buttons"", say Yes!
* Skip all choices except ""up"", which we recommend mapping to `X`.

While a keyboard and mouse isn't strictly necessary to play most action games, it is definitely recommended for the full Amiga experience, and many games have controls that make use of them.

## MiSTer: Video Modes

We care deeply about preserving the correct aspect ratio for all games. That means going beyond just NTSC and PAL, and ensuring that the Pixel Aspect Ratio (PAR) is also correct. Pixels on the Amiga were close to square (16:15) in PAL resolutions on a CRT, but quite tall on NTSC displays (5:6). Additionally, when we apply a 5×PAL or 6×PAL [Dynamic Crop](https://amiga.vision/5x), 1:1 gives us great results that are near indistinguishable from the original PAR at those sizes, while modernizing the output to fit 16:9 displays.

You no longer have to interact with the MiSTer OSD menu to switch aspect ratios in certain cases like what we informally refer to as ""Jim Sachs mode"" — NTSC, tall pixels at 5:6 PAR, seen in e.g. Defender of the Crown. Most emulators and captures get this wrong and use 1:1 pixels instead, so we built an implementation that handles all the variants correctly on MiSTer:

* **PAL title, 50Hz:** PAL, 16:15 PAR at 4×, 1:1 PAR at 5× and 6×
* **PAL title, 60Hz:** PAL60, 1:1 PAR at 5×
* **NTSC title, 60Hz:**  NTSC, 1:1 PAR at 5×
* **""World"" title, 60Hz:** NTSC, 1:1 PAR at 5×
* **""Sachs NTSC"" title, 60Hz:**  NTSC, 5:6 PAR at 5×

All these align to the 1080p/4K 16:9 pixel grid while having the correct Pixel Aspect Ratio, so you will not get any shimmering or non-integer pixels.

On the MiSTer side of things, always, *always* run the AmigaVision setup in the `40:27` aspect ratio that we supply to ensure that this is handled correctly. This is what AmigaVision sets as the default as long as you copy over the supplied config file and have the correct `MiSTer.ini` definition for the core. 

The `Original` aspect ratio supplied by the core should not be used. The `Full Screen` aspect ratio is *only* used for 6×PAL on 16:9 widescreen displays.

Make absolutely sure that you update your `MiSTer.ini` settings for the core [according to the documentation](#setup-for-mister)!

### vsync_adjust setting

The optimal `vsync_adjust` setting in `MiSTer.ini` will depend on your HDMI display. A setting of `2` ensures the lowest possible latency, but it may come at the cost of a short period of no video or audio on video mode changes — something Amiga games and demos do quite often. Setting `vsync_adjust` to `1` introduces a buffer that will smooth over most of these changes, although it will add a frame of latency.

### Dynamic cropping *&* 5× scaling on 1080p/4K displays

A unique feature of the Amiga (Minimig) core on MiSTer is the ability to do viewport cropping. By default the full overscan area will be fed to the HDMI scaler, resulting in huge borders for most content. But fear not! AmigaVision leverages the custom `vadjust` feature of the core to dynamically apply viewport settings on a per-game basis. This depends on MiSTer's ""shared folder"" functionality, which is enabled in AmigaVision if the ""games/Amiga/shared"" directory exists. So, make sure you copied all the archive contents as described in the Setup section.

Also note that dynamic cropping *only* applies if you are using 1080p output. Most Amiga games fit on the screen using 5× zoom in this resolution. Any other resolution or analog output is *not* affected by dynamic viewport cropping, as it only makes sense for 1080p/4K 16:9 displays.

With dynamic vadjust enabled, most titles will enjoy a nicely centered viewport at a perfect 5× scale using 1080p output resolution, by cropping the viewport to 216 lines. Games using more than 216 active video lines will instead get a perfect 4× scale by applying a 270 line crop.

## MiSTer: CPU Performance Notes

The D-Cache option in MiSTer's Amiga core is essentially a turbo switch for the CPU, making it perform on par with an accelerated Amiga with a Motorola 68030 CPU at 50MHz in many benchmarks. Unfortunately, running with it enabled introduces lots of subtle glitches in many (mostly older) games and demos, so it's recommended is to leave it `OFF` by default.

The CPU D-Cache option is available in the `OSD` under the `System` menu.

On the other hand, some titles — mostly 3D polygon games and demos — will benefit greatly from the CPU boost D-Cache offers. So it's an option worth experimenting with on a case by case basis. Whenever switching this on or off, we recommend reloading the core to avoid any issues.

## Workbench

From the launcher, you can hit the `ESC` key to exit into Workbench, the AmigaOS graphical desktop environment.

You can explore the world's first multitasking 16-bit computer from 1985 with the addition of a more modern desktop from 1992, AmigaOS 3.

To change from the default 640×200 resolution to something like 1280×720 or 1920×1080 for use with a 16:9 HD display, hold down the right mouse button and select your preferred resolution from the ScreenMode menu. 540p is a nice compromise, a very usable screen resolution that doubles every pixel on a modern 1080p/4K 16:9 display.

## CD³² Games Support

*This section is for MiSTer only. Most emulators have a way to run CD³² games, so consult the documentation there for instructions on how to play CD³² games. The Amiga core on Analogue Pocket does not support mounting/loading disc images yet.*

If you are unfamiliar with the Amiga CD³², it was essentially an Amiga 1200 with a 2× speed CD drive, packaged with gamepads and in a console form factor. It was released in September 1993.

While the CD³² never really got its time to shine because of Commodore’s bankruptcy soon after launch, there are some fun expansions of existing Amiga games with great CD audio and Full Motion Video intros, so some of its ~150 games are worth checking out.

We include a setup in AmigaVision that puts an entry in your `Console` section of MiSTer's core selector. This boots directly into an Amiga setup preconfigured to let you load CD³² games. You will need a mouse, a keyboard or an analog stick set up to act as a mouse to operate this UI.

### Starting a CD³² Game

* Put your CD³² disc images in `games/AmigaCD32` — CHD format is recommended, but BIN/CUE and ISO are also supported.
* Navigate to `Consoles` → `Amiga CD32` and start it.
* You will be presented with a UI, but don't click anything yet.
* Open the MiSTer menu (`F12`), and navigate to the Drives section.
* Navigate to the `Removable/CD` section, and select the CD³² disc image you want to play.
* Dismiss the menu with `ESC`
* Select ""Boot""
* Enjoy your game!

Do note that while the majority of CD³² games work (and, even a few CDTV games), this is using a shim combined with MiSTer's CD Audio support in the Amiga core, so not every game will work perfectly — but the majority of them do.

We maintain a compatibility list, including any special settings needed at [amiga.vision/cd32] — make sure to consult the listing for the game you are trying to play if it does not work.

## Non-working Games

About 10 games are currently not working due to CPU or graphics chipset features not yet implemented in MiSTer's Minimig core. Over the past years compatibility has improved a lot, and that trend is likely to continue. The launcher will specify when a game is known not to work in the `Issues` section of a given game.

## Custom Scripts

If you want to run additional scripts on startup, AmigaVision looks for a file named `Saves:custom-startup` on boot and runs it, so if you need to run scripts that will survive upgrades of the main image, this is where to put them.

As an example, here's what you would add to `Saves:custom-startup` if you wanted to make changes to screen resolution, colors, pointers or any other Workbench setting copied from `ENV:Sys/` (which is where Workbench settings are stored temporarily) to `Saves:Custom/Prefs-Env/` before rebooting:

```
copy >NIL: Saves:Custom/Prefs-Env ENV:Sys/
```

This will take the setting you copied to `Saves:Custom/Prefs-Env` and put them in RAM: when booting the image, so you can keep your own settings even when replacing the `AmigaVision.hdf` file with a future version. You can also install new apps/games to Saves: and add `Assign` statements etc to the `Saves:` drive, or do anything else you want to keep permanent after upgrading.

## Bug Reports *&* Feature Requests

While AmigaVision has been tested for many years, the sheer volume of games and demos makes it all but certain that something has been overlooked somewhere. If you find something that doesn't work or seems like it's running with the wrong settings, or something is missing — tell us about it in the issue tracker found under the Development section at [amiga.vision].

## Credits

* [David Lindecrantz] — Creator, original developer
* [Alex Limi] — Developer, current maintainer
* [Per Olofsson] — Creator of [AGS], the launcher software
* [Ben Squibb] — Improvements to AGS to enable hi-res launcher with thumbnail cycling + IFF conversion of hi-res thumbnails
* Simon ""[hitm4n]"" Quincey — Hi-resolution screenshots for demos
* LamerDeluxe — MT-32 support
* [Frode Solheim] — Creator of [OpenRetro.org], screenshots used with kind permission

## Troubleshooting

### Why doesn't AmigaVision work on my network drive or external drive?

The configuration is likely still looking for your Kickstart file on the path to the SD card. There is currently no way to make this configuration relative in a way that works both on the standard SD card location as well as an SSD or network drive, so you will have to adjust this manually: 

Go to the MiSTer menu → System → ROM, and point it to the location of the file on your SSD or network drive instead. Save this new configuration.

### I get a bunch of errors when starting up!

Unfortunately, there's a lot of variables in what could go wrong, but one useful thing to verify is to make sure the `HDF` file didn't get corrupted on its way to the MiSTer, your Amiga or emulation setup. It's a large file, and there's a lot that can go wrong along the way. The reasons for this happening are legion, but among them:

* You are using FTP to transfer the file, but your FTP client defaults to ASCII instead of binary transfers.
* You are using SMB/Samba, but the implementation of your OS isn't great, and might abort mid-write.
* Your SD card has issues writing the file — very common with cheap or old SD cards.

Let's make sure the image made it over to the target location intact.

The easiest — but a little bit time-consuming — is to compare SHA-1 checksums. MiSTer, Windows, Mac and Linux all have these tools installed by default, but you have to issue some command line instructions to make it work.

1. Do a new, clean transfer of the `HDF` file to MiSTer — ideally not over the network, but if that's your only option, go for it. The reason why you need to do this again is that if the Amiga does any writes to the `HDF` during earlier attempts to launch it, the checksum will be different, and useless for this purpose.
2. Do a clean boot of your MiSTer, and press the `F9` key on a keyboard connected to it. This will let you log in to the terminal. Username is `root`, and password is `1` unless you have changed it in the past.
3. Run the following command, which will take 5-10 minutes to complete: `shasum /media/fat/games/Amiga/AmigaVision.hdf`
4. It will report back a checksum, you will compare this to the checksum on your computer.

Depending on what operating system you are on, you will do one of the following:

* **If you are on macOS:** Open the Terminal, and run the command: `shasum /path/to/AmigaVision.hdf` Compare this to the checksum you got on the MiSTer.

* **If you are on Windows:** Open PowerShell, and run the command: `Get-FileHash C:\path\to\AmigaVision.hdf -Algorithm SHA1` Compare this to the checksum you got on the MiSTer.

* **If you are on Linux:** Open a terminal, and run the command: `shasum /path/to/AmigaVision.hdf` Compare this to the checksum you got on the MiSTer.

If these checksums do not match, something is wrong with either the way you transfer your file, or your SD card. 

If they do match, launching should work without issues, assuming you don't have a bad `HDF` file on your computer.

## Frequently Asked Questions

### Why do you have Scanline and Shadow Mask presets by default?

The Amiga was almost exclusively used with RGB-based CRTs or consumer TVs, and graphics do not look correct without scanlines and shadow masks. We have included a set of Amiga-specific scanline and shadow mask setups to more accurately represent the graphics output of the system that we highly recommend.

Thus, the default setup is configured for this to make sure the most people see the most representative setup. Since configuring this can be intimidating to new users, we chose to have a default that we think best represents the Amiga's original look.

You can of course set your own scanline and shadow mask presets and save those configurations if you don't want to use ours. 

If you are using resolutions lower than 1080p, 1440p or 1536p, we recommend turning them off, but since most users are on 1080p/4K TVs, equally capable monitors, they are on by default.

If you are using analog output to CRTs, these will of course not be used.

### Can I add my own games?

It’s pretty straightforward to add your own games as long as you are a little bit familiar with AmigaOS. Just install the game on the `Saves` HDF drive, and you can add your own launcher entries and thumbnails in the Favorites folder, also located on the `Saves` drive. If you need a template, favorite a random game and open it in a text editor.

### Can I make a setup with only a few games?

The simplest way to do this is to make your own personal collection using the Favorites feature in the launcher. They will be stored on the `Saves` drive, and will survive upgrades.

### Should I worry about Amiga viruses?

In short: No. 

Viruses on the Amiga were quite common, and some retail games even shipped with infected disks in the box. 

Even though we don't control what's being used as inputs to the script that creates the AmigaVision image, pretty much all games and demos run inside WHDL containers, you can think of them as ""virtualization for the Amiga"". Their job is to insulate the game from the rest of the system, reset CPU vectors, and other system state preservation. So even if a game or demo contains a virus, it cannot stay resident in memory, and will not spread to the rest of the system outside of the sandbox it has been given.

If you want to check the state of a given setup, or whether you have viruses in memory, just run the included VirusZ scanner in the System folder. Again, if you see virus warnings inside of WHDLoad containers, it's nothing to worry about.

We check all files that are under our control for viruses before release.

### I can't get the game started from its title screen! Do I ever need to use ""Joystick Swap"" on the MiSTer?

Unless you are using the ""Arcadia Systems"" games (see below), no. On the Commodore 64, games sometimes used Port 1 and sometimes Port 2 for controlling games, necessitating this setting; but since the (Commodore) Amiga shipped with a mouse and it was always plugged into port 1, the main controller is pretty much always connected to port 2. 

The MiSTer core will handle these mappings for you, and joystick port configuration is pretty much never the reason you can't start or control a given game. It's way more likely that you have to hit the space bar, F1, or click a mouse button to get the game started. If you're stuck, look up the controls for a given game in an online manual — which is always a good idea anyway, as there are often additional keyboard or mouse controls needed for a given game for full enjoyment.

### Does AmigaVision work with Kickstart 3.2 or 3.1.4?

While the standard AmigaVision setup expects Kickstart 3.1 (not 3.1.4 or 3.2) — which was the last release from Commodore in 1993 — we have had reports of the setup working if you replace `icon.library` and `workbench.library` with their respective Workbench 3.2 (or 3.1.4) versions. The recommended and tested setup is still Kickstart 3.1.

The files needed can be found on the Workbench Install disk, in the `Libs` drawer.

If you get an error on startup saying ""Please insert a volume containing LIBS/workbench.library in any drive"", this is likely the reason.

### Are there any plans to support original Amiga 500, Amiga 600, or Amiga 1000 hardware?

Not at the moment, AmigaVision is currently AGA-only and requires at least a 68020 processor. Unless you have added a fair bit of upgrades to these systems, using the AmigaVision setup would be an exercise in frustration, and we also don't have the real hardware to test with when we do a release. 

We welcome contributions, though — so if you're interested in maintaining this part of the AmigaVision setup scripts, let us know!

### What is ""Arcadia Systems""?

Arcadia was an unsuccessful venture by Mastertronic to create an Amiga 500-based multi-game arcade system. Most titles released for the system have been dumped and are available via AmigaVision. The games are not great (to put it kindly),",VRAI
anderseknert/opa-policy-composition,Documentations,Documentations,2024-06-18T19:11:06Z,2021-03-24T21:07:35Z,0,8,0,0,0,0,0,0,2021-03-24T21:06:15Z,2025-03-04T14:27:00Z,7,7,Open Policy Agent,VRAI,5,FAUX,0,"opa,open-policy-agent,rego",0,Example policies demonstrating policy composition.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,2,"# opa-policy-composition

Example policies demonstrating policy composition in Rego. These example accompany the [blog post](https://blog.styra.com/blog/dynamic-policy-composition-for-opa) on the same topic as found on the [Styra](https://www.styra.com) blog.

## Running

`opa run --server --set default_decision=/main/decision .`

Query OPA (use team1-3 as input, and try removing the `explain` attribute):

`curl -d '{""resource"": ""volume"", ""explain"": true}' http://localhost:8181`

Or if you prefer using `opa eval`:

```shell
opa eval -f pretty -d . 'data.main.decision with input as {""resource"": ""volume"", ""explain"": true}'
```",FAUX
aptakube/kubespec.dev,DevOPs,DevOPs,2025-04-27T13:19:31Z,2025-01-22T09:26:59Z,0,0,0,0,181,0,0,0,2024-10-06T12:35:46Z,2025-04-08T03:43:01Z,4944,419,TypeScript,VRAI,19,FAUX,7,kubernetes,7,Kubernetes Spec Explorer,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,11,"# Kubernetes Spec Explorer

👉 Live at https://kubespec.dev

- Tree view of all Kubernetes resources
- History changes since Kubernetes v1.12
- Examples that you can use copy and modify
- Links to official Kubernetes documentation and useful resources
- Support for popular CRDs

![](./screenshot.png)

## Contributing

Contributions are welcome!

- clone the repo
- run `npm install`
- run `npm run dev`

## 📃 License

MIT

## ❤️ Sponsored by

<a href=""https://aptakube.com"">
    <img src=""https://aptakube.com/og.png"" alt=""Aptakube"">
</a>",FAUX
aquasecurity/postee,Application System,Application System,2024-09-28T05:52:51Z,2023-06-01T20:37:52Z,0,38,0,0,0,0,0,0,2016-12-04T13:29:28Z,2025-04-04T03:49:41Z,8417,210,Go,VRAI,64,FAUX,45,"aqua,automation,cloud-native,devsecops,docker,golang,jira,kubernetes,messaging,opa,rego,security,slack,soar",45,Notice: Postee is no longer under active development or maintenance.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,28,"# Notice: Postee is no longer under active development or maintenance.

<p align=""center"">
  <img src=""./docs/img/postee.png"">
</p>

![Docker Pulls][docker-pull]
[![Go Report Card][report-card-img]][report-card]
![](https://github.com/aquasecurity/postee/workflows/Go/badge.svg)
[![License][license-img]][license]
<a href=""https://slack.aquasec.com/?_ga=2.51428586.2119512742.1655808394-1739877964.1641199050"">
<img src=""https://img.shields.io/static/v1?label=Slack&message=Join+our+Community&color=4a154b&logo=slack"">
</a>


[download]: https://img.shields.io/github/downloads/aquasecurity/postee/total?logo=github
[release-img]: https://img.shields.io/github/release/aquasecurity/postee.png?logo=github
[release]: https://github.com/aquasecurity/postee/releases
[docker-pull]: https://img.shields.io/docker/pulls/aquasec/postee?logo=docker&label=docker%20pulls%20%2F%20postee
[go-doc-img]: https://godoc.org/github.com/aquasecurity/postee?status.svg
[report-card-img]: https://goreportcard.com/badge/github.com/aquasecurity/postee
[report-card]: https://goreportcard.com/report/github.com/aquasecurity/postee
[license-img]: https://img.shields.io/badge/License-mit-blue.svg
[license]: https://github.com/aquasecurity/postee/blob/master/LICENSE


Postee is a simple message routing application that receives input messages through a webhook interface, and can take enforce actions using predefined outputs via integrations.

Watch a quick demo of how you can use Postee:


[![Postee Demo Video](./docs/img/postee-video-thumbnail.jpg)](https://www.youtube.com/watch?v=HZ5Z8jAVH8w)

Primary use of Postee is to act as a message relay and notification service that integrates with a variety of third-party services. Postee can also be used for sending vulnerability scan results or audit alerts from Aqua Platform to collaboration systems.

In addition, Postee can also be used to enforce pre-defined behaviours that can orchestrate actions based on input messages as triggers.

![Postee v2 scheme](docs/img/postee-v2-scheme.png)

## Status
Although we are trying to keep new releases backward compatible with previous versions, this project is still incubating,
and some APIs and code structures may change.

## Documentation
The official [Documentation] provides detailed installation, configuration, troubleshooting, and quick start guides.

---
Postee is an [Aqua Security](https://aquasec.com) open source project.  
Learn about our [Open Source Work and Portfolio].  
Join the community, and talk to us about any matter in [GitHub Discussions] or [Slack].

[Documentation]: https://aquasecurity.github.io/postee/latest
[Open Source Work and Portfolio]: https://www.aquasec.com/products/open-source-projects/
[Slack]: https://slack.aquasec.com/
[GitHub Discussions]: https://github.com/aquasecurity/postee/discussions


## Release

1. Bump version of [helm chart](https://github.com/aquasecurity/postee/blob/main/deploy/helm/postee/Chart.yaml).
1. (By repository admin) Create a new tag. Postee and helm charts are automatically released by github actions.
1. (By repository admin) Run [publish-docs workflow](https://github.com/aquasecurity/postee/blob/main/.github/workflows/publish-docs.yml), if document has been updated.",FAUX
aquasecurity/tracee-action,Documentations,Application System,2025-02-08T18:43:35Z,2023-01-29T18:52:23Z,0,2,0,0,0,0,0,0,2021-05-12T23:28:29Z,2025-04-04T03:49:42Z,73,81,Open Policy Agent,VRAI,8,FAUX,3,"ebpf,github-actions,runtime-scanner,security",3,Protect GitHub Actions with Tracee,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,4,"**This project is for demonstration purpose only. It is not under active development. Use at your own risk**
Watch a detailed introduction to the project: https://youtu.be/nNqrPNEqtw0

# Protect your GitHub Actions with Tracee

[Tracee](https://github.com/aquasecurity/tracee) is a powerful open source runtime security and forensics solution. It is using eBPF to trace your system, produce rich events that gives you visibility into what is happening under the hood, and also detects suspicious behavior in those event.

This project is using Tracee to protect GitHub Actions workflow against supply chain attacks.

## Protection

tracee-action offers two kinds of protection that complements each other: Events, and Profile.

### Signatures

Tracee runs in the background and hunts for suspicious behavior in the runner and in the workflow. It uses the powerful set of behavioral signatures that is available for Tracee, and you can add your own specific signatures to detect unwanted behavior.
Signatures detections are reported to you as a comment on the PR that triggered the action for your review.
You can review the list of events in the default policy [here](policies/signatures.yaml.tmpl)

### Profile

While the profile is running Tracee builds a profile that describes how your workflow normally behaves. Once you approve this initial profile as the baseline, tracee-action will detect and report any deviation from it.
Profile deviations are reported to you as a new PR that add commits the changes to a `.tracee` directory in the project.
You can review the contents of the default profile [here](docs/profile.md)

## Getting Started

Add tracee-action to the beginning of your workflow with the tag ending with `-start`, and to the end of your workflow with the tag ending with `-stop`.
Example:

```yaml
name: My pipeline
jobs:
  my-job:
    runs-on: ubuntu-latest
    steps:
    - name: Start Tracee
      uses: aquasecurity/tracee-action@v0.3.0-start

    ...

    - name: Stop Tracee
      uses: aquasecurity/tracee-action@v0.3.0-stop
```

There are some configuration options the are detailed [here](docs/config.md)",VRAI
aquasecurity/trivy,Toolkit,Application System,2025-05-15T05:47:04Z,2025-04-29T00:56:54Z,0,43,0,0,0,0,0,0,2019-04-11T01:01:07Z,2025-04-07T09:24:29Z,901518,25337,Go,VRAI,2487,FAUX,216,"containers,devsecops,docker,go,golang,hacktoberfest,iac,infrastructure-as-code,kubernetes,misconfiguration,security,security-tools,vulnerability,vulnerability-detection,vulnerability-scanners",216,"Find vulnerabilities, misconfigurations, secrets, SBOM in containers, Kubernetes, code repositories, clouds and more",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,446,"<div align=""center"">
<img src=""docs/imgs/logo.png"" width=""200"">

[![GitHub Release][release-img]][release]
[![Test][test-img]][test]
[![Go Report Card][go-report-img]][go-report]
[![License: Apache-2.0][license-img]][license]
[![GitHub Downloads][github-downloads-img]][release]
![Docker Pulls][docker-pulls]

[📖 Documentation][docs]
</div>

Trivy ([pronunciation][pronunciation]) is a comprehensive and versatile security scanner.
Trivy has *scanners* that look for security issues, and *targets* where it can find those issues.

Targets (what Trivy can scan):

- Container Image
- Filesystem
- Git Repository (remote)
- Virtual Machine Image
- Kubernetes

Scanners (what Trivy can find there):

- OS packages and software dependencies in use (SBOM)
- Known vulnerabilities (CVEs)
- IaC issues and misconfigurations
- Sensitive information and secrets
- Software licenses

Trivy supports most popular programming languages, operating systems, and platforms. For a complete list, see the [Scanning Coverage] page.

To learn more, go to the [Trivy homepage][homepage] for feature highlights, or to the [Documentation site][docs] for detailed information.

## Quick Start

### Get Trivy

Trivy is available in most common distribution channels. The full list of installation options is available in the [Installation] page. Here are a few popular examples:

- `brew install trivy`
- `docker run aquasec/trivy`
- Download binary from <https://github.com/aquasecurity/trivy/releases/latest/>
- See [Installation] for more

Trivy is integrated with many popular platforms and applications. The complete list of integrations is available in the [Ecosystem] page. Here are a few popular examples:

- [GitHub Actions](https://github.com/aquasecurity/trivy-action)
- [Kubernetes operator](https://github.com/aquasecurity/trivy-operator)
- [VS Code plugin](https://github.com/aquasecurity/trivy-vscode-extension)
- See [Ecosystem] for more

### Canary builds
There are canary builds ([Docker Hub](https://hub.docker.com/r/aquasec/trivy/tags?page=1&name=canary), [GitHub](https://github.com/aquasecurity/trivy/pkgs/container/trivy/75776514?tag=canary), [ECR](https://gallery.ecr.aws/aquasecurity/trivy#canary) images and [binaries](https://github.com/aquasecurity/trivy/actions/workflows/canary.yaml)) as generated every push to main branch.

Please be aware: canary builds might have critical bugs, it's not recommended for use in production.

### General usage

```bash
trivy <target> [--scanners <scanner1,scanner2>] <subject>
```

Examples:

```bash
trivy image python:3.4-alpine
```

<details>
<summary>Result</summary>

https://user-images.githubusercontent.com/1161307/171013513-95f18734-233d-45d3-aaf5-d6aec687db0e.mov

</details>

```bash
trivy fs --scanners vuln,secret,misconfig myproject/
```

<details>
<summary>Result</summary>

https://user-images.githubusercontent.com/1161307/171013917-b1f37810-f434-465c-b01a-22de036bd9b3.mov

</details>

```bash
trivy k8s --report summary cluster
```

<details>
<summary>Result</summary>

![k8s summary](docs/imgs/trivy-k8s.png)

</details>

## FAQ

### How to pronounce the name ""Trivy""?

`tri` is pronounced like **tri**gger, `vy` is pronounced like en**vy**.

## Want more? Check out Aqua

If you liked Trivy, you will love Aqua which builds on top of Trivy to provide even more enhanced capabilities for a complete security management offering.  
You can find a high level comparison table specific to Trivy users [here](https://trivy.dev/latest/commercial/compare/).
In addition check out the <https://aquasec.com> website for more information about our products and services.
If you'd like to contact Aqua or request a demo, please use this form: <https://www.aquasec.com/demo>

## Community

Trivy is an [Aqua Security][aquasec] open source project.  
Learn about our open source work and portfolio [here][oss].  
Contact us about any matter by opening a GitHub Discussion [here][discussions]

Please ensure to abide by our [Code of Conduct][code-of-conduct] during all interactions.

[test]: https://github.com/aquasecurity/trivy/actions/workflows/test.yaml
[test-img]: https://github.com/aquasecurity/trivy/actions/workflows/test.yaml/badge.svg
[go-report]: https://goreportcard.com/report/github.com/aquasecurity/trivy
[go-report-img]: https://goreportcard.com/badge/github.com/aquasecurity/trivy
[release]: https://github.com/aquasecurity/trivy/releases
[release-img]: https://img.shields.io/github/release/aquasecurity/trivy.svg?logo=github
[github-downloads-img]: https://img.shields.io/github/downloads/aquasecurity/trivy/total?logo=github
[docker-pulls]: https://img.shields.io/docker/pulls/aquasec/trivy?logo=docker&label=docker%20pulls%20%2F%20trivy
[license]: https://github.com/aquasecurity/trivy/blob/main/LICENSE
[license-img]: https://img.shields.io/badge/License-Apache%202.0-blue.svg
[homepage]: https://trivy.dev
[docs]: https://trivy.dev/latest/docs/
[pronunciation]: #how-to-pronounce-the-name-trivy
[code-of-conduct]: https://github.com/aquasecurity/community/blob/main/CODE_OF_CONDUCT.md

[Installation]:https://trivy.dev/latest/getting-started/installation/
[Ecosystem]: https://trivy.dev/latest/ecosystem/
[Scanning Coverage]: https://trivy.dev/latest/docs/coverage/

[alpine]: https://ariadne.space/2021/06/08/the-vulnerability-remediation-lifecycle-of-alpine-containers/
[rego]: https://www.openpolicyagent.org/docs/latest/#rego
[sigstore]: https://www.sigstore.dev/

[aquasec]: https://aquasec.com
[oss]: https://www.aquasec.com/products/open-source-projects/
[discussions]: https://github.com/aquasecurity/trivy/discussions",VRAI
aquasecurity/trivy-checks,Toolkit,Documentations,2025-05-14T07:25:19Z,2025-04-03T06:37:32Z,0,1052,0,0,0,0,0,0,2023-09-06T01:08:17Z,2025-04-05T06:35:22Z,4356,48,Open Policy Agent,FAUX,36,FAUX,13,,13,,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,22,"# trivy-checks

_trivy-checks_ contains misconfiguration checks for Trivy

_trivy-checks_ is an [Aqua Security](https://aquasec.com) open source project.
Learn about our open source work and portfolio [here](https://www.aquasec.com/products/open-source-projects/).
Join the community, and talk to us about any matter in [GitHub Discussion](https://github.com/aquasecurity/trivy/discussions).

## Project Layout

The directory structure is broken down as follows:

- `cmd/` - These CLI tools are primarily used during development for end-to-end testing without requiring the use of a library.
  - `cmd/id` - This command helps generate the next available ID that is free when writing a new check.
- `checks/` - All the checks are defined in this directory.
  - `kubernetes/` - Kubernetes-specific security checks
    - `access/` - RBAC, authentication, and authorization related checks
    - `network/` - Network security checks including network policies, host network access, and service configurations
    - `resources/` - Resource quotas, limits, and management checks
    - `security/` - Core security checks including Pod Security Standards
    - `workloads/` - Workload-specific security checks
  - `cloud/kubernetes/` - Cloud-specific Kubernetes security checks
- `commands/` - All [Node-collector](https://github.com/aquasecurity/k8s-node-collector) commands are defined in this directory.
- `test/` - Integration tests and other high-level tests that require a full build of the project.",VRAI
argoproj/argo-cd,DevOPs,DevOPs,2025-05-15T20:58:43Z,2025-05-09T00:14:31Z,0,0,0,0,11,0,0,0,2018-02-09T11:12:01Z,2025-04-08T07:54:53Z,132997,19144,Go,VRAI,5877,FAUX,3587,"argo,argo-cd,cd,ci-cd,cicd,continuous-delivery,continuous-deployment,devops,docker,gitops,hacktoberfest,helm,jsonnet,kubernetes,kustomize,pipeline",3587,Declarative Continuous Deployment for Kubernetes,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,1654,"**Releases:**
[![Release Version](https://img.shields.io/github/v/release/argoproj/argo-cd?label=argo-cd)](https://github.com/argoproj/argo-cd/releases/latest)
[![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/argo-cd)](https://artifacthub.io/packages/helm/argo/argo-cd)
[![SLSA 3](https://slsa.dev/images/gh-badge-level3.svg)](https://slsa.dev)

**Code:** 
[![Integration tests](https://github.com/argoproj/argo-cd/workflows/Integration%20tests/badge.svg?branch=master)](https://github.com/argoproj/argo-cd/actions?query=workflow%3A%22Integration+tests%22)
[![codecov](https://codecov.io/gh/argoproj/argo-cd/branch/master/graph/badge.svg)](https://codecov.io/gh/argoproj/argo-cd)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/4486/badge)](https://bestpractices.coreinfrastructure.org/projects/4486)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/argoproj/argo-cd/badge)](https://scorecard.dev/viewer/?uri=github.com/argoproj/argo-cd)

**Social:**
[![Twitter Follow](https://img.shields.io/twitter/follow/argoproj?style=social)](https://twitter.com/argoproj)
[![Slack](https://img.shields.io/badge/slack-argoproj-brightgreen.svg?logo=slack)](https://argoproj.github.io/community/join-slack)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-argoproj-blue.svg?logo=linkedin)](https://www.linkedin.com/company/argoproj/)

# Argo CD - Declarative Continuous Delivery for Kubernetes

## What is Argo CD?

Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.

![Argo CD UI](docs/assets/argocd-ui.gif)

[![Argo CD Demo](https://img.youtube.com/vi/0WAm0y2vLIo/0.jpg)](https://youtu.be/0WAm0y2vLIo)

## Why Argo CD?

1. Application definitions, configurations, and environments should be declarative and version controlled.
1. Application deployment and lifecycle management should be automated, auditable, and easy to understand.

## Who uses Argo CD?

[Official Argo CD user list](USERS.md)

## Documentation

To learn more about Argo CD [go to the complete documentation](https://argo-cd.readthedocs.io/).
Check live demo at https://cd.apps.argoproj.io/.

## Community

### Contribution, Discussion and Support

 You can reach the Argo CD community and developers via the following channels:

* Q & A : [Github Discussions](https://github.com/argoproj/argo-cd/discussions)
* Chat : [The #argo-cd Slack channel](https://argoproj.github.io/community/join-slack)
* Contributors Office Hours: [Every Thursday](https://calendar.google.com/calendar/u/0/embed?src=argoproj@gmail.com) | [Agenda](https://docs.google.com/document/d/1xkoFkVviB70YBzSEa4bDnu-rUZ1sIFtwKKG1Uw8XsY8)
* User Community meeting: [First Wednesday of the month](https://calendar.google.com/calendar/u/0/embed?src=argoproj@gmail.com) | [Agenda](https://docs.google.com/document/d/1ttgw98MO45Dq7ZUHpIiOIEfbyeitKHNfMjbY5dLLMKQ)


Participation in the Argo CD project is governed by the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md)


### Blogs and Presentations

1. [Awesome-Argo: A Curated List of Awesome Projects and Resources Related to Argo](https://github.com/terrytangyuan/awesome-argo)
1. [Unveil the Secret Ingredients of Continuous Delivery at Enterprise Scale with Argo CD](https://akuity.io/blog/secret-ingredients-of-continuous-delivery-at-enterprise-scale-with-argocd/)
1. [GitOps Without Pipelines With ArgoCD Image Updater](https://youtu.be/avPUQin9kzU)
1. [Combining Argo CD (GitOps), Crossplane (Control Plane), And KubeVela (OAM)](https://youtu.be/eEcgn_gU3SM)
1. [How to Apply GitOps to Everything - Combining Argo CD and Crossplane](https://youtu.be/yrj4lmScKHQ)
1. [Couchbase - How To Run a Database Cluster in Kubernetes Using Argo CD](https://youtu.be/nkPoPaVzExY)
1. [Automation of Everything - How To Combine Argo Events, Workflows & Pipelines, CD, and Rollouts](https://youtu.be/XNXJtxkUKeY)
1. [Environments Based On Pull Requests (PRs): Using Argo CD To Apply GitOps Principles On Previews](https://youtu.be/cpAaI8p4R60)
1. [Argo CD: Applying GitOps Principles To Manage Production Environment In Kubernetes](https://youtu.be/vpWQeoaiRM4)
1. [Creating Temporary Preview Environments Based On Pull Requests With Argo CD And Codefresh](https://codefresh.io/continuous-deployment/creating-temporary-preview-environments-based-pull-requests-argo-cd-codefresh/)
1. [Tutorial: Everything You Need To Become A GitOps Ninja](https://www.youtube.com/watch?v=r50tRQjisxw) 90m tutorial on GitOps and Argo CD.
1. [Comparison of Argo CD, Spinnaker, Jenkins X, and Tekton](https://www.inovex.de/blog/spinnaker-vs-argo-cd-vs-tekton-vs-jenkins-x/)
1. [Simplify and Automate Deployments Using GitOps with IBM Multicloud Manager 3.1.2](https://www.ibm.com/cloud/blog/simplify-and-automate-deployments-using-gitops-with-ibm-multicloud-manager-3-1-2)
1. [GitOps for Kubeflow using Argo CD](https://v0-6.kubeflow.org/docs/use-cases/gitops-for-kubeflow/)
1. [GitOps Toolsets on Kubernetes with CircleCI and Argo CD](https://www.digitalocean.com/community/tutorials/webinar-series-gitops-tool-sets-on-kubernetes-with-circleci-and-argo-cd)
1. [CI/CD in Light Speed with K8s and Argo CD](https://www.youtube.com/watch?v=OdzH82VpMwI&feature=youtu.be)
1. [Machine Learning as Code](https://www.youtube.com/watch?v=VXrGp5er1ZE&t=0s&index=135&list=PLj6h78yzYM2PZf9eA7bhWnIh_mK1vyOfU). Among other things, describes how Kubeflow uses Argo CD to implement GitOPs for ML
1. [Argo CD - GitOps Continuous Delivery for Kubernetes](https://www.youtube.com/watch?v=aWDIQMbp1cc&feature=youtu.be&t=1m4s)
1. [Introduction to Argo CD : Kubernetes DevOps CI/CD](https://www.youtube.com/watch?v=2WSJF7d8dUg&feature=youtu.be)
1. [GitOps Deployment and Kubernetes - using Argo CD](https://medium.com/riskified-technology/gitops-deployment-and-kubernetes-f1ab289efa4b)
1. [Deploy Argo CD with Ingress and TLS in Three Steps: No YAML Yak Shaving Required](https://itnext.io/deploy-argo-cd-with-ingress-and-tls-in-three-steps-no-yaml-yak-shaving-required-bc536d401491)
1. [GitOps Continuous Delivery with Argo and Codefresh](https://codefresh.io/events/cncf-member-webinar-gitops-continuous-delivery-argo-codefresh/)
1. [Stay up to date with Argo CD and Renovate](https://mjpitz.com/blog/2020/12/03/renovate-your-gitops/)
1. [Setting up Argo CD with Helm](https://www.arthurkoziel.com/setting-up-argocd-with-helm/)
1. [Applied GitOps with Argo CD](https://thenewstack.io/applied-gitops-with-argocd/)
1. [Solving configuration drift using GitOps with Argo CD](https://www.cncf.io/blog/2020/12/17/solving-configuration-drift-using-gitops-with-argo-cd/)
1. [Decentralized GitOps over environments](https://blogs.sap.com/2021/05/06/decentralized-gitops-over-environments/)
1. [Getting Started with ArgoCD for GitOps Deployments](https://youtu.be/AvLuplh1skA)
1. [Using Argo CD & Datree for Stable Kubernetes CI/CD Deployments](https://youtu.be/17894DTru2Y)
1. [How to create Argo CD Applications Automatically using ApplicationSet? ""Automation of GitOps""](https://amralaayassen.medium.com/how-to-create-argocd-applications-automatically-using-applicationset-automation-of-the-gitops-59455eaf4f72)
1. [Progressive Delivery with Service Mesh – Argo Rollouts with Istio](https://www.cncf.io/blog/2022/12/16/progressive-delivery-with-service-mesh-argo-rollouts-with-istio/)",FAUX
ArthurVardevanyan/HomeLab,Toolkit,Documentations,2025-05-15T16:33:21Z,2025-04-17T12:06:44Z,0,0,0,0,6,0,0,0,2021-11-05T00:41:06Z,2025-04-08T01:54:56Z,54462,19,YAML,VRAI,12,FAUX,1,"homelab,kubernetes,okd,terraform",1,HomeLab Server & Desktop Configuration,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,4,"# HomeLab

HomeLab Server/Cluster, Virtual Sandbox Cluster, & Desktop Configuration

## Table of Contents

- [HomeLab](#homelab)
  - [Table of Contents](#table-of-contents)
  - [Remote VsCode](#remote-vscode)
  - [Server](#server)
    - [Networking](#networking)
    - [Kubernetes](#kubernetes)
      - [Machines](#machines)
        - [ZFS Storage](#zfs-storage)
      - [Kubernetes Nodes](#kubernetes-nodes)
        - [Ceph Performance Tests](#ceph-performance-tests)
      - [OKD WIF](#okd-wif)
      - [Kubernetes Commands](#kubernetes-commands)
        - [Delete Pod Using Graceful Termination Eviction Request](#delete-pod-using-graceful-termination-eviction-request)

## Remote VsCode

```bash
 ansible-playbook -i ansible/inventory --ask-become-pass ansible/vscode-server.yaml --ask-pass \
  -e 'ansible_python_interpreter=/usr/bin/python3'
```

## Server

```mermaid
graph TD
 linkStyle default interpolate basis

subgraph Networking
wan1[<center>WAN<br>192.168.100.1</center>]---|1000/50 Mb|router{<center>UDM Pro SE<br>10.0.0.1</center>}
wan2[<center>LTE<br>192.168.1.1</center>]---|750/25 Mb|router
router---|10GbE|switch1[<center>USW-Pro-Max-16<br></center>]
router---|2.5GbE|ap1{<center>U7 Pro Wall<br></center>}
switch1---|2.5GbE|ap2{<center>U7 Pro Max<br></center>}
switch1----|10GbE|switch10[<center>USW-Aggregation<br></center>]
switch10----|10GbE|switch11[<center>USW-Aggregation<br></center>]
router----|10GbE|switch11[<center>USW-Aggregation<br></center>]



end

subgraph Homelab
router----|1GbE|microshift[<center>MicroShift / PiHole<br>10.101.1.7</center>]
router-.-|1GbE Fail Over|truenas(<center>TrueNas<br>10.101.1.6</center>)
end

subgraph OpenShift/OKD
    switch1---|2.5GbE|truenas(<center>TrueNas<br>10.101.1.6</center>)
    switch1-.-|1GbE Fail Over Host/Pod/LB|server-1(<center>server-1<br>10.101.10.101</center>)
    switch1-.-|1GbE Fail Over Host/Pod/LB|server-2(<center>server-2<br>10.101.10.102</center>)
    switch1-.-|1GbE Fail Over Host/Pod/LB|server-3(<center>server-3<br>10.101.10.103</center>)

    switch1-.-|1GbE Fail Over NAD/LM|server-1(<center>server-1<br>10.101.10.101</center>)
    switch1-.-|1GbE Fail Over NAD/LM|server-2(<center>server-2<br>10.101.10.102</center>)
    switch1-.-|1GbE Fail Over NAD/LM|server-3(<center>server-3<br>10.101.10.103</center>)

    switch1-.-|1GbE Fail Over Ceph Public|server-1(<center>server-1<br>10.101.10.101</center>)
    switch1-.-|1GbE Fail Over Ceph Public|server-2(<center>server-2<br>10.101.10.102</center>)
    switch1-.-|1GbE Fail Over Ceph Public|server-3(<center>server-3<br>10.101.10.103</center>)

    switch1-.-|1GbE Fail Over Ceph Private|server-1(<center>server-1<br>10.101.10.101</center>)
    switch1-.-|1GbE Fail Over Ceph Private|server-2(<center>server-2<br>10.101.10.102</center>)
    switch1-.-|1GbE Fail Over Ceph Private|server-3(<center>server-3<br>10.101.10.103</center>)

    switch10----|10 GbE Host/Pod/LB|server-1(<center>server-1<br>10.101.10.101</center>)
    switch10----|10 GbE Host/Pod/LB|server-2(<center>server-2<br>10.101.10.102</center>)
    switch10----|10 GbE Host/Pod/LB|server-3(<center>server-3<br>10.101.10.103</center>)

    switch10----|10 GbE NAD/LM|server-1(<center>server-1<br>10.101.10.101</center>)
    switch10----|10 GbE NAD/LM|server-2(<center>server-2<br>10.101.10.102</center>)
    switch10----|10 GbE NAD/LM|server-3(<center>server-3<br>10.101.10.103</center>)

    switch11----|10 GbE Ceph Public|server-1(<center>server-1<br>10.101.10.101</center>)
    switch11----|10 GbE Ceph Public|server-2(<center>server-2<br>10.101.10.102</center>)
    switch11----|10 GbE Ceph Public|server-3(<center>server-3<br>10.101.10.103</center>)

    switch11----|10 GbE Ceph Private|server-1(<center>server-1<br>10.101.10.101</center>)
    switch11----|10 GbE Ceph Private|server-2(<center>server-2<br>10.101.10.102</center>)
    switch11----|10 GbE Ceph Private|server-3(<center>server-3<br>10.101.10.103</center>)
end
```

### Networking

**Networking Machines:**

| NAME             | Type   | Status |
| ---------------- | ------ | ------ |
| SB6183           | Modem  |        |
| UDM SE           | Router |        |
| USW Pro Max 16   | Switch |        |
| USW Aggregation  | Switch |        |
| USW Aggregation  | Switch |        |
| USWFlex XG 10GbE | Switch | Died   |
| U7 Pro Max       | AP     |        |
| U7 Pro Wall      | AP     |        |

**Vlans:**

| Site    | Range      |
| ------- | ---------- |
| 1       | 10.101.X.X |
| 2       | 10.102.X.X |
| Sandbox | 10.103.X.X |

| Name    | VLAN ID | Subnet        | Info                   |
| ------- | ------- | ------------- | ---------------------- |
| infra   | 1       | 10.102.1.0/24 |                        |
| clients | 2       | 10.102.2.0/24 |                        |
| iot     | 3       | 10.102.3.0/24 | TODO: Enable Isolation |

| Name                      | VLAN ID | Subnet         | Info                    |
| ------------------------- | ------- | -------------- | ----------------------- |
| infrastructure            | 111     | 10.101.1.0/24  |                         |
| clients                   | X       | 10.101.2.0/24  | TODO: Create            |
| iot                       | X       | 10.101.3.0/24  | TODO: Create            |
| openshift-machine-network | 10      | 10.101.10.0/24 | Host/Pod/Load Balancers |
| openshift-nad             | 11      | 10.101.11.0/24 | Virtual Machines        |
| openshift-ceph-private    | 12      | 10.101.12.0/24 | Isolated                |
| openshift-ceph-public     | 13      | 10.101.13.0/24 | Isolated                |
| openshift-lm              | 14      | 10.101.14.0/24 | Isolated                |
| openshift-ovn-network     | 15      | 10.101.15.0/24 | Isolated                |
| openshift-service-network | 16      | 10.101.16.0/22 | Isolated                |
| openshift-pod-network     | 32      | 10.101.32.0/19 | Isolated                |

| Name                           | VLAN ID | Subnet         | Info                    |
| ------------------------------ | ------- | -------------- | ----------------------- |
| virt-openshift-machine-network | 310     | 10.103.10.0/24 | Host/Pod/Load Balancers |
| virt-openshift-nad             | 311     | 10.103.11.0/24 | Virtual Machines        |
| virt-openshift-ceph-private    | 312     | 10.103.12.0/24 | Isolated                |
| virt-openshift-ceph-public     | 313     | 10.103.13.0/24 | Isolated                |
| virt-openshift-lm              | 314     | 10.103.14.0/24 | Isolated                |
| virt-openshift-ovn-network     | 315     | 10.103.15.0/24 | Isolated                |
| virt-openshift-service-network | 316     | 10.103.16.0/22 | Isolated                |
| virt-openshift-pod-network     | 332     | 10.103.32.0/19 | Isolated                |

### Kubernetes

<https://www.okd.io/>

| Kubernetes Channel | OKD Version | OKD Channel   | OKD OS                   | Storage Layer |
| ------------------ | ----------- | ------------- | ------------------------ | ------------- |
| v1.33.\*           | 4.19-\*     | stable-scos-4 | CentOS Stream CoreOS 9.0 | CEPH          |

#### Machines

[CPU Benchmark](https://www.cpubenchmark.net/compare/Intel-i5-6600-vs-AMD-RX-427BB-vs-Intel-i3-2130-vs-AMD-GX-415GA-SOC-vs-AMD-Ryzen-7-5700G/2594vs2496vs755vs2081vs4323)

| Machine    | Model          | CPU      | CPU | Mem  | Storage                           | Networking              | ZFS Storage  | Status         |
| ---------- | -------------- | -------- | --- | ---- | --------------------------------- | ----------------------- | ------------ | -------------- |
| MicroShift | Raspberry Pi 5 | BCM2712  | 4   | 8G   | 1TB NVME                          | 1x1GbE                  | N/A          | MicroShift     |
| server-1   | N/A            | R7-5700G | 16  | 128G | 2x4TB NVME, 2x1TB SSD, 2x.5TB SSD | 4x10Gbe (DAC) && 4x1GbE | N/A          | OpenShift/OKD  |
| server-2   | N/A            | R7-5700G | 16  | 128G | 2x4TB NVME, 2x1TB SSD,2x.5TB SSD  | 4x10Gbe (DAC) && 4x1GbE | N/A          | OpenShift/OKD  |
| server-3   | N/A            | R7-5700G | 16  | 128G | 2x4TB NVME, 2x1TB SSD,2x.5TB SSD  | 4x10Gbe (DAC) && 4x1GbE | N/A          | OpenShift/OKD  |
| TrueNas    | Hp ProDesk     | i5-6600  | 4   | 32G  | 120G SSD Boot Mirror              | 1x2.5Gbe && 1x1GbE      | 5x2TB RaidZ2 | TrueNas        |
| pfSense    | Hp t730        | RX-427BB | 4   | 4G   | 16G SSD                           | 4x1GbE                  | N/A          | Decommissioned |
| Bare Metal | Hp t620        | GX-415GA | 4   | 6G   | 16G SSD & 16G USB                 | 1x1GbE                  | N/A          | Decommissioned |
| Spare      | Hp p7-1226s    | i3-2130  | 4   | 8G   | 240G SSD                          |                         | N/A          | Decommissioned |

| Machine  | PPT | CPU Curve | GFX Curve | CPU Frequency | vMem | Memory Freq |
| -------- | --- | --------- | --------- | ------------- | ---- | ----------- |
| server-1 | 40W | -20       | -30       | -750          | 1.35 | 3200        |
| server-2 | 40W | -20       | -30       | -750          | 1.35 | 3200        |
| server-3 | 40W | -20       | -30       | -750          | 1.35 | 3200        |

##### ZFS Storage

| Machine | Use    | Dataset   | Size  | Dataset     | Size  | Dataset       | Size  | Disks (SSD)  |
| ------- | ------ | --------- | ----- | ----------- | ----- | ------------- | ----- | ------------ |
| TrueNas | Backup | Nextcloud | 750GB | Ceph Backup | 175GB | WindowsBackup | 750GB | 5x2TB RaidZ2 |

#### Kubernetes Nodes

| Attribute             | Value                 |
| --------------------- | --------------------- |
| **NAME**              | server-1/2/3          |
| **ROLES**             | control-plane, worker |
| **CPU**               | 16                    |
| **Mem**               | 128G                  |
| **OS Disk**           | mdadm 2x5TB SSD       |
| **Container Overlay** | mdadm 2x1TB SSD       |
| **Ceph Storage**      | 2x4TB CEPH NVME       |
| **Nic's**             | 4x10GbE/1GbE A/P      |
| **Bond 0:** v10       | Host/Pod/LoadBalancer |
| **Bond 2:** v12       | Ceph Private          |
| **Bond 3:** v13       | Ceph Public           |
| **Bond 4:** v14       | VM Live Migrate       |
| **Bond 4.3**          | IOT                   |
| **Bond 4.11**         | Virtual Machines      |
| **Bond 4.111**        | Infrastructure        |

##### Ceph Performance Tests

|  test   | class  | threads | bk-size | iops-min | iops-max | iops-avg |  MB/s   |
| :-----: | :----: | :-----: | :-----: | :------: | :------: | :------: | :-----: |
|  seq-r  | cephfs |    4    |  256k   |   3968   |   5120   |   4641   |  1216   |
|  seq-r  | block  |    4    |  256k   |   3958   |   5120   |   4584   |  1202   |
|  seq-w  | cephfs |    4    |  256k   |   1024   |   2048   |   1661   |   436   |
|  seq-w  | block  |    4    |  256k   |   1023   |   2048   |   1627   |   427   |
| rand-r  | cephfs |    4    |  256k   |   3972   |   4854   |   4441   |  1164   |
| rand-r  | block  |    4    |  256k   |   3596   |   4726   |   4385   |  1150   |
| rand-w  | cephfs |    4    |  256k   |   266    |   2048   |   1553   |   397   |
| rand-w  | block  |    4    |  256k   |   1402   |   2048   |   1617   |   423   |
| rand-rw | cephfs |    4    |  256k   |   952    |   1418   |   1202   | 313/316 |
| rand-rw | block  |    4    |  256k   |   876    |   1410   |   1190   | 310/312 |

#### OKD WIF

- <https://github.com/openshift/cloud-credential-operator/blob/master/docs/gcp_workload_identity.md>
- <https://github.com/openshift/cloud-credential-operator/blob/master/docs/ccoctl.md>

File Configuration Locations

```bash
ls ./terraform/gcp/HomeLab/homelab
ls ./terraform/gcp/HomeLab/homelab/wif
ls ./okd/okd-configuration/wif.yaml
```

CCOCTL Binary: <https://mirror.openshift.com/pub/openshift-v4/amd64/clients/ocp/stable/ccoctl-linux.tar.gz>

```bash
PROJECT_ID=""$(vault kv get -field=project_id secret/gcp/org/av/projects)""

./ccoctl gcp create-workload-identity-pool --name=okd-homelab-wif --project=homelab-${PROJECT_ID} --dry-run
./ccoctl gcp create-workload-identity-provider --name=okd-homelab-wif --region=us --project=homelab-${PROJECT_ID} \
  --public-key-file=serviceaccount-signer.public --workload-identity-pool=okd-homelab-wif --dry-run
```

#### Kubernetes Commands

```bash
oc login --web --server https://api.okd.homelab.arthurvardevanyan.com:6443

# Watch ALl Pods
watch kubectl get pods -A -o wide --sort-by=.metadata.creationTimestamp
# Delete Pods that Have a Restart
kubectl get pods -A | awk '$5>0' | awk '{print ""kubectl delete pod -n "" $1 "" "" $2}' | bash -
# Drain Node
 oc adm drain server-3 --delete-emptydir-data --ignore-daemonsets --force
# Nextcloud
kubectl exec -it nextcloud-0 -n nextcloud -- runuser -u www-data -- php -f /var/www/html/occ
```

##### Delete Pod Using Graceful Termination Eviction Request

```bash
NAMESPACE=homelab
POD=el-webhook-6b56cc5f84-clfc6

curl --header ""Authorization: Bearer $(oc whoami -t)"" -H 'Content-type: application/json' \
""$(oc whoami --show-server)/api/v1/namespaces/{$NAMESPACE}/pods/{$POD}/eviction"" \
-d '{""apiVersion"": ""policy/v1"",""kind"": ""Eviction"",""metadata"": {""name"": ""'""${POD}""'"",""namespace"": ""'""${NAMESPACE}""'""}}'
```

- <https://docs.okd.io/latest/rest_api/policy_apis/eviction-policy-v1.html#eviction-policy-v1>
- <https://unofficial-kubernetes.readthedocs.io/en/latest/tasks/configure-pod-container/configure-pod-disruption-budget/>",VRAI
artifacthub/hub,Toolkit,Application System,2025-05-06T10:22:52Z,2024-11-11T08:57:32Z,0,5,0,0,1,0,0,0,2020-01-14T09:53:51Z,2025-04-06T16:25:06Z,88109,1807,TypeScript,VRAI,249,FAUX,19,"cloud-native,cncf,kubernetes,packages",19,"Find, install and publish Cloud Native packages",FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,47,"# Artifact Hub

[![Go Report Card](https://goreportcard.com/badge/github.com/artifacthub/hub)](https://goreportcard.com/report/github.com/artifacthub/hub)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/4106/badge)](https://bestpractices.coreinfrastructure.org/projects/4106)
[![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/artifact-hub)](https://artifacthub.io/packages/helm/artifact-hub/artifact-hub)
[![CLOMonitor](https://img.shields.io/endpoint?url=https://clomonitor.io/api/projects/cncf/artifact-hub/badge)](https://clomonitor.io/projects/cncf/artifact-hub)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/artifacthub/hub/badge)](https://securityscorecards.dev/viewer/?uri=github.com/artifacthub/hub)
[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/artifacthub/hub)
[![Licenses](https://app.fossa.io/api/projects/git%2Bhttps%3A%2F%2Fgithub.com%2Fartifacthub%2Fhub.svg?type=shield)](https://app.fossa.io/projects/git%2Bhttps%3A%2F%2Fgithub.com%2Fartifacthub%2Fhub?ref=badge_shield)

[Artifact Hub](https://artifacthub.io) is a web-based application that enables finding, installing, and publishing packages and configurations for Cloud Native packages.

Discovering artifacts to use with CNCF projects can be difficult. If every CNCF project that needs to share artifacts creates its own Hub this creates a fair amount of repeat work for each project and a fractured experience for those trying to find the artifacts to consume. The Artifact Hub attempts to solve that by providing a single experience for consumers that any CNCF project can leverage.

At the moment, the following artifacts kinds are supported *(with plans to support more projects to follow)*:

- [Argo templates](https://argoproj.github.io/argo-workflows/)
- [Backstage plugins](https://backstage.io)
- [Bootable containers](https://containers.github.io/bootc/)
- [Containers images](https://opencontainers.org)
- [CoreDNS plugins](https://coredns.io/)
- [Falco configurations](https://falco.org/)
- [Gatekeeper policies](https://open-policy-agent.github.io/gatekeeper/website/docs/)
- [Headlamp plugins](https://headlamp.dev)
- [Helm charts](https://helm.sh/)
- [Helm plugins](https://helm.sh/docs/topics/plugins/)
- [Inspektor Gadgets](https://www.inspektor-gadget.io)
- [KCL modules](https://kcl-lang.io)
- [KEDA scalers](https://keda.sh/)
- [Keptn integrations](https://keptn.sh)
- [Knative client plugins](https://knative.dev)
- [KubeArmor policies](https://kubearmor.io)
- [Kubectl plugins (Krew)](https://krew.sigs.k8s.io/)
- [Kubewarden policies](https://www.kubewarden.io)
- [Kyverno policies](https://kyverno.io)
- [Meshery designs](https://meshery.io)
- [OLM operators](https://github.com/operator-framework)
- [OpenCost plugins](https://www.opencost.io)
- [Open Policy Agent (OPA) policies](https://www.openpolicyagent.org/)
- [Radius Recipes](https://radapp.io)
- [Tekton tasks, pipelines and stepactions](https://tekton.dev/)
- [Tinkerbell actions](https://tinkerbell.org/)

You can use Artifact Hub to:

- [Discover](https://artifacthub.io/packages/search), [install](https://artifacthub.io/packages/helm/artifact-hub/artifact-hub?modal=install) and [publish](https://artifacthub.io/docs/topics/repositories/) packages and configurations
- Explore content like Helm charts [schemas](https://artifacthub.io/packages/helm/artifact-hub/artifact-hub?modal=values-schema) and [templates](https://artifacthub.io/packages/helm/artifact-hub/artifact-hub/0.20.0?modal=template&template=db_migrator_install_job.yaml) in an interactive way
- Subscribe to packages' new releases and security alerts notifications, via email or webhooks
- Visualize packages' [security reports](https://artifacthub.io/packages/helm/artifact-hub/artifact-hub/0.19.0?modal=security-report)
- Inspect packages' [changelog](https://artifacthub.io/packages/helm/artifact-hub/artifact-hub?modal=changelog)

Feel free to ask any questions on the #artifact-hub channel in the CNCF Slack. To get an invite please visit [http://slack.cncf.io/](http://slack.cncf.io/).

Artifact Hub is a [CNCF Incubating Project](https://www.cncf.io/projects/).

<br/>
<table>
    <tr>
        <td width=""33%""><img src=""https://artifacthub.github.io/hub/screenshots/screenshot1.jpg""></td>
        <td width=""33%""><img src=""https://artifacthub.github.io/hub/screenshots/screenshot2.jpg""></td>
        <td width=""33%""><img src=""https://artifacthub.github.io/hub/screenshots/screenshot3.jpg""></td>
    </tr>
    <tr>
        <td width=""33%""><img src=""https://artifacthub.github.io/hub/screenshots/screenshot4.jpg""></td>
        <td width=""33%""><img src=""https://artifacthub.github.io/hub/screenshots/screenshot5.jpg""></td>
        <td width=""33%""><img src=""https://artifacthub.github.io/hub/screenshots/screenshot6.jpg""></td>
    </tr>
</table>

## Getting started

[Artifact Hub](https://artifacthub.io) allows publishers to list their content in an automated way. Please check out the [repositories guide](https://artifacthub.io/docs/topics/repositories/) for more details about how to add your repositories.

If you want to run your own Artifact Hub instance in your Kubernetes cluster, the easiest way is by deploying the Helm chart provided. For more details, please see the [Helm chart documentation in Artifact Hub](https://artifacthub.io/packages/helm/artifact-hub/artifact-hub).

## Contributing

Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for more details.

## Community

The Artifact Hub is an open source project. Aside from contributing code and feature suggestions you can also engage via:

- Attending a meeting. Meetings are on the 2nd Tuesday of the month at 10:30am PT / 1:30pm ET. [Meeting minutes and agenda are in Google Docs](https://docs.google.com/document/d/1nkIgFh4dNPawoDD_9fV7vicVSeKk2Zcdd0C5yovSiKQ/edit).
- Joining [CNCF slack](https://cloud-native.slack.com) ([join link](https://slack.cncf.io/)) and joining the room #artifact-hub.

## Changelog

The *changelog* is [available on Artifact Hub](https://artifacthub.io/packages/helm/artifact-hub/artifact-hub?modal=changelog).

## Code of Conduct

This project follows the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md).

## Roadmap

Please see [ROADMAP.md](./ROADMAP.md) for more details.

## Security

To report a security problem in Artifact Hub, please contact the Maintainers Team at <cncf-artifacthub-maintainers@lists.cncf.io>. Please see [SECURITY.md](./SECURITY.md) for more details.

## CLOMonitor Report

[![CloMonitor report summary](https://clomonitor.io/api/projects/cncf/artifact-hub/report-summary?theme=light)](https://clomonitor.io/projects/cncf/artifact-hub)

## License

Artifact Hub is an Open Source project licensed under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).

[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bhttps%3A%2F%2Fgithub.com%2Fartifacthub%2Fhub.svg?type=large)](https://app.fossa.io/projects/git%2Bhttps%3A%2F%2Fgithub.com%2Fartifacthub%2Fhub?ref=badge_large)",VRAI
aws-cloudformation/aws-cloudformation-templates,Documentations,Documentations,2025-04-28T18:33:24Z,2025-02-12T22:27:09Z,0,0,0,0,1,0,1,0,2015-12-08T20:52:52Z,2025-04-07T20:42:08Z,3103,4852,Python,VRAI,4403,FAUX,6,"aws-cloudformation,cloudformation-template",6,A collection of useful CloudFormation templates ,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,147,"# AWS CloudFormation Sample Templates

This repository contains sample CloudFormation templates that you can use
to help you get started on new infrastructure projects. Keep in mind that these 
templates are not meant to be production-ready ""QuickStarts"". You should 
take the time to learn how they work, adapt them to your needs, and make sure
that they meet your company's compliance standards.

Each template in this repository passes
[CloudFormation Linter](https://github.com/aws-cloudformation/cfn-lint)
(cfn-lint) checks, and also a basic set of
[CloudFormation Guard](https://github.com/aws-cloudformation/cloudformation-guard)
 rules based on the CIS Top 20, with exceptions for some rules where it
made sense to keep the sample focused on a single use case.

## Discord

Join us on Discord to discuss rain and all things CloudFormation! Connect and
interact with CloudFormation developers and experts, find channels to discuss
rain, the CloudFormation registry, StackSets, cfn-lint, Guard and more:

[![Join our Discord](https://discordapp.com/api/guilds/981586120448020580/widget.png?style=banner3)](https://discord.gg/9zpd7TTRwq)

## Submitting Templates

Before you submit a template, we suggest that you follow these guidelines:

- Fork the repository and create a fresh branch to work on your sample
  ```
  git remote add upstream git@github.com:aws-cloudformation/aws-cloudformation-templates.git
  git fetch upstream
  git checkout -b my-branch-name upstream/main
  git push -u origin
  ```
- Write the template in YAML, with a `.yaml` suffix (not `.yml` or
  `.template`). Our test scripts will auto-generate a JSON file based on the
  YAML. YAML is the source of truth for all templates in this repository.
- If your solution needs any other type of YAML file, like a K8s manifest 
  or a build spec, give it a `.yml` extension. This will cause it to be skipped
  by the test scripts.
- Test your template. Can you successfully create a stack with it?  When you
  delete a stack, is the stack (and all of its resources) successfully deleted?
  Make sure users aren't left with stray resources or stacks that have deletion
  errors.
- In the Description section, add a brief description of your template. The
  description should indicate what the template does and why it's useful. For
  example:
  ```
  Description: ""Create a LAMP stack using a single EC2 instance and
  a local MySQL database for storage. This template demonstrates using the AWS
  CloudFormation bootstrap scripts to install the packages and files necessary
  to deploy the Apache web server, PHP, and MySQL when the instance is
  launched.""
  ```
- Format your template to make it human readable:
  - Err on the side of human readability. If it makes your template easier to
    read, do it.
  - Use cfn-lint to lint your template and make sure it is valid.
  - Consider using two-space indents to reduce line wrapping.
- Review IAM resources. If you include IAM resources, follow the standard
  security advice of granting least privilege (granting only the permissions
  required to do a task).
- Remove secrets/credentials from your template. You might hardcode credentials
  or secrets in your template when you're testing. Don't forget to remove them
  before submitting your template. You can use this tool to help you scrub
  secrets:
  [https://github.com/awslabs/git-secrets](https://github.com/awslabs/git-secrets).
- Add your template to the correct folder so that others can discover it.
- Run the `scripts/test-all.sh` script in the directory where you're working to 
  make sure the template is valid.
- If you write any lambda function code, put it in a separate file and run
  `pylint` or `eslint` to make sure the code is valid.

When your template is ready, submit a pull request. A member of the AWS
organization will review your request and might suggest changes. 

## Additional Resources

### CloudFormation Linter (cfn-lint)

The [CloudFormation Linter](https://github.com/aws-cloudformation/cfn-lint) is
an indispensable tool for developing your templates. It should be a part of
every developer's workflow, and incorporated into your CI/CD pipelines.

Install cfn-lint with pip:

```sh
pip install cfn-lint
```

### CLoudFormation Rain

[Rain](https://github.com/aws-cloudformation/rain) is a command line interface
(CLI) for CloudFormation that greatly improves the experience for authoring and
deploying templates. It has many features, such as creating starter templates
for various use cases, interactive deployments, modules, and more.

Rain can be installed with Brew:

```sh
brew install rain
```

or if you are a Go user, you can install it like this:

```sh
go install github.com/aws-cloudformation/rain/cmd/rain@latest
```


In the *AWS CloudFormation User Guide*, you can view more information about the
following topics:

- Learn how to use templates to create AWS CloudFormation stacks using the
  [AWS Management Console](http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stack.html)
  or
  [AWS Command Line Interface (AWS CLI)](http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-cli-creating-stack.html).
- To view all the supported AWS resources and their properties, see the
  [Template Reference](http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-reference.html).",FAUX
aws-cloudformation/aws-guard-rules-registry,Toolkit,Documentations,2024-03-23T04:00:52Z,2022-10-26T03:33:26Z,0,0,0,0,0,0,210,0,2021-11-18T18:58:48Z,2025-03-07T15:07:06Z,7173,118,Python,VRAI,26,FAUX,27,"aws,cfn-guard,infrastructure-as-code,policy-as-code,static-application-security-testing",27,Rules Registry for Compliance Frameworks,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,12,"# AWS Guard Rules Registry

AWS Guard Rules Registry is an open-source repository of [AWS CloudFormation Guard](https://github.com/aws-cloudformation/cloudformation-guard) rule files and managed rule sets that help organizations *shift left* in their Software Development Life Cycle (SDLC) processes.

## TL;DR

**Leverage the existing AWS Guard Registry Rule Sets currently available:**
* Read the [Using Guard Rules Registry Guide](./docs/Using-Guard-Rules-Registry.md) for information on how to integrate into your existing continuous integration and development processes. Then pick from the list of [Guard Rules Registry Managed Rule Sets](#managed-rule-sets).

**Contribute to the individual AWS Guard Registry Rules:**
* Read the [Guard Rules Development Guide](./docs/Guard-Rules-Dev-Guide.md) for details in how to contribute and develop Guard Rules Registry rules. Additionally, Guard Rules Registry has several staged Guard rule files that have yet to be implemented. These Guard rules are to be a *best of effort* representation of AWS Config Managed rules. To get started look for an open issues labeled `good first issue`.

**Create and contribute your own open source AWS Guard Rules Registry custom rule set:**
* Read the [Guard Rule Sets Development Guide](./docs/Guard-Rule-Sets-Dev-Guide.md) for details on creating or updating the Guard Map rule set files.

## About

AWS Guard Rules Registry is an open-source repository of rule files and managed rule sets for AWS CloudFormation Guard. The intent of the registry is to give users Guard rules that provide policy as code solutions which complement the AWS Config Managed Rules as well as your Guard rules. Many of the Guard rules supported by AWS are best-effort Guard rule implementations of AWS Config Managed Rules.

> **Note:** Not all AWS Config Managed Rules are present in the AWS Guard Rules Registry. Some of the AWS Config Managed Rules are detective only in nature and are not able to be expressed in infrastructure as code relevant to development practices.

The Guard Rules Registry offers the following value:

* Easy to consume Managed Rules Sets based on many of the sample AWS Conformance Packs. *see [Guard Rules Registry Managed Rule Sets](#managed-rule-sets)*
* Individual Guard Rule files giving *best effort* to correspond to an AWS Config Managed Rule
* Rule Set mapping process to compile single rule set files for public consumption
* A centralized location for users, teams, and organizations to manage and open source their custom Guard rule sets
* Resource level rule suppress! See [Using Guard Rules Registry Rule Suppression](./docs/Using-Guard-Rules-Registry.md#guard-rules-registry-rule-suppression) for more details.

### Registry Rules Files

One of the intents of AWS Guard Rules Registry is to create modular single file Guard rule files that can be mapped into multiple managed rule sets similar to how AWS Config Conformance Packs work with AWS Config Managed Rules. The AWS Guard Rules Registry contains individual guard rule files associated to a single rule. The [rules directory](/rules) contains multiple sub-directories based on different technologies, providers, and services.

    ```
    rules
    ├── aws
    │   └── apigateway
    │   │   ├── apigw_method_auth_type_is_not_none.guard
    │   │   └── tests
    │   │       └── apigw_method_auth_type_is_not_none_tests.yml
    │   └── dynamodb
    │       ├── dynamodb_pitr_enabled.guard
    │       └── tests
    │           └── dynamodb_pitr_is_enabled_tests.yml
    ├── kubernetes
    └── terraform
    ```

Many of the Guard rules are supported by AWS and correspond or complement associated AWS Config Managed Rules. These rules can be identified by the all-uppercase naming convention which is identical to the AWS Config Managed Rule identifier.

> **Note:** Guard rule names that are in all uppercase are intentionally set this way. The names reflects the AWS Config Managed rule identifier the guard rule is satisfying.

Within each directory that contains Guard rules, there is a `tests` sub-directory contains unit tests for some of the corner cases we expect Guard rule to `PASS`/`FAIL`/`SKIP`. The `test` sub-directory contains the corresponding test file for the Guard rule with the suffix `_tests` and can have the extension of `.yml` or `.json`. To learn more, see [Guard Rules Dev Guide](./docs/Guard-Rules-Dev-Guide.md#writing-unit-tests) for more detail on how to create unit tests for your guard rule.

### Managed Rule Sets

AWS Guard Rules registry contains prebuilt managed rule sets compiled from rule mapping files found in the [mappings](/mappings) directory. The following managed Rule Sets are available for use:

| Managed Rule Set                         | Rules Set Name | Mapping File |
| -------------------------------- | -------- | ---------- |
| ABS Cloud Computing Implementation Guide 2.0 - Material Workloads | ABS-CCIGv2-Material | [Link](/mappings/rule_set_ABS_CCIGv2_Material.json) |
| ABS Cloud Computing Implementation Guide 2.0 - Standard Workloads | ABS-CCIGv2-Standard | [Link](/mappings/rule_set_ABS_CCIGv2_Standard.json) |
| Australian Cyber Security Centre (ACSC) Essential Eight Maturity Model | acsc-essential-8 | [Link](/mappings/rule_set_acsc_essential_8.json) |
| Australian Cyber Security Centre (ACSC) Information Security Manual (ISM) 2020-06 | acsc-ism | [Link](/mappings/rule_set_acsc_ism.json) |
| Australian Prudential Regulation Authority (APRA) CPG 234 | apra-cpg-234 | [Link](/mappings/rule_set_apra_cpg_234.json) |
| Bank Negara Malaysia (BNM) Risk Management in Technology (RMiT) | bnm-rmit | [Link](/mappings/rule_set_bnm_rmit.json) |
| Center for Internet Security (CIS) Amazon Web Services Foundation v1.4 Level 1 | cis-aws-benchmark-level-1 | [Link](/mappings/rule_set_cis_aws_benchmark_level_1.json) |
| Center for Internet Security (CIS) Amazon Web Services Foundation v1.4 Level2 | cis-aws-benchmark-level-2 | [Link](/mappings/rule_set_cis_aws_benchmark_level_2.json) |
| Center for Internet Security (CIS) Critical Security Controls v8 IG1 | cis-critical-security-controls-v8-ig1 | [Link](/mappings/rule_set_cis_critical_security_controls_v8_ig1.json) |
| Center for Internet Security (CIS) Critical Security Controls v8 IG2 | cis-critical-security-controls-v8-ig2 | [Link](/mappings/rule_set_cis_critical_security_controls_v8_ig2.json) |
| Center for Internet Security (CIS) Critical Security Controls v8 IG3 | cis-critical-security-controls-v8-ig3 | [Link](/mappings/rule_set_cis_critical_security_controls_v8_ig3.json) |
| Center for Internet Security (CIS) Top 20 Critical Security Controls | cis-top-20 | [Link](/mappings/rule_set_cis_top_20.json) |
| Cybersecurity & Infrastructure Security Agency (CISA) Cyber Essentials (CE) | cisa-ce | [Link](/mappings/rule_set_cisa_ce.json) |
| Cybersecurity Maturity Model Certification (CMMC) Level 1 | cmmc-level-1 | [Link](/mappings/rule_set_cmmc_level_1.json) |
| Cybersecurity Maturity Model Certification (CMMC) Level 2 | cmmc-level-2 | [Link](/mappings/rule_set_cmmc_level_2.json) |
| Cybersecurity Maturity Model Certification (CMMC) Level 3 | cmmc-level-3 | [Link](/mappings/rule_set_cmmc_level_3.json) |
| Cybersecurity Maturity Model Certification (CMMC) Level 4 | cmmc-level-4 | [Link](/mappings/rule_set_cmmc_level_4.json) |
| Cybersecurity Maturity Model Certification (CMMC) Level 5 | cmmc-level-5 | [Link](/mappings/rule_set_cmmc_level_5.json) |
| European Union Agency for Cybersecurity (ENISA) Cybersecurity guide for SMEs  | enisa-cybersecurity-guide-for-smes | [Link](/mappings/rule_set_enisa_cybersecurity_guide_for_smes.json) |
| Spain Esquema Nacional de Seguridad (ENS) High framework controls | ens-high | [Link](/mappings/rule_set_ens_high.json) |
| Spain Esquema Nacional de Seguridad (ENS) Low framework controls | ens-low | [Link](/mappings/rule_set_ens_low.json) |
| Spain Esquema Nacional de Seguridad (ENS) Medium framework controls | ens-medium | [Link](/mappings/rule_set_ens_medium.json) |
| Title 21 of the Code of Federal Regulations (CFR) Part 11 | FDA-21CFR-Part-11 | [Link](/mappings/rule_set_FDA_21CFR_Part_11.json) |
| Federal Risk and Authorization Management Program (FedRAMP) Moderate | fedramp-moderate | [Link](/mappings/rule_set_fedramp_moderate.json) |
| Federal Risk and Authorization Management Program (FedRAMP) Low | fedramp-low  | [Link](/mappings/rule_set_fedramp_low.json) |
| Federal Financial Institutions Examination Council (FFIEC) Cyber Security Assessment Tool domains | ffiec | [Link](/mappings/rule_set_ffiec.json) |
| Health Insurance Portability and Accountability Act (HIPAA) | hipaa-security | [Link](/mappings/rule_set_hipaa_security.json) |
| Korea – Information Security Management System (ISMS) | k-isms | [Link](/mappings/rule_set_k_isms.json) |
| Monetary Authority of Singapore (MAS) Notice 655 – Cyber Hygiene | mas-notice-655 | [Link](/mappings/rule_set_mas_notice_655.json) |
| Monetary Authority of Singapore (MAS) Technology Risk Management Guidelines (TRMG) January 2021 | mas-trmg | [Link](/mappings/rule_set_mas_trmg.json) |
| National Bank of Cambodia’s (NBC) Technology Risk Management (TRM) Guidelines framework | nbc-trmg | [Link](/mappings/rule_set_nbc_trmg.json) |
| UK National Cyber Security Centre (NCSC) Cyber Assessment Framework (CAF) controls | ncsc-cafv3 | [Link](/mappings/rule_set_ncsc_cafv3.json) |
| UK National Cyber Security Centre (NCSC) Cloud Security Principles | ncsc | [Link](/mappings/rule_set_ncsc.json) |
| North American Electric Reliability Corporation Critical Infrastructure Protection Standards (NERC CIP) for BES Cyber System Information (BCSI), CIP-004-7 & CIP-011-3  | nerc | [Link](/mappings/rule_set_nerc.json) |
| NIST 1800-25 | nist-1800-25 | [Link](/mappings/rule_set_nist_1800_25.json) |
| NIST 800-171 | nist-800-171 | [Link](/mappings/rule_set_nist_800_171.json) |
| NIST 800-172 | nist-800-172 | [Link](/mappings/rule_set_nist_800_172.json) |
| NIST 800-181 | nist-800-181 | [Link](/mappings/rule_set_nist_800_181.json) |
| NIST 800-53 Revision 4 | nist800-53rev4 | [Link](/mappings/rule_set_nist800_53rev4.json) |
| NIST 800-53 Revision 5| nist800-53rev5 | [Link](/mappings/rule_set_nist800_53rev5.json) |
| NIST Cyber Security Framework (CSF)  | nist-csf | [Link](/mappings/rule_set_nist_csf.json) |
| NIST Privacy Framework | nist-privacy-framework | [Link](/mappings/rule_set_nist_privacy_framework.json) |
| New Zealand Government Communications Security Bureau (GCSB) Information Security Manual (NZISM) | nzism | [Link](/mappings/rule_set_nzism.json) |
| Payment Card Industry Data Security Standard (PCI DSS) 3.2.1 | PCI-DSS-3-2-1 | [Link](/mappings/rule_set_pci_dss_3_2_1.json) |
| Reserve Bank of India (RBI) Cyber Security Framework for Urban Cooperative Banks (UCBs) | rbi-bcsf-ucb | [Link](/mappings/rule_set_rbi_bcsf_ucb.json) |
| Reserve Bank of India (RBI) Master Direction – Information Technology Framework | rbi-md-itf | [Link](/mappings/rule_set_rbi_md_itf.json) |
| New York State Department Of Financial Services (NYDFS) cybersecurity requirements for financial services companies (23 NYCRR 500) | us-nydfs | [Link](/mappings/rule_set_us_nydfs.json) |
| Amazon Web Services' Well-Architected Framework Reliability Pillar | wa-Reliability-Pillar | [Link](/mappings/rule_set_wa-Reliability-Pillar.json) |
| AWS Guard rule set for Amazon Web Services' Well-Architected Framework Security Pillar | wa-Security-Pillar | [Link](/mappings/rule_set_wa-Security-Pillar.json) |


## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This project is licensed under the [Apache-2.0 License](LICENSE).",VRAI
aws-cloudformation/cloudformation-guard,Toolkit,Application System,2025-05-07T13:32:20Z,2024-10-08T14:33:39Z,0,0,0,0,0,0,45,0,2020-06-12T20:56:45Z,2025-04-04T21:05:21Z,13354,1330,Rust,VRAI,184,FAUX,34,"cfn-guard,cloudformation,compliance,governance,k8s,policy-as-code,policy-rule-evaluation,security,terraform",34,"Guard offers a policy-as-code domain-specific language (DSL) to write rules and validate JSON- and YAML-formatted data such as CloudFormation Templates, K8s configurations, and Terraform JSON plans/configurations against those rules. Take this survey to provide feedback about cfn-guard: https://amazonmr.au1.qualtrics.com/jfe/form/SV_bpyzpfoYGGuuUl0",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,40,"# AWS CloudFormation Guard

**Validate Cloud Environments with Policy-as-Code**

AWS CloudFormation Guard is an open-source general-purpose policy-as-code evaluation tool. It provides developers with a simple-to-use, yet powerful and expressive domain-specific language (DSL) to define policies and enables developers to validate JSON- or YAML- formatted structured data with those policies.

The Guard 3.0 release introduces support for stateful rules through built-in functions, SAM CLI as an alternative deployment method for `cfn-guard-lambda`, command auto-completions for shell, advanced regular expressions, improved handling of intrinsic functions for the test command, as well as the `--structured` flag to the validate command to emit JSON/YAML parseable output. Note that previously written tests may have to be reviewed due to the corrected behavior of intrinsic function handling. Please see the release notes for full details and examples.

Guard can be used for the following domains:

1. **Preventative Governance and Compliance (shift left):** validate Infrastructure-as-code (IaC) or infrastructure/service compositions such as CloudFormation Templates, CloudFormation ChangeSets, Terraform JSON configuration files, Kubernetes configurations, and more against Guard policies representing your organizational best practices for security, compliance, and more. For example, developers can use Guard policies with
   1. Terraform plan (**in JSON format**) for deployment safety assessment checks or Terraform state files to detect live state deviations.
   2. Static assessment of IaC templates to determine network reachability like Amazon Redshift cluster deployed inside a VPC and prevent the provision of such stacks.
2. **Detective Governance and Compliance:** validate conformity of Configuration Management Database (CMDB) resources such as AWS Config-based configuration items (CIs). For example, developers can use Guard policies against AWS Config CIs to continuously monitor state of deployed AWS and non-AWS resources, detect violations from policies, and trigger remediation.
3. **Deployment Safety:** validate CloudFormation ChangeSets to ensure changes are safe before deployment. For example, renaming an Amazon DynamoDB Table will cause a replacement of the Table. With Guard 3.0, you can prevent such changes in your CI/CD pipelines.

> **NOTE**: If you are using Guard 1.0 rules, we highly recommend adopting the latest version of Guard to simplify your current policy-as-code experience. Guard 2.0 and higher versions are backward incompatible with your Guard 1.0 rules and can result in breaking changes. The Guard 2.0 release was a complete re-write of the earlier 1.0 version to make the tool general-purpose.
>
> To migrate from Guard 1.0 rules to use the updated grammar, follow the steps given below.
>
> 1. Pull the release artifacts for the latest `2.x.x` release from the corresponding release page listed [here](https://github.com/aws-cloudformation/cloudformation-guard/releases).
> 2. Use `migrate` command to transition your existing 1.0 rules to use the updated grammar
> 3. Read about all new Guard features from the latest release, and modify your rules for enhanced experience

**Guard In Action**

![Guard In Action](images/guard-demo.gif)

**Customer Feedback**
Take this [survey](https://amazonmr.au1.qualtrics.com/jfe/form/SV_bpyzpfoYGGuuUl0) to provide feedback about cfn-guard

## Table of Contents

<!-- no toc -->

- [FAQs](#faqs)
- [Guard DSL](#guard-dsl)
  - [Tenets](#tenets)
  - [Features of Guard DSL](#features-of-guard-dsl)
- [Guard CLI](#guard-cli)
  - [Installation](#installation)
  - [How does Guard CLI work?](#how-does-guard-cli-work?)
- [Rule authoring references](#references)
- [Built-in functions & stateful rules](#functions)
- [AWS Rule Registry](#registry)
- [Use Guard as a Docker Image](#docker)
- [Use Guard as a Github Action](https://github.com/aws-cloudformation/cloudformation-guard/tree/main/action#readme)
- [Use Guard as a CI tool](#ci)
- [Use Guard as a pre-commit hook](#pre-commit-hook)
- [Contribute using the DevContainer in VSCode](#devcontainer)
- [License](#license)

## FAQs

**1) What is Guard?**

> Guard is an open-source command line interface (CLI) that provides developers a general purpose domain-specific language (DSL) to express policy-as-code and then validate their JSON- and YAML-formatted data against that code. Guard’s DSL is a simple, powerful, and expressive declarative language to define policies. It is built on the foundation of clauses, which are assertions that evaluate to `true` or `false`. Examples clauses can include simple validations like all Amazon Simple Storage Service (S3) buckets must have versioning enabled, or combined to express complex validations like preventing public network reachability of Amazon Redshift clusters placed in a subnet. Guard has support for looping, queries with filtering, cross query joins, single shot variable assignments, conditional executions, and composable rules. These features help developers to express simple and advanced policies for various domains.

**2) What Guard is not?**

> Guard **is not** a general-purpose programming language. It is a **purpose-built** DSL that is designed for policy definition and evaluation. Both non-technical people and developers can easily pick up Guard. Guard is human-readable and machine enforceable.

**3) Where can I use Guard?**

> You can use Guard to define any type of policy for evaluation. You can apply Guard in the context of multiple domains: a) validating IaC/service compositions such as [CloudFormation Templates](https://aws.amazon.com/cloudformation/resources/templates/), Terraform JSON configuration files, and Kubernetes configurations, b) verifying conformity of CMDB resources such as AWS Config-based CIs, and c) assessing security postures across resources like AWS Security Hub. The policy language and expression is common to all of them, based on simple Guard clauses.

**3) What is a clause in Guard?**

> Clause is an assertion that evaluates to true or false. Clauses can either use binary operations to compare two values (e.g `==, >` and `in`), or unary operations that takes only one value (e.g. `exists, empty,` and `is_list`). Here is a sample clause that compares `Type` to be a `AWS::S3::Bucket` :

```
Type == /AWS::S3::Bucket/
```

**4) What are the supported** **types** **that can I use to define clauses?**

> Guard supports all primitives `string, integer (64), float (64), bool, char, regex` and specialized range expression like `r(10, 200)`, for specifying ranges of values. It supports general key value pair maps (a.k.a associative arrays/struct) like `{ ""my-map"": { ""nested-maps"": [ { ""key"": 10, ""value"": 20 } ] } },` and arrays of primitives or key-value pair maps like `[10, 20, 30] or [{ Key: ""MyApp"", Value: ""PROD}, ..]`.

**5) What binary and unary comparison operators can I use?**

> _Unary Operators:_ `exists, empty, is_string, is_list, is_struct, is_bool, is_int, is_float, not(!)` > _Binary Operators:_ `==, !=, >, >=, <, <=, IN `
>
> Most operators are self-explanatory. A few important points:
>
> 1. Refer [Guard: Clauses](docs/CLAUSES.md) to understand the usage of `exists` and `empty` operators
> 2. Clause `ports >= [10, 20, 30]` implies that every element for `ports` is `>= 30`. If your intention is range, then express it as `r[10, 30]` .
> 3. Clause `ports >= 100` can have ports resolve to an array `[121, 200, 443]`. This check ensures that every element returned was >= 100, and in the example shown this evaluates to `true.`
> 4. `IN` operator for collections (does not work for `string` type) to check if any value matches. For example:

```
Properties.SslPolicy IN [""ELBSecurityPolicy-TLS-1-2-2017-01"", ""ELBSecurityPolicy-TLS-1-2-Ext-2018-06""]
```

**6) How can I define advanced policy rules?**

> You can define advanced policy rules using Conjunctive normal form. For example, here is a clause that asserts that all S3 buckets have a) names that start with a common prefix, b) encryption turned on, and c) only KMS-based algorithm is used (to know more about the query part read [Guard: Query and Filtering](docs/QUERY_AND_FILTERING.md)) for IaC template.

```
let s3_buckets = Resources.*[ Type == /S3::Bucket/ ]

# Skip the checks if there are no S3 buckets present
rule s3_bucket_name_encryption_check when %s3_buckets !empty {
    %s3_buckets {
        Properties {
             # common prefix
             BucketName == /^MyCompanyPrefix/

             # encryption MUST BE on
             BucketEncryption.ServerSideEncryptionConfiguration[*] {
                # only KMS
                ServerSideEncryptionByDefault.SSEAlgorithm IN
                    [""aws:KMS""]
             }
        }
    }
}
```

**7) Can I easily test policy rules?**

> Yes. Guard supports a built-in unit testing framework to test policy rules and clauses. This gives customers confidence that their guard policy rules work as intended. You can learn more about this unit testing framework in this doc [Guard: Unit Testing](docs/UNIT_TESTING.md)

**8)** **Does Guard support rule categories?**

> Yes. Guard supports running several rule-sets together for validating policies. You can create multiple rule files, each with its own intended purpose. For example, you can create one rules file for S3, second one for Dynamo DB, third one for access management, and so on. Alternatively, you can create a rules file for all your security related rules, second one for cost compliance, and so on. You can run Guard against all these rule files at once for evaluation. Refer example rules file [Guard: Clauses](docs/CLAUSES.md), [Guard: Complex Composition](docs/COMPLEX_COMPOSITION.md).

**9) Where can I evaluate Guard policies?**

> Guard supports the entire spectrum of end-to-end evaluation of policy checks. The tool supports bringing in shift-left practices as close as running it directly at development time, integrated into code repositories via hooks like GitHub Actions for pull requests, and into CI/CD pipelines such as AWS CodePipeline pipelines and Jenkins (just exec process).

**10) What are you not telling me? This sounds too good to be true.**

> Guard is a DSL and an accompanying CLI tool that allows easy-to-use definitions for declaring and enforcing policies. Today the tool supports local file-based execution of a category of policies. Guard doesn’t support the following things today, along with workarounds for some:
>
> 1. Sourcing of rules from external locations such as GitHub Release and S3 bucket. If you want this feature natively in Guard, please raise an issue or +1 an existing issue.
> 2. Ability to import Guard policy file by reference (local file or GitHub, S3, etc.). It currently only supports a directory on disk of policy files, that it would execute.
> 3. Parameter/Vault resolution for IaC tools such as CloudFormation or Terraform. Before you ask, the answer is NO. We will not add native support in Guard as the engine is general-purpose. If you need CloudFormation resolution support, raise an issue and we might have a solution for you. We do not support HCL natively. We do, however, support Terraform Plan in JSON to run policies against for deployment safety. If you need HCL support, raise an issue as well.
> 4. Ability to reference variables like `%s3_buckets`, inside error messages. Both JSON/Console output for evaluation results contain some of this information for inference. We also do not support using variable references to create dynamic regex expressions. However, we support variable references inside queries for cross join support, like `Resources.%references.Properties.Tags`.
> 5. Support for specifying variable names when accessing map or list elements to capture these values. For example, consider this check `Resources[resource_name].Properties.Tags not empty`, here `resource_name` captures the key or index value. The information is tracked as a part of the evaluation context today and present in both console/JSON outputs. This support will be extended to regex expression variable captures as well.
> 6. There are [known issues](docs/KNOWN_ISSUES.md) with potential workarounds that we are tracking towards resolution

**11) What are we really thankful about?**

> Where do we start? Hmm.... we want to thank Rust [language’s forums](https://users.rust-lang.org/), [build management, and amazing ecosystem](https://crates.io/) without which none of this would have been possible. We are not the greatest Rust practitioners, so if we did something that is not idiomatic Rust, please raise a PR.
>
> We want to make a special mention to [nom](https://github.com/Geal/nom) combinator parser framework to write our language parser in. This was an excellent decision that improved readability, testability, and composition. We highly recommend it. There are some rough edges, but it’s just a wonderful, awesome library. Thank you. Apart from that, we are consumers of many crates including [hyper](https://crates.io/crates/hyper) for HTTP handling, [simple logger](https://crates.io/crates/simple_logger), and many more. We also want to thank the open-source community for sharing their feedback with us through GitHub issues/PRs.
>
> And of course AWS for supporting the development and commitment to this project. Now read the docs and take it for a ride and tell us anything and everything.

## Guard DSL

### Tenets

**(Unless you know better ones)**

These tenets help guide the development of the Guard DSL:

- **Simple**: The language must be simple for customers to author policy rules, simple to integrate with an integrated development environment (IDE), readable for human comprehension, and machine enforceable.

- **Unambiguous**: The language must not allow for ambiguous interpretations that make it hard for customers to comprehend the policy evaluation. The tool is targeted for security and compliance related attestations that need the auditor to consistently and unambiguously understand rules and their evaluations.

- **Deterministic**: The language design must allow language implementations to have deterministic, consistent, and isolated evaluations. Results for repeated evaluations for the same context and rules must evaluate to the same result every time. Time to evaluate results inside near-identical environments must be within acceptable tolerance limits.

- **Composable**: The language must support composition to help build higher order functionality such as checks for PCI compliance, by easily combining building blocks together. Composition should not increase the complexity for interpreting outcomes, syntax, or navigation.

### Features of Guard DSL

- **Clauses:** Provides the foundational underpinning for Guard. They are assertions that evaluate to true or false. You can combine clauses using [Conjunctive Normal Form](https://en.wikipedia.org/wiki/Conjunctive_normal_form). You can use them for direct assertions, as part of filters to select values, or for conditional evaluations. To learn more read [Guard: Clauses](docs/CLAUSES.md)

- **Context-Aware Evaluations, `this` binding and Loops:** Automatic binding for context values when traversing hierarchical data with support for implicit looping over collections with an easy-to-use syntax. Collections can arise from accessing an array of elements, values for a map along with a filter, or from a query. To learn more read [Guard: Context-Aware Evaluations, this and Loops](docs/CONTEXTAWARE_EVALUATIONS_AND_LOOPS.md)

- **Query & Filtering:** Queries support simple decimal dotted format syntax to access properties in the hierarchical data. Arrays/Collections are accessed using `[]` . Map or Struct’s values can use `*` for accessing values for all keys. All collections can be further narrowed to target specific instances inside the collection using filtering. To learn more read [Guard: Query and Filtering](docs/QUERY_AND_FILTERING.md)

- **Variables, Projections, and Query Interpolation:** Guard supports single shot assignment to variables using a **`let`** keyword for assignment. All variable assignments resulting from a query is a list (result set). One can also assign static literals to variables. Variables are assessed using a prefix **`%`** and can be used inside the Query for interpolation. To learn more read [Guard: Query, Projection and Interpolation](docs/QUERY_PROJECTION_AND_INTERPOLATION.md)

- **Complex Composition**: As stated earlier, clauses can be expressed in Conjunctive Normal Form. Clauses on separates lines are ANDs. Disjunctions are expressed using the `or|OR` keyword. You can group clauses in a named rule. You can then use named rules in other rules to create more advanced compositions. Furthermore, you can have multiple files containing named rules that together form a category of checks for a specific compliance like “ensure encryption at rest”. To learn more read [Guard: Complex Composition](docs/COMPLEX_COMPOSITION.md)

## Guard CLI

### Installation

#### Installation from Pre-Built Release Binaries

##### MacOS

By default this is built for macOS-12 (Monterey). See [OS Matrix](https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#github-hosted-runners)

1. Open terminal of your choice. Default `Cmd+Space`, type `terminal`
2. Cut-n-paste the commands below (default latest, add `-s -- -v <VERSION>` for other versions ie. `-s -- -v 3.1.2`)

```bash
$ curl --proto '=https' --tlsv1.2 -sSf https://raw.githubusercontent.com/aws-cloudformation/cloudformation-guard/main/install-guard.sh | sh
```

Remember to add `~/.guard/bin/` to your `$PATH`.

Alternatively, you can install the latest version with [Homebrew](https://brew.sh/).

```bash
$ brew install cloudformation-guard
```

You would not need to modify `$PATH` this way.

##### Ubuntu

1. Open any terminal of your choice
2. Cut-n-paste the commands below (default latest, add `-s -- -v <VERSION>` for other versions ie. `-s -- -v 3.1.2`)

```bash
$ curl --proto '=https' --tlsv1.2 -sSf https://raw.githubusercontent.com/aws-cloudformation/cloudformation-guard/main/install-guard.sh | sh
```

Remember to add `~/.guard/bin/` to your `$PATH`.

##### Windows

1. Open PowerShell as Administrator
2. Cut-n-paste the command below

```shell
$GuardWindowsInstallScript = Invoke-WebRequest https://raw.githubusercontent.com/aws-cloudformation/cloudformation-guard/main/install-guard.ps1; Invoke-Expression $($GuardWindowsInstallScript.Content)
```

If you get an error regarding authorization to execute the script run the follow before retrying step 2:

```shell
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
```

#### Installation of Rust and Cargo

##### Ubuntu/MacOS: Install Rust and Cargo

```bash
$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

If you have not already, run `source $HOME/.cargo/env` as recommended by the rust installer. Read [here](https://rustup.rs/) for more information.

If building on `Ubuntu`, it is recommended to run `sudo apt-get update; sudo apt install build-essential`.

##### Windows 10: Install Rust and Cargo

1. Create a Windows 10 workspace.
2. Install the version of Microsoft Visual C++ Build Tools 2019 which provides just the Visual C++ build tools: https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2019.
3. Download the installer and run it.
4. Select the ""Individual Components"" tab and check ""Windows 10 SDK"".
5. Select the ""Language Packs"" tab and make sure that at least ""English"" is selected.
6. Click ""Install"".
7. Let it download and reboot if asked.
8. Install [Rust](https://forge.rust-lang.org/infra/other-installation-methods.html#other-ways-to-install-rustup).
9. Download [rust-init.exe](https://static.rust-lang.org/rustup/dist/i686-pc-windows-gnu/rustup-init.exe).
10. Run it and accept the defaults.

#### Cargo-based Installation

Now that you have [rust and cargo installed](https://doc.rust-lang.org/cargo/getting-started/installation.html), installation of cfn-guard is easy:

```bash
$ cargo install cfn-guard
```

Check `help` to see if it is working.

```bash
$ cfn-guard help
cfn-guard 3.1.2

  Guard is a general-purpose tool that provides a simple declarative syntax to define
  policy-as-code as rules to validate against any structured hierarchical data (like JSON/YAML).
  Rules are composed of clauses expressed using Conjunctive Normal Form
  (fancy way of saying it is a logical AND of OR clauses). Guard has deep
  integration with CloudFormation templates for evaluation but is a general tool
  that equally works for any JSON- and YAML- data.

Usage: cfn-guard [COMMAND]

Commands:
  parse-tree   Prints out the parse tree for the rules defined in the file.
  test         Built in unit testing capability to validate a Guard rules file against
               unit tests specified in YAML format to determine each individual rule's success
               or failure testing.

  validate     Evaluates rules against the data files to determine success or failure.
               You can point rules flag to a rules directory and point data flag to a data directory.
               When pointed to a directory it will read all rules in the directory file and evaluate
               them against the data files found in the directory. The command can also point to a
               single file and it would work as well.
               Note - When pointing the command to a directory, the directory may not contain a mix of
               rules and data files. The directory being pointed to must contain only data files,
               or rules files.

  rulegen      Autogenerate rules from an existing JSON- or YAML- formatted data. (Currently works with only CloudFormation templates)
  completions  Generate auto-completions for all the sub-commands in shell.
  help         Print this message or the help of the given subcommand(s)

Options:
  -h, --help     Print help
  -V, --version  Print version

```

### How does Guard CLI work?

The two common Guard CLI commands are `validate` and `test`.

#### Validate

Validate command is used when you need to assess the compliance or security posture as defined by a set of policy files against incoming JSON/YAML data. Common data payloads used are CloudFormation Templates, CloudFormation ChangeSets, Kubernetes Pod policies, Terraform Plan/Configuration in JSON format, and more. Here is an example run of the `validate` command for assessing Kubernetes Pod Container configurations

1. Save the sample policy rules file below as `k8s-pod-containers-limits.guard`:

```
#
# Kubernetes container based limit checks
#

#
# These set of rules primarily apply to the version 1 of the API spec (including v1Beta) and
# the kind of document is a 'Pod'
#
rule version_and_kind_match
{
    apiVersion == /v1/
    kind == 'Pod'
}

#
# For the 'Pod' document ensure that containers have resource limits set
# for memory
#
rule ensure_container_has_memory_limits when version_and_kind_match
{
    spec.containers[*]
    {
       resources.limits
       {
            #
            # Ensure that memory attribute is set
            #
            memory exists
            <<
                Id: K8S_REC_22
                Description: Memory limit must be set for the container
            >>
        }
   }

}

#
# For the 'Pod' document ensure that containers have resource limits set
# for cpu
#
rule ensure_container_has_cpu_limits when version_and_kind_match
{
    spec.containers[*]
    {
       resources.limits
       {
            #
            # Ensure that cpu attribute is set
            #
            cpu exists
            <<
                Id: K8S_REC_24
                Description: Cpu limit must be set for the container
            >>
       }
   }
}
```

2. Paste the command below and hit `enter`

```bash
cfn-guard validate -r k8s-pod-containers-limits.guard
```

3. Cut-n-paste the sample configuration below for k8s pods on STDIN and then hit `CTRL+D`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
    - name: app
      image: ""images.my-company.example/app:v4""
      resources:
        requests:
          memory: 64Mi
          cpu: 0.25
        limits:
          memory: 128Mi
    - name: log-aggregator
      image: ""images.my-company.example/log-aggregator:v6""
      resources:
        requests:
          memory: 64Mi
          cpu: 0.25
        limits:
          memory: 128Mi
          cpu: 0.75
```

![Execution of validate](images/guard-validate.png)

The container `app` does not contain CPU limits specified, which fails the overall evaluation as shown in the screenshot.

#### Using Input Parameters

Guard allows you to use input parameters for dynamic data lookups during validation. This feature is particularly useful when you need to reference external data in your rules. However, when specifying input parameter keys, Guard requires that there are no conflicting paths.

##### How to use

1. Use the `--input-parameters` or `-i` flag to specify files containing input parameters. Multiple input parameter files can be specified and will be combined to form a common context. Input parameter keys can not have conflicting paths.
2. Use the `--data` or `-d` flag to specify the actual template file to be validated.

##### Example Usage

1. Create an input parameter file (e.g., `network.yaml`):

```yaml
NETWORK:
  allowed_security_groups: [""sg-282850"", ""sg-292040""]
  allowed_prefix_lists: [""pl-63a5400a"", ""pl-02cd2c6b""]
```

2. Reference these parameters in your guard rule file (e.g., `security_groups.guard`):

```
let groups = Resources.*[ Type == 'AWS::EC2::SecurityGroup' ]

let permitted_sgs = NETWORK.allowed_security_groups
let permitted_pls = NETWORK.allowed_prefix_lists
rule check_permitted_security_groups_or_prefix_lists(groups) {
    %groups {
        this in %permitted_sgs or
        this in %permitted_pls
    }
}

rule CHECK_PERMITTED_GROUPS when %groups !empty {
    check_permitted_security_groups_or_prefix_lists(
       %groups.Properties.GroupName
    )
}
```

3. Create a failing data template (e.g., `security_groups_fail.yaml`):

```yaml
# ---
# AWSTemplateFormatVersion: 2010-09-09
# Description: CloudFormation - EC2 Security Group

Resources:
  mySecurityGroup:
    Type: ""AWS::EC2::SecurityGroup""
    Properties:
      GroupName: ""wrong""
```

4. Run the validate command:

```
cfn-guard validate -r security_groups.guard -i network.yaml -d security_groups_fail.yaml
```

In this command:

- `-r` specifies the rule file
- `-i` specifies the input parameter file
- `-d` specifies the data file (template) to be validated

##### Multiple Input Parameters

You can specify multiple input parameter files:

```
cfn-guard validate -r rules.guard -i params1.yaml -i params2.yaml -d template.yaml
```

All files specified with `-i` will be combined to form a single context for parameter lookup.

#### Test

Test command is used during the development of guard policy rules files. Test provides a simple integrated unit-test frameworks that allows authors to individually test each policy file for different types of inputs. Unit testing helps authors gain confidence that the rule does indeed conform to expectations. It can also be used as regression tests for rules. Here is example run for `test` command

1. Save the sample policy rules file below as `api_gateway_private_access.guard`:

```
#
# Select from Resources section of the template all ApiGateway resources
# present in the template.
#
let api_gws = Resources.*[ Type == 'AWS::ApiGateway::RestApi' ]

#
# Rule intent
# a) All ApiGateway instances deployed must be private
# b) All ApiGateway instances must have atleast one IAM policy condition key to allow access from a VPC
#
# Expectations:
# 1) SKIP when there are not API Gateway instances in the template
# 2) PASS when ALL ApiGateway instances MUST be ""PRIVATE"" and
#              ALL ApiGateway instances MUST have one IAM Condition key with aws:sourceVpc or aws:SourceVpc
# 3) FAIL otherwise
#
#

rule check_rest_api_is_private when %api_gws !empty {
  %api_gws {
    Properties.EndpointConfiguration.Types[*] == ""PRIVATE""
  }
}

rule check_rest_api_has_vpc_access when check_rest_api_is_private {
  %api_gws {
    Properties {
      #
      # ALL ApiGateways must have atleast one IAM statement that has Condition keys with
      #     aws:sourceVpc
      #
      some Policy.Statement[*] {
        Condition.*[ keys == /aws:[sS]ource(Vpc|VPC|Vpce|VPCE)/ ] !empty
      }
    }
  }
}
```

2. Save the sample test file below as `api_gateway_private_access_tests.yaml`:

```yaml
---
- input: {}
  expectations:
    rules:
      check_rest_api_is_private: SKIP
      check_rest_api_has_vpc_access: SKIP
- input:
    Resources: {}
  expectations:
    rules:
      check_rest_api_is_private: SKIP
      check_rest_api_has_vpc_access: SKIP
- input:
    Resources:
      apiGw:
        Type: AWS::ApiGateway::RestApi
  expectations:
    rules:
      check_rest_api_is_private: FAIL
      check_rest_api_has_vpc_access: SKIP
- input:
    Resources:
      apiGw:
        Type: AWS::ApiGateway::RestApi
        Properties:
          EndpointConfiguration:
            Types: PRIVATE
  expectations:
    rules:
      check_rest_api_is_private: PASS
      check_rest_api_has_vpc_access: FAIL
- input:
    Resources:
      apiGw:
        Type: AWS::ApiGateway::RestApi
        Properties:
          EndpointConfiguration:
            Types: [PRIVATE, REGIONAL]
  expectations:
    rules:
      check_rest_api_is_private: FAIL
      check_rest_api_has_vpc_access: SKIP
- input:
    Resources:
      apiGw:
        Type: AWS::ApiGateway::RestApi
        Properties:
          EndpointConfiguration:
            Types: PRIVATE
          Policy:
            Statement:
              - Action: Allow
                Resource: ""*""
                Condition:
                  StringLike:
                    ""aws:sourceVPC"": vpc-12345678
  expectations:
    rules:
      check_rest_api_is_private: PASS
      check_rest_api_has_vpc_access: PASS
```

3. Run the test command

```bash
cfn-guard test -r api_gateway_private_access.guard -t api_gateway_private_access_tests.yaml
```

![Execution of test](images/guard-test.png)

Read [Guard: Unit Testing](docs/UNIT_TESTING.md) for more information on unit testing. To know about other commands read the [Readme in the guard directory](guard/README.md).

## <a name=""references""></a> Rule authoring references

As a starting point for writing Guard rules for yourself or your organisation we recommend following [this official guide](https://docs.aws.amazon.com/cfn-guard/latest/ug/writing-rules.html)

### Quick links:

[Writing AWS CloudFormation Guard rules](https://docs.aws.amazon.com/cfn-guard/latest/ug/writing-rules.html)

1. [Clauses](https://docs.aws.amazon.com/cfn-guard/latest/ug/writing-rules.html#clauses)
2. [Using queries in clauses](https://docs.aws.amazon.com/cfn-guard/latest/ug/writing-rules.html#clauses-queries)
3. [Using operators in clauses](https://docs.aws.amazon.com/cfn-guard/latest/ug/writing-rules.html#clauses-operators)
4. [Using custom messages in clauses](https://docs.aws.amazon.com/cfn-guard/latest/ug/writing-rules.html#clauses-custom-messages)
5. [Combining clauses](https://docs.aws.amazon.com/cfn-guard/latest/ug/writing-rules.html#combining-clauses)
6. [Using blocks with Guard rules](https://docs.aws.amazon.com/cfn-guard/latest/ug/writing-rules.html#blocks)
7. [Defining queries and filtering](https://docs.aws.amazon.com/cfn-guard/latest/ug/query-and-filtering.html)
8. [Assigning and referencing variables in AWS CloudFormation Guard rules](https://docs.aws.amazon.com/cfn-guard/latest/ug/variables.html)
9. [Composing named-rule blocks in AWS CloudFormation Guard](https://docs.aws.amazon.com/cfn-guard/latest/ug/named-rule-block-composition.html)
10. [Writing clauses to perform context-aware evaluations](https://docs.aws.amazon.com/cfn-guard/latest/ug/context-aware-evaluations.html)

## <a name=""functions""></a> Built-in functions & stateful rules

Guard 3.0 introduces support for functions, allowing for stateful rules that can run on a value that's evaluated based
on some properties extracted out of a data template.

### Sample template

Imagine we have a property in our template which consists of a list called as `Collection` and we need to ensure
it has at least 3 items in it.

```yaml
Resources:
  newServer:
    Type: AWS::New::Service
    Collection:
      - a
      - b
```

### Sample rule

We can write a rule to check this condition as follows:

```
let server = Resources.*[ Type == 'AWS::New::Service' ]
rule COUNT_CHECK when %server !empty {
    let collectio",VRAI
aws-samples/awsome-distributed-training,MLOps,Documentations,2025-05-15T19:19:25Z,2025-04-03T14:13:56Z,0,0,0,0,2,0,0,0,2023-09-30T16:22:35Z,2025-04-06T22:40:12Z,147793,277,Shell,VRAI,111,FAUX,47,"aws,awsbatch,distributed-training,efa,eks,generative-ai,gpu,hyperpod,llm-training,parallelcluster",47,"Collection of best practices, reference architectures, model training examples and utilities to train large models on AWS. ",FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,57,"# ML Training Reference Architectures & Tests <!-- omit from toc -->

> **Warning**
> We are currently undergoing a major refactoring of this repository, particularly focused on the test cases section. If you prefer to use the previous directory structure and deprecated test cases, please refer to [v1.1.0](https://github.com/aws-samples/awsome-distributed-training/releases/tag/v1.1.0).


This repository contains reference architectures and test cases for distributed model training with [Amazon SageMaker Hyperpod](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html), [AWS ParallelCluster](https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html), [AWS Batch](https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html), and [Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html). The test cases cover different types and sizes of models as well as different frameworks and parallel optimizations (Pytorch DDP/FSDP, MegatronLM, NemoMegatron...).

The major components of this directory are:

```bash
reference-architectures/
|-- 1.architectures/               # CloudFormation templates for reference arch
|-- 2.ami_and_containers/          # Scripts to create AMIs and container images
|-- 3.test_cases/                  # Reference test cases and/or benchmark scripts
|-- 4.validation_observability/    # Tools to measure performance or troubleshoot
`-- ...
```

**NOTE**: the architectures are designed to work with the S3 bucket and VPC created using reference templates `1.architectures/0.s3/` and `1.architectures/1.vpc_network/`. _You're strongly recommended to deploy these two templates **before** deploying any of the reference architectures._

## 0. Workshops

You can follow the workshop below to train models on AWS. Each contains examples for several test cases as well as nuggets of information on operating a cluster for LLM training.

| Name                                                                           | Comments
| ------------------------------------------------------------------------------ | ------------------------------------------------------------------- |
| [Amazon SageMaker HyperPod](https://catalog.workshops.aws/sagemaker-hyperpod/en-US)   | Workshop for SageMaker HyperPod, shows how to deploy and monitor it |
| [AWS ParallelCluster](https://catalog.workshops.aws/ml-on-aws-parallelcluster) | Similar workshop as HyperPod but on ParallelCluster                 |
| [Amazon SageMaker HyperPod EKS](https://catalog.workshops.aws/sagemaker-hyperpod-eks)   | Workshop for SageMaker HyperPod EKS, shows how to deploy and monitor it |

## 1. Architectures

Architectures are located in `1.architectures` and consists of utilities and service related architectures.

| Name                                                               | Category | Usage                                               |
| ------------------------------------------------------------------ | -------- | --------------------------------------------------- |
| [`0.s3`](./1.architectures/0.s3)                                   | Storage  | Create an S3 bucket                                 |
| [`1.vpc_network`](./1.architectures/1.vpc_network)                 | Network  | Create a VPC with subnets required resources        |
| [`2.aws-parallelcluster`](./1.architectures/2.aws-parallelcluster) | Compute  | Cluster templates for GPU & custom silicon training |
| [`3.aws-batch`](./1.architectures/3.aws-batch)                     | Compute  | AWS Batch template for distributed training         |
| [`4.amazon-eks`](./1.architectures/4.amazon-eks)                   | Compute  | Manifest files to train with Amazon EKS             |
| [`5.sagemaker-hyperpod`](./1.architectures/5.sagemaker-hyperpod)   | Compute  | SageMaker HyperPod template for distributed training|

More will come, feel free to add new ones (ex. Ray?). You will also find [documentation](./1.architectures/efa-cheatsheet.md) for EFA and the recommended environment variables.

## 2. Custom Amazon Machine Images

Custom machine images can be built using [Packer](www.packer.io) for AWS ParallelCluster, Amazon EKS and plain EC2. These images are based are on Ansible roles and playbooks.

## 3. Test cases

Test cases are organized by framework and cover various distributed training scenarios. Each test case includes the necessary scripts and configurations to run distributed training jobs.

### PyTorch Test Cases
- [`FSDP/`](./3.test_cases/pytorch/FSDP) - Fully Sharded Data Parallel training examples
- [`megatron-lm/`](./3.test_cases/pytorch/megatron-lm) - Megatron-LM distributed training examples
- [`nemo-launcher/`](./3.test_cases/pytorch/nemo-launcher) - NeMo Launcher examples for distributed training. This test case is for NeMo version 1.0 only.
- [`nemo-run/`](./3.test_cases/pytorch/nemo-run) - NeMo framework distributed training examples. This test case is for NeMo version 2.0+.
- [`neuronx-distributed/`](./3.test_cases/pytorch/neuronx-distributed) - AWS Trainium distributed training examples
- [`mosaicml-composer/`](./3.test_cases/pytorch/mosaicml-composer) - MosaicML Composer examples
- [`picotron/`](./3.test_cases/pytorch/picotron) - PicoTron distributed training examples
- [`torchtitan/`](./3.test_cases/pytorch/torchtitan) - TorchTitan examples
- [`cpu-ddp/`](./3.test_cases/pytorch/cpu-ddp) - CPU-based Distributed Data Parallel examples
- [`bionemo/`](./3.test_cases/pytorch/bionemo) - BioNeMo distributed training examples

### JAX Test Cases
- [`jax/`](./3.test_cases/jax) - JAX-based distributed training examples using PaxML

Each test case includes:
- Training scripts and configurations
- Container definitions (where applicable)
- Launch scripts for different cluster types
- Performance monitoring and validation tools

## 4. Validation scripts

Utilities scripts and micro-benchmarks examples are set under `4.validation_scripts/`. The EFA Prometheus exporter can be found in this [directory](./4.validation_and_observability/3.efa-node-exporter) 


| Name                                                                                    | Comments                                                        |
| --------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| [`1.pytorch-env-validation`](./4.validation_and_observability/1.pytorch-env-validation) | Validates your PyTorch environment                              |
| [`3.efa-node-exporter`](./4.validation_and_observability/3.efa-node-exporter)           | Node exporter with Amazon EFA monitoring modules                |
| [`4.prometheus-grafana`](./4.validation_and_observability/4.prometheus-grafana)         | Deployment assets to monitor SageMaker Hyperpod Clusters        |
| [`5.nsight`](./4.validation_and_observability/5.nsight)                                 | Shows how to run Nvidia Nsight Systems to profile your workload |
| [`efa-versions.py`](./1.architectures/efa-versions.py)                                  | Get the versions of Nvidia libraries, drivers and EFA drivers   |


## 5. CI

Integration tests are written in [pytest](https://docs.pytest.org). Just run:

```bash
pytest .
```

Alternatively you can run tests with out capturing stdout and keeping all docker images an other artifacts.

```bash
pytest -s --keep-artifacts=t
```

## 6. Contributors

Thanks to all the contributors for building, reviewing and testing.

[![Contributors](https://contrib.rocks/image?repo=aws-samples/awsome-distributed-training)](https://github.com/aws-samples/awsome-distributed-training/graphs/contributors)

## 7.Star History

[![Star History Chart](https://api.star-history.com/svg?repos=aws-samples/awsome-distributed-training&type=Date)](https://star-history.com/#aws-samples/awsome-distributed-training&Date)",VRAI
aws-samples/cfn101-workshop,Documentations,Documentations,2025-05-07T15:06:08Z,2023-09-05T02:29:56Z,0,0,0,0,0,0,2,0,2019-10-15T22:52:40Z,2025-01-31T04:22:26Z,37781,157,Shell,VRAI,152,FAUX,2,"aws,cloudformation,workshop",2,AWS CloudFormation Workshop,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,39,"<h1 align=""center"">
AWS CloudFormation - Workshop
<br>
    <a href=""https://cfn101.workshop.aws""><img alt=""Website"" src=""https://img.shields.io/website?down_color=red&down_message=down&up_color=green&up_message=up&url=https%3A%2F%2Fcfn101.workshop.aws""></a>
    <a href=""https://github.com/aws-samples/cfn101-workshop/actions""><img alt=""GitHub Workflow Status"" src=""https://github.com/aws-samples/cfn101-workshop/workflows/Unit%20Tests/badge.svg""></a>
</h1>

This repository provides all the resources referenced in the [CloudFormation](https://cfn101.workshop.aws/) workshop as
well as the code used to build it.

## Usage
1. Clone the repository to your working directory or Download the ZIP file from GitHub.
2. Open the downloaded files in your code editor or IDE of your choice.

The working directory is located in [code/workspace](code/workspace) where you can follow along and write your code to.

In the [code/solutions](code/solutions), you can find the completed solution for each lab. This can be used as a
reference, in case you get stuck or things don't work as intended.

## Local development
To set-up a local development environment for changing the workshop, please follow the instructions in
[local development](docs/LOCAL_DEVELOPMENT.md) file.

## Contributing
Contributions are more than welcome. Please read the [code of conduct](CODE_OF_CONDUCT.md) and the
[contributing guidelines](CONTRIBUTING.md).

## License
This library is licensed under the MIT-0 License. See the [license](LICENSE) file.",FAUX
aws-samples/container-patterns,Documentations,Documentations,2025-03-11T13:35:23Z,2023-11-19T20:43:24Z,0,0,6,0,0,0,7,0,2023-06-01T20:06:05Z,2025-03-11T13:35:29Z,15021,39,TypeScript,VRAI,13,FAUX,14,,14,,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,12,"## Containers on AWS

[![](./docs/images/preview.png)](https://containersonaws.com)

This repository hosts the source content for patterns on https://containersonaws.com/

If you would like to add your own pattern to the website please read the guide below.

## Documentation

- [Understand the repository structure](./docs/structure.md)
- [Add yourself as an author](./docs/add-yourself-as-an-author.md)
- [Add your pattern](./docs/add-a-pattern.md)
- [Add a great SVG diagram for your pattern](./docs/svg-diagram-process.md)
- [General contribution guidelines](./CONTRIBUTING.md)

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This project is licensed under the Apache-2.0 License.",FAUX
aws-samples/generative-ai-newsletter-app,MLOps,MLOps,2024-07-25T15:57:14Z,2024-06-04T13:58:44Z,0,0,0,7,0,0,0,0,2024-04-04T11:41:24Z,2025-03-24T06:57:46Z,5918,45,TypeScript,VRAI,6,FAUX,15,"amazon-verified-permissions,appsync,appsync-graphql-api,appsync-resolvers,aws,aws-cdk,bedrock,cdk,cloudscape-design,genai,genai-usecase,generative-ai,generative-ai-samples,lambda,pinpoint,react,serverless,typescript",15,The Generative AI Newsletter Application sample is a ready-to-use serverless solution designed to allow users to create rich newsletters automatically with content summaries that are AI-generated.,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,3,"# Generative AI Newsletter Application
[![Release Notes](https://img.shields.io/github/v/release/aws-samples/generative-ai-newsletter-app)](https://github.com/aws-samples/generative-ai-newsletter-app/releases)
[![GitHub star chart](https://img.shields.io/github/stars/aws-samples/generative-ai-newsletter-app?style=social)](https://star-history.com/#aws-samples/generative-ai-newsletter-app)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[![Full Documentation](https://img.shields.io/badge/Full%20Documentation-blue?style=for-the-badge&logo=Vite&logoColor=white)](https://aws-samples.github.io/generative-ai-newsletter-app/)


The Generative AI Newsletter Application sample is a ready-to-use serverless solution designed to allow users to create rich newsletters automatically with content summaries that are AI-generated. 

The application offers users the ability to influence the generative AI prompts to customize how content is summarized such as the tone, intended audience, and more. Users can stylize the HTML newsletter, define how frequently newsletters are created and share the newsletters with others. 

![Generative AI Newsletter Application Welcome Page](./pages/images/Generative-AI-Newsletter-App-Landing-Image.png)


## Author

* [Addie Rudy](https://www.linkedin.com/in/addierudy/)

# Contributors
[![contributors](https://contrib.rocks/image?repo=aws-samples/generative-ai-newsletter-app&max=2000)](https://github.com/aws-samples/generative-ai-newsletter-app/graphs/contributors)

## Threat Model

The threat model for this solution can be found [here](https://aws-samples.github.io/generative-ai-newsletter-app/threat-model).

## License

This library is licensed under the MIT-0 License. See the [LICENSE](LICENSE) file. 

Third-party dependencies have their own licenses. See the [NOTICE](NOTICE) file.

## Additional Resources

- [Changelog](CHANGELOG.md)
- [License](LICENSE)
- [Code of Content](CODE_OF_CONDUCT.md)
- [Contributing](CONTRIBUTING.md)",FAUX
aws-samples/observability-with-amazon-opensearch,Application System,Documentations,2024-12-23T16:22:36Z,2024-01-25T14:51:42Z,0,0,0,0,2,0,0,0,2022-04-26T10:45:23Z,2024-12-23T16:22:42Z,85114,83,Python,VRAI,38,FAUX,1,"amazon-web-services,aws,aws-cloudformation,aws-eks,cloudformation,eks,fluentbit,kubernetes,microservices,observability,observability-demo,opensearch,opensearch-dashboards,opentelemetry,opentelemetry-collector",1,This repository contains a microservice-based Sample App demonstrating observability capabilities in the Amazon OpenSearch Service.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,15,"# Microservice Observability with Amazon OpenSearch Service Workshop

Amazon OpenSearch Service’s Trace Analytics functionality allows you to go beyond simple monitoring to understand not just what events are happening, but why they are happening. In this workshop, learn how to instrument, collect, and analyze metrics, traces, and log data all the way from user front ends to service backends and everything in between. Put this together with Amazon OpenSearch Service, AWS Distro for OpenTelemetry, FluentBit, and Data Prepper.

## Architecture
![architecture](/assets/arch.jpg)

## Instructions 🚀
Detailed Workshop instructions should be followed in this [guide](https://catalog.us-east-1.prod.workshops.aws/workshops/1abb648b-2ef8-442c-a731-efbcb69c1e1e).

## (For Information Only) Manual Instrumentation to collect traces

As our sample microservice application is built using Python and Java, we have used OpenTelemetry Python packages to manually instrument our code.

In manual instrumentation, developers need to add trace capture code to the application. It provides customization in terms of capturing traces for a custom code block, name various components in OpenTelemetry like traces and spans, add attributes, events and handle specific exception within the code.

Following are dependencies installed using pip. This could be found in the requirement.txt within each microservice under sample-apps.
```
opentelemetry-exporter-otlp==1.20.0
opentelemetry-instrumentation-flask==0.41b0
opentelemetry-instrumentation-mysql==0.41b0
opentelemetry-instrumentation-requests==0.41b0
opentelemetry-instrumentation-logging==0.41b0
opentelemetry-sdk==1.20.0
```
Lets take a sample microservice ```sample-apps/08-paymentservice/paymentService.py``` and try to understand the instrumentation specific code.

Import dependent packages in the code.
```
from opentelemetry import trace
from opentelemetry.instrumentation.logging import LoggingInstrumentor
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import (
    ConsoleSpanExporter,
    SimpleSpanProcessor,
)
```
Configure ```TraceProvider``` and set it for the application. This includes exporting the logs to AWS Distro for OpenTelemetry Collector via OTLP protocol over port 55680.
```
trace.set_tracer_provider(
    TracerProvider(
        resource=Resource.create(
            {
                ""service.name"": ""payment"",
                ""service.instance.id"": str(id(app)),
                ""telemetry.sdk.name"": ""opentelemetry"",
                ""telemetry.sdk.language"": ""python"",
                ""telemetry.sdk.version"": pkg_resources.get_distribution(""opentelemetry-sdk"").version,
                ""host.hostname"": socket.gethostname(),
            }
        )
    )
)
tracerProvider = trace.get_tracer_provider()
tracer = tracerProvider.get_tracer(__name__)
tracerProvider.add_span_processor(
    SimpleSpanProcessor(ConsoleSpanExporter())
)
otlp_exporter = OTLPSpanExporter(endpoint=""{}:55680"".format(OTLP), insecure=True)
tracerProvider.add_span_processor(
    SimpleSpanProcessor(otlp_exporter)
)
```

Use ```LoggingInstrumentor``` to instrument application to inject trace id, span id and service name within logs for correlation purpose.
```
LoggingInstrumentor().instrument(set_logging_format=True)
```
```FlaskInstrumentor``` track web request in Flask application. It supports Flask specific feature such as - 
•	The Flask url rule pattern is used as the Span name.
•	The ```http.route``` Span attribute is set so that one can see which URL rule matched a request.
```
FlaskInstrumentor().instrument_app(app)
```
Trace HTTP requests made by the Python requests library
```
RequestsInstrumentor().instrument(tracer_provider=tracerProvider)
```
To capture the work done within a block, start a span with a name and put the code block within the span as shown using
```
 ...
 with tracer.start_as_current_span(""checkout""):
    ...
```
The full python function looks like this - 
```
@app.route(""/checkout"", methods=[""POST"", ""GET""])
def payment():
    errorRate = random.randint(0,99)
    if errorRate < ERROR_RATE_THRESHOLD:
        logs('Payment', 'Checkout operation failed - Service Unavailable: 503')
        logger.error('Payment - Checkout operation failed - Service Unavailable: 503')
        raise Error('Checkout Failed - Service Unavailable', status_code=503)
    else:
        with tracer.start_as_current_span(""checkout""):
            rawData = request.form
            data = {}
            for itemId in rawData.keys():
                data[itemId] = sum([-val for val in rawData.getlist(itemId, type=int)])

            soldInventorySession = requests.Session()
            soldInventorySession.mount(""http://"", HTTPAdapter(max_retries=retry_strategy))
            soldInventoryUpdateResponse = soldInventorySession.post(
                ""http://{}:80/update_inventory"".format(INVENTORY),
                data=data,
            )
            soldInventorySession.close()
            if soldInventoryUpdateResponse.status_code == 200:
                logs('Payment', 'Customer successfully checked out cart')
                logger.info('Payment - Customer successfully checked out cart')
                return ""success""
            else:
                failedItems = soldInventoryUpdateResponse.json().get(""failed_items"")
                return make_response(
                    ""Failed to checkout following items: {}"".format(','.join(failedItems)),
                    soldInventoryUpdateResponse.status_code)
```
Similarly other application services are instrumented to capture trace data from application.",VRAI
awslabs/aws-config-rdk,Toolkit,Toolkit,2025-05-06T19:00:09Z,2024-08-28T17:00:51Z,0,0,0,0,1,0,1,0,2017-08-03T18:06:51Z,2025-04-04T02:07:01Z,4020,466,Python,VRAI,172,FAUX,47,"amazon-web-services,aws,aws-config,aws-config-rules,rdk",47,"The AWS Config Rules Development Kit helps developers set up, author and test custom Config rules. It contains scripts to enable AWS Config, create a Config rule and test it with sample ConfigurationItems. ",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,69,"# AWS RDK

[![pypibadge](https://static.pepy.tech/personalized-badge/rdk?period=total&units=international_system&left_color=black&right_color=blue&left_text=downloads)](https://pepy.tech/project/rdk)
![PyPI](https://img.shields.io/pypi/v/rdk)

AWS Config Rules Development Kit

We greatly appreciate feedback and bug reports at
<rdk-maintainers@amazon.com>! You may also create an issue on this repo.

The RDK is designed to support a ""Compliance-as-Code"" workflow that is
intuitive and productive. It abstracts away much of the undifferentiated
heavy lifting associated with deploying AWS Config rules backed by
custom lambda functions, and provides a streamlined
develop-deploy-monitor iterative process.

For complete documentation, including command reference, check out the
[ReadTheDocs documentation](https://aws-config-rdk.readthedocs.io/).

## Getting Started

Uses Python 3.7+ and is installed via pip. Requires you to have
an AWS account and sufficient permissions to manage the Config service,
and to create S3 Buckets, Roles, and Lambda Functions. An AWS IAM Policy
Document that describes the minimum necessary permissions can be found
at `policy/rdk-minimum-permissions.json`.

Under the hood, rdk uses boto3 to make API calls to AWS, so you can set
your credentials any way that boto3 recognizes (options 3 through 8
[here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#guide-credentials))
or pass them in with the command-line parameters `--profile`,
`--region`, `--access-key-id`, or `--secret-access-key`

If you just want to use the RDK, go ahead and install it using pip.

```bash
pip install rdk
```

Alternately, if you want to see the code and/or contribute you can clone
the git repo, and then from the repo directory use pip to install the
package. Use the `-e` flag to generate symlinks so that any edits you
make will be reflected when you run the installed package.

If you are going to author your Lambda functions using Java you will
need to have Java 8 and gradle installed. If you are going to author
your Lambda functions in C# you will need to have the dotnet CLI and the
.NET Core Runtime 1.08 installed.

```bash
pip install -e .
```

To make sure the rdk is installed correctly, running the package from
the command line without any arguments should display help information.

```bash
rdk
usage: rdk [-h] [-p PROFILE] [-k ACCESS_KEY_ID] [-s SECRET_ACCESS_KEY]
           [-r REGION] [-f REGION_FILE] [--region-set REGION_SET]
           [-v] <command> ...
rdk: error: the following arguments are required: <command>, <command arguments>
```

## Usage

### Configure your env

To use the RDK, it's recommended to create a directory that will be
your working directory. This should be committed to a source code repo,
and ideally created as a python virtualenv. In that directory, run the
`init` command to set up your AWS Config environment.

```bash
rdk init
Running init!
Creating Config bucket config-bucket-780784666283
Creating IAM role config-role
Waiting for IAM role to propagate
Config Service is ON
Config setup complete.
Creating Code bucket config-rule-code-bucket-780784666283ap-southeast-1
```

Running `init` subsequent times will validate your AWS Config setup and
re-create any S3 buckets or IAM resources that are needed.

- If you have config delivery bucket already present in some other AWS account then use `--config-bucket-exists-in-another-account` as argument.

```bash
rdk init --config-bucket-exists-in-another-account
```

- If you have AWS Organizations/ControlTower Setup in your AWS environment then additionally, use `--control-tower` as argument.

```bash
rdk init --control-tower --config-bucket-exists-in-another-account
```

- If bucket for custom lambda code is already present in current account then use `--skip-code-bucket-creation` argument.

```bash
rdk init --skip-code-bucket-creation
```

- If you want rdk to create/update and upload the rdklib-layer for you, then use `--generate-lambda-layer` argument. In supported regions, rdk will deploy the layer using the Serverless Application Repository, otherwise it will build a local lambda layer archive and upload it for use.

```bash
rdk init --generate-lambda-layer
```

- If you want rdk to give a custom name to the lambda layer for you, then use `--custom-layer-namer` argument. The Serverless Application Repository currently cannot be used for custom lambda layers.

```bash
rdk init --generate-lambda-layer --custom-layer-name <LAYER_NAME>
```

## Create Rules

In your working directory, use the `create` command to start creating a
new custom rule. You must specify the runtime for the lambda function
that will back the Rule, and you can also specify a resource type (or
comma-separated list of types) that the Rule will evaluate or a maximum
frequency for a periodic rule. This will add a new directory for the
rule and populate it with several files, including a skeleton of your
Lambda code.

```bash
rdk create MyRule --runtime python3.12 --resource-types AWS::EC2::Instance --input-parameters '{""desiredInstanceType"":""t2.micro""}'
Running create!
Local Rule files created.
```

On Windows it is necessary to escape the double-quotes when specifying
input parameters, so the `--input-parameters` argument would instead
look something like this:

`'{\""desiredInstanceType\"":\""t2.micro\""}'`

As of RDK v0.17.0, you can also specify `--resource-types ALL` to include all resource types.

Note that you can create rules that use EITHER resource-types OR
maximum-frequency, but not both. We have found that rules that try to be
both event-triggered as well as periodic wind up being very complicated
and so we do not recommend it as a best practice.

### Edit Rules Locally

Once you have created the rule, edit the python file in your rule
directory (in the above example it would be `MyRule/MyRule.py`, but may
be deeper into the rule directory tree depending on your chosen Lambda
runtime) to add whatever logic your Rule requires in the
`evaluate_compliance` function. You will have access to the CI that was
sent by Config, as well as any parameters configured for the Config
Rule. Your function should return either a simple compliance status (one
of `COMPLIANT`, `NON_COMPLIANT`, or `NOT_APPLICABLE`), or if you're
using the python or node runtimes you can return a JSON object with
multiple evaluation responses that the RDK will send back to AWS Config.

An example would look like:

```python
for sg in response['SecurityGroups']:
    evaluations.append(
    {
        'ComplianceResourceType': 'AWS::EC2::SecurityGroup',
        'ComplianceResourceId': sg['GroupId'],
        'ComplianceType': 'COMPLIANT',
        'Annotation': 'This is an important note.',
        'OrderingTimestamp': str(datetime.datetime.now())
    })
return evaluations
```

This is necessary for periodic rules that are not triggered by any CI
change (which means the CI that is passed in will be null), and also for
attaching annotations to your evaluation results.

If you want to see what the JSON structure of a CI looks like for
creating your logic, you can use

```bash
rdk sample-ci <Resource Type>
```

to output a formatted JSON document.

### Write and Run Unit Tests

If you are writing Config Rules using either of the Python runtimes
there will be a `<rule name>_test.py` file deployed along with your
Lambda function skeleton. This can be used to write unit tests according
to the standard Python unittest framework (documented
[here](https://docs.python.org/3/library/unittest.html)), which can be
run using the `test-local` rdk command:

```bash
rdk test-local MyTestRule
Running local test!
Testing MyTestRule
Looking for tests in /Users/mborch/Code/rdk-dev/MyTestRule

---------------------------------------------------------------------

Ran 0 tests in 0.000s

OK
<unittest.runner.TextTestResult run=0 errors=0 failures=0>
```

The test file includes setup for the MagicMock library that can be used
to stub boto3 API calls if your rule logic will involve making API calls
to gather additional information about your AWS environment. For some
tips on how to do this, check out this blog post:
[Mock Is Magic](https://sgillies.net/2017/10/19/mock-is-magic.html)

### Modify Rule

If you need to change the parameters of a Config rule in your working
directory you can use the `modify` command. Any parameters you specify
will overwrite existing values, any that you do not specify will not be
changed.

```bash
rdk modify MyRule --runtime python3.12 --maximum-frequency TwentyFour_Hours --input-parameters '{""desiredInstanceType"":""t2.micro""}'
Running modify!
Modified Rule 'MyRule'.  Use the `deploy` command to push your changes to AWS.
```

Again, on Windows the input parameters would look like:

`'{\""desiredInstanceType\"":\""t2.micro\""}'`

It is worth noting that until you actually call the `deploy` command
your rule only exists in your working directory, none of the Rule
commands discussed thus far actually makes changes to your account.

### Deploy Rule

Once you have completed your compliance validation code and set your
Rule's configuration, you can deploy the Rule to your account using the
`deploy` command. This will zip up your code (and the other associated
code files, if any) into a deployable package (or run a gradle build if
you have selected the java8 runtime or run the Lambda packaging step
from the dotnet CLI if you have selected the dotnetcore1.0 runtime),
copy that zip file to S3, and then launch or update a CloudFormation
stack that defines your Config Rule, Lambda function, and the necessary
permissions and IAM Roles for it to function. Since CloudFormation does
not deeply inspect Lambda code objects in S3 to construct its changeset,
the `deploy` command will also directly update the Lambda function for
any subsequent deployments to make sure code changes are propagated
correctly.

```bash
rdk deploy MyRule
Running deploy!
Zipping MyRule
Uploading MyRule
Creating CloudFormation Stack for MyRule
Waiting for CloudFormation stack operation to complete...
...
Waiting for CloudFormation stack operation to complete...
Config deploy complete.
```

The exact output will vary depending on Lambda runtime. You can use the
`--all` flag to deploy all of the rules in your working directory. If
you used the `--generate-lambda-layer` flag in rdk init, use the
`--generated-lambda-layer` flag for rdk deploy.

### Deploy Organization Rule

You can also deploy the Rule to your AWS Organization using the
`deploy-organization` command. For successful evaluation of custom rules
in child accounts, please make sure you do one of the following:

1. Set ASSUME_ROLE_MODE in Lambda code to True, to get the Lambda to assume the Role attached on the Config Service and confirm that the role trusts the master account where the Lambda function is going to be deployed.
2. Set ASSUME_ROLE_MODE in Lambda code to True, to get the Lambda to assume a custom role and define an optional parameter with key as ExecutionRoleName and set the value to your custom role name; confirm that the role trusts the master account of the organization where the Lambda function will be deployed.

```bash
rdk deploy-organization MyRule
Running deploy!
Zipping MyRule
Uploading MyRule
Creating CloudFormation Stack for MyRule
Waiting for CloudFormation stack operation to complete...
...
Waiting for CloudFormation stack operation to complete...
Config deploy complete.
```

The exact output will vary depending on Lambda runtime. You can use the
`--all` flag to deploy all of the rules in your working directory. This
command uses `PutOrganizationConfigRule` API for the rule deployment. If
a new account joins an organization, the rule is deployed to that
account. When an account leaves an organization, the rule is removed.
Deployment of existing organizational AWS Config Rules will only be
retried for 7 hours after an account is added to your organization if a
recorder is not available. You are expected to create a recorder if one
doesn't exist within 7 hours of adding an account to your organization.

### View Logs For Deployed Rule

Once the Rule has been deployed to AWS you can get the CloudWatch logs
associated with your Lambda function using the `logs` command.

```bash
rdk logs MyRule -n 5
2017-11-15 22:59:33 - START RequestId: 96e7639a-ca15-11e7-95a2-b1521890638d Version: $LATEST
2017-11-15 23:41:13 - REPORT RequestId: 68e0304f-ca1b-11e7-b735-81ebae95acda    Duration: 0.50 ms    Billed Duration: 100 ms     Memory Size: 256 MB     Max Memory Used: 36 MB
2017-11-15 23:41:13 - END RequestId: 68e0304f-ca1b-11e7-b735-81ebae95acda
2017-11-15 23:41:13 - Default RDK utility class does not yet support Scheduled Notifications.
2017-11-15 23:41:13 - START RequestId: 68e0304f-ca1b-11e7-b735-81ebae95acda Version: $LATEST
```

You can use the `-n` and `-f` command line flags just like the UNIX
`tail` command to view a larger number of log events and to continuously
poll for new events. The latter option can be useful in conjunction with
manually initiating Config Evaluations for your deploy Config Rule to
make sure it is behaving as expected.

## Running the tests

The `testing` directory contains scripts and buildspec files that I use
to run basic functionality tests across a variety of CLI environments
(currently Ubuntu Linux running Python 3.7/3.8/3.9/3.10, and Windows Server
running Python 3.10). If there is interest I can release a CloudFormation
template that could be used to build the test environment, let me know
if this is something you want!

## Advanced Features

### Cross-Account Deployments

Features have been added to the RDK to facilitate the cross-account
deployment pattern that enterprise customers have standardized for
custom Config Rules. A cross-account architecture is one in which the
Lambda functions are deployed to a single central ""Compliance"" account
(which may be the same as a central ""Security"" account), and the
Config Rules are deployed to any number of ""Satellite"" accounts that
are used by other teams or departments. This gives the compliance team
confidence that their rule logic cannot be tampered with and makes it
much easier for them to modify rule logic without having to go through a
complex deployment process to potentially hundreds of AWS accounts. The
cross-account pattern uses two advanced RDK features:

- `--functions-only` (`-f`) deployment
- `create-rule-template` command

#### Functions-Only Deployment

By using the `-f` or `--functions-only` flag on the `deploy` command the
RDK will deploy only the necessary Lambda Functions, Lambda Execution
Role, and Lambda Permissions to the account specified by the execution
credentials. It accomplishes this by batching up all of the Lambda
function CloudFormation snippets for the selected Rule(s) into a single
dynamically generated template and deploy that CloudFormation template.
One consequence of this is that subsequent deployments that specify a
different set of rules for the same stack name will update that
CloudFormation stack, and any Rules that were included in the first
deployment but not in the second will be removed. You can use the
`--stack-name` parameter to override the default CloudFormation stack
name if you need to manage different subsets of your Lambda Functions
independently. The intended usage is to deploy the functions for all of
the Config rules in the Security/Compliance account, which can be done
simply by using `rdk deploy -f --all` from your working directory.

#### create-rule-template command

This command generates a CloudFormation template that defines the AWS
Config rules themselves, along with the Config Role, Config data bucket,
Configuration Recorder, and Delivery channel necessary for the Config
rules to work in a satellite account. You must specify the file name for
the generated template using the `--output-file` or
`-o` command line flags. The generated template takes a
single parameter of the AccountID of the central compliance account that
contains the Lambda functions that will back your custom Config Rules.
The generated template can be deployed in the desired satellite accounts
through any of the means that you can deploy any other CloudFormation
template, including the console, the CLI, as a CodePipeline task, or
using StackSets. The `create-rule-template` command takes all of the
standard arguments for selecting Rules to include in the generated
template, including lists of individual Rule names, an `--all` flag, or
using the RuleSets feature described below.

```bash
rdk create-rule-template -o remote-rule-template.json --all
Generating CloudFormation template!
CloudFormation template written to remote-rule-template.json
```

### Disable the supported resource types check

It is now possible to define a resource type that is not yet supported
by rdk. To disable the supported resource check use the optional flag
'--skip-supported-resource-check' during the create command.

```bash
rdk create MyRule --runtime python3.12 --resource-types AWS::New::ResourceType --skip-supported-resource-check
'AWS::New::ResourceType' not found in list of accepted resource types.
Skip-Supported-Resource-Check Flag set (--skip-supported-resource-check), ignoring missing resource type error.
Running create!
Local Rule files created.
```

### Custom Lambda Function Name

As of version 0.7.14, instead of defaulting the lambda function names to
`RDK-Rule-Function-<RULE_NAME>` it is possible to customize the name for
the Lambda function to any 64 characters string as per Lambda's naming
standards using the optional `--custom-lambda-name` flag while
performing `rdk create`. This opens up new features like :

1. Longer config rule name.
2. Custom lambda function naming as per personal or enterprise standards.

```bash
rdk create MyLongerRuleName --runtime python3.12 --resource-types AWS::EC2::Instance --custom-lambda-name custom-prefix-for-MyLongerRuleName
Running create!
Local Rule files created.
```

The above example would create files with config rule name as
`MyLongerRuleName` and lambda function with the name
`custom-prefix-for-MyLongerRuleName` instead of
`RDK-Rule-Function-MyLongerRuleName`

### RuleSets

New as of version 0.3.11, it is possible to add RuleSet tags to rules
that can be used to deploy and test groups of rules together. Rules can
belong to multiple RuleSets, and RuleSet membership is stored only in
the parameters.json metadata. The [deploy](docs/commands/deploy.md),
[create-rule-template](docs/commands/create-rule-template.md), and [test-local](docs/commands/test-local.md)
commands are RuleSet-aware such that a RuleSet can be passed in as the
target instead of `--all` or a specific named Rule.

A comma-delimited list of RuleSets can be added to a Rule when you
create it (using the `--rulesets` flag), as part of a `modify` command,
or using new `ruleset` subcommands to add or remove individual rules
from a RuleSet.

Running `rdk rulesets list` will display a list of the RuleSets
currently defined across all of the Rules in the working directory

```bash
rdk rulesets list
RuleSets:  AnotherRuleSet MyNewSet
```

Naming a specific RuleSet will list all of the Rules that are part of
that RuleSet.

```bash
rdk rulesets list AnotherRuleSet
Rules in AnotherRuleSet :  RSTest
```

Rules can be added to or removed from RuleSets using the `add` and
`remove` subcommands:

```bash
rdk rulesets add MyNewSet RSTest
RSTest added to RuleSet MyNewSet

rdk rulesets remove AnotherRuleSet RSTest
RSTest removed from RuleSet AnotherRuleSet
```

RuleSets are a convenient way to maintain a single repository of Config
Rules that may need to have subsets of them deployed to different
environments. For example your development environment may contain some
of the Rules that you run in Production but not all of them; RuleSets
gives you a way to identify and selectively deploy the appropriate Rules
to each environment.

### Managed Rules

The RDK is able to deploy AWS Managed Rules.

To do so, create a rule using `rdk create` and provide a valid
SourceIdentifier via the `--source-identifier` CLI option. The list of
Managed Rules can be found
[here](https://docs.aws.amazon.com/config/latest/developerguide/managed-rules-by-aws-config.html)
, and note that the Identifier can be obtained by replacing the dashes
with underscores and using all capitals (for example, the
""guardduty-enabled-centralized"" rule has the SourceIdentifier
""GUARDDUTY_ENABLED_CENTRALIZED""). Just like custom Rules you will need
to specify source events and/or a maximum evaluation frequency, and also
pass in any Rule parameters. The resulting Rule directory will contain
only the parameters.json file, but using `rdk deploy` or
`rdk create-rule-template` can be used to deploy the Managed Rule like
any other Custom Rule.

### Deploying Rules Across Multiple Regions

The RDK is able to run init/deploy/undeploy across multiple regions with
a `rdk -f <region file> -t <region set>`

If no region group is specified, rdk will deploy to the `default` region
set.

To create a sample starter region group, run `rdk create-region-set` to
specify the filename, add the `-o <region set output file name>` this
will create a region set with the following tests and regions
`""default"":[""us-east-1"",""us-west-1"",""eu-north-1"",""ap-east-1""],""aws-cn-region-set"":[""cn-north-1"",""cn-northwest-1""]`

### Using RDK to Generate a Lambda Layer in a region (Python3)

By default `rdk init --generate-lambda-layer` will generate an rdklib
lambda layer while running init in whatever region it is run, to force
re-generation of the layer, run `rdk init --generate-lambda-layer` again
over a region

To use this generated lambda layer, add the flag
`--generated-lambda-layer` when running `rdk deploy`. For example:
`rdk -f regions.yaml deploy LP3_TestRule_P39_lib --generated-lambda-layer`

If you created layer with a custom name (by running
`rdk init --custom-lambda-layer`), add a similar `custom-lambda-layer`
flag when running deploy.

### Proactive Rules

As of version `1.0.0`, RDK now supports proactive rule creation. Proactive evaluation mode only applies to CloudFormation template deployment, and does not apply to already-deployed resources. Proactive rules are therefore only evaluated as ""configuration changes"", not periodic rules.

You can create a proactive rule using `rdk create`'s flag `--evaluation-mode` and specifying an argument as outlined by `rdk create`'s help text. This will set the evaluation mode in the `parameters.json`.

For more detail on proactive rules, see [this blog post](https://aws.amazon.com/blogs/mt/how-to-use-aws-config-proactive-rules-and-aws-cloudformation-hooks-to-prevent-creation-of-non-complaint-cloud-resources/). Note that the presence of a proactive rule does NOT automatically block misconfigured resources. You need to configure [CloudFormation Hooks](https://docs.aws.amazon.com/cloudformation-cli/latest/hooks-userguide/what-is-cloudformation-hooks.html) in order to use the Config rule to assess (and potentially block) the CFT deployment.

Note that proactive rules are **NOT** supported for Organization Rules, as of May 2024. This is a limitation of the Config service. Proactive evaluation mode is supported for single-account custom and managed rules.

## Support & Feedback

This project is maintained by AWS Solution Architects and Consultants.
It is not part of an AWS service and support is provided best-effort by
the maintainers. To post feedback, submit feature ideas, or report bugs,
please use the [Issues section](https://github.com/awslabs/aws-config-rdk/issues) of this repo.

## Contributing

Email us at <rdk-maintainers@amazon.com> if you have any questions. We
are happy to help and discuss.

## Contacts

- **Benjamin Morris** - [bmorrissirromb](https://github.com/bmorrissirromb) - _current lead maintainer_
- **Nima Fotouhi** - [nimaft](https://github.com/nimaft) - _current maintainer_

## Past Contributors

- **Michael Borchert** - _Original Python version_
- **Jonathan Rault** - _Original Design, testing, feedback_
- **Greg Kim and Chris Gutierrez** - _Initial work and CI definitions_
- **Henry Huang** - _Original CFN templates and other code_
- **Santosh Kumar** - _maintainer_
- **Jose Obando** - _maintainer_
- **Jarrett Andrulis** - [jarrettandrulis](https://github.com/jarrettandrulis) - _maintainer_
- **Sandeep Batchu** - [batchus](https://github.com/batchus) - _maintainer_
- **Mark Beacom** - [mbeacom](https://github.com/mbeacom) - _maintainer_
- **Ricky Chau** - [rickychau2780](https://github.com/rickychau2780) - _maintainer_
- **Julio Delgado Jr** - [tekdj7](https://github.com/tekdj7) - _maintainer_
- **Carlo DePaolis** - [depaolism](https://github.com/depaolism) - _current maintainer_

## License

This project is licensed under the Apache 2.0 License

## Acknowledgments

- the boto3 team makes all of this magic possible.

## Link

- to view example of rules built with the RDK: [https://github.com/awslabs/aws-config-rules/tree/master/python](https://github.com/awslabs/aws-config-rules/tree/master/python)",FAUX
awslabs/cedar-access-control-for-k8s,Toolkit,DevOPs,2025-04-16T20:37:02Z,2024-11-11T19:49:13Z,0,0,0,0,0,0,0,0,2024-10-23T18:50:08Z,2025-04-06T15:43:44Z,631,123,Go,VRAI,7,FAUX,29,,29,"Cedar for Kubernetes brings the power of Cedar to Kubernetes authorization and admission validation, showing how cluster administrators can enable a unified access control language for principals making API calls and giving policy authors a single language to write and reason about",FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,5,,VRAI
awslabs/crossplane-on-eks,Toolkit,Documentations,2024-07-24T00:56:29Z,2024-03-13T20:20:32Z,0,0,0,0,3,0,0,7,2022-03-05T09:28:48Z,2025-04-04T02:07:17Z,6466,360,HCL,VRAI,112,FAUX,26,"aws,crossplane,crossplane-provider,terrajet",26,Crossplane bespoke composition blueprints for AWS resources,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,31,"# Blueprints for Crossplane on Amazon EKS
> **Note**: AWS Blueprints for Crossplane on Amazon Elastic Kubernetes Service is under active development and should be considered a pre-production framework.

Welcome to the AWS Crossplane Blueprints.

## Introduction
[AWS](https://aws.amazon.com/) Crossplane Blueprints is an open source repo to bootstrap Amazon Elastic Kubernetes Service Clusters.
and provision AWS resources with a library of [Crossplane Compositions (XRs)](https://crossplane.io/docs/master/concepts/composition.html) with Composite Resource Definitions (XRDs).

If you are new to Crossplane, it is highly recommended to get yourself familiarized with Crossplane concepts. The [official documentation](https://docs.crossplane.io/master/getting-started/introduction/) and this [blog post](https://blog.upbound.io/crossplane-first-look/) are good starting points. 

Compositions in this repository enable platform teams to define and offer bespoke AWS infrastructure APIs to the teams of application developers based on
predefined [Composite Resources (XRs)](https://crossplane.io/docs/master/concepts/composition.html), encompassing one or more of AWS [Managed Resources (MRs)](https://crossplane.io/docs/master/concepts/managed-resources.html)

## Features

✅   Bootstrap [Amazon EKS](https://aws.amazon.com/eks/) Cluster and Crossplane with [Terraform](https://www.terraform.io/) \
✅   Bootstrap [Amazon EKS](https://aws.amazon.com/eks/) Cluster and Crossplane with [eksctl](https://eksctl.io/) \
✅   [AWS Provider](https://github.com/crossplane/provider-aws) - Crossplane Compositions for AWS Services \
✅   [Upbound AWS Provider](https://github.com/upbound/provider-aws) - Upbound Crossplane Compositions for AWS Services \
✅   [AWS IRSA on EKS](https://github.com/crossplane/provider-aws/blob/master/AUTHENTICATION.md#using-iam-roles-for-serviceaccounts) - AWS Provider Config with IRSA enabled  \
✅ [Patching 101](doc/patching-101.md) - Learn how patches work.
✅   Example deployment patterns for [Composite Resources (XRs)](https://crossplane.io/docs/master/concepts/composition.html) for AWS Provider\
✅   Example deployment patterns for [Crossplane Managed Resources (MRs)](https://crossplane.io/docs/master/concepts/managed-resources.html)

## Getting Started

✅   Bootstrap EKS Cluster

This repo provides multiple options to bootstrap Amazon EKS Clusters with Crossplane and AWS Providers.
Checkout the following README for full deployment configuration

- [Bootstrap EKS Cluster with eksctl](bootstrap/eksctl/README.md)
- [Bootstrap EKS Cluster with Terraform](bootstrap/terraform/README.md)

✅   Configure the EKS cluster

Enable IRSA support for your EKS cluster for the necessary permissions to spin up other AWS services.
Depending on the provider, refer to the bootstrap README for this configuration.

 - [AWS Provider](https://github.com/crossplane/provider-aws) - Crossplane Compositions for AWS Services
 - [Upbound AWS Provider](https://github.com/upbound/provider-aws) - Upbound Crossplane Compositions for AWS Services

✅   Deploy the Examples

With the setup complete, you can then follow instructions on deploying
crossplane compositions or managed resources you want to experiment with. Keep
in mind that the list of compositions and managed resources in this repository
are evolving.

- Deploy the Examples by following [this README](examples/aws-provider/README.md)

✅   Work with nested compositions.

Compositions can be nested to further define and abstract application specific needs.

- Take a quick tour of a [nested composition example](doc/nested-compositions.md)

✅   Work with external secrets.

Crossplane can be configured to publish secrets external to the cluster in which it runs. 

- Try it out with [this guide](doc/vault-integration.md)

✅   Check out the [RDS day 2 operation doc](./doc/rds-day-2.md) 

✅   Checkout example [Gatekeeper configurations](./examples/gatekeeper/).

✅   Upbound AWS provider examples

- Deploy the Examples by following [this README](examples/upbound-aws-provider/README.md)

## Learn More

- [Amazon EKS](https://aws.amazon.com/eks/)
- [Crossplane](https://crossplane.io/)
- [AWS Provider](https://github.com/crossplane/provider-aws) for Crossplane
  - [API Docs](https://doc.crds.dev/github.com/crossplane/provider-aws) provider-aws

## Debugging
For debugging Compositions, CompositionResourceDefinitions, etc, [please see the debugging guide](doc/debugging.md).

## Adopters

A list of publicly known users of the Crossplane Blueprints for Amazon EKS project can be found in [ADOPTERS.md](ADOPTERS.md).

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the Apache 2.0 License.",VRAI
awslabs/idf-modules,Toolkit,DevOPs,2025-04-25T16:57:07Z,2024-07-26T19:12:52Z,0,0,0,0,38,0,0,0,2023-03-10T15:00:31Z,2025-04-04T02:07:24Z,1500,26,Python,VRAI,13,FAUX,15,,15,Industry Data Framework (IDF) IAC modules repository,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,15,"# Industry Dataframework (IDF)

IDF is a collection of resuable Infrastructure as Code (IAC) modules that works with [SeedFarmer CLI](https://github.com/awslabs/seed-farmer). Please see the [DOCS](https://seed-farmer.readthedocs.io/en/latest/) for all things seed-farmer.

The modules in this repository are decoupled from each other and can be aggregated together using GitOps (manifest file) principles provided by `seedfarmer` and achieve the desired use cases. It removes the undifferentiated heavy lifting for an end user by providing hardended modules and enables them to focus on building business on top of them.

## General Information

The modules in this repository are / must be generic for resuse without affiliation to any one particular project or use case or any vertical.

All modules in this repository adhere to the module strutucture defined in the the [SeedFarmer Guide](https://seed-farmer.readthedocs.io/en/latest)

- [Project Structure](https://seed-farmer.readthedocs.io/en/latest/project_development.html)
- [Module Development](https://seed-farmer.readthedocs.io/en/latest/module_development.html)
- [Module Manifest Guide](https://seed-farmer.readthedocs.io/en/latest/manifests.html)

#### Industry-Specific SeedFarmer Module Repositories
- [ADDF](https://github.com/awslabs/autonomous-driving-data-framework): A collection of modules for Scene Detection, Simulation (mock), Visualization, Compute, Storage, Centralized logging, etc.
- [AiOps](https://github.com/awslabs/aiops-modules): A collection of modules for use-cases in the Artificial Intelligence & Machine Learning spaces. 

## Modules supported by IDF

### Networking Modules

| Type | Description |
| --- | --- |
|  [Networking Module](modules/network/basic-cdk/README.md)  |  Deploys standard networking resources such as VPC, Public/Private/Isolated subnets and Interface/Gateway endpoints   |

### Compute Modules

| Type | Description |
| --- | --- |
|  [EKS Module](modules/compute/eks/README.md)  |  Deploys EKS Cluster with the documented list of addons  |
|  [AWS Batch Module](modules/compute/aws-batch/README.md)  |  Deploys AWS Batch resources   |

### Database Modules

| Type | Description |
| --- | --- |
|  [Neptune Module](modules/database/neptune/README.md)  |  Deploys Amazon Managed Neptune Cluster   |

### Storage Modules

| Type | Description |
| --- | --- |
|  [Opensearch Module](modules/storage/opensearch/README.md)  |  Deploys Amazon Opensearch Cluster   |
|  [S3 Buckets Module](modules/storage/buckets/README.md)  |  Deploys AWS S3 buckets for logging and artifacts purpose   |
|  [EFS Module](modules/storage/efs/README.md)  |  Deploys Amazon EFS for shared artifacts purpose   |
|  [FSX-Lustre Module](modules/storage/fsx-lustre/README.md)  |  Deploys Amazon FSX Lustre for HPC/Bigdata workloads   |

### Orchestration Modules

| Type | Description |
| --- | --- |
|  [Amazon Managed Workflows for Apache Airflow (MWAA) Module](modules/orchestration/mwaa/README.md)  |  Deploys an Amazon MWAA module   |

### Replication Modules

| Type | Description |
| --- | --- |
|  [DockerImages Replication Module](modules/replication/dockerimage-replication/README.md)  |  Deploys docker images replication module which replicates any docker image from public registry to an internal ECR repo(s)   |",VRAI
awslabs/sustainability-scanner,Toolkit,Application System,2024-08-29T08:47:58Z,2023-12-18T11:29:55Z,0,0,0,0,0,0,10,0,2023-02-20T15:12:41Z,2025-04-04T02:07:37Z,23119,116,Python,VRAI,11,FAUX,2,,2,,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,9,"# Sustainability Scanner (SusScanner)

<a href=""https://github.com/marketplace/actions/aws-sustainability-scanner-github-action""><img src=""https://img.shields.io/badge/GitHub%20Action-green?style=social&logo=github"" /></a>

**Validate AWS CloudFormation templates against AWS Well-Architected Sustainability Pillar best practices.**

Sustainability scanner is an open source tool that helps you create a more sustainable infrastructure on AWS. It takes in your Cloudformation template as input, evaluates it against a set of sustainability best practices and generates a report with a sustainability score and suggested improvements to apply to your template.
SusScanner comes with a set of rule implementations aligned to the [AWS Well-Architected Pillar for Sustainability](https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html). However, this is not an exhaustive list and new rules will come out as the tool evolves. Furthermore, you can extend these rules (located in the `susscanner/rules` dir) in accordance with your company-specific sustainability policies.

**Sustainability Scanner in action**  
![demo of susscanner][demo]

[demo]: https://raw.githubusercontent.com/awslabs/sustainability-scanner/main/demo.gif

Scroll down to the getting started section to get detailed examples on how to use the tool.

## Table of Contents

* Installation
  * Prerequisites
  * Getting Started
    1. Install via pip
    2. Install from source
* Sustainability Score
* Rule Set
  * Disabling Rules
  * Extending the rule set
* FAQs
* Security
* License

## Installation

To install Sustainability Scanner please follow the following instructions.

### Prerequisites

* [AWS CloudFormation Guard](https://github.com/aws-cloudformation/cloudformation-guard)
  * an open-source general-purpose policy-as-code evaluation tool which SusScanner builds on top of
* Python 3.6 or later
  * check version with `python3 -V`
* [AWS CDK] (https://docs.aws.amazon.com/cdk/v2/guide/home.html), if you use CDK to define your AWS infrastructure.

### Getting Started

There are two options to install the tool:
  
### 1. Install via pip

To install the project via pip, you simply have to call

```sh
pip3 install sustainability-scanner
```

#### Scanning an AWS CloudFormation Template

Run `susscanner --help` to get a list of options and arguments for the tool.
You should see an output like below:

```sh
susscanner --help
 Usage: susscanner [OPTIONS] CFN_TEMPLATE...                                                                                                                                                                                                                                                                          
                                                                                                                                                                                                                                                                                                                      
╭─ Arguments ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ *    cfn_template      CFN_TEMPLATE...  List of template names (for CloudFormation format) or stack name (for CDK format) [default: None] [required]     │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─ Options ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ --version  -v                Show the application's version and exit.                                                                                    │
│ --rules    -r      PATH      Location for a custom rules metadata file.                                                                                  │
│ --format   -f      [cf|cdk]  Template format [default: cf]                                                                                               │
│ --help                       Show this message and exit.                                                                                                 │   
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
```

You can scan a template by using the command:

```sh
susscanner [path/to/cloudformation/template_or_templates]
```

Or you can scan a CDK stack by using the command, **NB! you have to run this command in your CDK application root directory**:

```sh
susscanner -f cdk <STACK_NAME>
```

You should see an output like below;

```sh
susscanner test.yaml
{
    ""title"": ""Sustainability Scanner Report"",
    ""file"": ""test.yaml"",
    ""version"": ""1.3.0"",
    ""sustainability_score"": 8,
    ""failed_rules"": [
        {
            ""rule_name"": ""rest_api_compression_max"",
            ""severity"": ""MEDIUM"",
            ""message"": ""Consider configuring the payload compression with MinimumCompressionSize. Compressing the payload will in general reduce the network traffic."",
            ""links"": [
                ""https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-gzip-compression-decompression.html"",
                ""https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sus_sus_data_a8.html""
            ],
            ""resources"": [
                {
                    ""name"": ""/Resources/API-GW-2/Properties/MinimumCompressionSize"",
                    ""line"": ""15""
                }
            ]
        }
    ]
}
```

If you want to use your own `rules_metadata` file you can specify one using the `-r` or `--rules` options.  

### 2. Install from source
#### Clone this project

```sh
git clone https://github.com/awslabs/sustainability-scanner.git
```

#### Move into the project directory

```sh
cd sustainability-scanner
```

#### Create and activate virtual environment (optional)

```sh
# from the root directory of the project
python3 -m venv .venv
source .venv/bin/activate
```

#### Install dependencies

```sh
python3 -m pip install -r requirements.txt
```

That's it! You're ready to use Sustainability Scanner.

#### Scanning an AWS CloudFormation Template

You can scan a template by using the command;

```sh
#from the root directory of the project
python3 -m susscanner [path/to/cloudformation/template_or_templates]
```

## Sustainability Score

After you've scanned your AWS CloudFormation template, as part of the report, you will get a Sustainability Score. It follows inverted scoring and increases your score for each best practice you can improve; the lower the score the better. Higher severity rules have a greater scope for improvement e.g. Failing a HIGH SEV rule will increase your score more than a LOW SEV rule. If you are following all the best practices or none of the rules apply to your infrastructure this score will be 0. 
Find the scoring by severity in the table below

| SEVERITY    | SCORE       |
| ----------- | ----------- |
| LOW         | 1           |
| MEDIUM      | 2           |
| HIGH        | 3           |

## Rule set

SusScanner comes with a set of best practices/rules that align with [best practices for sustainability in the cloud](https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/best-practices-for-sustainability-in-the-cloud.html). You can find the list of best practices, the service they apply to and their improvement actions in the [rules_metadata.json](https://github.com/awslabs/sustainability-scanner/blob/main/susscanner/rules_metadata.json) file.

### Disabling rules

As mentioned before, the tool comes with a pre-defined set of rules, all of which are enabled by default. However, you can disable a rule if it is not applicable to your setup.
In the susscanner directory you can find a file called `rules_metadata.json`. This configuration file can be used to specify which rules to include. The structure of this file is as follows:

```
01:{
02:    ""all_rules"": 
03:    {
04:        ""rule_on_service_level"": {
05:            ""enabled"": true,
06:            ""rules"": [
07:            {
08:                ""rule_name"": ""name_of_the_rule"",
09:                ""severity"": ""MEDIUM"",
10:                ""message"": ""message_of_the_rule"",
11:                ""enabled"": true,
12:                ""links"": [
13:                    ""link_1"",
14:                    ""link_2""
15:                ]
16:            }
17:        }
18:    }
19:}
```

Rules can be enabled or disabled on both a service level and rule level. If you want to disable the checks for a service, for example Amazon Elastic Compute Cloud (EC2), you can set `enabled` to `false` on line 5 of the example above. Since a service can have multiple rules you can opt to disable rules on a per rule base. This can be done by setting `enabled` to `false`, in the example shown on line 11.

### Extending the rule set

If you wish to extend the pre-existing set of rules you can define your own by adding AWS CloudFormation Guard rules to the `susscanner/rules` directory. For each rule that you add, don't forget to add test cases to validate it. You can [validate](https://docs.aws.amazon.com/cfn-guard/latest/ug/validating-rules.html) a rule by running:

```sh
cfn-guard test --rules-file ./susscanner/rules/<RULE_FILE> --test-data ./susscanner/rules/test_cases/<TEST_FILE>
```

AWS CloudFormation Guard uses a domain-specific language (DSL) to define the rules. More information can be found at the [AWS CloudFormation Guard documentation page](https://docs.aws.amazon.com/cfn-guard/latest/ug/writing-rules.html). When defining a new rule there are 2 requirements to ensure compatibility with the Sustainability Scanner project.

1. Rules `FAIL` when the resulting state is not desirable in terms of sustainability and `PASS` when the outcome is sustainable.
2. Add the rule created in the `susscanner/rules` directory to the `rules_metadata.json` file. Define the name of the rule in the `rule_name` variable.

## FAQs

### Are all the recommendations mandatory to implement?

No, the recommendations are not mandatory to implement, if you categorize a best practice as not applicable or prefer the status quo given your workload, you can choose to either ignore the failed rule or disable it.

### What happens if there are no suggested improvements?

You will get a Sustainability Scanner Report without failed rules. This looks as follows:

```
{
    ""title"": ""Sustainability Scanner Report"",
    ""file"": ""cloudformation.yaml"",
    ""version"": ""1.3.0"",
    ""sustainability_score"": 0,
    ""failed_rules"": []
}
```

### Can I use it as part of a Github workflow?

Yes, a Github Action to run the scanner is available on the [marketplace](https://github.com/marketplace/actions/aws-sustainability-scanner-github-action).

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License
This project is licensed under the MIT-0 License.",VRAI
aws/aws-app-mesh-examples,Toolkit,DevOPs,2025-03-20T16:10:25Z,2023-02-08T23:01:35Z,0,0,0,0,1,0,0,0,2018-11-23T19:27:28Z,2025-04-02T08:33:34Z,26899,865,Shell,VRAI,394,FAUX,35,"app-mesh,aws,service-mesh",35,AWS App Mesh is a service mesh that you can use with your microservices to manage service to service communication.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,113,"# AWS App Mesh

## Introduction

*After careful consideration, we have made the decision to discontinue support for AWS App Mesh, effective September 30th, 2026. Until this date, existing AWS App Mesh customers will be able to use the service as normal, including creating new resources and onboarding new accounts via the AWS CLI and AWS CloudFormation. Additionally, AWS will continue to provide critical security and availability updates to AWS App Mesh during this period. However, starting from September 24th, 2024, new customers will be unable to onboard to AWS App Mesh.*

App Mesh makes it easy to run microservices by providing consistent visibility and network traffic controls for every microservice in an application. App Mesh separates the logic needed for monitoring and controlling communications into a proxy that runs next to every microservice. App Mesh removes the need to coordinate across teams or update application code to change how monitoring data is collected or traffic is routed. This allows you to quickly pinpoint the exact location of errors and automatically re-route network traffic when there are failures or when code changes need to be deployed.

You can use App Mesh with AWS Fargate, Amazon Elastic Container Service (ECS), Amazon Elastic Container Service for Kubernetes (EKS), and Kubernetes on EC2 to better run containerized microservices at scale. App Mesh uses [Envoy](https://www.envoyproxy.io/), an open source proxy, making it compatible with a wide range of AWS partner and open source tools for monitoring microservices.

Learn more at https://aws.amazon.com/app-mesh

## Availability

Today, AWS App Mesh is generally available for production use. You can use App Mesh with AWS Fargate, Amazon Elastic Container Service (ECS), Amazon Elastic Container Service for Kubernetes (EKS), applications running on Amazon EC2, and Kubernetes on EC2 to better run containerized microservices at scale. App Mesh uses Envoy, an open source proxy, making it compatible with a wide range of AWS partner and open source tools for monitoring microservices.

Learn more at https://aws.amazon.com/app-mesh

## Getting started
For help getting started with App Mesh, take a look at the [examples](https://github.com/aws/aws-app-mesh-examples/tree/master/examples) in this repo.  

### ARM64 support    
All the [walkthrough](https://github.com/aws/aws-app-mesh-examples/tree/main/walkthroughs) examples in this repo are compatible only
with amd64 linux instances. arm64 is only supported from version v1.20.0.1 or later of [aws-appmesh-envoy](https://gallery.ecr.aws/appmesh/aws-appmesh-envoy) and [v1.4.2](https://github.com/aws/aws-app-mesh-controller-for-k8s/releases/tag/v1.4.2) and later for
Appmesh-controller. We are working on updating these walkthroughs to be arm64 compatible as well. See https://github.com/aws/aws-app-mesh-examples/issues/473 for more up-to-date information.  

### China Regions
All the examples and walkthrough are written for commercial regions. You need to make few changes to make them work for China regions, below are some changes that will be needed:

* Change ARN:
  For China regions include aws-cn in all arns. So instead of 'arn:aws:' it starts with 'arn:aws-cn:'.
  Replace 'arn:aws:' with 'arn:${AWS::Partition}:' to make it work for all partitions.
* Change Endpoints:
  The endpoint domain for China regions is amazonaws.com.cn. Replace the endpoints from amazonaws.com to amazonaws.com.cn Refer [this](https://docs.amazonaws.cn/en_us/aws/latest/userguide/endpoints-Beijing.html) doc for a list of endpoints for cn-north-1.
  Do not change the Service Principal like ecs-tasks.amazonaws.com, it is a Service Principal not an endpoint.
* Change TCP ports 80/8080/443
  By default all AWS China accounts are blocked for TCP ports 80/8080/443 with EC2 and S3 services. These ports will be unlocked when an ICP license has been provided by customers. As a workaround you can use some other port for ex: 9090. The url that you curl for, needs to explicitly mention the port now.
  For example: http://appme-.....us-west-2.elb.amazonaws.com.cn:9090/color

### Roadmap

The AWS App Mesh team maintains a [public roadmap](https://github.com/aws/aws-app-mesh-roadmap).

### Participate

If you have a suggestion, request, submission, or bug fix for the examples in this repo, please open it as an [Issue](https://github.com/aws/aws-app-mesh-examples/issues).  

If you have a feature request for AWS App Mesh, please open an Issue on the [public roadmap](https://github.com/aws/aws-app-mesh-roadmap).

## Security disclosures

If you think you’ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).

### Why use  App Mesh?

1. Streamline operations by offloading communication management logic from application code and libraries into configurable infrastructure.
2. Reduce troubleshooting time required by having end-to-end visibility into service-level logs, metrics and traces across your application.
3. Easily roll out of new code by dynamically configuring routes to new application versions.
4. Ensure high-availability with custom routing rules that help ensure every service is highly available during deployments, after failures, and as your application scales.
5. Manage all service to service traffic using one set of APIs regardless of how the services are implemented.

### What makes AWS App Mesh unique?

AWS App Mesh is built in direct response to our customers needs implementing a 'service mesh' for their applications. Our customers asked us to:

* Make it easy to manage microservices deployed across accounts, clusters, container orchestration tools, and compute services with simple and consistent abstractions.
* Minimize the cognitive and operational overhead in running a microservices application and handling its monitoring and traffic control.
* Remove the need to build or operate a control plane for service mesh.
* Use open source software to allow extension to new tools and different use cases.

In order to best meet the needs of our customers, we have invested into building a service that includes a control plane and API that follows the AWS best practices. Specifically, App Mesh:

* Is an AWS managed service that works across container services with a design that allows us to add support for other computer services in the future.
* Works with the open source Envoy proxy
* Is designed to pluggable and will support bringing your own Envoy images and Istio Mixer in the future.
* Implemented as a multi-tenant control plane to be scalable, robust, cost-effective, and efficient.
* Built to work independently of any particular container orchestration system. Today, App Mesh works with both Kubernetes and Amazon ECS.",VRAI
aws/aws-cdk,Toolkit,Toolkit,2025-05-15T21:54:06Z,2025-05-13T08:20:02Z,0,0,0,0,0,0,0,0,2017-10-04T19:22:36Z,2025-04-08T00:36:13Z,1349480,12063,TypeScript,VRAI,4082,FAUX,2535,"aws,cloud-infrastructure,hacktoberfest,infrastructure-as-code,typescript",2535,The AWS Cloud Development Kit is a framework for defining cloud infrastructure in code,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,1729,"# AWS Cloud Development Kit (AWS CDK)

![Build Status](https://codebuild.us-east-1.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiSy9rWmVENzRDbXBoVlhYaHBsNks4OGJDRXFtV1IySmhCVjJoaytDU2dtVWhhVys3NS9Odk5DbC9lR2JUTkRvSWlHSXZrNVhYQ3ZsaUJFY3o4OERQY1pnPSIsIml2UGFyYW1ldGVyU3BlYyI6IlB3ODEyRW9KdU0yaEp6NDkiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=main)
[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/aws/aws-cdk)
[![NPM version](https://badge.fury.io/js/aws-cdk.svg)](https://badge.fury.io/js/aws-cdk)
[![PyPI version](https://badge.fury.io/py/aws-cdk-lib.svg)](https://badge.fury.io/py/aws-cdk-lib)
[![NuGet version](https://badge.fury.io/nu/Amazon.CDK.Lib.svg)](https://badge.fury.io/nu/Amazon.CDK.Lib)
[![Maven Central](https://maven-badges.herokuapp.com/maven-central/software.amazon.awscdk/aws-cdk-lib/badge.svg)](https://maven-badges.herokuapp.com/maven-central/software.amazon.awscdk/aws-cdk-lib)
[![Go Reference](https://pkg.go.dev/badge/github.com/aws/aws-cdk-go/awscdk/v2.svg)](https://pkg.go.dev/github.com/aws/aws-cdk-go/awscdk/v2)

[![View on Construct Hub](https://constructs.dev/badge?package=aws-cdk-lib)](https://constructs.dev/packages/aws-cdk-lib)

The **AWS Cloud Development Kit (AWS CDK)** is an open-source software development
framework to define cloud infrastructure in code and provision it through AWS CloudFormation.

It offers a high-level object-oriented abstraction to define AWS resources imperatively using
the power of modern programming languages. Using the CDK’s library of
infrastructure constructs, you can easily encapsulate AWS best practices in your
infrastructure definition and share it without worrying about boilerplate logic.

The CDK is available in the following languages:

* JavaScript, TypeScript ([Node.js ≥ 14.15.0](https://nodejs.org/download/release/latest-v14.x/))
  * We recommend using a version in [Active LTS](https://nodejs.org/en/about/previous-releases)
* Python ([Python ≥ 3.8](https://www.python.org/downloads/))
* Java ([Java ≥ 8](https://www.oracle.com/technetwork/java/javase/downloads/index.html) and [Maven ≥ 3.5.4](https://maven.apache.org/download.cgi))
* .NET ([.NET ≥ 6.0](https://dotnet.microsoft.com/download))
* Go ([Go ≥ 1.16.4](https://golang.org/))

Third-party Language Deprecation: language version is only supported until its EOL (End Of Life) shared by the vendor or community and is subject to change with prior notice.

\
Jump To:
[Developer Guide](https://docs.aws.amazon.com/cdk/latest/guide) |
[API Reference](https://docs.aws.amazon.com/cdk/api/v2/docs/aws-construct-library.html) |
[Getting Started](#getting-started) |
[Getting Help](#getting-help) |
[Contributing](#contributing) |
[RFCs](https://github.com/aws/aws-cdk-rfcs) |
[Roadmap](https://github.com/aws/aws-cdk/blob/main/ROADMAP.md) |
[More Resources](#more-resources)

-------

Developers use the [CDK framework] in one of the
supported programming languages to define reusable cloud components called [constructs], which
are composed together into [stacks], forming a ""CDK app"".

They then use the [AWS CDK CLI] to interact with their CDK app. The CLI allows developers to
synthesize artifacts such as AWS CloudFormation Templates, deploy stacks to development AWS accounts and ""diff""
against a deployed stack to understand the impact of a code change.

The [AWS Construct Library] includes a module for each
AWS service with constructs that offer rich APIs that encapsulate the details of
how to use AWS. The AWS Construct Library aims to reduce the complexity and
glue-logic required when integrating various AWS services to achieve your goals
on AWS.

Modules in the AWS Construct Library are designated Experimental while we build
them; experimental modules may have breaking API changes in any release.  After
a module is designated Stable, it adheres to [semantic versioning](https://semver.org/),
and only major releases can have breaking changes. Each module's stability designation
is available on its Overview page in the [AWS CDK API Reference](https://docs.aws.amazon.com/cdk/api/latest/docs/aws-construct-library.html).
For more information, see [Versioning](https://docs.aws.amazon.com/cdk/latest/guide/reference.html#versioning)
in the CDK Developer Guide.

[CDK framework]: https://docs.aws.amazon.com/cdk/latest/guide/home.html
[constructs]: https://docs.aws.amazon.com/cdk/latest/guide/constructs.html
[stacks]: https://docs.aws.amazon.com/cdk/latest/guide/stacks.html
[apps]: https://docs.aws.amazon.com/cdk/latest/guide/apps.html
[Developer Guide]: https://docs.aws.amazon.com/cdk/latest/guide
[AWS CDK CLI]: https://docs.aws.amazon.com/cdk/latest/guide/tools.html
[AWS Construct Library]: https://docs.aws.amazon.com/cdk/api/latest/docs/aws-construct-library.html

## Getting Started

For a detailed walkthrough, see the [tutorial](https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html#hello_world_tutorial) in the AWS CDK [Developer Guide](https://docs.aws.amazon.com/cdk/latest/guide/home.html).

### At a glance

Install or update the [AWS CDK CLI] from npm (requires [Node.js ≥ 14.15.0](https://nodejs.org/download/release/latest-v14.x/)). We recommend using a version in [Active LTS](https://nodejs.org/en/about/previous-releases)

```sh
npm i -g aws-cdk
```

(See [Manual Installation](./MANUAL_INSTALLATION.md) for installing the CDK from a signed .zip file).

Initialize a project:

```sh
mkdir hello-cdk
cd hello-cdk
cdk init sample-app --language=typescript
```

This creates a sample project looking like this:

```ts
export class HelloCdkStack extends cdk.Stack {
  constructor(scope: cdk.App, id: string, props?: cdk.StackProps) {
    super(scope, id, props);

    const queue = new sqs.Queue(this, 'HelloCdkQueue', {
      visibilityTimeout: cdk.Duration.seconds(300)
    });

    const topic = new sns.Topic(this, 'HelloCdkTopic');

    topic.addSubscription(new subs.SqsSubscription(queue));
  }
}
```

Deploy this to your account:

```sh
cdk deploy
```

Use the `cdk` command-line toolkit to interact with your project:

* `cdk deploy`: deploys your app into an AWS account
* `cdk synth`: synthesizes an AWS CloudFormation template for your app
* `cdk diff`: compares your app with the deployed stack

## Getting Help

The best way to interact with our team is through GitHub. You can open an [issue](https://github.com/aws/aws-cdk/issues/new/choose) and choose from one of our templates for bug reports, feature requests, documentation issues, or guidance.

If you have a support plan with AWS Support, you can also create a new [support case](https://console.aws.amazon.com/support/home#/).

You may also find help on these community resources:

* Look through the [API Reference](https://docs.aws.amazon.com/cdk/api/latest/docs/aws-construct-library.html) or [Developer Guide](https://docs.aws.amazon.com/cdk/latest/guide)
* The #aws-cdk Slack channel in [cdk.dev](https://cdk.dev)
* Ask a question on [Stack Overflow](https://stackoverflow.com/questions/tagged/aws-cdk)
  and tag it with `aws-cdk`

## Roadmap

The AWS CDK Roadmap lets developers know about our upcoming features and priorities to help them plan how to best leverage the CDK and identify opportunities to contribute to the project. See [ROADMAP.md](https://github.com/aws/aws-cdk/blob/main/ROADMAP.md) for more information and FAQs.

## Contributing

We welcome community contributions and pull requests. See
[CONTRIBUTING.md](./CONTRIBUTING.md) for information on how to set up a development
environment and submit code.

## Metrics collection

This solution collects anonymous operational metrics to help AWS improve the
quality and features of the CDK. For more information, including how to disable
this capability, please see the [developer guide](https://docs.aws.amazon.com/cdk/latest/guide/cli.html#version_reporting).

## More Resources

* [AWS CDK Immersion Day Workshop](https://catalog.us-east-1.prod.workshops.aws/workshops/10141411-0192-4021-afa8-2436f3c66bd8/en-US)
* [Construct Hub](https://constructs.dev) - Find and use open-source Cloud Development Kit (CDK) libraries
* Best Practices
  * [Best practices for developing cloud applications with AWS CDK](https://aws.amazon.com/blogs/devops/best-practices-for-developing-cloud-applications-with-aws-cdk/)
  * [Align with best practices while creating infrastructure using cdk aspects](https://aws.amazon.com/blogs/devops/align-with-best-practices-while-creating-infrastructure-using-cdk-aspects/)
  * [Recommended AWS CDK project structure for Python applications](https://aws.amazon.com/blogs/developer/recommended-aws-cdk-project-structure-for-python-applications/)
  * [Best practices for discoverability of a construct library on Construct Hub](https://aws.amazon.com/blogs/opensource/best-practices-for-discoverability-of-a-construct-library-on-construct-hub/)
* [All developer blog posts about AWS CDK](https://aws.amazon.com/blogs/developer/category/developer-tools/aws-cloud-development-kit/)
* **[CDK Construction Zone](https://www.twitch.tv/collections/9kCOGphNZBYVdA)** - A Twitch live coding series hosted by the CDK team, season one episodes:
  * Triggers: Join us as we implement [Triggers](https://github.com/aws/aws-cdk-rfcs/issues/71), a Construct for configuring deploy time actions. Episodes 1-3:
    * [S1E1](https://www.twitch.tv/videos/917691798): Triggers (part 1); **Participants:** @NetaNir, @eladb, @richardhboyd
    * [S1E2](https://www.twitch.tv/videos/925801382): Triggers (part 2); **Participants:** @NetaNir, @eladb, @iliapolo
    * [S1E3](https://www.twitch.tv/videos/944565768): Triggers (part 3); **Participants:** @NetaNir, @eladb, @iliapolo, @RomainMuller
  * [S1E4](https://www.twitch.tv/aws/video/960287598): [Tokens](https://docs.aws.amazon.com/cdk/latest/guide/tokens.html) Deep Dive; **Participants:** @NetaNir,@rix0rrr, @iliapolo, @RomainMuller
  * [S1E5](https://www.twitch.tv/videos/981481112): [Assets](https://docs.aws.amazon.com/cdk/latest/guide/assets.html) Deep Dive; **Participants:** @NetaNir, @eladb, @jogold
  * [S1E6](https://www.twitch.tv/aws/video/1005334364): [Best Practices](https://aws.amazon.com/blogs/devops/best-practices-for-developing-cloud-applications-with-aws-cdk/); **Participants:** @skinny85, @eladb, @rix0rrr, @alexpulver
  * [S1E7](https://www.twitch.tv/videos/1019059654): Tips and Tricks From The CDK Team; **Participants:** All the CDK team!
* [Examples](https://github.com/aws-samples/aws-cdk-examples)
* [Changelog](./CHANGELOG.md)
* [NOTICE](./NOTICE)
* [License](./LICENSE)",FAUX
aws/aws-eks-best-practices,Documentations,Documentations,2025-04-04T00:54:53Z,2025-02-19T19:21:20Z,0,0,0,0,5,0,0,23,2019-10-23T23:50:53Z,2025-04-05T00:20:34Z,79145,2101,Python,VRAI,522,FAUX,88,,88,"A best practices guide for day 2 operations, including operational excellence, security, reliability, performance efficiency, and cost optimization.",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,219,"## Amazon Elastic Kubernetes Service (Amazon EKS) Best Practices

A best practices guide for day 2 operations, including operational excellence, security, reliability, performance efficiency, and cost optimization.

This guide is now published to the official [Amazon EKS Docs platform](https://docs.aws.amazon.com/eks/latest/best-practices/introduction.html). While this repo continues to be the source, the GitHub.io page will be phased out.

See [latest/bpg](latest/bpg) for the new AsciiDoc formatted files. Contribution guidelines coming soon!

## Contributing

While the best practices were originally authored by AWS employees, we encourage and welcome contributions from the Kubernetes user community. If you have a best practice that you would like to share, please review the [Contributing Guidelines](https://github.com/aws/aws-eks-best-practices/blob/master/CONTRIBUTING.md) before submitting a PR. 

## License Summary

The documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.

The sample code within this documentation is made available under the MIT-0 license. See the LICENSE-SAMPLECODE file.",FAUX
aws/aws-parallelcluster,Toolkit,DevOPs,2025-01-06T16:39:37Z,2025-04-15T19:26:39Z,0,0,0,0,3,0,0,0,2014-05-12T22:42:19Z,2025-04-04T17:46:03Z,27248,860,Python,VRAI,314,FAUX,151,,151,AWS ParallelCluster is an AWS supported Open Source cluster management tool to deploy and manage HPC clusters in the AWS cloud.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,123,"AWS ParallelCluster - HPC for the Cloud
=======================================

[![PyPI Version](https://img.shields.io/pypi/v/aws-parallelcluster)](https://pypi.org/project/aws-parallelcluster/)
[![Spack Version](https://img.shields.io/spack/v/aws-parallelcluster)](https://spack.readthedocs.io/en/latest/package_list.html#aws-parallelcluster)
[![Conda Verseion](https://img.shields.io/conda/vn/conda-forge/aws-parallelcluster)](https://anaconda.org/conda-forge/aws-parallelcluster)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![codecov](https://codecov.io/gh/aws/aws-parallelcluster/branch/develop/graph/badge.svg)](https://codecov.io/gh/aws/aws-parallelcluster)
[![ParallelCluster CI](https://github.com/aws/aws-parallelcluster/workflows/ParallelCluster%20CI/badge.svg)](https://github.com/aws/aws-parallelcluster/actions)

AWS ParallelCluster is an AWS supported Open Source cluster management tool that makes it easy for you to deploy and
manage High Performance Computing (HPC) clusters in the AWS cloud.
Built on the Open Source CfnCluster project, AWS ParallelCluster enables you to quickly build an HPC compute environment in AWS.
It automatically sets up the required compute resources and a shared filesystem and offers a variety of batch schedulers such as AWS Batch and Slurm.
AWS ParallelCluster facilitates both quick start proof of concepts (POCs) and production deployments.
You can build higher level workflows, such as a Genomics portal that automates the entire DNA sequencing workflow, on top of AWS ParallelCluster.

Quick Start
-----------
**IMPORTANT**: you will need an **Amazon EC2 Key Pair** to be able to complete the following steps.
Please see the [Official AWS Guide](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html).

First, prepare a Python Virtual Environment for ParallelCluster, note ParallelCluster >= 3.0.0 requires Python >= 3.7.
```
python3 -m pip install --upgrade pip
python3 -m pip install --user --upgrade virtualenv
python3 -m virtualenv ~/hpc-ve
source ~/hpc-ve/bin/activate
```

Make sure you have installed the [AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html):

```
$ pip3 install awscli
```

[Node.js](https://nodejs.org/en/) is required by AWS CDK library used by ParallelCluster.
Please reference installation instructions [in the AWS CDK documentaton](https://docs.aws.amazon.com/cdk/v2/guide/work-with.html#work-with-prerequisites).

Then you can install AWS ParallelCluster:

```
$ pip3 install aws-parallelcluster
```

Next, configure your aws credentials and default region:

```
$ aws configure
AWS Access Key ID [None]: YOUR_KEY
AWS Secret Access Key [None]: YOUR_SECRET
Default region name [us-east-1]:
Default output format [None]:
```

Then, run ``pcluster configure``. A list of valid options will be displayed for each
configuration parameter. Type an option number and press ``Enter`` to select a specific option,
or just press ``Enter`` to accept the default option.

```
$ pcluster configure --config /dir/cluster-config.yaml
INFO: Configuration file /dir/cluster-config.yaml will be written.
Press CTRL-C to interrupt the procedure.


Allowed values for AWS Region ID:
1. eu-north-1
...
15. us-west-1
16. us-west-2
AWS Region ID [us-east-1]:
```

Be sure to select a region containing the EC2 key pair you wish to use. You can also import a public key using
[these instructions](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#how-to-generate-your-own-key-and-import-it-to-aws).

During the process you will be asked to set up your networking environment. The wizard will offer you the choice of
using an existing VPC or creating a new one on the fly.

```
Automate VPC creation? (y/n) [n]:
```

Enter ``n`` if you already have a VPC suitable for the cluster. Otherwise you can let ``pcluster configure``
create a VPC for you. The same choice is given for the subnet: you can select a valid subnet ID for
both the head node and compute nodes, or you can let ``pcluster configure`` set up everything for you.
The same choice is given for the subnet configuration: you can select a valid subnet ID for both
the head node and compute nodes, or you can let pcluster configure set up everything for you.
In the latter case, just select the configuration you prefer.

```
Automate Subnet creation? (y/n) [y]: y
Allowed values for Network Configuration:
1. Head node in a public subnet and compute fleet in a private subnet
2. Head node and compute fleet in the same public subnet
```


At the end of the process a message like this one will be shown:

```
Configuration file written to /dir/conf_file
You can edit your configuration file or simply run 'pcluster create-cluster --cluster-name cluster-name --cluster-configuration /dir/cluster-config.yaml' to create your cluster.
```


Now you can create your first cluster:

```
$ pcluster create-cluster --cluster-name myfirstcluster --cluster-configuration /dir/cluster-config.yaml
```


After the cluster finishes creating, log in:

```
$ pcluster ssh --cluster-name myfirstcluster
```

You can view the running compute hosts:

```
$ sinfo
```

For more information on any of these steps see the [Getting Started Guide](https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3.html).

Documentation
-------------

We've been working hard to greatly improve the [Documentation](https://docs.aws.amazon.com/parallelcluster/latest/ug/), it's now published in 10 languages, one of the many benefits of being hosted on AWS Docs. Of most interest to new users is
the [Getting Started Guide](https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3.html).

If you have changes you would like to see in the docs, please either submit feedback using the feedback link at the bottom
of each page or create an issue or pull request for the project at:
https://github.com/awsdocs/aws-parallelcluster-user-guide.

Issues
------

[![GitHub issues](https://img.shields.io/github/issues/aws/aws-parallelcluster.svg)](https://github.com/aws/aws-parallelcluster/issues)
[![GitHub closed issues](https://img.shields.io/github/issues-closed-raw/aws/aws-parallelcluster.svg)](https://github.com/aws-parallelcluster/issues?q=is%3Aissue+is%3Aclosed)

Please open a GitHub issue for any feedback or issues:
https://github.com/aws/aws-parallelcluster/issues.  There is also an active AWS
HPC forum which may be helpful: https://repost.aws/tags/TAbl-DsTlyQMe0T2i-d5Rr8g/aws-parallel-cluster.

Changes
-------

### CfnCluster to AWS ParallelCluster
In Version `2.0.0`, we changed the name of CfnCluster to AWS ParallelCluster. With that name change we released several new features, which you can read about in the [Change Log](https://github.com/aws/aws-parallelcluster/blob/develop/CHANGELOG.md#200).",VRAI
aws/aws-sdk-go,Toolkit,Toolkit,2025-04-23T02:27:13Z,2024-07-10T19:11:19Z,0,0,0,0,0,0,13,0,2014-12-05T05:29:41Z,2025-04-07T13:12:05Z,520242,8688,Go,VRAI,2068,FAUX,2,"aws,aws-sdk,go",2,"AWS SDK for the Go programming language (In Maintenance Mode, End-of-Life on 07/31/2025). The AWS SDK for Go v2 is available here: https://github.com/aws/aws-sdk-go-v2",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,306,"# AWS SDK for Go

[![API Reference](https://img.shields.io/badge/api-reference-blue.svg)](https://docs.aws.amazon.com/sdk-for-go/api) [![Join the chat at https://gitter.im/aws/aws-sdk-go](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/aws/aws-sdk-go?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Build status](https://github.com/aws/aws-sdk-go/actions/workflows/go.yml/badge.svg?branch=main)](https://github.com/aws/aws-sdk-go/actions/workflows/go.yml) [![Apache V2 License](https://img.shields.io/badge/license-Apache%20V2-blue.svg)](https://github.com/aws/aws-sdk-go/blob/main/LICENSE.txt)

aws-sdk-go is the v1 AWS SDK for the Go programming language.

## :warning: This SDK is in maintenance mode

We previously [announced](https://aws.amazon.com/blogs/developer/announcing-end-of-support-for-aws-sdk-for-go-v1-on-july-31-2025)
the upcoming **end-of-support for AWS SDK for Go (v1)**.

Per that announcement, as of 7/31/2024, **the SDK has entered maintenance mode**.
Going forward, we will limit releases to address critical bug fixes and security
issues only. The SDK will not receive API updates for new or existing services,
or be updated to support new regions.

Maintenance mode will last for 1 year. After 7/31/2025, the SDK will enter end-of-support, and no updates at all will be made.
We recommend that you migrate to [AWS SDK for Go v2](https://aws.github.io/aws-sdk-go-v2/docs/).
For additional details as well as information on how to migrate, please refer
to the linked announcement.

Jump To:
* [Getting Started](#Getting-Started)
* [Quick Examples](#Quick-Examples)
* [Getting Help](#Getting-Help)
* [Contributing](#Contributing)
* [More Resources](#Resources)

## Getting Started

### Installing
Use `go get` to retrieve the SDK to add it to your project's Go module dependencies.

	go get github.com/aws/aws-sdk-go

To update the SDK use `go get -u` to retrieve the latest version of the SDK.

	go get -u github.com/aws/aws-sdk-go

## Quick Examples 

### Complete SDK Example

This example shows a complete working Go file which will upload a file to S3
and use the Context pattern to implement timeout logic that will cancel the
request if it takes too long. This example highlights how to use sessions,
create a service client, make a request, handle the error, and process the
response.

```go
  package main

  import (
  	""context""
  	""flag""
  	""fmt""
  	""os""
  	""time""

  	""github.com/aws/aws-sdk-go/aws""
  	""github.com/aws/aws-sdk-go/aws/awserr""
  	""github.com/aws/aws-sdk-go/aws/request""
  	""github.com/aws/aws-sdk-go/aws/session""
  	""github.com/aws/aws-sdk-go/service/s3""
  )

  // Uploads a file to S3 given a bucket and object key. Also takes a duration
  // value to terminate the update if it doesn't complete within that time.
  //
  // The AWS Region needs to be provided in the AWS shared config or on the
  // environment variable as `AWS_REGION`. Credentials also must be provided
  // Will default to shared config file, but can load from environment if provided.
  //
  // Usage:
  //   # Upload myfile.txt to myBucket/myKey. Must complete within 10 minutes or will fail
  //   go run withContext.go -b mybucket -k myKey -d 10m < myfile.txt
  func main() {
  	var bucket, key string
  	var timeout time.Duration

  	flag.StringVar(&bucket, ""b"", """", ""Bucket name."")
  	flag.StringVar(&key, ""k"", """", ""Object key name."")
  	flag.DurationVar(&timeout, ""d"", 0, ""Upload timeout."")
  	flag.Parse()

  	// All clients require a Session. The Session provides the client with
 	// shared configuration such as region, endpoint, and credentials. A
 	// Session should be shared where possible to take advantage of
 	// configuration and credential caching. See the session package for
 	// more information.
  	sess := session.Must(session.NewSession())

 	// Create a new instance of the service's client with a Session.
 	// Optional aws.Config values can also be provided as variadic arguments
 	// to the New function. This option allows you to provide service
 	// specific configuration.
  	svc := s3.New(sess)

  	// Create a context with a timeout that will abort the upload if it takes
  	// more than the passed in timeout.
  	ctx := context.Background()
  	var cancelFn func()
  	if timeout > 0 {
  		ctx, cancelFn = context.WithTimeout(ctx, timeout)
  	}
  	// Ensure the context is canceled to prevent leaking.
  	// See context package for more information, https://golang.org/pkg/context/
	if cancelFn != nil {
  		defer cancelFn()
	}

  	// Uploads the object to S3. The Context will interrupt the request if the
  	// timeout expires.
  	_, err := svc.PutObjectWithContext(ctx, &s3.PutObjectInput{
  		Bucket: aws.String(bucket),
  		Key:    aws.String(key),
  		Body:   os.Stdin,
  	})
  	if err != nil {
  		if aerr, ok := err.(awserr.Error); ok && aerr.Code() == request.CanceledErrorCode {
  			// If the SDK can determine the request or retry delay was canceled
  			// by a context the CanceledErrorCode error code will be returned.
  			fmt.Fprintf(os.Stderr, ""upload canceled due to timeout, %v\n"", err)
  		} else {
  			fmt.Fprintf(os.Stderr, ""failed to upload object, %v\n"", err)
  		}
  		os.Exit(1)
  	}

  	fmt.Printf(""successfully uploaded file to %s/%s\n"", bucket, key)
  }
```

### Overview of SDK's Packages

The SDK is composed of two main components, SDK core, and service clients.
The SDK core packages are all available under the aws package at the root of
the SDK. Each client for a supported AWS service is available within its own
package under the service folder at the root of the SDK.

  * aws - SDK core, provides common shared types such as Config, Logger,
    and utilities to make working with API parameters easier.

      * awserr - Provides the error interface that the SDK will use for all
        errors that occur in the SDK's processing. This includes service API
        response errors as well. The Error type is made up of a code and message.
        Cast the SDK's returned error type to awserr.Error and call the Code
        method to compare returned error to specific error codes. See the package's
        documentation for additional values that can be extracted such as RequestID.

      * credentials - Provides the types and built in credentials providers
        the SDK will use to retrieve AWS credentials to make API requests with.
        Nested under this folder are also additional credentials providers such as
        stscreds for assuming IAM roles, and ec2rolecreds for EC2 Instance roles.

      * endpoints - Provides the AWS Regions and Endpoints metadata for the SDK.
        Use this to lookup AWS service endpoint information such as which services
        are in a region, and what regions a service is in. Constants are also provided
        for all region identifiers, e.g UsWest2RegionID for ""us-west-2"".

      * session - Provides initial default configuration, and load
        configuration from external sources such as environment and shared
        credentials file.

      * request - Provides the API request sending, and retry logic for the SDK.
        This package also includes utilities for defining your own request
        retryer, and configuring how the SDK processes the request.

  * service - Clients for AWS services. All services supported by the SDK are
    available under this folder.

### How to Use the SDK's AWS Service Clients

The SDK includes the Go types and utilities you can use to make requests to
AWS service APIs. Within the service folder at the root of the SDK you'll find
a package for each AWS service the SDK supports. All service clients follow common pattern of creation and usage.

When creating a client for an AWS service you'll first need to have a Session
value constructed. The Session provides shared configuration that can be shared
between your service clients. When service clients are created you can pass
in additional configuration via the aws.Config type to override configuration
provided by in the Session to create service client instances with custom
configuration.

Once the service's client is created you can use it to make API requests the
AWS service. These clients are safe to use concurrently.

### Configuring the SDK

In the AWS SDK for Go, you can configure settings for service clients, such
as the log level and maximum number of retries. Most settings are optional;
however, for each service client, you must specify a region and your credentials.
The SDK uses these values to send requests to the correct AWS region and sign
requests with the correct credentials. You can specify these values as part
of a session or as environment variables.

See the SDK's [configuration guide][config_guide] for more information.

See the [session][session_pkg] package documentation for more information on how to use Session
with the SDK.

See the [Config][config_typ] type in the [aws][aws_pkg] package for more information on configuration
options.

[config_guide]: https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html
[session_pkg]: https://docs.aws.amazon.com/sdk-for-go/api/aws/session/
[config_typ]: https://docs.aws.amazon.com/sdk-for-go/api/aws/#Config
[aws_pkg]: https://docs.aws.amazon.com/sdk-for-go/api/aws/

### Configuring Credentials

When using the SDK you'll generally need your AWS credentials to authenticate
with AWS services. The SDK supports multiple methods of supporting these
credentials. By default the SDK will source credentials automatically from
its default credential chain. See the session package for more information
on this chain, and how to configure it. The common items in the credential
chain are the following:

  * Environment Credentials - Set of environment variables that are useful
    when sub processes are created for specific roles.

  * Shared Credentials file (~/.aws/credentials) - This file stores your
    credentials based on a profile name and is useful for local development.

  * EC2 Instance Role Credentials - Use EC2 Instance Role to assign credentials
    to application running on an EC2 instance. This removes the need to manage
    credential files in production.

Credentials can be configured in code as well by setting the Config's Credentials
value to a custom provider or using one of the providers included with the
SDK to bypass the default credential chain and use a custom one. This is
helpful when you want to instruct the SDK to only use a specific set of
credentials or providers.

This example creates a credential provider for assuming an IAM role, ""myRoleARN""
and configures the S3 service client to use that role for API requests.

```go
  // Initial credentials loaded from SDK's default credential chain. Such as
  // the environment, shared credentials (~/.aws/credentials), or EC2 Instance
  // Role. These credentials will be used to to make the STS Assume Role API.
  sess := session.Must(session.NewSession())

  // Create the credentials from AssumeRoleProvider to assume the role
  // referenced by the ""myRoleARN"" ARN.
  creds := stscreds.NewCredentials(sess, ""myRoleArn"")

  // Create service client value configured for credentials
  // from assumed role.
  svc := s3.New(sess, &aws.Config{Credentials: creds})
```

See the [credentials][credentials_pkg] package documentation for more information on credential
providers included with the SDK, and how to customize the SDK's usage of
credentials.

The SDK has support for the shared configuration file (~/.aws/config). This
support can be enabled by setting the environment variable, ""AWS_SDK_LOAD_CONFIG=1"",
or enabling the feature in code when creating a Session via the
Option's SharedConfigState parameter.

```go
  sess := session.Must(session.NewSessionWithOptions(session.Options{
      SharedConfigState: session.SharedConfigEnable,
  }))
```

[credentials_pkg]: https://docs.aws.amazon.com/sdk-for-go/api/aws/credentials

### Configuring AWS Region

In addition to the credentials you'll need to specify the region the SDK
will use to make AWS API requests to. In the SDK you can specify the region
either with an environment variable, or directly in code when a Session or
service client is created. The last value specified in code wins if the region
is specified multiple ways.

To set the region via the environment variable set the ""AWS_REGION"" to the
region you want to the SDK to use. Using this method to set the region will
allow you to run your application in multiple regions without needing additional
code in the application to select the region.

    AWS_REGION=us-west-2

The endpoints package includes constants for all regions the SDK knows. The
values are all suffixed with RegionID. These values are helpful, because they
reduce the need to type the region string manually.

To set the region on a Session use the aws package's Config struct parameter
Region to the AWS region you want the service clients created from the session to
use. This is helpful when you want to create multiple service clients, and
all of the clients make API requests to the same region.

```go
  sess := session.Must(session.NewSession(&aws.Config{
      Region: aws.String(endpoints.UsWest2RegionID),
  }))
```

See the [endpoints][endpoints_pkg] package for the AWS Regions and Endpoints metadata.

In addition to setting the region when creating a Session you can also set
the region on a per service client bases. This overrides the region of a
Session. This is helpful when you want to create service clients in specific
regions different from the Session's region.

```go
  svc := s3.New(sess, &aws.Config{
      Region: aws.String(endpoints.UsWest2RegionID),
  })
```

See the [Config][config_typ] type in the [aws][aws_pkg] package for more information and additional
options such as setting the Endpoint, and other service client configuration options.

[endpoints_pkg]: https://docs.aws.amazon.com/sdk-for-go/api/aws/endpoints/

### Making API Requests

Once the client is created you can make an API request to the service.
Each API method takes a input parameter, and returns the service response
and an error. The SDK provides methods for making the API call in multiple ways.

In this list we'll use the S3 ListObjects API as an example for the different
ways of making API requests.

  * ListObjects - Base API operation that will make the API request to the service.

  * ListObjectsRequest - API methods suffixed with Request will construct the
    API request, but not send it. This is also helpful when you want to get a
    presigned URL for a request, and share the presigned URL instead of your
    application making the request directly.

  * ListObjectsPages - Same as the base API operation, but uses a callback to
    automatically handle pagination of the API's response.

  * ListObjectsWithContext - Same as base API operation, but adds support for
    the Context pattern. This is helpful for controlling the canceling of in
    flight requests. See the Go standard library context package for more
    information. This method also takes request package's Option functional
    options as the variadic argument for modifying how the request will be
    made, or extracting information from the raw HTTP response.

  * ListObjectsPagesWithContext - same as ListObjectsPages, but adds support for
    the Context pattern. Similar to ListObjectsWithContext this method also
    takes the request package's Option function option types as the variadic
    argument.

In addition to the API operations the SDK also includes several higher level
methods that abstract checking for and waiting for an AWS resource to be in
a desired state. In this list we'll use WaitUntilBucketExists to demonstrate
the different forms of waiters.

  * WaitUntilBucketExists. - Method to make API request to query an AWS service for
    a resource's state. Will return successfully when that state is accomplished.

  * WaitUntilBucketExistsWithContext - Same as WaitUntilBucketExists, but adds
    support for the Context pattern. In addition these methods take request
    package's WaiterOptions to configure the waiter, and how underlying request
    will be made by the SDK.

The API method will document which error codes the service might return for
the operation. These errors will also be available as const strings prefixed
with ""ErrCode"" in the service client's package. If there are no errors listed
in the API's SDK documentation you'll need to consult the AWS service's API
documentation for the errors that could be returned.

```go
  ctx := context.Background()

  result, err := svc.GetObjectWithContext(ctx, &s3.GetObjectInput{
      Bucket: aws.String(""my-bucket""),
      Key: aws.String(""my-key""),
  })
  if err != nil {
      // Cast err to awserr.Error to handle specific error codes.
      aerr, ok := err.(awserr.Error)
      if ok && aerr.Code() == s3.ErrCodeNoSuchKey {
          // Specific error code handling
      }
      return err
  }

  // Make sure to close the body when done with it for S3 GetObject APIs or
  // will leak connections.
  defer result.Body.Close()

  fmt.Println(""Object Size:"", aws.Int64Value(result.ContentLength))
```

### API Request Pagination and Resource Waiters

Pagination helper methods are suffixed with ""Pages"", and provide the
functionality needed to round trip API page requests. Pagination methods
take a callback function that will be called for each page of the API's response.

```go
   objects := []string{}
   err := svc.ListObjectsPagesWithContext(ctx, &s3.ListObjectsInput{
       Bucket: aws.String(myBucket),
   }, func(p *s3.ListObjectsOutput, lastPage bool) bool {
       for _, o := range p.Contents {
           objects = append(objects, aws.StringValue(o.Key))
       }
       return true // continue paging
   })
   if err != nil {
       panic(fmt.Sprintf(""failed to list objects for bucket, %s, %v"", myBucket, err))
   }

   fmt.Println(""Objects in bucket:"", objects)
```

Waiter helper methods provide the functionality to wait for an AWS resource
state. These methods abstract the logic needed to check the state of an
AWS resource, and wait until that resource is in a desired state. The waiter
will block until the resource is in the state that is desired, an error occurs,
or the waiter times out. If a resource times out the error code returned will
be request.WaiterResourceNotReadyErrorCode.

```go
  err := svc.WaitUntilBucketExistsWithContext(ctx, &s3.HeadBucketInput{
      Bucket: aws.String(myBucket),
  })
  if err != nil {
      aerr, ok := err.(awserr.Error)
      if ok && aerr.Code() == request.WaiterResourceNotReadyErrorCode {
          fmt.Fprintf(os.Stderr, ""timed out while waiting for bucket to exist"")
      }
      panic(fmt.Errorf(""failed to wait for bucket to exist, %v"", err))
  }
  fmt.Println(""Bucket"", myBucket, ""exists"")
```    

## Getting Help

Please use these community resources for getting help. We use the GitHub issues
for tracking bugs and feature requests.

* Ask a question on [StackOverflow](http://stackoverflow.com/) and tag it with the [`aws-sdk-go`](http://stackoverflow.com/questions/tagged/aws-sdk-go) tag.
* Come join the AWS SDK for Go community chat on [gitter](https://gitter.im/aws/aws-sdk-go).
* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).
* If you think you may have found a bug, please open an [issue](https://github.com/aws/aws-sdk-go/issues/new/choose).

This SDK implements AWS service APIs. For general issues regarding the AWS services and their limitations, you may also take a look at the [Amazon Web Services Discussion Forums](https://forums.aws.amazon.com/).

### Opening Issues

If you encounter a bug with the AWS SDK for Go we would like to hear about it.
Search the [existing issues](https://github.com/aws/aws-sdk-go/issues) and see
if others are also experiencing the issue before opening a new issue. Please
include the version of AWS SDK for Go, Go language, and OS you’re using. Please
also include reproduction case when appropriate.

The GitHub issues are intended for bug reports and feature requests. For help
and questions with using AWS SDK for Go please make use of the resources listed
in the [Getting Help](https://github.com/aws/aws-sdk-go#getting-help) section.
Keeping the list of open issues lean will help us respond in a timely manner.

## Contributing

We work hard to provide a high-quality and useful SDK for our AWS services, and we greatly value feedback and contributions from our community. Please review our [contributing guidelines](./CONTRIBUTING.md) before submitting any [issues] or [pull requests][pr] to ensure we have all the necessary information to effectively respond to your bug report or contribution.

## Maintenance and support for SDK major versions

For information about maintenance and support for SDK major versions and our underlying dependencies, see the following in the AWS SDKs and Tools Shared Configuration and Credentials Reference Guide:

* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)
* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)

### Go version support policy

The v2 SDK follows the upstream [release policy](https://go.dev/doc/devel/release#policy)
with an additional six months of support for the most recently deprecated
language version.

**AWS reserves the right to drop support for unsupported Go versions earlier to
address critical security issues.**

## Resources

[Developer guide](https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/welcome.html) - This document
is a general introduction on how to configure and make requests with the SDK.
If this is your first time using the SDK, this documentation and the API
documentation will help you get started. This document focuses on the syntax
and behavior of the SDK. The [Service Developer Guide](https://aws.amazon.com/documentation/) 
will help you get started using specific AWS services.

[SDK API Reference Documentation](https://docs.aws.amazon.com/sdk-for-go/api/) - Use this
document to look up all API operation input and output parameters for AWS
services supported by the SDK. The API reference also includes documentation of
the SDK, and examples how to using the SDK, service client API operations, and
API operation require parameters.

[Service Documentation](https://aws.amazon.com/documentation/) - Use this
documentation to learn how to interface with AWS services. These guides are
great for getting started with a service, or when looking for more 
information about a service. While this document is not required for coding, 
services may supply helpful samples to look out for.

[SDK Examples](https://github.com/aws/aws-sdk-go/tree/main/example) -
Included in the SDK's repo are several hand crafted examples using the SDK
features and AWS services.

[Forum](https://forums.aws.amazon.com/forum.jspa?forumID=293) - Ask questions, get help, and give feedback

[Issues][issues] - Report issues, submit pull requests, and get involved
  (see [Apache 2.0 License][license])


[issues]: https://github.com/aws/aws-sdk-go/issues
[pr]: https://github.com/aws/aws-sdk-go/pulls
[license]: http://aws.amazon.com/apache2.0/",FAUX
aws/aws-sdk-net,Toolkit,Toolkit,2025-05-15T19:25:05Z,2025-05-12T18:24:48Z,0,0,0,0,0,0,19,0,2012-01-23T23:50:43Z,2025-04-08T01:06:58Z,1634991,2126,C#,VRAI,865,FAUX,88,,88,"The official AWS SDK for .NET. For more information on the AWS SDK for .NET, see our web site:",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,184,"",FAUX
aws/aws-sdk-php,Toolkit,Toolkit,2025-05-15T18:14:32Z,2025-04-25T18:09:39Z,0,0,0,0,0,0,13,0,2012-10-17T21:28:32Z,2025-04-07T18:35:02Z,446467,6090,PHP,VRAI,1229,FAUX,40,,40,Official repository of the AWS SDK for PHP (@awsforphp),FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,236,"# AWS SDK for PHP - Version 3

[![Total Downloads](https://img.shields.io/packagist/dt/aws/aws-sdk-php.svg?style=flat)](https://packagist.org/packages/aws/aws-sdk-php)
[![Apache 2 License](https://img.shields.io/packagist/l/aws/aws-sdk-php.svg?style=flat)](http://aws.amazon.com/apache-2-0/)
[![Gitter](https://badges.gitter.im/aws/aws-sdk-php.svg)](https://gitter.im/aws/aws-sdk-php?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
[![codecov](https://codecov.io/gh/aws/aws-sdk-php/branch/master/graph/badge.svg)](https://codecov.io/gh/aws/aws-sdk-php)

The **AWS SDK for PHP** makes it easy for developers to access [Amazon Web
Services][aws] in their PHP code, and build robust applications and software
using services like Amazon S3, Amazon DynamoDB, Amazon Glacier, etc. You can
get started in minutes by [installing the SDK through Composer][docs-installation]
or by downloading a single zip or phar file from our [latest release][latest-release].

Jump To:
* [Getting Started](#Getting-Started)
* [Quick Examples](#Quick-Examples)
* [Getting Help](#Getting-Help)
* [Features](#Features)
* [Contributing](#Contributing)
* [More Resources](#Resources)
* [Related AWS Projects](#Related-AWS-Projects)

## Getting Started

1. **Sign up for AWS** – Before you begin, you need to
   sign up for an AWS account and retrieve your [AWS credentials][docs-signup].
2. **Minimum requirements** – To run the SDK, your system will need to meet the
   [minimum requirements][docs-requirements], including having **PHP >= 8.1**.
   We highly recommend having it compiled with the cURL extension and cURL
   7.16.2+ compiled with a TLS backend (e.g., NSS or OpenSSL).
3. **Install the SDK** – Using [Composer] is the recommended way to install the
   AWS SDK for PHP. The SDK is available via [Packagist] under the
   [`aws/aws-sdk-php`][install-packagist] package. If Composer is installed globally on your system, you can run the following in the base directory of your project to add the SDK as a dependency:
   ```
   composer require aws/aws-sdk-php
   ```
   Please see the
   [Installation section of the User Guide][docs-installation] for more
   detailed information about installing the SDK through Composer and other
   means.
4. **Using the SDK** – The best way to become familiar with how to use the SDK
   is to read the [User Guide][docs-guide]. The
   [Getting Started Guide][docs-quickstart] will help you become familiar with
   the basic concepts.
5. **Beta: Removing unused services** — To date, there are over 300 AWS services available for use with this SDK.
   You will likely not need them all. If you use Composer and would like to learn more about this feature,
    please read the [linked documentation][docs-script-composer].


## Quick Examples

### Create an Amazon S3 client

```php
<?php
// Require the Composer autoloader.
require 'vendor/autoload.php';

use Aws\S3\S3Client;

// Instantiate an Amazon S3 client.
$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-west-2'
]);
```

### Upload a file to Amazon S3

```php
<?php
// Upload a publicly accessible file. The file size and type are determined by the SDK.
try {
    $s3->putObject([
        'Bucket' => 'my-bucket',
        'Key'    => 'my-object',
        'Body'   => fopen('/path/to/file', 'r'),
        'ACL'    => 'public-read',
    ]);
} catch (Aws\S3\Exception\S3Exception $e) {
    echo ""There was an error uploading the file.\n"";
}
```

## Getting Help

Please use these community resources for getting help. We use the GitHub issues for tracking bugs and feature requests and have limited bandwidth to address them.

* Ask a question on [StackOverflow](https://stackoverflow.com/) and tag it with [`aws-php-sdk`](http://stackoverflow.com/questions/tagged/aws-php-sdk)
* Come join the AWS SDK for PHP [gitter](https://gitter.im/aws/aws-sdk-php)
* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home/)
* If it turns out that you may have found a bug, please [open an issue](https://github.com/aws/aws-sdk-php/issues/new/choose)

This SDK implements AWS service APIs. For general issues regarding the AWS services and their limitations, you may also take a look at the [Amazon Web Services Discussion Forums](https://forums.aws.amazon.com/).


## Maintenance and support for SDK major versions

For information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the [AWS SDKs and Tools Shared Configuration and Credentials Reference Guide](https://docs.aws.amazon.com/credref/latest/refdocs/overview.html):

* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)
* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)


### Opening Issues

If you encounter a bug with `aws-sdk-php` we would like to hear about it. Search the existing issues and try to make sure your problem doesn’t already exist before opening a new issue. It’s helpful if you include the version of `aws-sdk-php`, PHP version and OS you’re using. Please include a stack trace and a simple workflow to reproduce the case when appropriate, too.

The GitHub issues are intended for bug reports and feature requests. For help and questions with using `aws-sdk-php` please make use of the resources listed in the Getting Help section. There are limited resources available for handling issues and by keeping the list of open issues lean we can respond in a timely manner.

## Features

* Provides easy-to-use HTTP clients for all supported AWS
  [services][docs-services], [regions][docs-rande], and authentication
  protocols.
* Is built on [Guzzle][guzzle-docs], and utilizes many of its features,
  including persistent connections, asynchronous requests, middlewares, etc.
* Provides convenience features including easy result pagination via
  [Paginators][docs-paginators], [Waiters][docs-waiters], and simple
  [Result objects][docs-results].
* Provides a [multipart uploader tool][docs-s3-multipart] for Amazon S3 and
  Amazon Glacier that can be paused and resumed.
* Provides an [Amazon S3 Stream Wrapper][docs-streamwrapper], so that you can
  use PHP's native file handling functions to interact with your S3 buckets and
  objects like a local filesystem.
* Provides an [Amazon S3 Encryption Client][docs-s3-encryption] for creating and interacting with encrypted objects in your S3 buckets.
* Provides the [Amazon DynamoDB Session Handler][docs-ddbsh] for easily scaling
  sessions on a fast, NoSQL database.
* Automatically uses [IAM Instance Profile Credentials][aws-iam-credentials] on
  configured Amazon EC2 instances.

## Contributing

We work hard to provide a high-quality and useful SDK for our AWS services, and we greatly value feedback and contributions from our community. Please review our [contributing guidelines](./CONTRIBUTING.md) before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.


## Resources

* [User Guide][docs-guide] – For both getting started and in-depth SDK usage information
* [API Docs][docs-api] – For details about operations, parameters, and responses
* [Blog][sdk-blog] – Tips & tricks, articles, and announcements
* [Sample Project][sdk-sample] - A quick, sample project to help get you started
* [Forum][sdk-forum] – Ask questions, get help, and give feedback
* [Issues][sdk-issues] – Report issues, submit pull requests, and get involved
  (see [Apache 2.0 License][sdk-license])

## Related AWS Projects

* [AWS Service Provider for Laravel][mod-laravel]
* [AWS SDK ZF2 Module][mod-zf2]
* [AWS Service Provider for Silex][mod-silex]
* [AWS SDK Bundle for Symfony][mod-symfony]
* [Amazon SNS Message Validator for PHP][sns-validator] - SNS validator without requiring SDK
* [Guzzle Version 7][guzzle-docs] – PHP HTTP client and framework
* For Version 2 of the SDK (deprecated):
  * [User Guide][docs-guide-v2]
  * [API Docs][docs-api-v2]
* [Serverless LAMP stack guide][serverless-LAMP-stack-guide] - A guide to building and deploying a serverless PHP application
* Other [AWS SDKs & Tools][aws-tools] (e.g., js, cli, ruby, python, java, etc.)

[sdk-website]: http://aws.amazon.com/sdkforphp
[sdk-forum]: https://forums.aws.amazon.com/forum.jspa?forumID=80
[sdk-issues]: https://github.com/aws/aws-sdk-php/issues
[sdk-license]: http://aws.amazon.com/apache2.0/
[sdk-blog]: https://aws.amazon.com/blogs/developer/category/php/
[sdk-sample]: http://aws.amazon.com/developers/getting-started/php

[install-packagist]: https://packagist.org/packages/aws/aws-sdk-php
[latest-release]: https://github.com/aws/aws-sdk-php/releases

[docs-api]: http://docs.aws.amazon.com/aws-sdk-php/v3/api/index.html
[docs-guide]: http://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/welcome.html
[docs-api-v2]: http://docs.aws.amazon.com/aws-sdk-php/v2/api/index.html
[docs-guide-v2]: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/index.html
[docs-contribution]: https://github.com/aws/aws-sdk-php/blob/master/CONTRIBUTING.md
[docs-migration]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_migration.html
[docs-signup]: https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key/
[docs-requirements]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_requirements.html
[docs-installation]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_installation.html
[docs-quickstart]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/welcome.html#getting-started
[docs-paginators]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_paginators.html
[docs-waiters]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_waiters.html
[docs-results]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_basic-usage.html#result-objects
[docs-exceptions]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_basic-usage.html#handling-errors
[docs-wire-logging]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/faq.html#how-can-i-see-what-data-is-sent-over-the-wire
[docs-ddbsh]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/service_dynamodb-session-handler.html
[docs-services]: https://aws.amazon.com/products/
[docs-rande]: http://docs.aws.amazon.com/general/latest/gr/rande.html
[docs-streamwrapper]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/s3-stream-wrapper.html
[docs-s3-transfer]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/s3-transfer.html
[docs-s3-multipart]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/s3-multipart-upload.html
[docs-s3-encryption]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/s3-encryption-client.html
[docs-script-composer]: https://github.com/aws/aws-sdk-php/tree/master/src/Script/Composer

[aws]: http://aws.amazon.com
[aws-iam-credentials]: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/UsingIAM.html#UsingIAMrolesWithAmazonEC2Instances
[aws-tools]: http://aws.amazon.com/tools
[guzzle-docs]: http://guzzlephp.org
[composer]: http://getcomposer.org
[packagist]: http://packagist.org
[psr-7]: https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-7-http-message.md
[psr-4]: https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-4-autoloader.md
[psr-1]: https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-1-basic-coding-standard.md
[psr-2]: https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-2-coding-style-guide.md

[mod-laravel]: https://github.com/aws/aws-sdk-php-laravel
[mod-zf2]: https://github.com/aws/aws-sdk-php-zf2
[mod-silex]: https://github.com/aws/aws-sdk-php-silex
[mod-symfony]: https://github.com/aws/aws-sdk-php-symfony
[sns-validator]: https://github.com/aws/aws-php-sns-message-validator
[serverless-LAMP-stack-guide]: https://github.com/aws-samples/php-examples-for-aws-lambda",FAUX
aws/aws-sdk-ruby,Toolkit,Toolkit,2025-05-15T18:21:03Z,2025-04-18T18:10:49Z,0,0,0,0,0,0,13,0,2011-07-14T22:21:47Z,2025-04-08T00:15:46Z,505201,3607,Ruby,VRAI,1220,FAUX,26,"aws,aws-sdk,cloud,ruby",26,The official AWS SDK for Ruby,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,251,"# AWS SDK for Ruby - Version 3

[![Gem Version](https://badge.fury.io/rb/aws-sdk-core.svg)](https://badge.fury.io/rb/aws-sdk-core)
[![Build Status](https://github.com/aws/aws-sdk-ruby/workflows/CI/badge.svg)](https://github.com/aws/aws-sdk-ruby/actions)
[![Github forks](https://img.shields.io/github/forks/aws/aws-sdk-ruby.svg)](https://github.com/aws/aws-sdk-ruby/network)
[![Github stars](https://img.shields.io/github/stars/aws/aws-sdk-ruby.svg)](https://github.com/aws/aws-sdk-ruby/stargazers)

## Links of Interest

* [API Documentation](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/index.html)
* [Developer Guide](https://docs.aws.amazon.com/sdk-for-ruby/v3/developer-guide/welcome.html)
* [V3 Upgrading Guide](https://github.com/aws/aws-sdk-ruby/blob/version-3/V3_UPGRADING_GUIDE.md)
* [AWS Developer Blog](https://aws.amazon.com/blogs/developer/category/programing-language/ruby/)
* [Github Discussions](https://github.com/aws/aws-sdk-ruby/discussions)

## Installation

The AWS SDK for Ruby is available from RubyGems. With V3 modularization, you
should pick the specific AWS service gems to install.

```ruby
gem 'aws-sdk-s3', '~> 1'
gem 'aws-sdk-ec2', '~> 1'
```

Alternatively, the `aws-sdk` gem contains every available AWS service gem. This
gem is very large; it is recommended to use it only as a quick way to migrate
from V2 or if you depend on many AWS services.

```ruby
gem 'aws-sdk', '~> 3'
```

**Please use a pessimistic version constraint on the major version when
depending on service gems.**

## Configuration

You will need to configure credentials and a region, either in 
[configuration files](https://docs.aws.amazon.com/sdkref/latest/guide/file-location.html)
or environment variables, to make API calls. It is recommended that you
provide these via your environment. This makes it easier to rotate credentials
and it keeps your secrets out of source control. 

The SDK searches the following locations for credentials:

* `ENV['AWS_ACCESS_KEY_ID']` and `ENV['AWS_SECRET_ACCESS_KEY']`
* The shared credentials ini file at `~/.aws/credentials`.  The location used can be changed with the `AWS_CREDENTIALS_FILE` ENV variable.
  * Credential options supported in this file are:
    * Static Credentials (`aws_access_key_id`, `aws_secret_access_key`, `aws_session_token`)
    * Assume Role Web Identity Credentials (`web_identity_token_file`, `role_arn`, `source_profile`)
    * Assume Role Credentials (`role_arn`, `source_profile`)
    * Process Credentials (`credential_process`)
    * SSO Credentials (`sso_session`, `sso_account_id`, `sso_role_name`, `sso_region`)
  * Unless `ENV['AWS_SDK_CONFIG_OPT_OUT']` is set, the shared configuration ini file at `~/.aws/config` will also be parsed for credentials.
* From an instance profile when running on EC2 or from the ECS credential provider when running in an ECS container with that feature enabled.

**Shared configuration is loaded only a single time, and credentials are provided statically at client creation time. Shared credentials do not refresh.**

The SDK searches the following locations for a region:

* `ENV['AWS_REGION']`
* `ENV['AMAZON_REGION']`
* `ENV['AWS_DEFAULT_REGION']`
* Unless `ENV['AWS_SDK_CONFIG_OPT_OUT']` is set, the shared configuration files (`~/.aws/credentials` and `~/.aws/config`) will also be checked for a region selection.

**The region is used to construct an SSL endpoint**. If you need to connect to a non-standard endpoint, you may specify the `:endpoint` option.

### Configuration Options

You can also configure default credentials and the region via the `Aws.config`
hash. The `Aws.config` hash takes precedence over environment variables.

```ruby
require 'aws-sdk-core'

Aws.config.update(
  region: 'us-west-2',
  credentials: Aws::Credentials.new('akid', 'secret')
)
```

Valid region and credentials options are:

* `:region` - A string like `us-west-2`. See [this page](https://docs.aws.amazon.com/general/latest/gr/aws-service-information.html) for a list of supported regions by service.
* `:credentials` - An instance of one of the following classes:
  * [`Aws::Credentials`](http://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/Credentials.html)
  * [`Aws::AssumeRoleWebIdentityCredentials`](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/AssumeRoleWebIdentityCredentials.html)
  * [`Aws::AssumeRoleCredentials`](http://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/AssumeRoleCredentials.html)
  * [`Aws::SharedCredentials`](http://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/SharedCredentials.html)
  * [`Aws::ProcessCredentials`](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/ProcessCredentials.html)
  * [`Aws::InstanceProfileCredentials`](http://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/InstanceProfileCredentials.html)
  * [`Aws::ECSCredentials`](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/ECSCredentials.html)
  * [`Aws::CognitoIdentityCredentials`](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/CognitoIdentity/CognitoIdentityCredentials.html)

You may also pass configuration options directly to Client and Resource
constructors. These options take precedence over the environment and
`Aws.config` defaults. A `:profile` Client option can also be used to choose a
specific profile defined in your configuration file.

```ruby
# using a credentials object
ec2 = Aws::EC2::Client.new(region: 'us-west-2', credentials: credentials)

# using a profile name
ec2 = Aws::EC2::Client.new(profile: 'my_profile')
```

Please take care to **never commit credentials to source control**. We strongly
recommended loading credentials from an external source.

```ruby
require 'aws-sdk'
require 'json'

creds = JSON.load(File.read('secrets.json'))
Aws.config[:credentials] = Aws::Credentials.new(
  creds['AccessKeyId'],
  creds['SecretAccessKey']
)
```

For more information on how to configure credentials, see the developer guide
for [configuring AWS SDK for Ruby](https://docs.aws.amazon.com/sdk-for-ruby/v3/developer-guide/setup-config.html).

## API Clients

Construct a service client to make API calls. Each client provides a 1-to-1
mapping of methods to API operations. Refer to the
[API documentation](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/index.html)
for a complete list of available methods.

```ruby
# list buckets in Amazon S3
s3 = Aws::S3::Client.new
resp = s3.list_buckets
resp.buckets.map(&:name)
#=> [""bucket-1"", ""bucket-2"", ...]
```

API methods accept a hash of additional request parameters and return
structured response data.

```ruby
# list the first two objects in a bucket
resp = s3.list_objects(bucket: 'aws-sdk', max_keys: 2)
resp.contents.each do |object|
  puts ""#{object.key} => #{object.etag}""
end
```

### Paging Responses

Many AWS operations limit the number of results returned with each response.
To make it easy to get the next page of results, every AWS response object
is enumerable:

```ruby
# yields one response object per API call made, this will enumerate
# EVERY object in the named bucket
s3.list_objects(bucket:'aws-sdk').each do |response|
  puts response.contents.map(&:key)
end
```

If you prefer to control paging yourself, response objects have helper methods
that control paging:

```ruby
# make a request that returns a truncated response
resp = s3.list_objects(bucket: 'aws-sdk')

resp.last_page? #=> false
resp.next_page? #=> true
resp = resp.next_page # send a request for the next response page
resp = resp.next_page until resp.last_page?
```

### Waiters

Waiters are utility methods that poll for a particular state. To invoke a
waiter, call `#wait_until` on a client:

```ruby
begin
  ec2.wait_until(:instance_running, instance_ids:['i-12345678'])
  puts ""instance running""
rescue Aws::Waiters::Errors::WaiterFailed => error
  puts ""failed waiting for instance running: #{error.message}""
end
```

Waiters have sensible default polling intervals and maximum attempts. You can
configure these per call to `#wait_until`. You can also register callbacks
that are triggered before each polling attempt and before waiting.
See the API documentation for more examples and for a list of supported
waiters per service.

## Resource Interfaces

Resource interfaces are object oriented classes that represent actual
resources in AWS. Resource interfaces built on top of API clients and provide
additional functionality.

**Only a few services implement a resource interface. They are defined by hand
in JSON and have limitations. Please use the Client API instead.**

```ruby
s3 = Aws::S3::Resource.new

# reference an existing bucket by name
bucket = s3.bucket('aws-sdk')

# enumerate every object in a bucket
bucket.objects.each do |obj|
  puts ""#{obj.key} => #{obj.etag}""
end

# batch operations, delete objects in batches of 1k
bucket.objects(prefix: '/tmp-files/').delete

# single object operations
obj = bucket.object('hello')
obj.put(body:'Hello World!')
obj.etag
obj.delete
```

## REPL - AWS Interactive Console

The `aws-sdk` gem ships with a REPL that provides a simple way to test
the Ruby SDK. You can access the REPL by running `aws-v3.rb` from the command line.

```ruby
$ aws-v3.rb
[1] pry(Aws)> ec2.describe_instances.reservations.first.instances.first
[Aws::EC2::Client 200 0.216615 0 retries] describe_instances()
<struct
 instance_id=""i-1234567"",
 image_id=""ami-7654321"",
 state=<struct  code=16, name=""running"">,
 ...>
```

You can enable HTTP wire logging by setting the verbose flag:

```
$ aws-v3.rb -v
```

In the REPL, every service class has a helper that returns a new client object.
Simply downcase the service module name for the helper:

* `s3` => `#<Aws::S3::Client>`
* `ec2` => `#<Aws::EC2::Client>`
* etc

## Functionality requiring AWS Common Runtime (CRT)

The AWS SDK for Ruby has optional functionality that requires the 
[AWS Common Runtime (CRT)](https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html) 
bindings to be included as a dependency with your application. This functionality includes:
* CRC-32c support for [S3 Additional Checksums](https://aws.amazon.com/blogs/aws/new-additional-checksum-algorithms-for-amazon-s3/)

AWS CRT bindings are in developer preview and are available in the
the [aws-crt](https://rubygems.org/gems/aws-crt/) gem.  You can install them by adding the `aws-crt` gem to your Gemfile.

## Getting Help

Please use any of these resources for getting help:

* Ask a question on [Github Discussions](https://github.com/aws/aws-sdk-ruby/discussions).
* Ask a question on StackOverflow and [tag it](http://stackoverflow.com/questions/tagged/aws-sdk-ruby) with `aws-sdk-ruby`.
* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home).

## Maintenance and support for SDK major versions

For information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the [AWS SDKs and Tools Shared Configuration and Credentials Reference Guide](https://docs.aws.amazon.com/credref/latest/refdocs/overview.html):

* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)
* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)

## Opening Issues

If you encounter a bug or have a feature request, we would like to hear about
it. Search the existing issues and try to make sure your problem doesn’t already
exist before opening a new issue.

The GitHub issues are intended for bug reports and feature requests. For help
and questions with using `aws-sdk-ruby` please make use of the resources listed
in the Getting Help section.

## Versioning

This project uses [semantic versioning](http://semver.org/). You can safely
express a dependency on a major version and expect all minor and patch versions
to be backwards compatible.

A CHANGELOG can be found at each gem's root path (i.e. `aws-sdk-s3` can be found
at `gems/aws-sdk-s3/CHANGELOG.md`). The CHANGELOG is also accessible via the
RubyGems.org page under ""LINKS"" section.

## Supported Services

| Service Name                                          | Service Module                           | gem_name                                    | API Version |
| ----------------------------------------------------- | ---------------------------------------- | ------------------------------------------- | ----------- |
| AWS ARC - Zonal Shift                                 | Aws::ARCZonalShift                       | aws-sdk-arczonalshift                       | 2022-10-30  |
| AWS Account                                           | Aws::Account                             | aws-sdk-account                             | 2021-02-01  |
| AWS Amplify                                           | Aws::Amplify                             | aws-sdk-amplify                             | 2017-07-25  |
| AWS Amplify UI Builder                                | Aws::AmplifyUIBuilder                    | aws-sdk-amplifyuibuilder                    | 2021-08-11  |
| AWS App Mesh                                          | Aws::AppMesh                             | aws-sdk-appmesh                             | 2019-01-25  |
| AWS App Runner                                        | Aws::AppRunner                           | aws-sdk-apprunner                           | 2020-05-15  |
| AWS AppConfig Data                                    | Aws::AppConfigData                       | aws-sdk-appconfigdata                       | 2021-11-11  |
| AWS AppSync                                           | Aws::AppSync                             | aws-sdk-appsync                             | 2017-07-25  |
| AWS Application Cost Profiler                         | Aws::ApplicationCostProfiler             | aws-sdk-applicationcostprofiler             | 2020-09-10  |
| AWS Application Discovery Service                     | Aws::ApplicationDiscoveryService         | aws-sdk-applicationdiscoveryservice         | 2015-11-01  |
| AWS Artifact                                          | Aws::Artifact                            | aws-sdk-artifact                            | 2018-05-10  |
| AWS Audit Manager                                     | Aws::AuditManager                        | aws-sdk-auditmanager                        | 2017-07-25  |
| AWS Auto Scaling Plans                                | Aws::AutoScalingPlans                    | aws-sdk-autoscalingplans                    | 2018-01-06  |
| AWS B2B Data Interchange                              | Aws::B2bi                                | aws-sdk-b2bi                                | 2022-06-23  |
| AWS Backup                                            | Aws::Backup                              | aws-sdk-backup                              | 2018-11-15  |
| AWS Backup Gateway                                    | Aws::BackupGateway                       | aws-sdk-backupgateway                       | 2021-01-01  |
| AWS Backup Search                                     | Aws::BackupSearch                        | aws-sdk-backupsearch                        | 2018-05-10  |
| AWS Batch                                             | Aws::Batch                               | aws-sdk-batch                               | 2016-08-10  |
| AWS Billing                                           | Aws::Billing                             | aws-sdk-billing                             | 2023-09-07  |
| AWS Billing and Cost Management Data Exports          | Aws::BCMDataExports                      | aws-sdk-bcmdataexports                      | 2023-11-26  |
| AWS Billing and Cost Management Pricing Calculator    | Aws::BCMPricingCalculator                | aws-sdk-bcmpricingcalculator                | 2024-06-19  |
| AWS Budgets                                           | Aws::Budgets                             | aws-sdk-budgets                             | 2016-10-20  |
| AWS Certificate Manager                               | Aws::ACM                                 | aws-sdk-acm                                 | 2015-12-08  |
| AWS Certificate Manager Private Certificate Authority | Aws::ACMPCA                              | aws-sdk-acmpca                              | 2017-08-22  |
| AWS Chatbot                                           | Aws::Chatbot                             | aws-sdk-chatbot                             | 2017-10-11  |
| AWS Clean Rooms ML                                    | Aws::CleanRoomsML                        | aws-sdk-cleanroomsml                        | 2023-09-06  |
| AWS Clean Rooms Service                               | Aws::CleanRooms                          | aws-sdk-cleanrooms                          | 2022-02-17  |
| AWS Cloud Control API                                 | Aws::CloudControlApi                     | aws-sdk-cloudcontrolapi                     | 2021-09-30  |
| AWS Cloud Map                                         | Aws::ServiceDiscovery                    | aws-sdk-servicediscovery                    | 2017-03-14  |
| AWS Cloud9                                            | Aws::Cloud9                              | aws-sdk-cloud9                              | 2017-09-23  |
| AWS CloudFormation                                    | Aws::CloudFormation                      | aws-sdk-cloudformation                      | 2010-05-15  |
| AWS CloudHSM V2                                       | Aws::CloudHSMV2                          | aws-sdk-cloudhsmv2                          | 2017-04-28  |
| AWS CloudTrail                                        | Aws::CloudTrail                          | aws-sdk-cloudtrail                          | 2013-11-01  |
| AWS CloudTrail Data Service                           | Aws::CloudTrailData                      | aws-sdk-cloudtraildata                      | 2021-08-11  |
| AWS CodeBuild                                         | Aws::CodeBuild                           | aws-sdk-codebuild                           | 2016-10-06  |
| AWS CodeCommit                                        | Aws::CodeCommit                          | aws-sdk-codecommit                          | 2015-04-13  |
| AWS CodeConnections                                   | Aws::CodeConnections                     | aws-sdk-codeconnections                     | 2023-12-01  |
| AWS CodeDeploy                                        | Aws::CodeDeploy                          | aws-sdk-codedeploy                          | 2014-10-06  |
| AWS CodePipeline                                      | Aws::CodePipeline                        | aws-sdk-codepipeline                        | 2015-07-09  |
| AWS CodeStar Notifications                            | Aws::CodeStarNotifications               | aws-sdk-codestarnotifications               | 2019-10-15  |
| AWS CodeStar connections                              | Aws::CodeStarconnections                 | aws-sdk-codestarconnections                 | 2019-12-01  |
| AWS Comprehend Medical                                | Aws::ComprehendMedical                   | aws-sdk-comprehendmedical                   | 2018-10-30  |
| AWS Compute Optimizer                                 | Aws::ComputeOptimizer                    | aws-sdk-computeoptimizer                    | 2019-11-01  |
| AWS Config                                            | Aws::ConfigService                       | aws-sdk-configservice                       | 2014-11-12  |
| AWS Control Catalog                                   | Aws::ControlCatalog                      | aws-sdk-controlcatalog                      | 2018-05-10  |
| AWS Control Tower                                     | Aws::ControlTower                        | aws-sdk-controltower                        | 2018-05-10  |
| AWS Cost Explorer Service                             | Aws::CostExplorer                        | aws-sdk-costexplorer                        | 2017-10-25  |
| AWS Cost and Usage Report Service                     | Aws::CostandUsageReportService           | aws-sdk-costandusagereportservice           | 2017-01-06  |
| AWS Data Exchange                                     | Aws::DataExchange                        | aws-sdk-dataexchange                        | 2017-07-25  |
| AWS Data Pipeline                                     | Aws::DataPipeline                        | aws-sdk-datapipeline                        | 2012-10-29  |
| AWS DataSync                                          | Aws::DataSync                            | aws-sdk-datasync                            | 2018-11-09  |
| AWS Database Migration Service                        | Aws::DatabaseMigrationService            | aws-sdk-databasemigrationservice            | 2016-01-01  |
| AWS Device Farm                                       | Aws::DeviceFarm                          | aws-sdk-devicefarm                          | 2015-06-23  |
| AWS Direct Connect                                    | Aws::DirectConnect                       | aws-sdk-directconnect                       | 2012-10-25  |
| AWS Directory Service                                 | Aws::DirectoryService                    | aws-sdk-directoryservice                    | 2015-04-16  |
| AWS Directory Service Data                            | Aws::DirectoryServiceData                | aws-sdk-directoryservicedata                | 2023-05-31  |
| AWS EC2 Instance Connect                              | Aws::EC2InstanceConnect                  | aws-sdk-ec2instanceconnect                  | 2018-04-02  |
| AWS Elastic Beanstalk                                 | Aws::ElasticBeanstalk                    | aws-sdk-elasticbeanstalk                    | 2010-12-01  |
| AWS Elemental MediaConvert                            | Aws::MediaConvert                        | aws-sdk-mediaconvert                        | 2017-08-29  |
| AWS Elemental MediaLive                               | Aws::MediaLive                           | aws-sdk-medialive                           | 2017-10-14  |
| AWS Elemental MediaPackage                            | Aws::MediaPackage                        | aws-sdk-mediapackage                        | 2017-10-12  |
| AWS Elemental MediaPackage VOD                        | Aws::MediaPackageVod                     | aws-sdk-mediapackagevod                     | 2018-11-07  |
| AWS Elemental MediaPackage v2                         | Aws::MediaPackageV2                      | aws-sdk-mediapackagev2                      | 2022-12-25  |
| AWS Elemental MediaStore                              | Aws::MediaStore                          | aws-sdk-mediastore                          | 2017-09-01  |
| AWS Elemental MediaStore Data Plane                   | Aws::MediaStoreData                      | aws-sdk-mediastoredata                      | 2017-09-01  |
| AWS End User Messaging Social                         | Aws::SocialMessaging                     | aws-sdk-socialmessaging                     | 2024-01-01  |
| AWS EntityResolution                                  | Aws::EntityResolution                    | aws-sdk-entityresolution                    | 2018-05-10  |
| AWS Fault Injection Simulator                         | Aws::FIS                                 | aws-sdk-fis                                 | 2020-12-01  |
| AWS Free Tier                                         | Aws::FreeTier                            | aws-sdk-freetier                            | 2023-09-07  |
| AWS Global Accelerator                                | Aws::GlobalAccelerator                   | aws-sdk-globalaccelerator                   | 2018-08-08  |
| AWS Glue                                              | Aws::Glue                                | aws-sdk-glue                                | 2017-03-31  |
| AWS Glue DataBrew                                     | Aws::GlueDataBrew                        | aws-sdk-gluedatabrew                        | 2017-07-25  |
| AWS Greengrass                                        | Aws::Greengrass                          | aws-sdk-greengrass                          | 2017-06-07  |
| AWS Ground Station                                    | Aws::GroundStation                       | aws-sdk-groundstation                       | 2019-05-23  |
| AWS Health APIs and Notifications                     | Aws::Health                              | aws-sdk-health                              | 2016-08-04  |
| AWS Health Imaging                                    | Aws::MedicalImaging                      | aws-sdk-medicalimaging                      | 2023-07-19  |
| AWS Identity and Access Management                    | Aws::IAM                                 | aws-sdk-iam                                 | 2010-05-08  |
| AWS Import/Export                                     | Aws::ImportExport                        | aws-sdk-importexport                        | 2010-06-01  |
| AWS Invoicing                                         | Aws::Invoicing                           | aws-sdk-invoicing                           | 2024-12-01  |
| AWS IoT                                               | Aws::IoT                                 | aws-sdk-iot                                 | 2015-05-28  |
| AWS IoT Analytics                                     | Aws::IoTAnalytics                        | aws-sdk-iotanalytics                        | 2017-11-27  |
| AWS IoT Core Device Advisor                           | Aws::IoTDeviceAdvisor                    | aws-sdk-iotdeviceadvisor                    | 2020-09-18  |
| AWS IoT Data Plane                                    | Aws::IoTDataPlane                        | aws-sdk-iotdataplane                        | 2015-05-28  |
| AWS IoT Events                                        | Aws::IoTEvents                           | aws-sdk-iotevents                           | 2018-07-27  |
| AWS IoT Events Data                                   | Aws::IoTEventsData                       | aws-sdk-ioteventsdata                       | 2018-10-23  |
| AWS IoT Fleet Hub                                     | Aws::IoTFleetHub                         | aws-sdk-iotfleethub                         | 2020-11-03  |
| AWS IoT FleetWise                                     | Aws::IoTFleetWise                        | aws-sdk-iotfleetwise                        | 2021-06-17  |
| AWS IoT Greengrass V2                                 | Aws::GreengrassV2                        | aws-sdk-greengrassv2                        | 2020-11-30  |
| AWS IoT Jobs Data Plane                               | Aws::IoTJobsDataPlane                    | aws-sdk-iotjobsdataplane                    | 2017-09-29  |
| AWS IoT Secure Tunneling                              | Aws::IoTSecureTunneling                  | aws-sdk-iotsecuretunneling                  | 2018-10-05  |
| AWS IoT SiteWise                                      | Aws::IoTSiteWise                         | aws-sdk-iotsitewise                         | 2019-12-02  |
| AWS IoT Things Graph                                  | Aws::IoTThingsGraph                      | aws-sdk-iotthingsgraph                      | 2018-09-06  |
| AWS IoT TwinMaker                                     | Aws::IoTTwinMaker                        | aws-sdk-iottwinmaker                        | 2021-11-29  |
| AWS IoT Wireless                                      | Aws::IoTWireless                         | aws-sdk-iotwireless                         | 2020-11-22  |
| AWS Key Management Service                            | Aws::KMS                                 | aws-sdk-kms                                 | 2014-11-01  |
| AWS Lake Formation                                    | Aws::LakeFormation                       | aws-sdk-lakeformation                       | 2017-03-31  |
| AWS Lambda                                            | Aws::Lambda                              | aws-sdk-lambda                              | 2015-03-31  |
| AWS Launch Wizard                                     | Aws::LaunchWizard                        | aws-sdk-launchwizard                        | 2018-05-10  |
| AWS License Manager                                   | Aws::LicenseManager                      | aws-sdk-licensemanager                      | 2018-08-01  |
| AWS License Manager Linux Subscriptions               | Aws::LicenseManagerLinuxSubscriptions    | aws-sdk-licensemanagerlinuxsubscriptions    | 2018-05-10  |
| AWS License Manager User Subscriptions                | Aws::LicenseManagerUserSubscriptions     | aws-sdk-licensemanagerusersubscriptions     | 2018-05-10  |
| AWS Mainframe Modernization Application Testing       | Aws::AppTest                             | aws-sdk-apptest                             | 2022-12-06  |
| AWS Marketplace Agreement Service                     | Aws::MarketplaceAgreement                | aws-sdk-marketplaceagreement                | 2020-03-01  |
| AWS Marketplace Catalog Service                       | Aws::MarketplaceCatalog                  | aws-sdk-marketplacecatalog                  | 2018-09-17  |
| AWS Marketplace Commerce Analytics                    | Aws::MarketplaceCommerceAnalytics        | aws-sdk-marketplacecommerceanalytics        | 2015-07-01  |
| AWS Marketplace Deployment Service                    | Aws::MarketplaceDeployment               | aws-sdk-marketplacedeployment               | 2023-01-25  |
| AWS Marketplace Entitlement Service                   | Aws::MarketplaceEntitlementService       | aws-sdk-marketplaceentitlementservice       | 2017-01-11  |
| AWS Marketplace Reporting Service                     | Aws::MarketplaceReporting                | aws-sdk-marketplacereporting                | 2018-05-10  |
| AWS MediaConnect                                      | Aws::MediaConnect                        | aws-sdk-mediaconnect                        | 2018-11-14  |
| AWS MediaTailor                                       | Aws::MediaTailor                         | aws-sdk-mediatailor                         | 2018-04-23  |
| AWS Migration Hub                                     | Aws::MigrationHub                        | aws-sdk-migrationhub                        | 2017-05-31  |
| AWS Migration Hub Config                              | Aws::MigrationHubConfig                  | aws-sdk-migrationhubconfig                  | 2019-06-30  |
| AWS Migration Hub Orchestrator                        | Aws::MigrationHubOrchestrator            | aws-sdk-migrationhuborchestrator            | 2021-08-28  |
| AWS Migration Hub Refactor Spaces                     | Aws::MigrationHubRefactorSpaces          | aws-sdk-migrationhubrefactorspaces          | 2021-10-26  |
| AWS Network Firewall                                  | Aws::NetworkFirewall                     | aws-sdk-networkfirewall                     | 2020-11-12  |
| AWS Network Manager                                   | Aws::NetworkManager                      | aws-sdk-networkmanager                      | 2019-07-05  |
| AWS OpsWorks                                          | Aws::OpsWorks                            | aws-sdk-opsworks                            | 2013-02-18  |
| AWS OpsWorks CM                                       | Aws::OpsWorksCM                          | aws-sdk-opsworkscm                          | 2016-11-01  |
| AWS Organizations                                     | Aws::Organizations                       | aws-sdk-organizations                       | 2016-11-28  |
| AWS Outposts                                          | Aws::Outposts                            | aws-sdk-outposts                            | 2019-12-03  |
| AWS Panorama                                          | Aws::Panorama                            | aws-sdk-panorama                            | 2019-07-24  |
| AWS Parallel Computing Service                        | Aws::PCS                                 | aws-sdk-pcs                                 | 2023-02-10  |
| AWS Performance Insights                              | Aws::PI                                  | aws-sdk-pi                                  | 2018-02-27  |
| AWS Price List Service                                | Aws::Pricing                             | aws-sdk-pricing                             | 2017-10-15  |
| AWS Private 5G                                        | Aws::PrivateNetworks                     | aws-sdk-privatenetworks                     | 2021-12-03  |
| AWS Proton                                            | Aws::Proton                              | aws-sdk-proton                              | 2020-07-20  |
| AWS RDS DataService                                   | Aws::RDSDataService                      | aws-sdk-r",FAUX
Azure-Samples/aks-labs,Documentations,Documentations,2025-05-14T14:47:11Z,2025-04-22T20:10:24Z,0,0,0,0,0,0,0,1,2025-01-15T17:05:51Z,2025-04-07T11:08:01Z,54203,18,TypeScript,VRAI,14,FAUX,6,,6,Grab and go resources for hands-on learning about Azure Kubernetes Service (AKS),FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,8,"# AKS-LABS

Grab and go resources to learn about Kubernetes and Azure Kubernetes Service (AKS)

The labs are organized by topic and are designed to be hands-on, with step-by-step instructions.

## Getting Started

There are a couple of ways to get started with the labs.

1. You can run the [Setting Up the Lab Environment](https://azure-samples.github.io/aks-labs/docs/getting-started/setting-up-lab-environment)

2. You can run the [Kubernetes the Easy Way with AKS Automatic](https://azure-samples.github.io/aks-labs/docs/getting-started/aks-automatic)

In either case, you will wind up a AKS cluster running in Azure on which you can run any of the labs.",FAUX
Azure-Samples/azure-digital-twins-getting-started,Documentations,Documentations,2024-10-16T14:57:59Z,2023-01-31T18:06:07Z,2,0,0,0,0,0,0,0,2021-03-22T16:07:14Z,2025-03-11T10:38:03Z,10329,28,ShaderLab,VRAI,25,FAUX,6,,6,Resources for getting started with Azure Digital Twins,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,12,"---
page_type: sample
languages:
- json
products:
- azure-digital-twins
name: Azure Digital Twins getting started samples
description: Contains getting started samples for Azure Digital Twins.
urlFragment: digital-twins-getting-started
---

# Azure Digital Twins getting started samples

These samples can help you get started with Azure Digital Twins.

## Contents

This project contains the following folders.

| Folder | Description |
| --- | --- |
| [3dscenes](/3dscenes/) | Sample files to be used with 3D Scenes Studio. |
| [ISS](/ISS) | Project to track the International Space Station with Azure Digital Twins and Azure Data Explorer. |
| [adt-adx-queries](/adt-adx-queries) | Contains sample queries that can be run with the Azure Digital Twins query plugin for Azure Data Explorer. |
| [azure-functions](/azure-functions) | Contains sample Azure functions that can be used (as an example) with Azure Digital Twins for managing data ingress and twin updates. |
| [bulk-import](/ndjson-generator) | Contains helper code to generate *ndjson* files that can be used for bulk import. |
| [models](/models) | Contains sets of ""getting started"" models to illustrate how certain subjects might be modeled in Digital Twins Definition Language (DTDL). |


## Resources

- [Azure Digital Twins documentation](https://learn.microsoft.com/azure/digital-twins/)
- [**DTDL Models** in the Azure Digital Twins documentation](https://learn.microsoft.com/azure/digital-twins/concepts-models)
- [DTDL v2 spec](https://github.com/Azure/opendigitaltwins-dtdl/blob/master/DTDL/v2/dtdlv2.md)",FAUX
Azure/aks-app-routing-operator,DevOPs,DevOPs,2025-05-13T17:03:48Z,2025-03-14T19:25:26Z,0,0,0,0,0,0,0,45,2022-03-29T17:30:46Z,2025-03-27T20:54:49Z,1749,33,Go,VRAI,21,FAUX,35,,35,Kubernetes operator that implements AKS Application Routing,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,16,"# AKS Web Application Routing Operator

[![Coverage Status](https://coveralls.io/repos/github/Azure/aks-app-routing-operator/badge.svg?branch=main)](https://coveralls.io/github/Azure/aks-app-routing-operator?branch=main)

A Kubernetes operator that manages resources related to AKS Application Routing functionality.

## Docs

View the [docs](docs/) folder for more information.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.",FAUX
Azure/aks-engine-azurestack,DevOPs,DevOPs,2025-04-29T21:23:19Z,2024-11-20T03:31:23Z,0,0,0,0,0,0,0,1,2022-08-24T18:31:04Z,2025-04-01T22:36:12Z,112056,19,Go,VRAI,13,FAUX,10,,10,AKS Engine: Units of Kubernetes on Azure Stack Hub!,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,364,"# AKS Engine - Units of Kubernetes on Azure Stack Hub

## Overview

AKS Engine is an ARM template-driven way to provision a self-managed Kubernetes cluster on Azure Stack Hub. By leveraging [ARM (Azure Resource Manager)][ARM], AKS Engine helps you create, destroy and maintain clusters provisioned with basic IaaS resources in Azure Stack Hub.

## Getting started

- Read the [CLI Overview](docs/tutorials/cli-overview.md) for a list of features provided by the `aks-engine-azurestack` command line tool.

- The [Quickstart Guide](docs/tutorials/quickstart.md) describes how to download the latest release of `aks-engine-azurestack` for your environment, and demonstrates how to use `aks-engine-azurestack` to create a Kubernetes cluster on Azure Stack Hub that you will manage and customize.

- The complete body of documentation can be found [here][docs] and [here][ms-docs].

Please see the [FAQ][] for answers about AKS Engine.

## Join the community

Want to get involved? The [community guide][community] covers everything you need to know about the AKS Engine community and how you can contribute. The [developer guide][developer-guide] will help you onboard as a developer.

## Support

Please see our [support policy][support-policy].

## Code of conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Data Collection

The software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry [as described in the repository][telemetry-config]. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.

For more information, please see the [telemetry documentation][telemetry].

[ARM]: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview
[community]: docs/community/README.md
[developer-guide]: docs/community/developer-guide.md
[docs]: docs/README.md
[FAQ]: docs/faq.md
[support-policy]: SUPPORT.md
[telemetry]: docs/topics/telemetry.md
[telemetry-config]: docs/topics/telemetry.md#configuration
[ms-docs]: https://learn.microsoft.com/azure-stack/user/azure-stack-kubernetes-aks-engine-overview",FAUX
Azure/ARO-RP,Toolkit,DevOPs,2025-05-15T20:47:18Z,2025-04-24T00:26:33Z,0,11,0,0,0,0,0,7,2019-11-29T15:27:53Z,2025-04-08T02:37:41Z,172510,105,Go,VRAI,182,FAUX,148,,148,Azure Red Hat OpenShift RP,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,153,"# Azure Red Hat OpenShift Resource Provider

## Welcome!

For information relating to the generally available Azure Red Hat OpenShift v4
service, please see the following links:

* https://azure.microsoft.com/en-us/services/openshift/
* https://www.openshift.com/products/azure-openshift
* https://docs.microsoft.com/en-us/azure/openshift/
* https://docs.openshift.com/aro/4/welcome/index.html


## Quickstarts

* If you are an end user and want to create an Azure Red Hat OpenShift 4
  cluster, follow [Create, access, and manage an Azure Red Hat OpenShift 4
  Cluster][1].

* If you want to deploy a development RP, follow [deploy development
  RP](docs/deploy-development-rp.md).

[1]: https://docs.microsoft.com/en-us/azure/openshift/howto-using-azure-redhat-openshift

## Contributing

This project welcomes contributions and suggestions. Most contributions require
you to agree to a Contributor License Agreement (CLA) declaring that you have
the right to, and actually do, grant us the rights to use your contribution. For
details, visit https://cla.microsoft.com.

Before you start development, please set up your local git hooks to conform to our
development standards:

```bash
make init-contrib
```

When you submit a pull request, a CLA-bot will automatically determine whether
you need to provide a CLA and decorate the PR appropriately (e.g., label,
comment). Simply follow the instructions provided by the bot. You will only need
to do this once across all repositories using our CLA.

This project has adopted the [Microsoft Open Source Code of
Conduct](https://opensource.microsoft.com/codeofconduct/). For more information
see the [Code of Conduct
FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact
[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional
questions or comments.


## Repository map

* .pipelines: CI workflows using Azure pipelines.

* cmd/aro: RP entrypoint.

* deploy: ARM templates to deploy RP in development and production.

* docs: Documentation.

* hack: Build scripts and utilities.

* pkg: RP source code:

  * pkg/api: RP internal and external API definitions.

  * pkg/backend: RP backend workers.

  * pkg/bootstraplogging: Bootstrap logging configuration

  * pkg/client: Autogenerated ARO service Go client.

  * pkg/cluster: Cluster create/update/delete operations wrapper for OCP installer.

  * pkg/database: RP CosmosDB wrapper layer.

  * pkg/deploy: /deploy ARM template generation code.

  * pkg/env: RP environment-specific shims for running in production,
    development or test

  * pkg/frontend: RP frontend webserver.

  * pkg/metrics: Handles RP metrics via statsd.

  * pkg/mirror: OpenShift release mirror tooling.

  * pkg/monitor: Monitors running clusters.

  * pkg/operator/controllers: A list of controllers instantiated by the operator
    component.

    * alertwebhook: Ensures that the receiver endpoint defined in the
      `alertmanager-main` secret matches the webserver endpoint at
      aro-operator-master.openshift-azure-operator:8080, to avoid the
      `AlertmanagerReceiversNotConfigured` warning.


    * checker: Watches the `Cluster` resource for changes and updates conditions
      of the resource based on checks mentioned below

      * internetchecker: validate outbound internet connectivity to the nodes

      * serviceprincipalchecker: validate cluster service principal has the
        correct role/permissions

    * clusteroperatoraro: Ensures that the ARO cluster object is consistent and
      immutable

    * dnsmasq: Ensures that a dnsmasq systemd service is defined as a machineconfig for all
      nodes. The dnsmasq config contains records for azure load balancers such as api, api-int and *.apps domains so they will resolve even if custom DNS on the VNET is set.

    * genevalogging: Ensures all the Geneva logging resources in the
      `openshift-azure-logging` namespace matches the pre-defined specification
      found in `pkg/operator/controllers/genevalogging/genevalogging.go`.

    * imageconfig: Ensures that required registries are not blocked in `image.config`

    * machine: validate machine objects have the correct provider spec,
      vm type, vm image, disk size, three master nodes exist, and the number of worker nodes
      match the desired worker replicas

    * machineset: Ensures that a minimum of two worker replicas are met.

    * machinehealthcheck: Ensures the MachineHealthCheck resource is running as configured. See [machinehealthcheck/doc.go](pkg/operator/controllers/machinehealthcheck/doc.go)
        * More information around the MHC CR can be found [in openshift documentation of MHC](https://docs.openshift.com/container-platform/4.9/machine_management/deploying-machine-health-checks.html)

    * monitoring: Ensures that the OpenShift monitoring configuration in the `openshift-monitoring` namespace is consistent and immutable.

    * node: Force deletes pods when a node fails to drain for 1 hour.  It should clear up any pods that refuse to be evicted on a drain due to violating a pod disruption budget.

    * pullsecret: Ensures that the ACR credentials in the
      `openshift-config/pull-secret` secret match those in the
      `openshift/azure-operator/cluster` secret.

    * rbac: Ensures that the `aro-sre` clusterrole and clusterrolebinding exist and are consistent.

    * routefix: Ensures all the routefix resources in the namespace
      `openshift-azure-routefix` remain on the cluster.

    * subnets: Ensures that the Network Security Groups (NSGs) are correct, and updates the Azure Machine Provider spec with subnet, vnet, and Network Resource Group.

    * workaround: Applies a set of temporary workarounds to the ARO cluster.

    * previewfeature: Allows toggling certain features that are not yet enabled by default.

  * pkg/portal: Portal for running promql queries against a cluster or requesting a kubeconfig for a cluster.

  * pkg/proxy: Proxy service for portal kubeconfig cluster access.

  * pkg/swagger: Swagger specification generation code.

  * pkg/util: Utility libraries.

* python: Autogenerated ARO service Python client and `az aro` client extension.

* swagger: Autogenerated ARO service Swagger specification.

* test: End-to-end tests.


## Basic architecture

* pkg/frontend is intended to become a spec-compliant RP web server.  It is
  backed by CosmosDB.  Incoming PUT/DELETE requests are written to the database
  with an non-terminal (Updating/Deleting) provisioningState.

* pkg/backend reads documents with non-terminal provisioningStates,
  asynchronously updates them and finally updates document with a terminal
  provisioningState (Succeeded/Failed).  The backend updates the document with a
  heartbeat - if this fails, the document will be picked up by a different
  worker.

* As CosmosDB does not support document patch, care is taken to correctly pass
  through any fields in the internal model which the reader is unaware of (see
  `github.com/ugorji/go/codec.MissingFielder`).  This is intended to help in
  upgrade cases and (in the future) with multiple microservices reading from the
  database in parallel.

* Care is taken to correctly use optimistic concurrency to avoid document
  corruption through concurrent writes (see `RetryOnPreconditionFailed`).

* The pkg/api architecture differs somewhat from
  `github.com/openshift/openshift-azure`: the intention is to fix the broken
  merge semantics and try pushing validation into the versioned APIs to improve
  error reporting.

* Everything is intended to be crash/restart/upgrade-safe, horizontally
  scaleable, upgradeable...


## Useful links

* https://github.com/AzureExpert/azure-resource-manager-rpc/tree/master

* https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/azure-resource-manager/management/overview.md

* https://github.com/cloud-and-ai-microsoft/resource-provider-contract

* https://github.com/microsoft/api-guidelines

* https://docs.microsoft.com/en-gb/rest/api/cosmos-db

* https://github.com/jewzaam/go-cosmosdb",VRAI
Azure/ato-toolkit,Documentations,Documentations,2025-04-10T17:48:58Z,2021-09-15T16:04:24Z,0,0,0,0,0,0,0,0,2020-03-26T20:42:12Z,2025-02-24T13:43:05Z,166604,80,PowerShell,VRAI,45,FAUX,14,,14,On this page you'll find everything you need to get started with the Azure blueprint for Zero Trust. This blueprint is currently in preview.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,18,"# Welcome to Azure's DoD DevSecOps Enterprise Open Source Solutions!

# [ :warning: This library and all its artifacts are currently not actively maintained or supported. Please review the code and instructions carefully, and make any necessary additions or edits before using them. These resources are not actively contributed to or supported.]

In this repository you will find DevSecOps solutions that will kickstart your path to DoD application development, authorization, and deployment.

1. [Zero Trust Architecture Blueprint](https://github.com/Azure/ato-toolkit/tree/master/zero%20trust%20architecture%20blueprint)
2. [Software Factory](https://github.com/Azure/ato-toolkit/tree/master/software%20factory)
3. [STIG](https://github.com/Azure/ato-toolkit/tree/master/stig)",FAUX
Azure/azure-policy,Documentations,Documentations,2025-05-12T18:49:35Z,2024-07-08T21:45:18Z,0,9,0,0,0,0,0,71,2017-07-27T03:58:06Z,2025-04-08T07:14:51Z,16796,1558,Open Policy Agent,VRAI,1109,FAUX,249,"azure,policy",249,Repository for Azure Resource Policy built-in definitions and samples,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,66,"# {{displayName}}

{{description}}

## Try on Portal

[![Deploy to Azure](http://azuredeploy.net/deploybutton.png)](https://portal.azure.com/?feature.customportal=false&microsoft_azure_policy=true#blade/Microsoft_Azure_Policy/CreatePolicyDefinitionBlade)

## Try on PowerShell

````powershell
$definition = New-AzPolicyDefinition -Name ""{{policyName}}"" -DisplayName ""{{displayName}}"" -description ""{{description}}"" -Policy 'https://raw.githubusercontent.com/Azure/azure-policy-samples/master/samples/{{path}}/azurepolicy.rules.json' -Parameter 'https://raw.githubusercontent.com/Azure/azure-policy-samples/master/samples/{{path}}/azurepolicy.parameters.json' -Mode All
$definition
$assignment = New-AzPolicyAssignment -Name <assignmentname> -Scope <scope> -PolicyDefinition $definition
$assignment 
````



## Try with CLI

````cli

az policy definition create –-name ""{{policyName}}"" --display-name ""{{displayName}}"" --description ""{{description}}"" --rules 'https://raw.githubusercontent.com/Azure/azure-policy-samples/master/samples/{{path}}/azurepolicy.rules.json' –-params 'https://raw.githubusercontent.com/Azure/azure-policy-samples/master/samples/{{path}}/azurepolicy.parameters.json' -mode All

az policy assignment create --name <assignmentname> --scope <scope> --policy ""{{policyName}}"" 

````",FAUX
Azure/azure-remote-rendering,Toolkit,Toolkit,2024-09-23T08:50:37Z,2023-03-29T08:38:37Z,4,0,0,0,0,0,0,0,2020-01-11T00:47:04Z,2025-02-02T16:48:11Z,250081,109,HTML,VRAI,39,FAUX,3,,3,SDK and samples for Azure Remote Rendering,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,19,"# Azure Remote Rendering

> [!NOTE]
> Please note that Azure Remote Rendering (ARR) will be retired on **September 30, 2025**. More details [here](https://azure.microsoft.com/en-us/updates/v2/azure-remote-rendering-retirement).

Welcome to [Azure Remote Rendering](https://azure.microsoft.com/services/remote-rendering/).
This service enables you to render highly complex 3D models in real time on devices such as HoloLens 2.

Full documentation for Azure Remote Rendering can be found here:
<https://docs.microsoft.com/azure/remote-rendering>

This repository contains the following folders:

* Unity - This folder contains sample projects for use in the Unity game engine.
   - Please note that you have to run a [script](https://docs.microsoft.com/azure/remote-rendering/quickstarts/render-model#clone-the-sample-app) before these projects can be opened in Unity.
* NativeCpp - This folder contains sample projects using Remote Rendering with native C++
* Scripts - This folder contains PowerShell scripts for interacting with the service (e.g. converting assets or launching rendering servers).
* Tools - This folder contains auxiliary utilities for working with Remote Rendering (e.g. tracing profiles to gather tracing information).

Another useful open-source tool which uses the C++ ARR SDK is Azure Remote Rendering Asset Tool (ARRT). This desktop application can be used to upload, convert and remotely render 3D models, using Azure Remote Rendering. Find the source code and binary releases on the [Azure Remote Rendering Asset Tool GitHub repository](https://github.com/Azure/azure-remote-rendering-asset-tool).

## Requirements

To check out this repository, [Git LFS](https://git-lfs.github.com/) must be installed.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Support

Have an idea or suggestion? [Give us your feedback](https://feedback.azure.com/forums/928696-azure-remote-rendering)

Have an issue? [Refer to our Troubleshooting Guide](https://docs.microsoft.com/azure/remote-rendering/resources/troubleshoot) OR Ask the community on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-remote-rendering) and [Microsoft Q&A](https://docs.microsoft.com/answers/topics/azure-remote-rendering.html)",FAUX
Azure/Community-Policy,Documentations,Documentations,2025-05-14T15:10:17Z,2024-09-18T15:57:09Z,0,26,0,0,0,0,0,18,2019-11-13T00:24:57Z,2025-04-02T15:07:18Z,5392,654,Open Policy Agent,VRAI,336,FAUX,2,,2,This repo is for Microsoft Azure customers and Microsoft teams to collaborate in making custom policies. ,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,125,"# Community Policy Repo

<!-- 
Guidelines on README format: https://review.learn.microsoft.com/help/contribute/samples/concepts/readme-template

Guidance on onboarding samples to learn.microsoft.com/samples: https://review.learn.microsoft.com/help/contribute/samples/process/onboarding
-->

The purpose of this repo is for Azure Policy users and Microsoft internal teams to share and collaborate on custom policies. These policies are built either by customers or Microsoft Support engineers for customers. These are NOT Built-in policies hence are not check, tested or validated in any form by the Azure Policy Release Team. Please be wary of this and always TEST your policies before enforcing. Happy Coding! 

For Built-in policies repo, please visit here: [azure-policy](https://github.com/Azure/azure-policy)


## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

To get started contributing to the samples, please visit our [**contribution guide**](https://github.com/Azure/Community-Policy/blob/master/CONTRIBUTING.md). We also have a PowerShell script that will validate your Policy against the contribution guide and fix problems for you. You can find it [**here**](https://github.com/Azure/Community-Policy/blob/main/Scripts/Confirm-PolicyDefinitionIsValid.ps1).

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.


# Getting Support

The general Azure Policy support role that this repository has is not supported by standard Azure support channels. See below for information about getting support help for Azure Policy.

### General Questions

If you have questions you haven't been able to answer from the [**Azure Policy documentation**](https://docs.microsoft.com/azure/governance/policy), there are a few places that host discussions on Azure Policy:

 - [Microsoft Tech Community](https://techcommunity.microsoft.com/) [**Azure Governance conversation space**](https://techcommunity.microsoft.com/t5/Azure-Governance/bd-p/AzureGovernance)
 - Join the Monthly Call on Azure Governance (register [here](https://forms.office.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxn7UD7lweFDnmuLj72r6E1UN1dLNTBZUVMyNVpHUjJLRE5PVDVGNlkyOC4u))
 - Search or add to Azure Policy discussions on [**StackOverflow**](https://stackoverflow.com/questions/tagged/azure-policy+or+azure+policy)

If your questions are more in-depth or involve information that is not public, open a new [**Azure Customer Support ticket**](https://azure.microsoft.com/support/create-ticket/).

### Documentation Corrections

To report issues in the Azure Policy online documentation, look for a feedback area at the bottom of the page.

### New built-in Policy Proposals

If you have ideas for new built-in policies you want to suggest to Microsoft, you can submit them to [**Azure Governance Ideas**](https://feedback.azure.com/d365community/forum/675ae472-f324-ec11-b6e6-000d3a4f0da0). These suggestions are actively reviewed and prioritized for implementation.

### Other Support for Azure Policy

If you are encountering livesite issues or difficulties in implementing new policies that may be due to problems in Azure Policy itself, open a support ticket at [**Azure Customer Support**](https://azure.microsoft.com/support/create-ticket/). If you want to submit an idea for consideration, add an idea or upvote an existing idea at [**Azure Governance User Voice**](https://feedback.azure.com/forums/915958-azure-governance).


# Azure Policy Resources

## Articles

- [Azure Policy overview](https://learn.microsoft.com/azure/governance/policy/overview)
- [How to assign policies using the Azure portal](https://learn.microsoft.com/azure/governance/policy/assign-policy-portal)
- [How to assign policies using Azure PowerShell](https://learn.microsoft.com/azure/governance/policy/assign-policy-powershell)
- [How to assign policies using Azure CLI](https://learn.microsoft.com/azure/governance/policy/assign-policy-azurecli)
- [Export and manage Azure policies as code with GitHub](https://learn.microsoft.com/en-us/azure/governance/policy/tutorials/policy-as-code-github)
- [Definition structure](https://learn.microsoft.com/azure/governance/policy/concepts/definition-structure)
- [Understand Policy effects](https://learn.microsoft.com/azure/governance/policy/concepts/effects)
- [Programmatically create policies](https://learn.microsoft.com/azure/governance/policy/how-to/programmatically-create)
- [Get compliance data](https://learn.microsoft.com/azure/governance/policy/how-to/get-compliance-data)
- [Remediate non-compliant resources](https://learn.microsoft.com/azure/governance/policy/how-to/remediate-resources)

## References

- [Azure CLI](https://learn.microsoft.com/cli/azure/policy)
- Azure PowerShell
  - [Policy](https://learn.microsoft.com/powershell/module/az.resources/#policies)
- REST API
  - [Events](https://learn.microsoft.com/en-us/rest/api/policy/policy-events)
  - [States](https://learn.microsoft.com/en-us/rest/api/policy/policy-states)
  - [Assignments](https://learn.microsoft.com/rest/api/policy/policy-assignments)
  - [Policy Definitions](https://learn.microsoft.com/rest/api/policy/policy-definitions)
  - [Policy Set Definitions](https://learn.microsoft.com/rest/api/policy/policy-set-definitions)
  - [Policy Tracked Resources](https://learn.microsoft.com/rest/api/policy/policy-tracked-resources)
  - [Remediations](https://learn.microsoft.com/rest/api/policy/remediations)",FAUX
Azure/draft,Toolkit,Application System,2025-05-15T21:01:35Z,2025-03-26T17:26:39Z,0,0,0,0,0,0,0,24,2021-11-18T16:54:22Z,2025-04-03T19:42:24Z,28007,595,Go,VRAI,66,FAUX,52,,52,A day 0 tool for getting your app on k8s fast,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,29,"![Draft logo](./ghAssets/Draft-Gradient@3x.png)
<div id=""top""></div>
<br />
<div align=""center"">
  <h1 align=""center"">Draft</h1>
  <p align=""center"">
    A tool to help developers hit the ground running with Kubernetes.
    <br />
    <br />
    <a href=""https://github.com/Azure/draft/issues"">Report Bug</a>
    ·
    <a href=""https://github.com/Azure/draft/issues"">Request Feature</a>
  
  [![Draft Unit Tests](https://github.com/Azure/draft/actions/workflows/unit-tests.yml/badge.svg?branch=main)](https://github.com/Azure/draft/actions/workflows/unit-tests.yml?query=branch:main)
  [![GoDoc](https://godoc.org/github.com/Azure/draft?status.svg)](https://godoc.org/github.com/Azure/draft)
  [![Go Report Card](https://goreportcard.com/badge/github.com/Azure/draft?branch=main)](https://goreportcard.com/report/github.com/Azure/draft)
  [![CodeQL](https://github.com/Azure/draft/actions/workflows/codeql-analysis.yml/badge.svg?branch=main)](https://github.com/Azure/draft/actions/workflows/codeql-analysis.yml?query=branch:main)
  [![Draft Linux Integrations](https://github.com/Azure/draft/actions/workflows/integration-linux.yml/badge.svg?branch=main)](https://github.com/Azure/draft/actions/workflows/integration-linux.yml?query=branch:main)
  [![Draft Release & Publish](https://github.com/Azure/draft/actions/workflows/release-and-publish.yml/badge.svg?branch=main)](https://github.com/Azure/draft/actions/workflows/release-and-publish.yml?query=branch:main)
  </p>
</div>

## Installation
### Homebrew

1. Run the following commands

```sh
brew install draft
```

### Shell Script

```sh
curl -fsSL https://raw.githubusercontent.com/Azure/draft/main/scripts/install.sh | bash
```

* Windows is supported via WSL Ubuntu

## Getting Started

Draft is a tool made for users who are just getting started with Kubernetes or want to simplify their experience with Kubernetes. This readme will give you a quick run down on Draft’s commands and what they do.

### `draft create`

In our directory that holds our application, we can run the CLI command ‘draft create’. Draft create will walk you through a series of questions prompting you on your application specification. At the end of it, you will have a Dockerfile as well as Kubernetes manifests to deploy your application. Below is a picture of running the Draft create command on our [Contoso Air repository](https://github.com/microsoft/ContosoAir).

![example of draft create command showing the prompt ""select k8s deployment type"" with three options ""helm"", ""kustomize"", and ""manifests""](./ghAssets/draft-create.png)

### `generate-workflow`

Next up, we can run the ‘draft generate-workflow’ command.
This command will automatically build out a GitHub Action for us.
![screenshot of command line executing ""draft generate-workflow"" printing ""Draft has successfully genereated a Github workflow for your project""](./ghAssets/generate-workflow.png)

### `setup-gh`

If you are using Azure, you can also run the ‘draft setup-gh’ command to automate the GitHub OIDC setup process. This process is needed to make sure your Azure account and your GitHub repository can talk to each other. If you plan on using the GitHub Action to deploy your application, this step must be completed.

![screenshot of command line executing ""draft setup-gh"" showing the prompt ""Which account do you want to log into?"" with two options ""Github.com"" and ""Github Enterprise Server""](./ghAssets/setup-gh.png)

At this point, you have all the files needed to deploy your application onto a Kubernetes cluster!

If you don’t plan on using the GitHub Action, you can directly apply your deployment files by using the `kubectl apply -f` command.

If you plan on deploying your application through your GitHub Action, commit all the files to your repository and watch your application get deployed!

### `draft validate`

Draft validate scans your manifests and populates warnings messages in your code terminal if your manifests are not following best practices. All best practices are sourced from AKS’s deployment safeguards feature. To learn more about deployment safeguards, visit the [offical documentation](https://learn.microsoft.com/azure/aks/deployment-safeguards). To view the best practices that are used in Draft, view the [safeguards package](https://github.com/Azure/draft/tree/main/pkg/safeguards/lib/v1.0.0).

![screenshot of draft-validate](./ghAssets/draft-validate.png)

### `draft info`
The `draft info` command prints information about supported languages and deployment types.

Example output (for brevity, only the first supported language is shown):
```
{
  ""supportedLanguages"": [
    {
      ""name"": ""clojure"",
      ""displayName"": ""Clojure"",
      ""variableExampleValues"": {
        ""VERSION"": [
          ""8-jdk-alpine"",
          ""11-jdk-alpine""
        ]
      }
    }
  ]
  ...,
  ""supportedDeploymentTypes"": [
    ""helm"",
    ""kustomize"",
    ""manifests""
  ]
}
```
<!-- ABOUT THE PROJECT -->

## About The Project

Draft makes it easier for developers to get started building apps that run on Kubernetes by taking a non-containerized application and generating the Dockerfiles, Kubernetes manifests, Helm charts, Kustomize configuration, and other artifacts associated with a containerized application. Draft can also generate a GitHub Actions workflow file to quickly build and deploy applications onto any Kubernetes cluster.

### Commands

- `draft create` adds the minimum required Dockerfile and manifest files for your deployment to the project directory.
  - Supported deployment types: Helm, Kustomize, Kubernetes manifest.
- `draft setup-gh` automates the GitHub OIDC setup process for your project.
- `draft generate-workflow` generates a GitHub Actions workflow for automatic build and deploy to a Kubernetes cluster.
- `draft update` automatically make your application to be internet accessible.
- `draft validate` scan your manifests to see if they are following Kubernetes best practices.
- `draft info` print supported language and field information in json format.

Use `draft [command] --help` for more information about a command.

### Dry Run
The following flags can be used for enabling dry running, which is currently supported by the following commands: `create`
- ` --dry-run` enables dry run mode in which no files are written to disk
-  `--dry-run-file` specifies a file to write the dry run summary in json format into

```json
// Example dry run output
{
  ""variables"": {
    ""APPNAME"": ""testapp"",
    ""BUILDERVERSION"": ""null"",
    ""IMAGENAME"": ""testapp"",
    ""LANGUAGE"": ""gomodule"",  // Note that this variable is in addition to the draft config variables
    ""NAMESPACE"": ""default"",
    ""PORT"": ""1323"",
    ""SERVICEPORT"": ""80"",
  },
  ""filesToWrite"": [
    ""langtest/.dockerignore"",
    ""langtest/Dockerfile"",
    ""langtest/charts/.helmignore"",
    ""langtest/charts/Chart.yaml"",
    ""langtest/charts/production.yaml"",
    ""langtest/charts/templates/_helpers.tpl"",
    ""langtest/charts/templates/deployment.yaml"",
    ""langtest/charts/templates/namespace.yaml"",
    ""langtest/charts/templates/service.yaml"",
    ""langtest/charts/values.yaml""
  ]
}
```
## Install from Source

### Prerequisites

Draft requires Go version 1.21.x. or above

Check your go version.

```sh
go version
```

1. Clone the repo

   ```sh
   git clone https://github.com/Azure/draft.git
   ```

2. Change to the `draft` directory and build the binary

   ```sh
   cd draft/
   make
   ```

3. Add the binary to your path (we use the same directory as [go install](https://pkg.go.dev/cmd/go#hdr-Compile_and_install_packages_and_dependencies))

   ```sh
   mv draft $HOME/go/bin/
   ```


## Draft as a Dependency

If you are looking to leverage Draft's file generation capabilities and templating within another project instead of using the CLI, you have two options: importing the Draft go packages, and wrapping the binary

### Importing Draft Go Packages
This option will provide the cleanest integration, as it directly builds Draft into your project. However, it requires that your project is written in Go.

Dockerfiles can be generated following the example in [examples/dockerfile.go](https://github.com/Azure/draft/blob/main/example/dockerfile.go) 

Deployment files can be generated following the example in [examples/deployment.go](https://github.com/Azure/draft/blob/main/example/deployment.go)

### Wrapping the Binary
For projects written in languages other than Go, or for projects that prefer to not import the packages directly, you can wrap the Draft binary.

Several features have been implemented to make consuming draft as easy as possible:
- `draft info` prints supported language and field information in json format for easy parsing
- `--dry-run` and `--dry-run-file` flags can be used on the `create` and `update` commands to generate a summary of the files that would be written to disk, and the variables that would be used in the templates
- `draft update` and `draft create` accept a repeatable `--variable` flag that can be used to set template variables
- `draft create` takes a `--create-config` flag that can be used to input variables through a yaml file instead of interactively

## Introduction Videos

[![Day Zero Draft](./ghAssets/videoasset1.png)](https://www.youtube.com/watch?v=K1VYLSm32wg&list=PLlrxD0HtieHg8On6t1l5_kj--7PMmyfGi)

[![Get your app & APIs on Kubernetes fast with Draft](./ghAssets/videoasset2.png)](https://www.youtube.com/watch?v=f98NDqKQRbs)

## Contributing

Draft is fully compatible with [Azure Kubernetes Service](https://docs.microsoft.com/azure/aks/draft). We strongly encourage contributions to make Draft available to other cloud providers 😊!

## Issues/Discussions

The Draft team will be monitoring both the [issues](https://github.com/Azure/draft/issues) and [discussions](https://github.com/Azure/draft/discussions) board. Please feel free to create issues for any problems you run into and the Draft team will be quick to respond. The discussions board will be used for community engagement. We look forward to see you there!

## License

Draft is under the MIT License. See [LICENSE](https://github.com/Azure/draft/blob/main/LICENSE) for more information.

## Trademark Notice

Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft’s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.",VRAI
Azure/ShieldGuard,Toolkit,Application System,2025-03-24T17:06:32Z,2023-11-07T23:37:29Z,0,11,0,0,0,0,0,0,2022-11-07T23:48:20Z,2025-03-24T17:06:37Z,456,9,Go,VRAI,7,FAUX,6,secops,6,Enables best security practices for your project from day zero.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,6,"<div align=""center"">
<!-- TODO: logo -->
<h1>ShieldGuard</h1>
<p>Enables best security practices for your project from day zero.</p>

[![Unit Test](https://github.com/Azure/ShieldGuard/actions/workflows/unit-test.yaml/badge.svg)](https://github.com/Azure/ShieldGuard/actions/workflows/unit-test.yaml)
[![Go Reference](https://pkg.go.dev/badge/github.com/Azure/ShieldGuard/sg.svg)](https://pkg.go.dev/github.com/Azure/ShieldGuard/sg)
</div>

## What's ShieldGuard?

ShieldGuard is a **modular tool** and a **process** for enforcing various kind of validations on structured data. These data can be
the JSON/YAML/TOML/XML/... configurations from project source code, or the runtime data from your production environments.

## Quick Start

Get ShieldGuard running in 5 minutes: [./docs/manual/get-started.md](/docs/manual/get-started.md)

## Documentations

Interested to more usage scenarios? Checkout [./docs/manual](/docs/manual/) for more examples!

### Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

For step by step development setup & contribution guides, please see [./docs/dev](/docs/dev/) folder.

### Security

Please follow [SECURITY.md](/SECURITY.md) to report security issues.

## History and Why

ShieldGuard is a based on [Open Policy Agent (OPA)][opa] and heavily inspired by:

- [Conftest][conftest]
- [defsec][]
- ... and many other tools!

We decided to build a new tool based on following reasons:

1. ShieldGuard aims to provide a unified way to write checks using **vanilla** Rego language. This means you can reuse the checks
   without the need to depend on ShieldGuard itself;
2. ShieldGuard makes policy and check documentation as first-class citizen: it provides a convention approach for writing, organizing
   and referencing the documentations alongside with the policies;
3. ShieldGuard exposes itself via a modular types and packages, which enables further composition and building new tools easily.

[opa]: https://github.com/open-policy-agent/opa
[conftest]: https://www.conftest.dev/
[defsec]: https://github.com/aquasecurity/defsec

## License

[MIT](/LICENSE)

### Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.",VRAI
babywyrm/sysadmin,Application System,Documentations,2025-05-15T01:25:54Z,2025-05-04T17:48:34Z,0,0,0,0,2,0,0,2,2019-09-22T23:42:50Z,2025-04-07T03:27:03Z,120407,19,Python,VRAI,12,FAUX,2,,2,the flow of time is always cruel,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,2,"# behold
# the construct
#
#

Replacing Bash Scripting with Python
====================================

If I didn't cover something you want to know about or you find another
problem, please open an issue on github_!

.. _github:
  https://github.com/ninjaaron/replacing-bash-scripting-with-python

.. contents::

Introduction
------------
The Unix shell is one of my favorite inventions ever. It's genius, plain
and simple. The idea is that the user environment is a Turing-complete,
imperative programming language. It has a dead-simple model for dealing
with I/O and concurrency, which are notoriously difficult in most other
languages.

For problems where the data can be expressed as a stream of similar
objects separated by newlines to be processed concurrently through a
series of filters and handles a lot of I/O, it's difficult to think of a
more ideal language than the shell. A lot of the core parts on a Unix or
Linux system is designed to express data in such formats.

This tutorial is NOT about getting rid of bash altogether! In fact, one
of the main goals of the section on `Command-Line Interfaces`_ is to
show how to write programs that integrate well with the process
orchestration faculties of the shell.

If the Shell is so great, what's the problem?
+++++++++++++++++++++++++++++++++++++++++++++
The problem is if you want to do basically anything else, e.g. write
logic, use control structures, handle data... You're going to have big
problems. When Bash is coordinating external programs, it's fantastic.
When it's doing any work whatsoever itself, it disintegrates into a pile
of garbage.

For me, the fundamental problem with Bash and many other shell dialects
is that text is identifiers and identifiers are text -- and basically
everything else is also text. In some sense, this makes the shell a
homoiconic language, which theoretically means it might have an
interesting metaprogramming story, until you realize that it basically
just amounts to running ``eval`` on strings, which is a feature in
basically any interpreted language today, and one that is frequently
considered harmful. The problem with ``eval`` is that it's a pretty
direct path to arbitrary code execution. This is great if arbitrary code
execution is actually what you're trying to accomplish (like, say, in an
HTML template engine), but it's not generally what you want.

Bash basically defaults to evaling everything. This is very handy for
interactive use, since it cuts down in the need for a lot of explicit
syntax when all you really want to do is, say, open a file in a text
editor. This is pretty darn bad in a scripting context because it turns
the entire language into an injection honeypot. Yes, it is possible and
not so difficult to write safe Bash once you know the tricks, but it
takes extra consideration and it is easy to forget or be lazy about it.
Writing three or four lines of safe Bash is easy; two-hundred is quite a
bit more challenging.

Bash has other problems. The syntax that isn't native to the Bourne
Shell feels really ugly and bolted-on. For example, most modern shells
have arrays. Let's look at the syntax for iterating on an array, but
let's take the long way there.

.. code:: bash

  $ foo='this   and   that' # variable assignment
  $ echo $foo
  this and that
  $ # Oh dear. Text inside the variable was split into arguments on
  $ # whitespace, because eval all the things.
  $
  $ # To avoid this insane behavior, do the obvious thing: use string
  $ # interpolation. :-(
  $ echo ""$foo""
  this   and   that

What does this have to do with iterating on arrays? Unfortunately, the
answer is ""something.""

To properly iterate on the strings inside of an array (the only thing
which an array can possibly contain), you also use variable
interpolation syntax.

.. code:: bash

  for item in ""${my_array[@]}""; do
      stuff with ""$item""
  done

Why would string interpolation syntax ever be used to iterate over items
in an array? I have some theories, but they are only that. I could tell
you, but it wouldn't make this syntax any less awful. If you're not too
familiar with Bash, you may also (rightly) wonder what this ``@`` is, or
why everything is in curly braces.

The answer to all these questions is more or less that they didn't want
to do anything that would break compatibility with ancient Unix shell
scripts, which didn't have these features. Everything just got
shoe-horned in with the weirdest syntax you can imagine. Bash actually
has a lot of features of modern programming languages, but the problem
is that the syntax provided to access them is completely contrary to
logic and dictated by legacy concerns.

The Bash IRC channel has a very helpful bot, greybot, written by one of
the more important Bash community members and experts, greycat. This bot
is written in Perl. I once asked why it wasn't written in Bash, and only
got one answer: ""greycat wanted to remain sane.""

And really, that answer should be enough. Do you want to remain sane? Do
you want people who maintain your code in the future not to curse your
name? Don't use Bash. Do your part in the battle against mental illness.

*Ok, that was a little hyperbolic. For an opinion about when it's aright
to use Bash, see:* `Epilogue: Choose the right tool for the job.`_

Why Python?
+++++++++++
No particular reason. Perl_ and Ruby_ are also flexible, easy-to-write
languages that have robust support for administrative scripting and
automation. I would recommend against Perl for beginners because it has
some similar issues to Bash: it was a much smaller language when it was
created, and a lot of the syntax for the newer features has a bolted-on
feeling [#]_. However, if one knows Perl well and is comfortable with it,
it's well suited to the task and is still a much saner choice for
non-trivial automation scripts, and that is one of its strongest domains.

`Node.js`_ is also starting to be used for administrative stuff these
days, so that could also be an option, though JavaScript has similar
issues to Perl. I've been investigating the possibility of using Julia_
for this as well. Anyway, most interpreted languages seem to have pretty
good support for this kind of thing, and you should just choose one that
you like and is widely available on Linux and other \*nix operating
systems.

The main reason I would recommend Python is if you already know it. If
you don't know anything besides BASH (or BASH and lower-level languages
like C or even Java), Python is a reasonable choice for your next
language. It has a lot of mature, fast third-party libraries in a lot of
domains -- science, math, web, machine learning, etc. It's also
generally considered easy to learn and has become a major teaching
language.

The other very compelling reason to learn Python is that it is the
language covered in this very compelling tutorial.

.. _Perl: https://www.perl.org/
.. _Ruby: http://rubyforadmins.com/
.. _Node.js: https://developer.atlassian.com/blog/2015/11/scripting-with-node/
.. _Julia: https://docs.julialang.org/en/stable/

.. [#] I'm referring specifically to Perl 5 here. Perl 6 is a better
       language, in my opinion, but suffers from a lack of adoption.
       https://perl6.org/

Learn Python
++++++++++++
This tutorial isn't going to teach you the Python core language, though
a few built-in features will be covered. If you need to learn it, I
highly recommend the `official tutorial`_, at least through chapter 5.
Through chapter 9 would be even better, and you might as well just read
the whole thing at that point.

If you're new to programming, you might try the book `Introducing
Python`_ or perhaps `Think Python`_. `Dive Into Python`_ is another
popular book that is available for free online. You may see a lot of
recommendations for `Learn Python the Hard Way`_. I think this method is
flawed, though I do appreciate that it was written by someone with
strong opinions about correctness, which has some benefits.

This tutorial assumes Python 3.5 or higher, though it may sometimes use
idioms from newer versions, and I will attempt to document when have used
an idiom which doesn't work in 3.4, which is apparently the version that
ships with the latest CentOS and SLES. Use at least 3.6 if you can. It
has some cool new features, but the implementation of dictionaries
(Python's hash map) was also overhauled in this version of Python, which
sort of undergirds the way the whole object system is implemented and
therefore is a major win all around.

Basically, always try to use whatever the latest version of Python is.
Do not use Python 2. It will be officially retired in 2020. That's two
years. If a library hasn't been ported to Python 3 yet, it's already
dead, just that its maintainers might not know it yet.

One last note about this tutorial: It doesn't explain *so much*. I have
no desire to rewrite things that are already in the official
documentation. It frequently just points to the relevant documentation
for those wishing to do the kinds of tasks that Bash scripting is
commonly used for.

.. _official tutorial: https://docs.python.org/3/tutorial/index.html
.. _Introducing Python: http://shop.oreilly.com/product/0636920028659.do
.. _Think Python: http://shop.oreilly.com/product/0636920045267.do
.. _Dive Into Python: http://www.diveintopython3.net/
.. _Learn Python the Hard Way: https://learncodethehardway.org/python/

Reading and Writing Files
-------------------------
If you're going to do any kind of administration or automation on a Unix
system, the idea of working with files is pretty central. The great
coreutils like ``grep``, ``sed``, ``awk``, ``tr``, ``sort``, etc., they
are all designed to go over text files line by line and do... something
with the content of that line. Any shell scripter knows that these
""files"" aren't always really files. Often as not, it's really dealing
with the output of another process and not a file at all. Whatever the
source, the organizing principle is streams of text divided by newline
characters. In Python, this is what we'd call a ""file-like object.""

Because the idea of working with text streams is so central to Unix
programming, we start this tutorial with the basics of working with text
files and will go from there to other streams you might want to work
with.

One handy thing in the shell is that you never really need file
handles.  All you have to type to loop over lines in a file would be
something like:

.. code:: Bash

  while read line; do
      stuff with ""$line""
  done < my_file.txt

(Don't use this code. You actually have to do some things with $IFS to
make it safe. Don't use any of my Bash examples. Don't use Bash! The
proper one is ``while IFS= read -r line``, but that just raises more
questions.)

In Python, you need to turn a path into a file object. The above loop
would be something like this:

.. code:: Python

  with open('my_file.txt') as my_file:
      for line in my_file:
          do_stuff_with(line.rstrip())

  ## the .rstrip() method is optional. It removes trailing whitespace
  ## from the line (including the newline character).

Let's take that apart.

The ``open()`` function returns a file object. If you just send it the
path name as a string, it's going to assume it's a text file in the
default system encoding (UTF-8, right?), and it is opened only for
reading. You can, of course, do ``my_file = open('my_file.txt')`` as
well. When you use ``with x as y:`` instead of assignment, it ensures the
object is properly cleaned up when the block is exited using something
called a ""context manager"". You can do ``my_file.close()`` manually, but
the ``with`` block will ensure that happens even if you hit an error
without having to write a lot of extra code.

The gross thing about context managers is that they add an extra
level of indentation. Here's a helper function you can use to open a
context manager for something you want to be cleaned up after you loop.

.. code:: Python

  def iter_with(obj):
      with obj:
          yield from obj

and then you use it like this:

.. code:: Python

  for line in iter_with(open('my_file.txt')):
      do_stuff_with(line)

``yield from`` means it's a `generator function`_, and it's
handing over control to a sub-iterator (the file object, in this case)
until that iterator runs out of things to return. Don't worry if that
doesn't make sense. It's a more advanced Python topic and not necessary
for administrative scripting.

If you don't want to iterate on lines, which is the most
memory-efficient way to deal with text files, you can slurp entire
contents of a file at once like this:

.. code:: Python

  with open('my_file.txt') as my_file:
      file_text = my_file.read()
      ## or
      lines = list(my_file)
      ## or with newline characters removed
      lines = my_file.read().splitlines()

  ## This code wouldn't actually run because the file hasn't been
  ## rewound to the beginning after it's been read through.

  ## Also note: list(my_file). Any function that takes an iterable can
  ## take a file object.



You can also open files for writing with, like this:

.. code:: Python

  with open('my_file.txt', 'w') as my_file:
      my_file.write('some text\n')
      my_file.writelines(['a\n', 'b\n', 'c\n'])
      print('another line', file=my_file)        # print adds a newline.


The second argument of ``open()`` is the *mode*. The default mode is
``'r'``, which opens the file for reading text. ``'w'`` deletes
everything in the file (or creates it if it doesn't exist) and opens it
for writing. You can also use the mode ``'a'``. This goes to the end of
a file and adds text there. In shell terms, ``'r'`` is a bit like ``<``,
``'w'`` is a bit like ``>``, and ``'a'`` is a bit like ``>>``.

This is just the beginning of what you can do with files. If you want to
know all their methods and modes, check the official tutorial's section
on `reading and writing files`_.  File objects provide a lot of cool
interfaces. These interfaces will come back with other ""file-like
objects"" which will come up many times later, including in the very next
section.

.. _generator function:
  https://docs.python.org/3/tutorial/classes.html#generators
.. _reading and writing files:
  https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files

Command-Line Interfaces
-----------------------

Working with ``stdin``, ``stdout`` and ``stderr``
+++++++++++++++++++++++++++++++++++++++++++++++++
Unix scripting is all about filtering text streams. You have a stream
that comes from lines in a file or output of a program and you pipe it
through other programs. Unix has a bunch of special-purpose programs
just for filtering text (some of the more popular of which are
enumerated at the beginning of the previous chapter). Everyone using a
\*nix system has probably done something like this at one point or
another:

.. code:: Bash

  program-that-prints-something | grep 'a pattern'

This is the ""normal"" way to search through the output of a program for
lines containing whatever it is you're searching for. Your setting the
``stdout`` of ``program-that-prints-something`` to the stdin of
``grep``.


Great CLI scripts should follow the same pattern so you can incorporate
them into your shell pipelines.  You can, of course, write your script
with its own ""interactive"" interface and read lines of user input one
at a time:

.. code:: Python

  username = input('What is your name? ')

This is fine in some cases, but it doesn't really promote the creation
of reusable, multi-purpose filters. With that in mind, allow me to
introduce the ``sys`` module.

The ``sys`` module has all kinds of great things as well as all kinds of
things you shouldn't really be messing with. We're going to start with
``sys.stdin``.

``sys.stdin`` is a file-like object that, you guessed it, allows you to
read from your script's ``stdin``. In Bash you'd write:

.. code:: Bash

  while read line; do # <- not actually safe. Don't use bash.
      stuff with ""$line""
  done

In Python, that looks like this:

.. code:: Python

  import sys
  for line in sys.stdin:
      do_stuff_with(line) # <- we didn't remove the newline char this
                          #    time. Just mentioning it because it's a
                          #    difference between python and shell.

Naturally, you can also slurp stdin in one go, though this isn't the
most Unix-y design choice and you could use up your RAM with a very
large file:

.. code:: Python

  text = sys.stdin.read()

As far as stdout is concerned, you can access it directly if you like,
but you'll typically just use the ``print()`` function.


.. code:: Python

  print(""Hello, stdout."")
  # ^ functionally same as:
  sys.stdout.write('Hello, stdout.\n')

Anything you print can be piped to another process. Pipelines are great.
For stderr, it's a similar story:

.. code:: Python

  print('a logging message.', file=sys.stderr)
  # or:
  sys.stderr.write('a logging message.\n')

If you want more advanced logging functions, check out the `logging
module`_.

Using ``stdin``, ``stdout`` and ``stderr``, you can write python
programs which behave as filters and integrate well into a Unix
workflow.

.. _logging module:
  https://docs.python.org/3/howto/logging.html#logging-basic-tutorial

CLI Arguments
+++++++++++++
Arguments are passed to your program as a list which you can access
using ``sys.argv``. This is a bit like ``$@`` in Bash, or ``$1 $2
$3...`` etc. e.g.:

.. code:: bash

  for arg in ""$@""; do
      stuff with ""$arg""
  done

looks like this in Python:

.. code:: Python

  import sys
  for arg in sys.argv[1:]:
      do_stuff_with(arg)

Why ``sys.argv[1:]``? ``sys.argv[0]`` is like ``$0`` in Bash or
``argv[0]`` in C. It's the name of the executable. Just a refresher
(because you read the tutorial, right?) ``a_list[1:]`` is list-slice
syntax that returns a new list starting on the second item of
``a_list``, going through to the end.

If you want to build a more complete set of flags and arguments for a
CLI program, the standard library module for that is argparse_. The
tutorial in that link leaves out some useful info, so here are the `API
docs`_. click_ is a popular and powerful third-party module for building
even more advanced CLI interfaces.

.. _argparse: https://docs.python.org/3/howto/argparse.html
.. _API docs: https://docs.python.org/3/library/argparse.html
.. _click: https://click.palletsprojects.com/

Environment Variables and Config files
++++++++++++++++++++++++++++++++++++++
Ok, environment variables and config files aren't necessarily only part
of CLI interfaces, but they are part of the user interface in general,
so I stuck them here. Environment variables are in the ``os.environ``
mapping, so you get to ``$HOME`` like this:

.. code:: Python

  >>> import os
  >>> os.environ['HOME']
  '/home/ninjaaron'

As far as config files, in Bash, you frequently just do a bunch of
variable assignments inside of a file and source it. You can also just
write valid python files and import them as modules or eval them... but
don't do that. Arbitrary code execution in a config file is generally
not what you want.

The standard library includes configparser_, which is a parser for .ini
files, and also a json_ parser. I don't really like the idea of
human-edited json, but go ahead and shoot yourself in the foot if you
want to. At least it's flexible.

PyYAML_, the YAML parser, and TOML_ are third-party libraries that are
useful for configuration files.

.. _configparser: https://docs.python.org/3/library/configparser.html
.. _json: https://docs.python.org/3/library/json.html
.. _PyYAML: http://pyyaml.org/wiki/PyYAMLDocumentation
.. _TOML: https://github.com/uiri/toml

Filesystem Stuff
----------------
Paths
+++++
So far, we've only seen paths as strings being passed to the ``open()``
function. You can certainly use strings for your paths, and the ``os``
and ``os.path`` modules contain a lot of portable functions for
manipulating paths as strings. However, since Python 3.4, we have
pathlib.Path_, a portable, abstract type for dealing with file paths,
which will be the focus of path manipulation in this tutorial.

.. code:: Python

  >>> from pathlib import Path
  >>> # make a path of the current directory
  >>> p = Path()
  >>> p
  PosixPath('.')
  >>> # iterate over directory contents
  >>> for i in p.iterdir():
  ...     print(repr(i))
  PosixPath('.git')
  PosixPath('out.html')
  PosixPath('README.rst')]
  >>> # use filename globbing
  >>> for i in p.glob('*.rst'):
  ...     print(repr(i))
  PosixPath('README.rst')
  >>> # get the full path
  >>> p = p.absolute()
  >>> p
  PosixPath('/home/ninjaaron/doc/replacing-bash-scripting-with-python')
  >>> # get the basename of the file
  >>> p.name
  'replacing-bash-scripting-with-python'
  >>> # name of the parent directory
  >>> p.parent
  PosixPath('/home/ninjaaron/doc')
  >>> # split path into its parts.
  >>> p.parts
  ('/', 'home', 'ninjaaron', 'doc', 'replacing-bash-scripting-with-python')
  >>> # do some tests about what the path is or isn't.
  >>> p.is_dir()
  True
  >>> p.is_file()
  False
  >>> # more detailed file stats.
  >>> p.stat()
  os.stat_result(st_mode=16877, st_ino=16124942, st_dev=2051, st_nlink=3, st_uid=1000, st_gid=100, st_size=4096, st_atime=1521557933, st_mtime=1521557860, st_ctime=1521557860)
  >>> # create new child paths with slash.
  >>> readme = p/'README.rst'
  >>> readme
  PosixPath('/home/ninjaaron/doc/replacing-bash-scripting-with-python/README.rst')
  >>> # open files
  >>> with readme.open() as file_handle:
  ...     pass
  >>> # make file executable with mode bits
  >>> readme.chmod(0o755)
  >>> # ^ note that octal notation is must be explicite.

Again, check out the documentation for more info. pathlib.Path_. Since
``pathlib`` came out, more and more builtin functions and functions in
the standard library that take a path name as a string argument can also
take a ``Path`` instance. If you find a function that doesn't, or you're
on an older version of Python, you can always get a string for a path
that is correct for your platform by using ``str(my_path)``. If you
need a file operation that isn't provided by the ``Path`` instance,
check the docs for os.path_ and os_ and see if they can help you out. In
fact, os_ is always a good place to look if you're doing system-level
stuff with permissions and UIDs and so forth.

If you're doing globbing with a ``Path`` instance, be aware that, like
ZSH, ``**`` may be used to glob recursively. It also (unlike the shell)
will include hidden files (files whose names begin with a dot). Given
this and the other kinds of attribute testing you can do on ``Path``
instances, it can do a lot of the kinds of stuff ``find`` can do.


.. code:: Python

  >>> [p for p in Path().glob('**/*') if p.is_dir()]

Oh. Almost forgot. ``p.stat()``, as you can see, returns an
os.stat_result_ instance. One thing to be aware of is that the
``st_mode``, (i.e. permissions bits) is represented as an integer, so
you might need to do something like ``oct(p.stat().st_mode)`` to show
what that number will look like in octal, which is how you set it with
``chmod`` in the shell.

.. _pathlib.Path:
  https://docs.python.org/3/library/pathlib.html#basic-use
.. _os.path: https://docs.python.org/3/library/os.path.html
.. _os: https://docs.python.org/3/library/os.html
.. _os.stat_result:
  https://docs.python.org/3/library/os.html#os.stat_result

Replacing miscellaneous file operations: ``shutil``
+++++++++++++++++++++++++++++++++++++++++++++++++++
There are certain file operations which are really easy in the shell,
but less nice than you might think if you're using python file objects
or the basic system calls in the ``os`` module. Sure, you can rename a
file with ``os.rename()``, but if you use ``mv`` in the shell, it will
check if you're moving to a different file system, and if so, copy the
data and delete the source -- and it can do that recursively without
much fuss. shutil_ is the standard library module that fills in the
gaps. The docstring gives a good summary: ""Utility functions for copying
and archiving files and directory trees.""

Here's the overview:

.. code:: Python

  >>> import shutil
  >>> # $ mv src dest
  >>> shutil.move('src', 'dest')
  >>> # $ cp src dest
  >>> shutil.copy2('src', 'dest')
  >>> # $ cp -r src dest
  >>> shutil.copytree('src', 'dest')
  >>> # $ rm a_file
  >>> os.remove('a_file') # ok, that's not shutil
  >>> # $ rm -r a_dir
  >>> shutil.rmtree('a_dir')
  >>> # $ tar caf 'my_archive.tar.gz' 'my_folder'
  >>> shutil.make_archive('my_archive.tar.gz', 'gztar', 'my_folder')
  >>> # $ tar xaf 'my_archive.tar.gz'
  >>> shutil.unpack_archive('my_archive.tar.gz')
  >>> # chown user:ninjaaron a_file.txt
  >>> shutil.chown('a_file.txt', 'ninjaaron', 'user')
  >>> # info about disk usage, a bit like `df`, but not exactly.
  >>> shutil.disk_usage('.')
  usage(total=123008450560, used=86878904320, free=36129546240)
  >>> #  ^ sizes in bytes
  >>> # which vi
  >>> shutil.which('vi')
  '/usr/bin/vi'
  >>> # info about the terminal you're running in.
  >>> shutil.get_terminal_size()
  os.terminal_size(columns=138, lines=30)

That's the thousand-foot view of the high-level functions you'll
normally be using. The module documentation is pretty good for examples,
but it also has a lot of details about the functions used to implement
the higher-level stuff I've shown which may or may not be interesting.

I should probably also mention ``os.link`` and ``os.symlink`` at this
point. They create hard and soft links respectively (like ``link`` and
``link -s`` in the shell). ``Path`` instances also have
``.symlink_to()`` method, if you want that.

.. _shutil: https://docs.python.org/3/library/shutil.html

Replacing ``sed``, ``grep``, ``awk``, etc: Python regex
-------------------------------------------------------
This section is not so much for experienced programmers who already know
more or less how to use regexes for matching and string manipulation in
other ""normal"" languages. Python is not so exceptional in this regard,
though if you're used to JavaScript, Ruby, Perl, and others, you may be
surprised to find that Python doesn't have regex literals. The regex
functionally is all encapsulated in the re_ module. (The official docs
have a `regex HOWTO`_, which is a good place to start if you don't know
anything about regular expressions. If you have some experience, I'd
recommend going straight for the re_ API docs.)

This section is for people who know how to use programs like ``sed``,
``grep`` and ``awk`` and wish to get similar results in Python, though
short explanations will be provided of what those utilities are commonly
used for. The intent is not that you should use Python wherever you
might use one-liners with these programs in the course of normal shell
usage (or in the the middle of the kinds of process orchestration
scripts that Bash does so well). The idea is rather that, when writing a
Python script, you won't be tempted to shell out for text processing.

I admit that writing simple text filters in Python will never be as
elegant as it is in Perl, since Perl was more or less created to be like
a super-powered version of the ``sh`` + ``awk`` + ``sed``. The same
thing can sort of be said about ``awk``, the original text-filtering
language on Unix. The main reason to use Python for these tasks is that
the project is going to scale a lot more easily when you want to do
something a bit more complex.

Another thing to keep in mind is that python has built-in operations
that you can use if you just need to match a string, rather than a
regular expression. Simple string operations are much faster than
regular expressions, though not as powerful.

.. Note::

  One thing to be aware of is that Python's regex is more like PCRE
  (Perl-style -- also similar to Ruby, JavaScript, etc.) than BRE or ERE
  that most shell utilities support. If you mostly do ``sed`` or
  ``grep`` without the ``-E`` option, you may want to look at the rules
  for Python regex (BRE is the regex dialect you know). If you're used
  to writing regex for ``awk`` or ``egrep`` (ERE), Python regex is more
  or less a superset of what you know. You still may want to look at the
  documentation for some of the more advanced things you can do. If you
  know regex from either vi/Vim or Emacs, they both use their own
  dialect of regex, but they are supersets of BRE, and Python's regex
  will have some major differences.

.. _re: https://docs.python.org/3/library/re.html
.. _regex HOWTO: https://docs.python.org/3/howto/regex.html

How to ``grep``
+++++++++++++++
``grep`` is the Unix utility that goes through each line of a file,
tests if it contains a certain pattern, and then prints the lines that
match. If you're a programmer and you don't use ``grep``, start using
it! Retrieving matching lines in a file is easy with Python, so we'll
start there.

If you don't need pattern matching (i.e. something you could do with
``fgrep``), you don't need regex to match a substring. You can simply
use built-in syntax:

.. code:: python

  >>> 'substring' in 'string containing substring'
  True

Otherwise, you need the regex module to match things:

.. code:: python

  >>> import re
  >>> re.search(r'a pattern', r'string containing a pattern')
  <_sre.SRE_Match object; span=(18, 27), match='a pattern'>
  >>> re.search(r'a pattern', r'string without the pattern')
  >>> # Returns None, which isn't printed in the Python REPL

I'm not going to go into the details of the ""match object"" that
is returned at the moment. The main thing for now is that it evaluates
to ``True`` in a boolean context. You may also notice I use raw strings
``r''``. This is to keep Python's normal escape sequences from being
interpreted, since regex uses its own escapes.

So, to use these to filter through strings:

.. code:: Python

  >>> ics = an_iterable_containing_strings
  >>> # like fgrep
  >>> filtered = (s for s in ics if substring in s)
  >>> # like grep (or, more like egrep)
  >>> filtered = (s for s in ics if re.search(pattern, s))

``an_iterable_containing_strings`` here could be a list, a generator or
even a file/file-like object. Anything that will give you strings when
you iterate on it. I use `generator expression`_ syntax here instead of
a list comprehension because that means each result is produced as
needed with lazy evaluation. This will save your RAM if you're working
with a large file. You can invert the result, like ``grep -v`` simply by
adding ``not`` to the ``if`` clause. There are also flags you can add to
do things like ignoring the case (``flags=re.I``), etc. Check out the docs
for more.

Example: searching logs for errors
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Say you want to look through the log file of a certain service on your
system for errors. With grep, you might do something like this:

.. code:: bash

  $ grep -i error: /var/log/some_service.log

This will search through ``/var/log/some_service.log`` for any line
containing the string ``error:``, ignoring case. To do the same thing in
Python:

.. code:: Python

  with open('/var/log/some_service.log') as log:
      matches = (line for line in log if 'error:' in line.lower())
      # line.lower() is a substitute for -i in grep, in this case

The difference here is that the bash version will print all the lines,
and the python version is just holding on to them for further
processing. If you want to print them, the next step is
``print(*matches)`` or ``for line in matches: print(line, end='')``.
However, this is in the context of a script, so you probably want to
extract further information from the line and do something
programmatically with it anyway.

.. _generator expression:
  https://docs.python.org/3/tutorial/classes.html#generator-expressions


How to ``sed``
++++++++++++++
``sed`` can do a LOT of things. It's more or less ""text editor"" without
a window. Instead of editing text manually, you give ``sed``
instructions about changes to apply to lines, and it does it all in one
shot. (The default is to print what the file would look like with
modification. The file isn't actually changed unless you use a special
flag.)

I'm not going to cover all of that. Back when I wrote more shell scripts
and less Python, the vast majority of my uses for ``sed`` were simply to
use the substitution facilities to change instances of one pattern into
something else, which is what I cover here.

.. code:: Python

  >>> # sed 's/a string/another string/g' -- i.e. doesn't regex
  >>> replaced = (s.replace('a string', 'another string') for s in ics)
  >>> # sed 's/pattern/replacement/g' -- needs regex
  >>> replaced = (re.sub(r'pattern', r'replacement', s) for s in ics)",VRAI
bacalhau-project/bacalhau,Toolkit,Toolkit,2025-05-15T18:56:36Z,2025-05-01T16:13:04Z,0,12,0,0,0,0,0,0,2021-11-05T15:30:29Z,2025-04-04T16:09:57Z,200164,776,Go,VRAI,98,FAUX,176,"ai-art,ai-data-collection,ai-pipeline,batch-processing,bioinformatics-pipeline,data-analysis,data-engineering,data-science,decentralized,decentralized-computing,distributed,gene-sequencing,insulators,iot,logging-framework,orchestration-framework,p2p,video-processing",176,"Community-driven, simple, yet powerful framework for fast, cost-effective distributed Compute over Data.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,68,"<p align=""center"">
  <a href=""https://github.com/bacalhau-project/bacalhau"">
    <img src=""./docs/logo/Bacalhau-horizontal.svg"" alt=""Bacalhau"" width=""300""/>
  </a>
</p>

<h1 align=""center"">Globally Distributed Compute Orchestrator ⚡<br>Compute Over Data (CoD)</h1>
<br>

<p align=""center"">
    <a href=""https://github.com/bacalhau-project/bacalhau/blob/main/LICENSE"" alt=""License"">
        <img src=""https://img.shields.io/badge/license-Apache-green"" />
    </a>
    <a href=""https://github.com/bacalhau-project/bacalhau/releases/"" alt=""Release"">
        <img src=""https://img.shields.io/github/v/release/bacalhau-project/bacalhau?display_name=tag"" />
    </a>
    <a href=""https://github.com/bacalhau-project/bacalhau/pulse"" alt=""Activity"">
        <img src=""https://img.shields.io/github/commit-activity/m/bacalhau-project/bacalhau"" />
    </a>
    <a href=""https://github.com/bacalhau-project/bacalhau/graphs/contributors"">
        <img src=""https://img.shields.io/github/contributors/bacalhau-project/bacalhau"" alt=""Bacalhau contributors"" >
    </a>
    <a href=""https://www.bacalhau.org/"">
        <img alt=""Bacalhau website"" src=""https://img.shields.io/badge/website-bacalhau.org-red"">
    </a>
    <a href=""https://bit.ly/bacalhau-project-slack"" alt=""Slack"">
        <img src=""https://img.shields.io/badge/slack-join_community-red.svg?color=0052FF&labelColor=090422&logo=slack"" />
    </a>
    <a href=""https://twitter.com/intent/follow?screen_name=BacalhauProject"">
        <img src=""https://img.shields.io/twitter/follow/BacalhauProject?style=social&logo=twitter"" alt=""follow on Twitter"">
    </a>
</p>

## What is Bacalhau?

[Bacalhau](https://www.bacalhau.org/) is an open-source distributed compute orchestration framework designed to bring compute to the data. Instead of moving large datasets around networks, Bacalhau makes it easy to execute jobs close to the data's location, drastically reducing latency and resource overhead.

## Why Bacalhau?

- ⚡ **Fast job processing**: Jobs in Bacalhau are processed where the data was created and all jobs are parallel by default
- 💰 **Low cost**: Reduce (or eliminate) ingress/egress costs since jobs are processed closer to the source
- 🔒 **Secure**: Data scrubbing and security can happen before migration, with a granular, code-based permission model
- 🚛 **Large-scale data**: Process petabytes of data efficiently without massive data transfers
- 🏢 **Data sovereignty**: Process sensitive data within security boundaries without requiring it to leave your premises
- 🤝 **Cross-organizational computation**: Allow specific vetted computations on protected datasets without exposing raw data

## Key Features

1. **Single Binary Simplicity**: Bacalhau is a single self-contained binary that functions as a client, orchestrator, and compute node—making it incredibly easy to set up and scale
   
2. **Modular Architecture**: Support for multiple execution engines (Docker, WebAssembly) and storage providers through clean interfaces

3. **Orchestrator-Compute Model**: A dedicated orchestrator coordinates job scheduling, while compute nodes run tasks

4. **Flexible Storage Integrations**: Integrates with S3, HTTP/HTTPS, IPFS, and local storage systems

5. **Multiple Job Types**: Support for batch, ops, daemon, and service job types for different workflow requirements

6. **Declarative & Imperative Submissions**: Define jobs in YAML (declarative) or pass arguments via CLI (imperative)

7. **Publisher Support**: Output results to local volumes, S3, or other storage backends

## Getting Started

### Quick Installation

```bash 
# Install Bacalhau CLI (Linux/macOS)
curl -sL https://get.bacalhau.org/install.sh | bash

# Verify installation
bacalhau version
```

For the complete quick start guide, including running your first job, see our [Quick Start Documentation](https://docs.bacalhau.org/getting-started/quick-start).

## Use Cases

Bacalhau's distributed compute framework enables a wide range of applications:

- **Log Processing**: Process logs efficiently at scale by running distributed jobs directly at the source
- **Distributed Data Warehousing**: Query and analyze data across multiple regions without moving large datasets
- **Fleet Management**: Efficiently manage distributed nodes across multiple environments
- **Distributed Machine Learning**: Train and deploy ML models across a distributed compute fleet
- **Edge Computing**: Run compute tasks closer to the data source for applications requiring low latency

## Documentation

📚 [Read the Bacalhau docs guide here](https://docs.bacalhau.org/)! 📚

The Bacalhau documentation contains all the information you need to get started:

- [Installation Tutorial](https://docs.bacalhau.org/getting-started/installation)
- [Basic Usage](https://docs.bacalhau.org/getting-started/cli)
- [Common Workflows](https://docs.bacalhau.org/common-workflows)

## Community & Contributing

Bacalhau has a very friendly community, and we are always happy to help:

- [Join the Slack Community](https://join.slack.com/t/bacalhauproject/shared_invite/zt-1sihp4vxf-TjkbXz6JRQpg2AhetPzYYQ) and go to the `#general` channel - it is the easiest way to engage with other members in the community and get help

If you are interested in contributing to the Bacalhau project:

- Set up your [local environment](docs/dev/local-env.md)
- Check out our [Contributing Guide](https://docs.bacalhau.org/community/community/ways-to-contribute)
- For issues and feature requests, please [open a GitHub issue](https://github.com/bacalhau-project/bacalhau/issues)

We are excited to hear your feedback!

## Open Source

This repository contains the Bacalhau software, covered under the [Apache-2.0](./LICENSE) license, except where noted (any Bacalhau logos or trademarks are not covered under the Apache License, and should be explicitly noted by a LICENSE file.)

Bacalhau is a product produced from this open source software, exclusively by Expanso, Inc. It is distributed under our commercial terms.

Others are allowed to make their own distribution of the software, but they cannot use any of the Bacalhau trademarks, cloud services, etc.

We explicitly grant permission for you to make a build that includes our trademarks while developing Bacalhau software itself. You may not publish or share the build, and you may not use that build to run Bacalhau software for any other purpose.

We have borrowed the above Open Source clause from the excellent [System Initiative](https://github.com/systeminit/si/blob/main/CONTRIBUTING.md)",FAUX
Bearer/bearer,Toolkit,Application System,2025-04-24T14:52:58Z,2024-08-12T14:31:29Z,0,6,0,0,0,0,0,0,2022-09-27T17:02:45Z,2025-04-04T03:38:35Z,25206,2250,Go,VRAI,125,FAUX,23,"appsec,code-quality,compliance,dataflow,devsecops,devsecops-tools,gdpr,owasp,privacy,sast,security,security-audit,security-automation,security-scanner,security-tools,static-analysis,static-code-analysis,vulnerabilities,vulnerability",23,"Code security scanning tool (SAST) to discover, filter and prioritize security and privacy risks.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,15,"<div align=""center"">
  <a href=""https://cycode.com/cygives/"" alt=""Bearer is part of Cygives, the community hub for free & open developer security tools.""/>
    <picture>
      <source media=""(prefers-color-scheme: dark)"" srcset=""./docs/assets/img/Cygives-darkmode.svg"">
      <img alt=""Cygives Banner"" src=""./docs/assets/img/Cygives-lightmode.svg"">
    </picture>
  </a>
  <br/>
  <br/>
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""./docs/assets/img/bearer-logo-dark.svg"">
    <img alt=""Bearer"" src=""./docs/assets/img/bearer-logo-light.svg"">
  </picture>
  <br />
  <hr/>
    Scan your source code against top <strong>security</strong> and <strong>privacy</strong> risks.
  <br /><br />
  Bearer is a static application security testing (SAST) tool designed to scan your source code and analyze data flows to identify, filter, and prioritize security and privacy risks.
  <br/><br/>
  Bearer offers a free, open solution, Bearer CLI, and a commercial solution, Bearer Pro, available through <a href=""https://cycode.com/"">Cycode</a>.
  <br /><br />

  [Getting Started](#rocket-getting-started) - [FAQ](#question-faqs) - [Documentation](https://docs.bearer.com) - [Report a Bug](https://github.com/Bearer/bearer/issues/new/choose) - [Discord Community][discord]

  [![GitHub Release][release-img]][release]
  [![Test][test-img]][test]
  [![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)
  [![Discord](https://img.shields.io/discord/1042147477765242973?label=discord)][discord]
</div>


## Language Support
**Bearer CLI**: Go • Java • JavaScript • TypeScript • PHP • Python • Ruby <br/>
**Bearer Pro by Cycode**: C# • Kotlin • Elixir *+ everything in Bearer OSS*

<a href=""https://docs.bearer.com/reference/supported-languages/"">Learn more about language suppport</a>

## Developer friendly static code analysis for security and privacy
<hr/>

<https://user-images.githubusercontent.com/1649672/230438696-9bb0fd35-2aa9-4273-9970-733189d01ff1.mp4>

Bearer CLI scans your source code for:
* **Security risks and vulnerabilities** using [built-in rules](https://docs.bearer.com/reference/rules/) covering the [OWASP Top 10](https://owasp.org/www-project-top-ten/) and [CWE Top 25](https://cwe.mitre.org/top25/archive/2023/2023_top25_list.html), such as:
  * A01: Access control (e.g. Path Traversal, Open Redirect, Exposure of Sensitive Information).
  * A02: Cryptographic Failures (e.g. Weak Algorithm, Insecure Communication).
  * A03: Injection (e.g. SQL Injection, Input Validation, XSS, XPath).
  * A04: Design (e.g. Missing Encryption of Sensitive Data, Persistent Cookies Containing Sensitive Information).
  * A05: Security Misconfiguration (e.g. Cleartext Storage of Sensitive Information in a Cookie or JWT).
  * A07: Identification and Authentication Failures (e.g. Use of Hard-coded Password, Improper Certificate Validation).
  * A08: Data Integrity Failures (e.g. Deserialization of Untrusted Data).
  * A09: Security Logging and Monitoring Failures (e.g. Insertion of Sensitive Information into Log File).
  * A10: Server-Side Request Forgery (SSRF).

  *Note: all the rules and their code patterns are accessible through the [documentation](https://docs.bearer.com/reference/rules/).*

* **Privacy risks** with the ability to detect [sensitive data flow](https://docs.bearer.com/explanations/discovery-and-classification/) such as the use of PII, PHI in your app, and [components](https://docs.bearer.com/reference/recipes/) processing sensitive data (e.g. databases like pgSQL, third-party APIs such as OpenAI, Sentry, etc.). This helps generate a [privacy report](https://docs.bearer.com/guides/privacy/) relevant for:
  * Privacy Impact Assessment (PIA).
  * Data Protection Impact Assessment (DPIA).
  * Records of Processing Activities (RoPA) input for GDPR compliance reporting.

## :rocket: Getting started

Discover your most critical security risks and vulnerabilities in only a few minutes. In this guide, you will install Bearer CLI, run a security scan on a local project, and view the results. Let's get started!

### Install Bearer CLI

The quickest way to install Bearer CLI is with the install script. It will auto-select the best build for your architecture. _Defaults installation to `./bin` and to the latest release version_:

```bash
curl -sfL https://raw.githubusercontent.com/Bearer/bearer/main/contrib/install.sh | sh
```

#### Other install options

<details>
  <summary>Homebrew</summary>

  Using [Bearer CLI's official Homebrew tap](https://github.com/Bearer/homebrew-tap):

  ```bash
  brew install bearer/tap/bearer
  ```

  Update an existing installation with the following:

  ```bash
  brew update && brew upgrade bearer/tap/bearer
  ```

</details>

<details>
  <summary>Debian/Ubuntu</summary>

  ```shell
  sudo apt-get install apt-transport-https
  echo ""deb [trusted=yes] https://apt.fury.io/bearer/ /"" | sudo tee -a /etc/apt/sources.list.d/fury.list
  sudo apt-get update
  sudo apt-get install bearer
  ```

  Update an existing installation with the following:

  ```bash
  sudo apt-get update
  sudo apt-get install bearer
  ```

</details>

<details>
  <summary>RHEL/CentOS</summary>

  Add repository setting:

  ```shell
  $ sudo vim /etc/yum.repos.d/fury.repo
  [fury]
  name=Gemfury Private Repo
  baseurl=https://yum.fury.io/bearer/
  enabled=1
  gpgcheck=0
  ```

  Then install with yum:

  ```shell
    sudo yum -y update
    sudo yum -y install bearer
  ```

  Update an existing installation with the following:

  ```bash
  sudo yum -y update bearer
  ```

</details>

<details>
  <summary>Docker</summary>

  Bearer CLI is also available as a Docker image on [Docker Hub](https://hub.docker.com/r/bearer/bearer) and [ghcr.io](https://github.com/bearer/bearer/pkgs/container/bearer).

  With docker installed, you can run the following command with the appropriate paths in place of the examples.

  ```bash
  docker run --rm -v /path/to/repo:/tmp/scan bearer/bearer:latest-amd64 scan /tmp/scan
  ```

  Additionally, you can use docker compose. Add the following to your `docker-compose.yml` file and replace the volumes with the appropriate paths for your project:

  ```yml
  version: ""3""
  services:
    bearer:
      platform: linux/amd64
      image: bearer/bearer:latest-amd64
      volumes:
        - /path/to/repo:/tmp/scan
  ```

  Then, run the `docker compose run` command to run Bearer CLI with any specified flags:

  ```bash
  docker compose run bearer scan /tmp/scan --debug
  ```

  The Docker configurations above will always use the latest release.
</details>

<details>
  <summary>Binary</summary>

  Download the archive file for your operating system/architecture from [here](https://github.com/Bearer/bearer/releases/latest/).

  Unpack the archive, and put the binary somewhere in your $PATH (on UNIX-y systems, /usr/local/bin or the like). Make sure it has permission to execute.

  To update Bearer CLI when using the binary, download the latest release and overwrite your existing installation location.
</details>
<br/>

### Scan your project

The easiest way to try out Bearer CLI is with the OWASP [Juice Shop](https://github.com/juice-shop/juice-shop) example project. It simulates a realistic JavaScript application with common security flaws. Clone or download it to a convenient location to get started.

```bash
git clone https://github.com/juice-shop/juice-shop.git
```

Now, run the scan command with `bearer scan` on the project directory:

```bash
bearer scan juice-shop
```

A progress bar will display the status of the scan.

Once the scan is complete, Bearer CLI will output, by default, a security report with details of any rule findings, as well as where in the codebase the infractions happened and why.

By default the `scan` command use the SAST scanner, other [scanner types](https://docs.bearer.com/explanations/scanners) are available.

### Analyze the report

The security report is an easily digestible view of the security issues detected by Bearer CLI. A report is made up of:

* The list of [rules](https://docs.bearer.com/reference/rules/) run against your code.
* Each detected finding, containing the file location and lines that triggered the rule finding.
* A stat section with a summary of rules checks, findings and warnings.

The [OWASP Juice Shop](https://github.com/juice-shop/juice-shop) example application will trigger rule findings and output a full report. Here's a section of the output:

```text
...
HIGH: Sensitive data stored in HTML local storage detected. [CWE-312]
https://docs.bearer.com/reference/rules/javascript_lang_session
To skip this rule, use the flag --skip-rule=javascript_lang_session

File: juice-shop/frontend/src/app/login/login.component.ts:102

 102       localStorage.setItem('email', this.user.email)


=====================================

59 checks, 40 findings

CRITICAL: 0
HIGH: 16 (CWE-22, CWE-312, CWE-798, CWE-89)
MEDIUM: 24 (CWE-327, CWE-548, CWE-79)
LOW: 0
WARNING: 0
```

In addition of the security report, you can also run a [privacy report](https://docs.bearer.com/explanations/reports/#privacy-report).

Ready for the next step? Additional options for using and configuring the `scan` command can be found in [configuring the scan command](https://docs.bearer.com/guides/configure-scan/).

For more guides and usage tips, [view the docs](https://docs.bearer.com/).

## :question: FAQs

### What makes Bearer CLI different from any other SAST tools?

SAST tools are known to bury security teams and developers under hundreds of issues with little context and no sense of priority, often requiring security analysts to triage issues manually.

The most vulnerable asset today is sensitive data, so we start there and [prioritize](https://github.com/Bearer/bearer/issues/728) findings by assessing sensitive data flows to highlight what is more critical, and what is not. This unique ability allows us to provide you with a privacy scanner too.

We believe that by linking security issues with a clear business impact and risk of a data breach, or data leak, we can build better and more robust software, at no extra cost.

In addition, by being Free and Open, extendable by design, and built with a great developer UX in mind, we bet you will see the difference for yourself.

### What is the privacy scanner?

In addition of detecting security flaws in your code, Bearer CLI allows you to automate the evidence gathering process needed to generate a privacy report for your compliance team.

When you run Bearer CLI on your codebase, it discovers and classifies data by identifying patterns in the source code. Specifically, it looks for data types and matches against them. Most importantly, it never views the actual values—it just can’t—but only the code itself. If you want to learn more, here is the [longer explanation](https://docs.bearer.com/explanations/discovery-and-classification/).

Bearer CLI is able to identify over 120+ data types from sensitive data categories such as Personal Data (PD), Sensitive PD, Personally identifiable information (PII), and Personal Health Information (PHI). You can view the full list in the [supported data types documentation](https://docs.bearer.com/reference/datatypes/).

Finally, Bearer CLI also lets you detect components storing and processing sensitive data such as databases, internal APIs, and third-party APIs. See the [recipe list](https://docs.bearer.com/reference/recipes/) for a complete list of components.

### Supported Language

Bearer CLI currently supports:
<table>
  <tr>
    <td>GA</td>
    <td>JavaScript/TypeScript, Ruby, PHP, Java, Go, Python</td>
  </tr>
  <tr>
    <td>Beta</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Alpha</td>
    <td>-</td>
  </tr>
</table>

[Learn more](https://docs.bearer.com/reference/supported-languages/) about language support.

### How long does it take to scan my code? Is it fast?

It depends on the size of your applications. It can take as little as 20 seconds, up to a few minutes for an extremely large code base.

As a rule of thumb, Bearer CLI should never take more time than running your test suite.

In the case of CI integration, we provide a diff scan solution to make it even faster. [Learn more](https://docs.bearer.com/guides/configure-scan/#only-report-new-findings-on-a-branch).

### What about false positives?

If you’re familiar with SAST tools, false positives are always a possibility.

By using the most modern static code analysis techniques and providing a native filtering and prioritizing solution on the most important issues, we believe we have dramatically improved the overall SAST experience.

We strive to provide the best possible experience for our users. [Learn more](https://docs.bearer.com/reference/supported-languages/#how-do-we-evaluate-language-support%3F) about how we achieve this.

### When and where to use Bearer CLI?

We recommend running Bearer CLI in your CI to check new PRs automatically for security issues, so your development team has a direct feedback loop to fix issues immediately.

You can also integrate Bearer CLI in your CD, though we recommend setting it to only fail on high criticality issues, as the impact for your organization might be important.

In addition, running Bearer CLI as a scheduled job is a great way to keep track of your security posture and make sure new security issues are found even in projects with low activity.

Make sure to read our [integration strategy guide](https://docs.bearer.com/guides/integration-strategy/) for more information.

## :raised_hand: Get in touch

Thanks for using Bearer CLI. Still have questions?

* Start with the [documentation](https://docs.bearer.com).
* Have a question or need some help? Find the Bearer team on [Discord][discord].
* Got a feature request or found a bug? [Open a new issue](https://github.com/Bearer/bearer/issues/new/choose).
* Found a security issue? Check out our [Security Policy](https://github.com/Bearer/bearer/security/policy) for reporting details.
* Find out more at [Bearer.com](https://www.bearer.com)

## :handshake: Contributing

Interested in contributing? We're here for it! For details on how to contribute, setting up your development environment, and our processes, review the [contribution guide](CONTRIBUTING.md).

## :rotating_light: Code of conduct

Everyone interacting with this project is expected to follow the guidelines of our [code of conduct](CODE_OF_CONDUCT.md).

## :shield: Security

To report a vulnerability or suspected vulnerability, [see our security policy](https://github.com/Bearer/bearer/security/policy). For any questions, concerns or other security matters, feel free to [open an issue](https://github.com/Bearer/bearer/issues/new/choose) or join the [Discord Community][discord].

## :mortar_board: License

Bearer CLI code is licensed under the terms of the [Elastic License 2.0](LICENSE.txt) (ELv2), which means you can use it freely inside your organization to protect your applications without any commercial requirements.

You are not allowed to provide Bearer CLI to third parties as a hosted or managed service without the explicit approval of Bearer Inc.

---

[test]: https://github.com/Bearer/bearer/actions/workflows/test.yml
[test-img]: https://github.com/Bearer/bearer/actions/workflows/test.yml/badge.svg
[release]: https://github.com/Bearer/bearer/releases
[release-img]: https://img.shields.io/github/release/Bearer/bearer.svg?logo=github
[discord]: https://discord.gg/eaHZBJUXRF",VRAI
beiciliang/aws-bootcamp-cruddur-2023,Documentations,Documentations,2024-02-22T18:32:34Z,2023-05-24T18:29:12Z,0,0,0,0,0,0,2,0,2023-02-15T14:20:44Z,2024-03-25T08:28:26Z,23870,11,Python,VRAI,11,FAUX,0,,0,"Disposable micro blogging platform using Flask and ReactJS, deployed on AWS",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,2,"# AWS Cloud Project Bootcamp - Cruddur

![](https://codebuild.us-east-1.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiV0xHWHgweStGNVB5S2VFbktBNUo2ZnV3M1VWL1NHeVRuUWdSdTJ4NDFieW5QUXpTOU5FS1cwL2tZQzVHUE1LdFZLT2hWV1ZQSForeXU0eUY4ZWFtTklvPSIsIml2UGFyYW1ldGVyU3BlYyI6IkpocjhlVndVYmxwbzRpd0siLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=main)

![Cruddur Graphic](_docs/assets/cruddur-banner.jpg)

- Codebase: this repo was created from [this template](https://github.com/ExamProCo/aws-bootcamp-cruddur-2023)
- Cohort: 2023-A1 ([bootcamp website](https://aws.cloudprojectbootcamp.com/))
- In the end, I obtained the AWS Cloud Project Bootcamp Certificate (Red Squad) with Demonstration of Exceptional Skills as a Cloud Engineer ([certificate at LinkedIn](https://www.linkedin.com/posts/beiciliang_aws-cloud-project-bootcamp-certificate-red-activity-7103294105505427457-TrRE?utm_source=share&utm_medium=member_desktop))!

## Journaling Homework

The `/journal` directory contains:

- [x] [Week 0 - Billing and Architecture](journal/week0.md)
- [x] [Week 1 - App Containerization](journal/week1.md)
- [x] [Week 2 - Distributed Tracing](journal/week2.md)
- [x] [Week 3 - Decentralized Authentication](journal/week3.md)
- [x] [Week 4 - Postgres and RDS](journal/week4.md)
- [x] [Week 5 - DynamoDB and Serverless Caching](journal/week5.md)
- [x] [Week 6 - Deploying Containers](journal/week6.md)
- [x] [Week 7 - Solving CORS with a Load Balancer and Custom Domain](journal/week7.md)
- [x] [Week 8 - Serverless Image Processing](journal/week8.md)
- [x] [Week 9 - CI/CD with CodePipeline, CodeBuild and CodeDeploy](journal/week9.md)
- [x] [Week 10 - CloudFormation](journal/week10.md)

Note:

- Week for clean up is also described in [Week 10 - CloudFormation](journal/week10.md).
- My demo can be seen on ~~https://beici-demo.xyz/~~ (sorry domain expired, not available anymore), which just serves as a static website. This is because the task of Fargate services has been set to 0 in order to save my AWS budget.

## Instructions

Follow [this playlist](https://www.youtube.com/playlist?list=PLBfufR7vyJJ7k25byhRXJldB5AiwgNnWv) to watch detailed instructions by Andrew Brown, who organized this bootcamp with other guest instructors.

In the end of this bootcamp, the Cruddur application looks like follows:

![Cruddur Screenshot](journal/assets/week10-home.png)

A user can:

- Sign up and sign in to post and reply a crud;
- Send messages to others;
- Edit bio and upload avatar.",FAUX
besley/Slickflow,Application System,Application System,2025-03-19T09:09:54Z,2024-08-05T03:09:11Z,2,0,0,0,0,0,0,0,2014-11-30T14:45:33Z,2025-04-02T07:33:25Z,508415,823,JavaScript,VRAI,251,FAUX,11,"bpmn,net8,workflow-engine",11,".NET Open Source Workflow Engine,  .NET 开源工作流",FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,3,"## Slickflow
**Current Version:NET8**

![](https://img.shields.io/github/stars/besley/slickflow.svg) 
![](https://img.shields.io/github/forks/besley/slickflow.svg) 
![](https://img.shields.io/github/tag/besley/slickflow.svg) 
![](https://img.shields.io/github/release/besley/slickflow.svg) 
![](https://img.shields.io/nuget/dt/Slickflow.Engine.svg) 
![](https://img.shields.io/github/issues/besley/slickflow.svg) 


**A Quick Design and Testing Demo:**  
![And Split Demo](https://github.com/besley/besley.github.io/blob/master/Gif/slickflow-andsplit-demo.gif)

**Quick Start Tutorial for Designer Project:**  
1). In the command console, using the command **npm install** to download the node package.

   please notice to run the command, the directory location is in the **ClientApp** path of the **sfd** project.

2). Set up the **sfdapi** project which is an asp.net webapi type project.(IIS is a choice)

3). Setting webapi variable in the **kcofnig.js** file

    kconfig.webApiUrl = ""http://localhost/sfdapic/"" //your sfd webapi backend service url

4). In the command console, using the command **npm run dev** to the the project

5). Access the web project in the browser by 

    **http://localhost:5000**

**.NET/.NETCore Workflow Engine With Full Source Code**  

0. **AI DeepSeek service supported**  
 Slickflow can use AI deepsesk service to generate BPMN flow chart through sending request by users.  
1. **.NET, .NET CORE version both supported**  
 Slickflow is an open-source project based on .NET8; It's easy to use engine product into the cross-platform application.  
2. **BPMN graphic style process diagram**   
 Slickflow is using BPMN2 notation to describe process diagram, the Slickflow designer is HTML5 graph editor and user-friendly to business process communication and business analysis.  
3. **High performance with Dapper.NET library**  
 Dapper is a simple object mapper for .NET and owns the title of King of Micro ORM in terms of speed and is virtually as fast as using a raw ADO.NET data reader. An ORM is an Object Relational Mapper, which is responsible for mapping between database and programming language.
(Ref: https://dapper-tutorial.net/dapper)  
 4. **Multiple databases supported**  
 Slickflow supports SQLSERVER, ORACLE, MySQL and another database, it implemented by the Dapper.NET extension library. The .net core version using EF core to support different database products.  
5. **Workflow patterns supported**  
![Wokflow Pattern](http://www.slickflow.com/content/img/sfterm-en.png)  
 **1). Sequence**  
    the most frequently process pattern   
 **2). Split/Merge**  
   support and/or gateway such as **and/or split**, **and/or join**, together with  condition variables on the transition  
 **3). Sub-process**  
    in the main process, a subprocess node can start a new process life cycle.  
 **4). Multi-instance**  
   multiple performers processing a task together by multiple task instances. All performers both compete for their task, then the process can be continued on. There are **sequence** and **parallel** pattern, and the **percentage** or **count** parameters can be set on it to ensue when can go to the next step.   
    ![Muliple Instance Pattern](http://www.slickflow.com/content/img/wfpattern-mi-en.png)  
 **5). Event interoperation**  
   process instance and activity instance event delegation service, such as process/activity start, execute and complete.  
 **6). Timer**  
   integrated with **HangFire** library, and with **CRON** expression supported  
 **7). Email**  
   todo or overdue tasks email notification  
 **8). Withdraw**  
	withdraw the task after just sent out to next step users.  
 **9). Sendback**  
    send back to the previous step user, because of some exceptions.  
 **10). Resend**  
    combined after sendback and re-send the task to original sendback users.  
 **11). Reverse**  
    reverse the process instance alive when completed.  
 **12). Jump**  
    jump the process over by several steps forward or backward.   
 **13). MessageQueue(RabbitMQ)**  
    message publishing and subscribing to implement message throwing and catching. 
    
**6. Process Version**  
     the process has version property to upgrade a new definition due to the business process changed.    
**7. XML Cache**    
     the runtime instance use cache to keep the XML process diagram by an expired duration.  
**8. Sequence Process Code Style**  
 **0). Model**  
	
    //create a simple sequence process diagram by hand code rather than a HTML designer  
    var pmb = ProcessModelBuilder.CreateProcess(""simple-process-name"", ""simple-process-code"");
	var process = pmb.Start(""Start"")
		.Task(""Task1"")
		.Task(""Task2"")
		.End(""End"")
		.Store();       

   ![simple sequence diagram](http://www.slickflow.com/content/img/simple-sequence.png)  
    
                
 **1). Start**  
    
    //start a new process instance
    IWorkflowService wfService = new WorkflowService();
    var wfResult = wfService.CreateRunner(""10"", ""jack"")
                .UseApp(""DS-100"", ""Book-Order"", ""DS-100-LX"")
                .UseProcess(""PriceProcessCode"")
                .Start();

 **2). Run**  
    
    //run a process instance to next step
    IWorkflowService wfService = new WorkflowService();
    var wfResult = wfService.CreateRunner(""10"", ""jack"")
                .UseApp(""DS-100"", ""Book-Order"", ""DS-100-LX"")
                .UseProcess(""PriceProcessCode"")
                .NextStepInt(""20"", ""Alice"")
                .Run();
				 
 **3). Withdraw**  
    
    //Withdraw a activity instance to previous step
    IWorkflowService wfService = new WorkflowService();
    var wfResult = wfService.CreateRunner(""10"", ""Jack"")
                .UseApp(""DS-100"", ""Book-Order"", ""DS-100-LX"")
                .UseProcess(""PriceProcessCode"")
                .OnTask(id)             //TaskID
                .Withdraw();

 **4). SendBack**  
    
    //Sendback a activity instance to previous step
    IWorkflowService wfService = new WorkflowService();
    var wfResult = wfService.CreateRunner(""20"", ""Alice"")
                .UseApp(""DS-100"", ""Book-Order"", ""DS-100-LX"")
                .UseProcess(""PriceProcessCode"")
                .PrevStepInt()
                .OnTask(id)             //TaskID
                .SendBack();

**9. Rich demo projects**  
  WebDemo, MvcDemo, and WinformDemo project are demonstrated for a different type of enterprise information systems.   
**10. Target**  
  Slickflow is very suitable for software teams or companies who want to integrate workflow engine into their products.  
**11. Suggestions**  
  Slickflow is suggested to give programmers a flexible way to integrate workflow engine components into their products or customer projects. The programmers can write their own code segments based on the engine component.   
**12. Open Source Project License**    
 The product is under **Slickflow Open Source Project license**.    
 1). Slickflow software must be legally used, and should not be used in violation of the law, morality and other acts that endanger social interests;  
 2). Non-transferable, non-transferable and indivisible authorization of this software;  
 3). The source code can be modified to apply Slickflow components in their own projects or products, but Slickflow source code can not be separately encapsulated for sale or distributed to third-party users;  
 4). The intellectual property rights of Slickflow software shall be protected by law, and no documents such as technical data shall be made public or sold.  
**13. Commercial license**  
 The enterprise, ultimate and universe version can be provided with a commercial license, technical support and upgrade service.
 
 if you have any further inquiry, please feel free to contact us:   

**Email: sales@ruochisoft.com**  
**QQ(Author): 47743901**

**Document:**  
http://doc.slickflow.net  
**Wiki Page:**  
https://github.com/besley/Slickflow/wiki  
**CodeProject Articles:**  
https://www.codeproject.com/Articles/5246528/Slickflow-NET-Core-Open-Source-Workflow-Engine 
https://www.codeproject.com/Articles/5252483/Slickflow-Coding-Graphic-Model-User-Manual  
**Slickflow website:**  
http://www.slickflow.net  
http://www.slickflow.com  
**Demo:**  
http://www.slickflow.com/demo/index  
**Designer Demo:**  
http://demo.slickflow.com/sfd/  
**Form Builder**  
The online dynamic form demo:http://demo.slickflow.com/fbd/   
The formbuilder project:http://github.com/besley/formbuilder/   
**YouTube Workflow Pattern Video**
https://www.youtube.com/@besley7836

![AskForLeave Form Approval](https://github.com/besley/besley.github.io/blob/master/Gif/SlickQua-Ask4Leave-Demo.gif)  

**Slickflow(3.0.0.0) 企业版：** 


0. 支持AI模型DeepSeek生成BPMN流程图;
1. 集成BpmnJS 设计器，XML模式为BPMN2;
2. 重构Slickflow.Engine项目;
3. 演示项目为MvcDemo和WebDemo；
4. 全部项目 .NET8 版本实现；
5. 技术开发文档网站发布
	http://doc.slickflow.com (中文) 
	http://doc.slickflow.net(English) 
	

引擎功能各个版本描述见产品页面：

http://www.slickflow.com/product/index 


**EMail: sales@ruochisoft.com**  
**QQ(Author): 47743901**  
**WeChat(Author): besley2008**  


**Slickflow 网站:**  
http://www.slickflow.com  (中文) 
http://www.slickflow.net (English) 

**DEMO:**  
http://www.slickflow.com/demo/index  

**Document 文档:**  
http://doc.slickflow.com (中文) 
http://doc.slickflow.net(English) 

**Donation 捐赠:**  
您的捐赠将用于产品的持续研发和社区建设

Your donation will be used for the continuous research and development of the product and community building

[![Donate with PayPal](https://github.com/besley/besley.github.io/blob/master/Images/paypal/donation.png)](https://paypal.me/slickflownet)",FAUX
bmuschko/cks-crash-course,Documentations,Documentations,2024-08-20T18:07:53Z,2023-06-18T12:04:47Z,0,0,0,0,0,0,0,1,2022-04-07T13:07:47Z,2025-04-02T01:45:14Z,26414,128,Shell,VRAI,143,FAUX,0,"certification,kubernetes,training",0,In-depth and hands-on practice for acing the exam.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,2,"# Certified Kubernetes Security Specialist (CKS) Crash Course

Vulnerabilities in software and IT infrastructure, once exploited, can pose a major threat to organizations. The Cloud Native Computing Foundation (CNCF) developed the [Certified Kubernetes Security Specialist (CKS) certification](https://www.cncf.io/certification/cks/) to verify a Kubernetes administrator’s proficiency to protect a Kubernetes cluster and the cloud-native software operated in it. The exam is different from the typical multi-choice format of other certifications. It’s completely performance-based and requires deep knowledge of the tasks at hand under immense time pressure. Are you ready to pass the test on the first go?

This practical course is designed to walk you through all the topics covered in the exam to fully prepare to pass the certification exam. The trainer, [CKAD](https://www.credly.com/badges/98ba0895-b669-47d5-8206-50b7223940e3), [CKA](https://www.credly.com/badges/9a599e63-6155-422e-b169-8eaaed5369ab), and [CKS](https://www.credly.com/badges/24cb66c7-74ac-461a-95a4-d272d42bfdaa), Benjamin Muschko, will also share his personal experience with preparing for all aspects of the exam.

## Prerequisites

All exercises in this repository practice a self-contained portion of the [CKS curriculum](https://github.com/cncf/curriculum). Please make sure to follow the [instructions](./prerequisites/instructions.md) for setting up your environment before joining the training.

## Exercises

All [exercises](./exercises) are numbered and live in dedicated directories starting with the name `exercise-`. You'll find instructions for each exercise in each folder. Solutions are available in the `solution` folder. Try to solve each exercise yourself before having a look at the solution.

## Additional Resources

* 💬 [Kubernetes Slack Channel #certifications](https://kubernetes.slack.com/)
* 📚 [O'Reilly: Certified Kubernetes Security Specialist (CKS) Study Guide](https://learning.oreilly.com/library/view/certified-kubernetes-security/9781098132965/)
* 📚 [O'Reilly: Certified Kubernetes Administrator (CKA) Study Guide](https://learning.oreilly.com/library/view/certified-kubernetes-administrator/9781098107215/)
* 🎞️ [KodeKloud: Certified Kubernetes Security Specialist (CKS)](https://kodekloud.com/courses/certified-kubernetes-security-specialist-cks/)
* 🎞️ [YouTube: Kubernetes CKS Full Course Theory + Practice + Browser Scenarios](https://www.youtube.com/watch?v=d9xfB5qaOfg)
* 🎞️ [A Cloud Guru: Certified Kubernetes Security Specialist (CKS)](https://learn.acloud.guru/course/certified-kubernetes-security-specialist)
* 🧪 [O'Reilly: CKAD Interactive Practice Labs](https://learning.oreilly.com/playlists/c94bd9b1-6277-4eb4-b442-9555ab6ad594/)
* 🧪 [Killer Shell: CKS Simulator](https://killer.sh/cks)
* 🧪 [Study4Exam: Certified Kubernetes Security Specialist Exam](https://www.study4exam.com/linux-foundation/info/cks)",FAUX
boostsecurityio/poutine,Toolkit,Application System,2025-05-09T20:59:02Z,2025-01-09T15:08:50Z,0,27,0,0,0,0,0,0,2024-04-09T17:59:41Z,2025-04-07T07:48:37Z,6139,263,Go,VRAI,27,FAUX,27,"ci,cli,devops,devsecops,gh-extension,github,github-actions,golang,security,security-scanner,supply-chain,supply-chain-security",27,boostsecurityio/poutine,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,11,"[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/8787/badge)](https://www.bestpractices.dev/projects/8787)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/boostsecurityio/poutine/badge)](https://securityscorecards.dev/viewer/?uri=github.com/boostsecurityio/poutine)
![build](https://github.com/boostsecurityio/poutine/actions/workflows/build_test.yml/badge.svg)
![CodeQL](https://github.com/boostsecurityio/poutine/actions/workflows/codeql.yml/badge.svg)
[![Go Reference](https://pkg.go.dev/badge/github.com/boostsecurityio/poutine/v4.svg)](https://pkg.go.dev/github.com/boostsecurityio/poutine)
[![Go Report Card](https://goreportcard.com/badge/github.com/boostsecurityio/poutine)](https://goreportcard.com/report/github.com/boostsecurityio/poutine)
[![SLSA 3](https://slsa.dev/images/gh-badge-level3.svg)](https://slsa.dev)

[![View site - GH Pages](https://img.shields.io/badge/View_site-GH_Pages-2ea44f?style=for-the-badge)](https://boostsecurityio.github.io/poutine/)

# `poutine`

Created by [BoostSecurity.io](https://boostsecurity.io), `poutine` is a security scanner that detects misconfigurations and vulnerabilities in the build pipelines of a repository. It supports parsing CI workflows from GitHub Actions and Gitlab CI/CD. When given an access token with read-level access, `poutine` can analyze all the repositories of an organization to quickly gain insights into the security posture of the organization's software supply chain.

<table>
<td>

![Finding raised by poutine about ""Arbitrary Code Execution from Untrusted Code Changes""](https://github.com/boostsecurityio/poutine/assets/172889/ca031a4f-afd8-4e3f-9e66-a2502bd0379b)

</td>
</table>

See the [documentation](docs/content/en/rules) for a list of rules currently supported by `poutine`.

## Why `poutine`?

In French, the word ""poutine"", when not referring to the [dish](https://en.wikipedia.org/wiki/Poutine), can be used to mean ""messy"". Inspired by the complexity and intertwined dependencies of modern open-source projects, `poutine` reflects both a nod to our Montreal roots and the often messy, complex nature of securing software supply chains.

## Supported Platforms

- GitHub Actions
- Gitlab Pipelines
- Azure DevOps
- Pipelines As Code Tekton

## Getting Started

### Installation

To install `poutine`, download the latest release from the [releases page](https://github.com/boostsecurityio/poutine/releases) and add the binary to your $PATH. 

<!-- TODO: cosign verify instructions? -->

#### Homebrew
``` bash
brew install poutine
```

#### Docker
``` bash
docker run -e GH_TOKEN ghcr.io/boostsecurityio/poutine:latest
```

#### GitHub Actions
```yaml
...
jobs:
  poutine:
    runs-on: ubuntu-latest
    permissions:
      security-events: write
      contents: read
    steps:
    - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
#################################################################################################
    - name: poutine - GitHub Actions SAST
      uses: boostsecurityio/poutine-action@main # We recommend to use a tagged version and pin it
#################################################################################################
    - name: Upload poutine SARIF file
      uses: github/codeql-action/upload-sarif@4355270be187e1b672a7a1c7c7bae5afdc1ab94a # v3.24.10
      with:
        sarif_file: results.sarif
```

### Usage
``` bash
poutine [command] [arguments] [options]
```

#### Analyze a local repository

``` bash
poutine analyze_local .
```

#### Analyze a remote GitHub repository

```bash
poutine analyze_repo org/repo --token ""$GH_TOKEN""
```

#### Analyze all repositories in a GitHub organization

```bash
poutine analyze_org org --token ""$GH_TOKEN""
```


#### Analyze all projects in a self-hosted Gitlab instance

``` bash
poutine analyze_org my-org/project --token ""$GL_TOKEN"" --scm gitlab --scm-base-uri https://gitlab.example.com
```

### Configuration Options

``` 
--token          SCM access token (required for the commands analyze_repo, analyze_org) (env: GH_TOKEN)
--format         Output format (default: pretty, json, sarif)
--ignore-forks   Ignore forked repositories in the organization(analyze_org)
--scm            SCM platform (default: github, gitlab)
--scm-base-uri   Base URI of the self-hosted SCM instance
--threads        Number of threads to use (default: 2)
--config         Path to the configuration file (default: .poutine.yml)
--verbose        Enable debug logging
```

See [.poutine.sample.yml](.poutine.sample.yml) for an example configuration file.

## Building from source

Building `poutine` requires Go 1.24+.

```bash
git clone https://github.com/boostsecurityio/poutine.git
cd poutine
make build
```

## Development
### Updating Build Platform CVE Database
```bash
go test -tags build_platform_vuln_database ./...
opa fmt -w opa/rego/external/build_platform.rego
```

## See Also 

For examples of vulnerabilities in GitHub Actions workflows, you can explore the [Messy poutine GitHub organization](https://github.com/messypoutine). It showcases real-world vulnerabilities from open-source projects readily exploitable for educational purposes. 

To get started with some hints, try using `poutine` to analyze the `messypoutine` organization:
``` bash
poutine analyze_org messypoutine --token `gh auth token`
```

You may submit the flags you find in a [private vulnerability disclosure](https://github.com/messypoutine/.github/security/advisories/new).

## License

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.",VRAI
borgeby/jarl,Toolkit,Toolkit,2024-01-18T14:16:15Z,2022-11-04T11:40:56Z,0,12,0,0,0,0,0,0,2022-04-04T00:43:31Z,2024-10-29T02:19:14Z,3593,41,Clojure,VRAI,5,FAUX,25,"clojure,clojurescript,jvm,opa,rego",25,"Jarl is an Open Policy Agent (OPA) evaluator for the JVM and Javascript, written in Clojure(Script)",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,5,"# Jarl

![build](https://github.com/borgeby/jarl/actions/workflows/check.yaml/badge.svg)
[![codecov](https://codecov.io/gh/borgeby/jarl/branch/main/graph/badge.svg?token=PHGLRDWE39)](https://codecov.io/gh/borgeby/jarl)

Jarl is an [Open Policy Agent](https://www.openpolicyagent.org/) (OPA) evaluator for the JVM and Javascript, written in Clojure(Script). Jarl allows you to evaluate policy directly in your JVM language or Javascript environment of choice. See [this blog](https://blog.openpolicyagent.org/i-have-a-plan-exploring-the-opa-intermediate-representation-ir-format-7319cd94b37d) for an introduction to the intermediate representation (IR) format of OPA that Jarl uses for evaluation, and why you may want to consider that as an option.

While originally an abbreviation for the ""JVM Alternative for the Rego Language"", Javascript support — for both the browser
and Node.js — has been made possible by ClojureScript, and it's not unthinkable that more platforms will be added to the
[support matrix](https://github.com/johanfylling/jarl/blob/main/doc/builtins.md) in the future.

## Usage

Simple example policy compiled to plan by OPA, and executed by Jarl.

**policy.rego**
```rego
package policy

import future.keywords.if
import future.keywords.in

default allow := false

# METADATA
# entrypoint: true
allow if ""admin"" in input.user.roles
```
```shell
opa build --target plan policy.rego
```

We now have a bunde containing the `plan.json` file that we may submit for execution by Jarl!

Before we do, we'll need to create an input to use for evaluation.

**input.json**
```json
{
    ""user"": {
        ""roles"": [
            ""admin""
        ]
    }
}
```

**Evaluate**
```shell
lein run bundle.tar.gz --input input.json
```

**Output**
```json
[{""result"":true}]
```

Note that the above constitutes a simple flow for development and testing only. Production artifacts, or a fixed API for
integrations, will be here at a later point in time.

### Clojure

In the `core` directory, run `lein repl` to launch a REPL in the `jarl.core` namespace:

```clojure
(jarl.logging/set-log-level :warn)

(def input {""user"" {""roles"" [""admin""]}})
(def data {})

(-> (slurp ""path/to/plan.json"")
    (parser/parse-json)
    (eval-plan ""policy/allow"" input data))

; [{""result"" true}]
```

### Java

Jarl provides a simple API for evaluating policies.

#### Evaluating a Plan

```java
var file = new File(""path/to/plan.json"");
var input = Map.of(""user"", ""alice"");
Map<String, ?> data = Map.of();
var allowed = Jarl.builder(file)
        .build()
        .getPlan(""my_policy/allow"")
        .eval(input, data)
        .allowed();
if (allowed) {
  ...
}
```

## Built-in Functions

While still in an early stage of development, Jarl already supports [most of the built-in functions](doc/builtins.md)
provided by OPA. Jarl intends not just to support all built-in functions out of the box, but to make it trivial to
implement custom built-in functions for any platform, or even replace existing implementations with custom ones.

## Development

See the [development guide](doc/development.md).",FAUX
btkrausen/hashicorp,Documentations,Documentations,2025-05-05T11:47:28Z,2024-10-31T12:55:44Z,2,2,0,0,0,0,0,0,2020-03-03T17:49:01Z,2025-04-07T11:12:44Z,40750,1290,HCL,VRAI,1896,FAUX,10,,10,"Random code for HashiCorp related projects, training, etc.",FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,13,"### Support My Content Here: 
[![Udemy](https://img.shields.io/badge/Udemy-A435F0?style=for-the-badge&logo=Udemy&logoColor=white)](https://btk.me/vb) [![Github Sponsor](https://img.shields.io/badge/sponsor-30363D?style=for-the-badge&logo=GitHub-Sponsors&logoColor=#EA4AAA)](https://github.com/sponsors/btkrausen?frequency=one-time&sponsor=btkrausen)

## HashiCorp-related training courses, projects, and book

This repo is intended to supplement training courses and external projects related to HashiCorp training and community-based engagement. It is not affiliated with HashiCorp and should not be treated as such. Content shared here is targeted for students looking for additional content or content referenced in material created for educational purposes.

Check out my profile on GitHub at [btk.me/btk](btk.me/btk)

*********************************************************************************

<a href=""https://amzn.to/3VUZcOW""> <img align=""center"" alt="""" src=""https://m.media-amazon.com/images/I/51KLqAx5-QL._SY522_.jpg"" width=""200"" height=""300"" /></a>
<a href=""https://amzn.to/2UeUjAI""> <img align=""center"" alt="""" src=""https://images-na.ssl-images-amazon.com/images/I/41SXDY4t6-L._SX404_BO1,204,203,200_.jpg"" width=""250"" height=""300"" /></a>
<a href=""https://amzn.to/3HAw4pF""> <img align=""center"" alt="""" src=""https://m.media-amazon.com/images/I/41MY0+EHAbL._SX331_BO1,204,203,200_.jpg"" width=""200"" height=""300"" /></a>

<br>

### **Buy my books on Amazon:**
- 👉 [HashiCorp Nomad Fundamentals - The Ultimate Beginner's Guide](https://amzn.to/3VUZcOW)
- 👉 [Running HashiCorp Vault in Production](https://amzn.to/2UeUjAI)
- 👉 [The Best-Kept Secrets of HashiCorp Vault](https://amzn.to/3HAw4pF)

### **Buy my books on Gumroad in PDF format**
- 👉 [HashiCorp Nomad Fundamentals - The Ultimate Beginner's Guide](https://btkrausen.gumroad.com/l/nomad)
- 👉 [Running HashiCorp Vault in Production in PDF](https://btkrausen.gumroad.com/l/vaultbook)
- 👉 [The Best-Kept Secrets of HashiCorp Vault](https://btkrausen.gumroad.com/l/secretsofvault)
<br>

## **Udemy Discount Coupons**

[Udemy Profile for Bryan Krausen](https://www.udemy.com/user/bryan-krausen/ ""Udemy Profile"")

## 🧑‍💻 🧑‍💻 Video Courses 🧑‍💻 🧑‍💻

| Course Link | Repo Link | Coupon Code |
| ----------- | :---------: |:------------: |
| [HashiCorp Terraform: The Ultimate Beginner's Guide (with Labs)](https://btk.me/tb) | [Link](https://github.com/btkrausen/terraform-codespaces) | APRIL2025 |
| [GitHub Copilot: Use GenAI to Write Terraform for You!](https://btk.me/cp) | [Link](https://github.com/btkrausen/terraform)| APRIL2025 |
| [HashiCorp Vault for the Absolute Beginner](https://btk.me/vb) | [Link](https://github.com/btkrausen/hashicorp/tree/master/vault)| APRIL2025 |
| [HashiCorp Certified: Vault Associate (w/ Hands-On Labs)](https://btk.me/v) | [Link](https://github.com/btkrausen/hashicorp/tree/master/vault)| APRIL2025 |
| [Integrating HashiCorp Vault with AWS](https://btk.me/vaws) | [Link](https://github.com/btkrausen/hashicorp/tree/master/vault)| APRIL2025 |
| [HashiCorp Certified: Vault Operations Professional](https://btk.me/vp) |[Link](https://github.com/btkrausen/hashicorp/tree/master/vault) | APRIL2025 |
| [Mastering Terraform Cloud with Hands-On Labs](https://btk.me/tfc) | [Link](https://github.com/btkrausen/hashicorp/tree/master/terraform-cloud)| APRIL2025 |
| [HashiCorp Certified: Terraform Associate Hands-On Lab Course](https://btk.me/tfhol) |[Link](https://github.com/btkrausen/hashicorp/tree/master/terraform) | APRIL2025 |
| [HashiCorp Certified: Consul Associate (w Hands-On Labs)](https://btk.me/c) | [Link](https://github.com/btkrausen/hashicorp/tree/master/consul)| APRIL2025 |
| [Building Automated Machine Images using HashiCorp Packer](https://btk.me/p) | [Link](https://github.com/btkrausen/hashicorp/tree/master/packer)| APRIL2025 |
| [HashiCorp Nomad Fundamentals: The Ultimate Beginner's Guide](https://btk.me/n) | [Link](https://github.com/btkrausen/hashicorp/tree/master/nomad)| APRIL2025 |
| [Amazon S3 Deep Dive: The Ultimate Guide to AWS Cloud Storage](https://btk.me/s3) | [Link](https://github.com/btkrausen/aws)| APRIL2025 |
| [Amazon Bedrock - The Complete Guide to AWS Generative AI](https://btk.me/ab) | [Link](https://github.com/alexhddev/Bedrock_course)| APRIL2025 |

## 📝 📝 Practice Exam Courses 📝 📝
| Course Link | Coupon Code |
| ----------- | :-----------: |
| [HashiCorp Certified: Terraform Associate Practice Exam](https://btk.me/tf) | APRIL2025 |
| [Kubernetes and Cloud Native Associate (KCNA) Practice Exams](https://btk.me/kcna) | APRIL2025 |
| [HashiCorp Certified: Vault Associate Practice Exam](https://btk.me/vpe) | APRIL2025 |
| [HashiCorp Certified: Consul Associate Practice Exam](https://btk.me/cpe) | APRIL2025 |
| [GitHub Foundations Certification Practice Exams](https://btk.me/ghp) | APRIL2025 |
| [GitHub Actions Certification Practice Exams](https://btk.me/gha) | APRIL2025 |

Please feel free to reach through [Twitter](https://twitter.com/btkrausen) or [LinkedIn](https://www.linkedin.com/in/bryan-krausen-5ab8794/) for questions or comments.",FAUX
build-on-aws/bookstore-demo-app-with-authz,Documentations,Documentations,2025-03-07T17:29:53Z,2024-03-29T12:54:44Z,0,0,0,1,0,0,0,0,2023-10-31T16:48:01Z,2025-04-01T19:23:32Z,1864,11,Python,VRAI,10,FAUX,0,"amazon-cognito,amazon-verified-permissions,authorization,aws-amplify,aws-amplify-vue,aws-lambda,aws-lambda-python,cedar,python,python3",0,"Authorization is one of the foundational needs when building your applications and services. Learn how, with the help of Cedar and Amazon Verified Permissions, to to add non-trivial authorization rules to your web application.",FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,3,"# Bookstore Demo Application with Authorization

## Context

This repository is the main asset for the [AWS re:Invent 2023](https://reinvent.awsevents.com) breakout session titled: ""*Build verifiable and effective application authorization in 40 minutes*"" ([BOA209](https://hub.reinvent.awsevents.com/attendee-portal/catalog/?search=boa209)).

It contains a sample application that demonstrates how you could add authorization layer using [Amazon Verified Permissions](https://aws.amazon.com/verified-permissions) and [Cedar](https://www.cedarpolicy.com/en) policy language. The backend as a serverless application, written in [Python](https://www.python.org) and exposed as a *REST API*, making use of [Amazon API Gateway](https://aws.amazon.com/api-gateway), [AWS Lambda](https://aws.amazon.com/lambda), and [Amazon Cognito](https://aws.amazon.com/cognito). The frontend is a [Vue.js](https://docs.amplify.aws/vue) application using the [AWS Amplify](https://docs.amplify.aws) SDK for authentication and communication with the provided *API*.

## Goal

As stated above, the goal of this application is to integrate an authorization (_AuthZ_ or _authz_) layer into a bookstore application, which already uses *Amazon Cognito* for authentication. The addition of *Amazon Verified Permissions (AVP)* with policies defined in *Cedar* language will allow you to enhance, maintain in an easier way, and reason about the authorization rules. The application itself is really a demo, as it is just listing books.

For integration of the *Amazon Verified Permissions (AVP)* with bookstore application, you need to define a set of test users with varying attributes and roles. These users will help you validate different authorization scenarios based on the policies defined in *AVP* and the user attributes managed by *Amazon Cognito*.

## Architecture & Design

![Architecture Diagram](./images/architecture-diagram.png)

### Overview of attributes inside Amazon Cognito User Pool Schema

In our bookstore application, you will use *Amazon Cognito* for managing user authentication. The user attributes in our *Cognito User Pool* are designed to support both the security and functionality of our application. Here's a brief overview of these attributes:

1. **ID**: `id`
   - **Description:** Every user has a unique ID, automatically generated by Cognito. This ID helps identify users distinctly.
2. **Email**: `email`
   - **Description:** Users register and sign in using their email addresses.
3. **Role**: `role`
   - **Description:** Users have assigned roles that define their access within the application. Here is list of available roles:
     - **Admin:** Has complete access to all books (premium offers, etc.).
     - **Publisher:** Can manage and view their own published books.
     - **Customer:** Can browse and purchase books.
4. **Years as Member**: `yearsAsMember`
   - **Description:** This attribute indicates how long a user has been a member of our service. It's used for offering specific features or content to loyal users.

### API Design

#### Product Service

```text
GET     /product                Returns details for all products (books).
GET     /product/{book_id}      Returns details for a single product (individual book).
```

### Authorization

You can review documentation for the authorization scenarios (including available roles and users), prepared *Cedar* policies, and *Policy Store* schema inside [docs](./docs/README.md) directory.

## Running the Example

Here you can find a list of the recommended prerequisites for this repository.

- Pre-installed tools:
  - Most recent *AWS CLI* (`2.13.37` or higher, [Installation Guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html)).
  - Most recent *AWS SAM CLI* (`1.103.0` or higher, [Installation Guide](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/install-sam-cli.html)).
  - Node.js in version `20.9.x` or higher.
  - Python in version `3.10.x` or higher.
- Configured profile in the installed *AWS CLI* with credentials for your *AWS IAM* user account of choice.

### Setup steps

[Fork](https://github.com/build-on-aws/bookstore-demo-app-with-authz/fork) the GitHub repo, then clone your fork locally:

```shell
$ git clone https://github.com/<YOUR-GITHUB-USERNAME>/bookstore-demo-app-with-authz && cd bookstore-demo-app-with-authz
```

If you wish to use a named profile for your AWS credentials, you can set the environment variable `AWS_PROFILE` before running the below commands. For a profile named `development` it looks as follows `export AWS_PROFILE=development`.

You now have 2 options: you can deploy the backend and run the frontend locally, or you can deploy the whole project using the *AWS Amplify* console.

#### Option 1: Deploy backend and run frontend locally

##### Deploy the Backend

A new S3 bucket will be automatically created for you which will be used for deploying source code to AWS. If you wish to  use an existing bucket instead, you can manually set the `S3_BUCKET` environment variable to the name of your bucket.

Build and deploy the resources:

```bash
# After cloning it, inside the the repository root:

# Creates S3 bucket if not existing already, then deploys AWS CloudFormation stacks for authentication and product service.
$ make backend
```

##### Run the Frontend Locally

Start the frontend locally:

```bash
# After cloning it, inside the the repository root:

# Retrieves backend config from AWS SSM parameter store to a .env file, then starts service.
$ make frontend-serve
```

Once the service is running, you can access the frontend on `http://localhost:8080`.

You can create an account by clicking on ""*Sign In*"" then ""*Create Account*"". Be sure to use a valid email address as you'll need to retrieve the verification code sent by *Amazon Cognito*. Or, you can automate the whole process by using a script:

```bash
$ python manage-app-users.py                                      \
    --command create   # ... or delete, if you want to clean-up   \
    --cognito-user-pool-id ""<COGNITO_USER_POOL_ID>""               \
    --email-prefix ""your-first-part-of-the-email""                 \
    --email-postfix ""domain.com""
```

**Note:** [CORS](https://aws.amazon.com/what-is/cross-origin-resource-sharing/#:~:text=Cross-origin%20resource%20sharing%20(CORS,resources%20in%20a%20different%20domain.) headers on the backend service default to allowing `http://localhost:8080`. You will see *CORS* errors if you access the frontend using the IP address (like `http://127.0.0.1:8080`), or using a port other than 8080.

##### Clean Up

Delete the AWS CloudFormation stacks created by this project:

```bash
# After cloning it, inside the the repository root:

$ make backend-delete
```

Keep in mind that there will be also one additional *Amazon S3* bucket created for storing code archives, also starting with a similar name: `bookstore-demo-app-with-authz-src-`.

#### Option 2: Automatically deploy backend and frontend using *Amplify Console*

[![One-click deployment](https://oneclick.amplifyapp.com/button.svg)](https://console.aws.amazon.com/amplify/home#/deploy?repo=https://github.com/build-on-aws/bookstore-demo-app-with-authz)

1. Use **1-click deployment** button above, and continue by clicking ""*Connect to GitHub*"".
2. If you don't have an *IAM Service Role* with administrative permissions, select ""*Create new role*"".
   - If you already have that, you can jump directly to the 5th step.
3. Select ""*Amplify*"" from the drop-down, and select ""*Amplify - Backend Deployment*"", then click ""*Next*"".
4. Click ""*Next*"" again, then give the role a name and click ""*Create role*"".
5. In the Amplify console and select the role you created, then click ""*Save and deploy*"".
6. Amplify Console will fork this repository into your *GitHub* account and deploy it for you.
7. You should now be able to see your app being deployed in the [Amplify Console](https://console.aws.amazon.com/amplify/home).
8. Within your new app in Amplify Console, wait for deployment to complete.
   - This should take approximately ~10-15 minutes for the first deploy.

##### Clean Up

Delete the AWS Amplify application and AWS CloudFormation stacks created by this project. There are 3 of them, with names starting with `bookstore-demo-app-with-authz-`.

## References

- [Amazon Verified Permissions - Documentation](https://docs.aws.amazon.com/verifiedpermissions/latest/userguide/what-is-avp.html)
- [Cedar Language Playground](https://www.cedarpolicy.com/en/playground)
- [Cedar Policy GitHub Organization](https://github.com/cedar-policy)
- [Cedar Policy Langauge - Documentation](https://docs.cedarpolicy.com)
- [Cedar Policy Language - Best Practices](https://docs.cedarpolicy.com/bestpractices/bp-naming-conventions.html)
- [AVP CLI Tool](https://github.com/Pigius/avp-cli) by [Daniel Aniszkiewicz (Pigius)](https://github.com/Pigius)
- Blog posts series [Authorization and Cedar/Amazon Verified Permissions](https://dev.to/aws-builders/authorization-and-cedar-a-new-way-to-manage-permissions-part-i-1nid) by [Daniel Aniszkiewicz (Pigius)](https://dev.to/pigius)

## License

This library is licensed under the MIT-0 License. See the [LICENSE](LICENSE) file.",FAUX
canonical/snapd,Toolkit,Application System,2025-05-14T20:36:19Z,2025-04-30T10:59:42Z,0,0,0,0,0,6,0,0,2015-10-21T11:51:46Z,2025-04-08T09:02:37Z,165226,1941,Go,FAUX,603,FAUX,135,,135,The snapd and snap tools enable systems to work with .snap files.,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,320,"[![Snapcraft](https://avatars2.githubusercontent.com/u/19532717?s=200)](https://snapcraft.io)

# Welcome to snapd

This is the code repository for **snapd**, the background service that manages
and maintains installed snaps.

Snaps are app packages for desktop, cloud and IoT that update automatically,
are easy to install, secure, cross-platform and dependency-free. They're being
used on millions of Linux systems every day.

Alongside its various service and management functions, snapd:
- provides the _snap_ command that's used to install and remove snaps and
  interact with the wider snap ecosystem
- implements the confinement policies that isolate snaps from the base system
  and from each other
- governs the interfaces that allow snaps to access specific system resources
  outside of their confinement

For general details, including
[installation](https://snapcraft.io/docs/installing-snapd) and [Getting
started](https://snapcraft.io/docs/getting-started) guides, head over to our
[Snap documentation](https://snapcraft.io/docs). If you're looking for
something to install, such as [Spotify](https://snapcraft.io/spotify) or
[Visual Studio Code](https://snapcraft.io/code), take a look at the [Snap
Store](https://snapcraft.io/store). And if you want to build your own snaps,
start with our [Creating a snap](https://snapcraft.io/docs/creating-a-snap)
documentation.

## Get involved

This is an [open source](COPYING) project and we warmly welcome community
contributions, suggestions, and constructive feedback. If you're interested in
contributing, please take a look at our [Code of Conduct](CODE_OF_CONDUCT.md)
first.

- to report an issue, please file [a bug
  report](https://bugs.launchpad.net/snapd/+filebug) on our [Launchpad issue
tracker](https://bugs.launchpad.net/snapd/)
- for suggestions and constructive feedback, create a post on the [Snapcraft
  forum](https://forum.snapcraft.io/c/snapd)
- to build snapd manually, or to get started with snapd development, see
  [HACKING.md](HACKING.md)

## Get in touch

We're friendly! We have a community forum at
[https://forum.snapcraft.io](https://forum.snapcraft.io) where we discuss
feature plans, development news, issues, updates and troubleshooting. You can
chat in realtime with the snapd team and our wider community on the
[#snappy](https://web.libera.chat?channel=#snappy) IRC channel on
[libera chat](https://libera.chat/).

For news and updates, follow us on [Twitter](https://twitter.com/snapcraftio)
and on [Facebook](https://www.facebook.com/snapcraftio).

## Project status

| Service | Status |
|-----|:---|
| [Github Actions](https://github.com/actions/) |  [![Build Status][actions-image]][actions-url]  |
| [GoReport](https://goreportcard.com/) |  [![Go Report Card][goreportcard-image]][goreportcard-url] |
| [Codecov](https://codecov.io/) |  [![codecov][codecov-image]][codecov-url] |

[actions-image]: https://github.com/snapcore/snapd/actions/workflows/test.yaml/badge.svg?branch=master
[actions-url]: https://github.com/snapcore/snapd/actions?query=branch%3Amaster+event%3Apush

[goreportcard-image]: https://goreportcard.com/badge/github.com/snapcore/snapd
[goreportcard-url]: https://goreportcard.com/report/github.com/snapcore/snapd

[codecov-url]: https://codecov.io/gh/snapcore/snapd
[codecov-image]: https://codecov.io/gh/snapcore/snapd/branch/master/graph/badge.svg",VRAI
carbonetes/brainiac,Toolkit,Application System,2024-09-12T03:05:12Z,2024-04-01T07:18:49Z,0,2916,0,0,0,0,0,0,2023-04-20T06:16:44Z,2025-01-24T00:44:34Z,78720,70,Open Policy Agent,VRAI,8,FAUX,1,"aws,devsecops,golang,security,security-tools",1,"BrainIAC uses static code analysis to analyze IAC code to detect security issues before deployment. This tool can scan for issues like security policy misconfigurations, insecure cloud-based services, and compliance issues.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,12,"<p align=""center"">
<img src=""assets/IACOrig.svg"" style=""display: block; margin-left: auto; margin-right: auto; width: 50%;"">
</p>


# BrainIAC
[![Github All Releases](https://img.shields.io/github/downloads/carbonetes/brainiac/total.svg)]()
[![Go Report Card](https://goreportcard.com/badge/github.com/carbonetes/brainiac)](https://goreportcard.com/report/github.com/carbonetes/brainiac)
[![GitHub release](https://img.shields.io/github/release/carbonetes/brainiac.svg)](https://github.com/carbonetes/brainiac/releases/latest)
[![GitHub go.mod Go version](https://img.shields.io/github/go-mod/go-version/carbonetes/brainiac.svg)](https://github.com/carbonetes/brainiac)
[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/carbonetes/brainiac/blob/main/LICENSE)
[![Docker Pulls](https://img.shields.io/docker/pulls/carbonetes/brainiac)](https://hub.docker.com/r/carbonetes/brainiac)

BrainIAC uses static code analysis to analyze IAC code to detect security issues before deployment. This tool can scan for issues like security policy misconfigurations, insecure cloud-based services, and compliance issues. The BrainIAC tool performs a comprehensive code scan and generates reports containing detailed insights into the identified issues.

## Features
- 🔍 | Scans IAC Code for misconfiguration.
- 📁 | Has hundreds of pre-defined rules.
- 📁 | Scans a target directory to fill in multiple results.
- ⛑ | Works with major platforms.
- 🗄 | Converts between formats such as JSON and Table BrainIAC own format.

### Supported Platform
- [Kubernetes](docs/Policy/KUBERNETES_POLICYRULES.md)
- Terraform([AWS](docs/Policy/TERRAFORM_AWS_POLICYRULES.md), [OCI](docs/Policy/TERRAFORM_OCI_POLICYRULES.md), [ARM](docs/Policy/TERRAFORM_ARM_POLICYRULES.md), [GCP](docs/Policy/TERRAFORM_GCP_POLICYRULES.md), [ALIBABA](docs/Policy/TERRAFORM_ALI_POLICYRULES.md), [Kubernetes](docs/Policy/TERRAFORM_KUBERNETES_POLICYRULES.md), [Yandex](docs/Policy/TERRAFORM_YANDEX_POLICYRULES.md), [NCP](docs/Policy/TERRAFORM_NCP_POLICYRULES.md), [Rancher](docs/Policy/TERRAFORM_RANCHER_POLICYRULES.md))
- [Docker](docs/Policy/DOCKER_POLICYRULES.md)
- [CloudFormation](docs/Policy/CLOUDFORMATION_POLICYRULES.md)
- [ARM template files](docs/Policy/AZURE_POLICYRULES.md)

# Installation 📥

## Installation Support OS 💽
- Mac
  - darwin_amd64.tar.gz
  - darwin_arm64.tar.gz
- Linux
  - deb
    - linux_amd64.deb
    - linux_arm64.deb
    - linux_ppc64le.deb
  - rpm
    - linux_amd64.rpm
    - linux_arm64.rpm
    - linux_ppc64le.rpm
  - tar.gz
    - linux_amd64.tar.gz
    - linux_arm64.tar.gz
    - linux_ppc64le.tar.gz
- Windows
  - windows_amd64.zip
## Recommended
```bash
curl -sSfL https://raw.githubusercontent.com/carbonetes/brainiac/main/install.sh | sh -s -- -d /usr/local/bin
```
you can specify a release version and destination directory for the installation:

```
curl -sSfL https://raw.githubusercontent.com/carbonetes/brainiac/main/install.sh | sh -s -- -d <DESTINATION_DIR> -v <RELEASE_VERSION>
```

## Getting Started 🚀 

### 
Scan a single file
```
brainiac -f <file>
```

Scan multiple file in a directory
```
brainiac -d .
```

## Using Docker
```
docker pull carbonetes/brainiac
```
Scan a directory
```
docker run -t -v {path_to_host_folder}:/tmpPath carbonetes/brainiac:latest -d /tmpPath
```

Scan a single file
```
docker run -t -v {path_to_host_folder}:/tmpPath carbonetes/brainiac:latest -f /path/{filename}.{extension}
```

The output format for BrainIAC is configurable as well using the
`-o` (or `--output`) option:

The available `formats` are:
- `table`: A Tabular summary (default).
- `json`: Use this to get as much information out of BrainIAC.

### Available Commands and their flags with description:
```
brainiac [flag]
```
|                Root Flags                |                                                      Description                                                           |
| :--------------------------------------- | :------------------------------------------------------------------------------------------------------------------------- |
| `-f`, `--file`                    | File to scan                                                            |
| `-d`, `--dir`                     | Read directly from a path on disk (any directory) (e.g. 'BrainIAC -d path/to/dir)' (can not be used together with --file).                                              |
| `-o`, `--output`                  | Format to display results (table, json) (default ""table"")  
| `-v`, `--version`                 | Print BrainIAC version 
| `c`, `--check`                    | Each item should be a BrainIAC check ID(CB_K8S_023), and you can enter multiple items separated by commas. Only the specified checks will be executed, and any other checks will be skipped.|
|  `--skip-check`                   | The same behavior applies to the --check flag, where you can enter multiple items separated by commas. However, only the specified checks will be skipped, and all other checks will be executed. |
| `--severity-criteria`             | This is used to specify the severity level for filtering results. Only checks with a severity equal to or higher than the specified criteria will be included.([low medium high critical])



## License

[Apache 2.0](https://choosealicense.com/licenses/apache-2.0/)",VRAI
cdklabs/cdk-validator-cfnguard,Toolkit,Toolkit,2025-05-16T00:16:36Z,2025-04-16T00:15:42Z,0,0,0,0,0,0,238,0,2023-03-01T11:33:06Z,2025-04-08T00:17:50Z,3217,74,TypeScript,VRAI,6,FAUX,20,,20,,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,8,"# CDK CFN Guard Validator Plugin

<!--BEGIN STABILITY BANNER-->

---

![cdk-constructs: Experimental](https://img.shields.io/badge/cdk--constructs-experimental-important.svg?style=for-the-badge)

> The APIs of higher level constructs in this module are experimental and under active development.
> They are subject to non-backward compatible changes or removal in any future version. These are
> not subject to the [Semantic Versioning](https://semver.org/) model and breaking changes will be
> announced in the release notes. This means that while you may use them, you may need to update
> your source code when upgrading to a newer version of this package.
---

<!--END STABILITY BANNER-->


## Installing

### TypeScript/JavaScript

```bash
npm install @cdklabs/cdk-validator-cfnguard
```

### Python

```bash
pip install cdklabs.cdk-validator-cfnguard
```

### Java

```xml
// add this to your pom.xml
<dependency>
    <groupId>io.github.cdklabs</groupId>
    <artifactId>cdk-validator-cfnguard</artifactId>
    <version>0.0.0</version> // replace with version
</dependency>
```

### .NET

```bash
dotnet add package Cdklabs.CdkValidatorCfnGuard --version X.X.X
```

## Usage

To use this plugin in your CDK application add it to the CDK App.

```ts
new App({
  policyValidationBeta1: [
    new CfnGuardValidator(),
  ],
});
```

By default the `CfnGuardValidator` plugin comes with the [Control Tower
proactive
controls](https://docs.aws.amazon.com/controltower/latest/userguide/proactive-controls.html)
enabled. In order to disable these rules you can use the
`controlTowerRulesEnabled: false` property.

```ts
new CfnGuardValidator({
  controlTowerRulesEnabled: false,
});
```

It is also possible to disable individual rules.

```ts
new CfnGuardValidator({
  disabledRules: [
    'ct-s3-pr-1',
  ],
});
```

### Additional rules

To provide additional rules to the plugin, provide a list of local
file or directory paths.

```ts
new CfnGuardValidator({
  rules: [
    'path/to/local-rules-directory',
    'path/to/s3/local-rules/my-rule.guard',
  ],
});
```

If the path provided is a directory then the directory must only
contain guard rule files, and all rules within the directory will be used.

## Using the bundled Control Tower proactive controls in CDK

The bundled Control Tower proactive controls use CloudFormation Guard
policies that are also used in managed controls from the Control Tower
service. You can use these CDK bundled controls without having a Control
Tower environment in AWS, but there are many benefits to using the two together.

When you enable Control Tower proactive controls in your Control Tower environment,
the controls can stop the deployment of non-compliant resources deployed via
CloudFormation. For more information about managed proactive controls and how they work,
see the [Control Tower documentation](https://docs.aws.amazon.com/controltower/latest/userguide/proactive-controls.html).

These CDK bundled controls and managed Control Tower proactive controls are best used together.
In this scenario you can configure this validation plugin with the same proactive controls that
are active in your Control Tower cloud environment. You can then quickly gain confidence
that your CDK application will pass the Control Tower controls by running cdk synth locally
or in a pipeline as described above.

Regardless of whether you or your organization use Control Tower, however, you should
understand the following things about these bundled controls when run locally using this plugin:

1. These CloudFormation guard policies accept a limited subset of CloudFormation syntax
   for the properties they evaluate. For instance, a property called EncryptionEnabled may
   pass if it is specified with the literal value true, but it may fail if it is specified with
   a reference to a CloudFormation stack parameter instead. Similarly, if a rule checks for a string
   value, it may fail for Fn::Join objects. If you discover that a rule can be bypassed with a
   particular configuration of a resource, please file an issue.
2. Some rules may check references to other resources, but this reference checking is limited.
   For instance, a rule may require that an access logging bucket is specified for each S3 bucket.
   In this case, the rule can check whether you have passed a reference to a bucket in the same
   template, but it cannot verify that a hardcoded bucket name like ""examplebucket"" actually refers
   to a real bucket or a bucket you own.

You can add a layer of security protection by enabling the same proactive controls in your Control Tower
cloud environment. There are different considerations for using these controls since they operate in a
different way. For more information, see the [Control Tower proactive controls documentation](https://docs.aws.amazon.com/controltower/latest/userguide/proactive-controls.html).

If you do not yet have a Control Tower environment, see [What is AWS Control Tower?](https://docs.aws.amazon.com/controltower/latest/userguide/what-is-control-tower.html).

### Bundled Control Tower Rules

| ID                                                                                                                                                     | Name                                                                                                                                                                                     | Evaluated Resource Types                                                                            |
| ------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| [CT.ACM.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/acm-rules.html#ct-acm-pr-1-description)                                        | Require an AWS Private CA certificate to have a single domain name                                                                                                                       | `AWS::CertificateManager::Certificate`<br/>                                                         |
| [CT.APIGATEWAY.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/api-gateway-rules.html#ct-apigateway-pr-1-description)                  | Require an Amazon API Gateway REST and WebSocket API to have logging activated                                                                                                           | `AWS::ApiGateway::Stage`<br/>                                                                       |
| [CT.APIGATEWAY.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/api-gateway-rules.html#ct-apigateway-pr-2-description)                  | Require an Amazon API Gateway REST API stage to have AWS X-Ray tracing activated                                                                                                         | `AWS::ApiGateway::Stage`<br/>                                                                       |
| [CT.APIGATEWAY.PR.3](https://docs.aws.amazon.com/controltower/latest/userguide/api-gateway-rules.html#ct-apigateway-pr-3-description)                  | Require that an Amazon API Gateway REST API stage has encryption at rest configured for cache data                                                                                       | `AWS::ApiGateway::Stage`<br/>                                                                       |
| [CT.APIGATEWAY.PR.4](https://docs.aws.amazon.com/controltower/latest/userguide/api-gateway-rules.html#ct-apigateway-pr-4-description)                  | Require an Amazon API Gateway V2 stage to have access logging activated                                                                                                                  | `AWS::ApiGatewayV2::Stage`<br/>                                                                     |
| [CT.APIGATEWAY.PR.5](https://docs.aws.amazon.com/controltower/latest/userguide/api-gateway-rules.html#ct-apigateway-pr-5-description)                  | Require Amazon API Gateway V2 Websocket and HTTP routes to specify an authorization type                                                                                                 | `AWS::ApiGatewayV2::Route`<br/>`AWS::ApiGatewayV2::ApiGatewayManagedOverrides`<br/>                 |
| [CT.APIGATEWAY.PR.6](https://docs.aws.amazon.com/controltower/latest/userguide/api-gateway-rules.html#ct-apigateway-pr-6-description)                  | Require an Amazon API Gateway REST domain to use a security policy that specifies a minimum TLS protocol version of TLSv1.2                                                              | `AWS::ApiGateway::DomainName`<br/>                                                                  |
| [CT.APPSYNC.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/appsync-rules.html#ct-appsync-pr-1-description)                            | Require an AWS AppSync GraphQL API to have logging enabled                                                                                                                               | `AWS::AppSync::GraphQLApi`<br/>                                                                     |
| [CT.APPSYNC.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/appsync-rules.html#ct-appsync-pr-2-description)                            | Require an AWS AppSync GraphQL API to be configured with private visibility                                                                                                              | `AWS::AppSync::GraphQLApi`<br/>                                                                     |
| [CT.APPSYNC.PR.3](https://docs.aws.amazon.com/controltower/latest/userguide/appsync-rules.html#ct-appsync-pr-3-description)                            | Require that an AWS AppSync GraphQL API is not authenticated with API keys                                                                                                               | `AWS::AppSync::GraphQLApi`<br/>                                                                     |
| [CT.APPSYNC.PR.4](https://docs.aws.amazon.com/controltower/latest/userguide/appsync-rules.html#ct-appsync-pr-4-description)                            | Require an AWS AppSync GraphQL API cache to have encryption in transit enabled.                                                                                                          | `AWS::AppSync::ApiCache`<br/>                                                                       |
| [CT.APPSYNC.PR.5](https://docs.aws.amazon.com/controltower/latest/userguide/appsync-rules.html#ct-appsync-pr-5-description)                            | Require an AWS AppSync GraphQL API cache to have encryption at rest enabled.                                                                                                             | `AWS::AppSync::ApiCache`<br/>                                                                       |
| [CT.ATHENA.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/athena-rules.html#ct-athena-pr-1-description)                               | Require an Amazon Athena workgroup to encrypt Athena query results at rest                                                                                                               | `AWS::Athena::WorkGroup`<br/>                                                                       |
| [CT.ATHENA.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/athena-rules.html#ct-athena-pr-2-description)                               | Require an Amazon Athena workgroup to encrypt Athena query results at rest with an AWS Key Management Service (KMS) key                                                                  | `AWS::Athena::WorkGroup`<br/>                                                                       |
| [CT.AUTOSCALING.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-auto-scaling-rules.html#ct-autoscaling-pr-1-description)           | Require an Amazon EC2 Auto Scaling group to have multiple Availability Zones                                                                                                             | `AWS::AutoScaling::AutoScalingGroup`<br/>                                                           |
| [CT.AUTOSCALING.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-auto-scaling-rules.html#ct-autoscaling-pr-2-description)           | Require an Amazon EC2 Auto Scaling group launch configuration to configure Amazon EC2 instances for IMDSv2                                                                               | `AWS::AutoScaling::LaunchConfiguration`<br/>                                                        |
| [CT.AUTOSCALING.PR.3](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-auto-scaling-rules.html#ct-autoscaling-pr-3-description)           | Require an Amazon EC2 Auto Scaling launch configuration to have a single-hop metadata response limit                                                                                     | `AWS::AutoScaling::LaunchConfiguration`<br/>                                                        |
| [CT.AUTOSCALING.PR.4](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-auto-scaling-rules.html#ct-autoscaling-pr-4-description)           | Require an Amazon EC2 Auto Scaling group associated with an AWS Elastic Load Balancer (ELB) to have ELB health checks activated                                                          | `AWS::AutoScaling::AutoScalingGroup`<br/>                                                           |
| [CT.AUTOSCALING.PR.5](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-auto-scaling-rules.html#ct-autoscaling-pr-5-description)           | Require that an Amazon EC2 Auto Scaling group launch configuration does not have Amazon EC2 instances with public IP addresses                                                           | `AWS::AutoScaling::LaunchConfiguration`<br/>                                                        |
| [CT.AUTOSCALING.PR.6](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-auto-scaling-rules.html#ct-autoscaling-pr-6-description)           | Require any Amazon EC2 Auto Scaling groups to use multiple instance types                                                                                                                | `AWS::AutoScaling::AutoScalingGroup`<br/>                                                           |
| [CT.AUTOSCALING.PR.8](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-auto-scaling-rules.html#ct-autoscaling-pr-8-description)           | Require an Amazon EC2 Auto Scaling group to have EC2 launch templates configured                                                                                                         | `AWS::AutoScaling::AutoScalingGroup`<br/>                                                           |
| [CT.AUTOSCALING.PR.9](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-auto-scaling-rules.html#ct-autoscaling-pr-9-description)           | Require an Amazon EBS volume configured through an Amazon EC2 Auto Scaling launch configuration to encrypt data at rest                                                                  | `AWS::AutoScaling::LaunchConfiguration`<br/>                                                        |
| [CT.AUTOSCALING.PR.10](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-auto-scaling-rules.html#ct-autoscaling-pr-10-description)         | Require an Amazon EC2 Auto Scaling group to use only AWS Nitro instance types when overriding a launch template                                                                          | `AWS::AutoScaling::AutoScalingGroup`<br/>                                                           |
| [CT.AUTOSCALING.PR.11](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-auto-scaling-rules.html#ct-autoscaling-pr-11-description)         | Require only AWS Nitro instance types that support network traffic encryption between instances to be added to an Amazon EC2 Auto Scaling group, when overriding a launch template       | `AWS::AutoScaling::AutoScalingGroup`<br/>                                                           |
| [CT.CLOUDFRONT.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/cloudfront-rules.html#ct-cloudfront-pr-1-description)                   | Require an Amazon CloudFront distribution to have a default root object configured                                                                                                       | `AWS::CloudFront::Distribution`<br/>                                                                |
| [CT.CLOUDFRONT.PR.3](https://docs.aws.amazon.com/controltower/latest/userguide/cloudfront-rules.html#ct-cloudfront-pr-3-description)                   | Require an Amazon CloudFront distribution to have encryption in transit configured                                                                                                       | `AWS::CloudFront::Distribution`<br/>                                                                |
| [CT.CLOUDFRONT.PR.4](https://docs.aws.amazon.com/controltower/latest/userguide/cloudfront-rules.html#ct-cloudfront-pr-4-description)                   | Require an Amazon CloudFront distribution to have origin failover configured                                                                                                             | `AWS::CloudFront::Distribution`<br/>                                                                |
| [CT.CLOUDFRONT.PR.5](https://docs.aws.amazon.com/controltower/latest/userguide/cloudfront-rules.html#ct-cloudfront-pr-5-description)                   | Require any Amazon CloudFront distribution to have logging enabled                                                                                                                       | `AWS::CloudFront::Distribution`<br/>                                                                |
| [CT.CLOUDFRONT.PR.6](https://docs.aws.amazon.com/controltower/latest/userguide/cloudfront-rules.html#ct-cloudfront-pr-6-description)                   | Require an Amazon CloudFront distribution to use custom SSL/TLS certificates                                                                                                             | `AWS::CloudFront::Distribution`<br/>                                                                |
| [CT.CLOUDFRONT.PR.7](https://docs.aws.amazon.com/controltower/latest/userguide/cloudfront-rules.html#ct-cloudfront-pr-7-description)                   | Require an Amazon CloudFront distribution to use SNI to serve HTTPS requests                                                                                                             | `AWS::CloudFront::Distribution`<br/>                                                                |
| [CT.CLOUDFRONT.PR.8](https://docs.aws.amazon.com/controltower/latest/userguide/cloudfront-rules.html#ct-cloudfront-pr-8-description)                   | Require an Amazon CloudFront distribution to encrypt traffic to custom origins                                                                                                           | `AWS::CloudFront::Distribution`<br/>                                                                |
| [CT.CLOUDFRONT.PR.9](https://docs.aws.amazon.com/controltower/latest/userguide/cloudfront-rules.html#ct-cloudfront-pr-9-description)                   | Require an Amazon CloudFront distribution to have a security policy of TLSv1.2 as a minimum                                                                                              | `AWS::CloudFront::Distribution`<br/>                                                                |
| [CT.CLOUDFRONT.PR.10](https://docs.aws.amazon.com/controltower/latest/userguide/cloudfront-rules.html#ct-cloudfront-pr-10-description)                 | Require any Amazon CloudFront distributions with Amazon S3 backed origins to have origin access control configured                                                                       | `AWS::CloudFront::Distribution`<br/>                                                                |
| [CT.CLOUDFRONT.PR.11](https://docs.aws.amazon.com/controltower/latest/userguide/cloudfront-rules.html#ct-cloudfront-pr-11-description)                 | Require an Amazon CloudFront distribution to use updated SSL protocols between edge locations and custom origins                                                                         | `AWS::CloudFront::Distribution`<br/>                                                                |
| [CT.CLOUDTRAIL.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/cloudtrail-rules.html#ct-cloudtrail-pr-1-description)                   | Require an AWS CloudTrail trail to have encryption at rest activated                                                                                                                     | `AWS::CloudTrail::Trail`<br/>                                                                       |
| [CT.CLOUDTRAIL.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/cloudtrail-rules.html#ct-cloudtrail-pr-2-description)                   | Require an AWS CloudTrail trail to have log file validation activated                                                                                                                    | `AWS::CloudTrail::Trail`<br/>                                                                       |
| [CT.CLOUDTRAIL.PR.3](https://docs.aws.amazon.com/controltower/latest/userguide/cloudtrail-rules.html#ct-cloudtrail-pr-3-description)                   | Require an AWS CloudTrail trail to have an Amazon CloudWatch log group configuration                                                                                                     | `AWS::CloudTrail::Trail`<br/>                                                                       |
| [CT.CLOUDTRAIL.PR.4](https://docs.aws.amazon.com/controltower/latest/userguide/cloudtrail-rules.html#ct-cloudtrail-pr-4-description)                   | Require an AWS CloudTrail Lake event data store to enable encryption at rest with an AWS KMS key                                                                                         | `AWS::CloudTrail::EventDataStore`<br/>                                                              |
| [CT.CLOUDWATCH.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/cloudwatch-rules.html#ct-cloudwatch-pr-1-description)                   | Require an Amazon CloudWatch alarm to have an action configured for the alarm state                                                                                                      | `AWS::CloudWatch::Alarm`<br/>                                                                       |
| [CT.CLOUDWATCH.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/cloudwatch-rules.html#ct-cloudwatch-pr-2-description)                   | Require an Amazon CloudWatch log group to be retained for at least one year                                                                                                              | `AWS::Logs::LogGroup`<br/>                                                                          |
| [CT.CLOUDWATCH.PR.3](https://docs.aws.amazon.com/controltower/latest/userguide/cloudwatch-rules.html#ct-cloudwatch-pr-3-description)                   | Require an Amazon CloudWatch log group to be encrypted at rest with an AWS KMS key                                                                                                       | `AWS::Logs::LogGroup`<br/>                                                                          |
| [CT.CLOUDWATCH.PR.4](https://docs.aws.amazon.com/controltower/latest/userguide/cloudwatch-rules.html#ct-cloudwatch-pr-4-description)                   | Require an Amazon CloudWatch alarm to have actions activated                                                                                                                             | `AWS::CloudWatch::Alarm`<br/>                                                                       |
| [CT.CODEBUILD.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/codebuild-rules.html#ct-codebuild-pr-1-description)                      | Require OAuth on GitHub or Bitbucket source repository URLs for AWS CodeBuild projects                                                                                                   | `AWS::CodeBuild::Project`<br/>                                                                      |
| [CT.CODEBUILD.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/codebuild-rules.html#ct-codebuild-pr-2-description)                      | Require any AWS CodeBuild project environment variable to encrypt credentials in environment variables                                                                                   | `AWS::CodeBuild::Project`<br/>                                                                      |
| [CT.CODEBUILD.PR.3](https://docs.aws.amazon.com/controltower/latest/userguide/codebuild-rules.html#ct-codebuild-pr-3-description)                      | Require any AWS CodeBuild project environment to have logging configured                                                                                                                 | `AWS::CodeBuild::Project`<br/>                                                                      |
| [CT.CODEBUILD.PR.4](https://docs.aws.amazon.com/controltower/latest/userguide/codebuild-rules.html#ct-codebuild-pr-4-description)                      | Require any AWS CodeBuild project to deactivate privileged mode when running                                                                                                             | `AWS::CodeBuild::Project`<br/>                                                                      |
| [CT.CODEBUILD.PR.5](https://docs.aws.amazon.com/controltower/latest/userguide/codebuild-rules.html#ct-codebuild-pr-5-description)                      | Require encryption on all AWS CodeBuild project artifacts                                                                                                                                | `AWS::CodeBuild::Project`<br/>                                                                      |
| [CT.CODEBUILD.PR.6](https://docs.aws.amazon.com/controltower/latest/userguide/codebuild-rules.html#ct-codebuild-pr-6-description)                      | Require encryption on all Amazon S3 logs for AWS CodeBuild projects                                                                                                                      | `AWS::CodeBuild::Project`<br/>                                                                      |
| [CT.DAX.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/dax-rules.html#ct-dax-pr-1-description)                                        | Require encryption at rest for all Amazon DynamoDB Accelerator (DAX) clusters                                                                                                            | `AWS::DAX::Cluster`<br/>                                                                            |
| [CT.DAX.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/dax-rules.html#ct-dax-pr-2-description)                                        | Require an Amazon DAX cluster to deploy nodes to at least three Availability Zones                                                                                                       | `AWS::DAX::Cluster`<br/>                                                                            |
| [CT.DAX.PR.3](https://docs.aws.amazon.com/controltower/latest/userguide/dax-rules.html#ct-dax-pr-3-description)                                        | Require an Amazon DAX cluster to encrypt data in transit with Transport Layer Security (TLS)                                                                                             | `AWS::DAX::Cluster`<br/>                                                                            |
| [CT.DMS.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/dms-rules.html#ct-dms-pr-1-description)                                        | Require that a public AWS DMS replication instance is not public                                                                                                                         | `AWS::DMS::ReplicationInstance`<br/>                                                                |
| [CT.DMS.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/dms-rules.html#ct-dms-pr-2-description)                                        | Require an AWS Database Migration Service (DMS) Endpoint to encrypt connections for source and target endpoints                                                                          | `AWS::DMS::Endpoint`<br/>                                                                           |
| [CT.DOCUMENTDB.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/documentdb-rules.html#ct-documentdb-pr-1-description)                   | Require an Amazon DocumentDB cluster to be encrypted at rest                                                                                                                             | `AWS::DocDB::DBCluster`<br/>                                                                        |
| [CT.DOCUMENTDB.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/documentdb-rules.html#ct-documentdb-pr-2-description)                   | Require an Amazon DocumentDB cluster to have a backup retention period greater than or equal to seven days                                                                               | `AWS::DocDB::DBCluster`<br/>                                                                        |
| [CT.DYNAMODB.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/dynamodb-rules.html#ct-dynamodb-pr-1-description)                         | Require that point-in-time recovery for an Amazon DynamoDB table is activated                                                                                                            | `AWS::DynamoDB::Table`<br/>                                                                         |
| [CT.DYNAMODB.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/dynamodb-rules.html#ct-dynamodb-pr-2-description)                         | Require an Amazon DynamoDB table to be encrypted at rest using an AWS KMS key                                                                                                            | `AWS::DynamoDB::Table`<br/>                                                                         |
| [CT.EC2.PR.1](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-rules.html#ct-ec2-pr-1-description)                                        | Require an Amazon EC2 launch template to have IMDSv2 configured                                                                                                                          | `AWS::EC2::LaunchTemplate`<br/>                                                                     |
| [CT.EC2.PR.2](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-rules.html#ct-ec2-pr-2-description)                                        | Require that Amazon EC2 launch templates restrict the token hop limit to a maximum of one                                                                                                | `AWS::EC2::LaunchTemplate`<br/>                                                                     |
| [CT.EC2.PR.3](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-rules.html#ct-ec2-pr-3-description)                                        | Require that any Amazon EC2 security group rule does not use the source IP range 0.0.0.0/0 or ::/0 for ports other than 80 and 443                                                       | `AWS::EC2::SecurityGroup`<br/>`AWS::EC2::SecurityGroupIngress`<br/>                                 |
| [CT.EC2.PR.4](https://docs.aws.amazon.com/controltower/latest/userguide/ec2-rules.html#ct-ec2-pr-4-description)                              ",FAUX
cedar-policy/cedar,Toolkit,Toolkit,2025-05-15T18:29:27Z,2025-04-25T16:52:36Z,0,0,0,66,0,0,0,0,2023-04-25T15:13:59Z,2025-04-08T14:19:16Z,8194,983,Rust,VRAI,93,FAUX,135,,135,Implementation of the Cedar Policy Language,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,42,"# Cedar

![Cedar Logo](./logo.svg)

[![Crates.io](https://img.shields.io/crates/v/cedar-policy.svg)](https://crates.io/crates/cedar-policy)
[![docs.rs](https://img.shields.io/docsrs/cedar-policy)](https://docs.rs/cedar-policy/latest/cedar_policy/)
![nightly](https://github.com/cedar-policy/cedar/actions/workflows/nightly_build.yml/badge.svg)
![audit](https://github.com/cedar-policy/cedar/actions/workflows/cargo_audit.yml/badge.svg)

This repository contains source code of the Rust crates that implement the [Cedar](https://www.cedarpolicy.com/) policy language.

Cedar is a language for writing and enforcing authorization policies in your applications. Using Cedar, you can write policies that specify your applications' fine-grained permissions. Your applications then authorize access requests by calling Cedar's authorization engine. Because Cedar policies are separate from application code, they can be independently authored, updated, analyzed, and audited. You can use Cedar's validator to check that Cedar policies are consistent with a declared schema which defines your application's authorization model.

Cedar is:

### Expressive

Cedar is a simple yet expressive language that is purpose-built to support authorization use cases for common authorization models such as RBAC and ABAC.

### Performant

Cedar is fast and scalable. The policy structure is designed to be indexed for quick retrieval and to support fast and scalable real-time evaluation, with bounded latency.

### Analyzable

Cedar is designed for analysis using Automated Reasoning. This enables analyzer tools capable of optimizing your policies and proving that your security model is what you believe it is.

## Using Cedar

Cedar can be used in your application by depending on the [`cedar-policy` crate](https://crates.io/crates/cedar-policy).

Just add `cedar-policy` as a dependency by running

```sh
cargo add cedar-policy
```

## Crates in This Workspace

* [cedar-policy](./cedar-policy) : Main crate for using Cedar to authorize access requests in your applications, and validate Cedar policies against a schema
* [cedar-policy-cli](./cedar-policy-cli) : Crate containing a simple command-line interface (CLI) for interacting with Cedar
* [cedar-policy-core](./cedar-policy-core) : Internal crate containing the Cedar parser and evaluator
* [cedar-policy-validator](./cedar-policy-validator) : Internal crate containing the Cedar validator
* [cedar-policy-formatter](./cedar-policy-formatter) : Internal crate containing an auto-formatter for Cedar policies
* [cedar-testing](./cedar-testing) : Internal crate containing integration testing code

## Quick Start

Let's put the policy in `policy.cedar` and the entities in `entities.json`.

`policy.cedar`:

```cedar
permit (
  principal == User::""alice"",
  action == Action::""view"",
  resource in Album::""jane_vacation""
);
```

This policy specifies that `alice` is allowed to view the photos in the `""jane_vacation""` album.

`entities.json`:

```json
[
    {
        ""uid"": { ""type"": ""User"", ""id"": ""alice""} ,
        ""attrs"": {""age"": 18},
        ""parents"": []
    },
    {
        ""uid"": { ""type"": ""Photo"", ""id"": ""VacationPhoto94.jpg""},
        ""attrs"": {},
        ""parents"": [{ ""type"": ""Album"", ""id"": ""jane_vacation"" }]
    }
]

```

Cedar represents principals, resources, and actions as entities. An entity has a type (e.g., `User`) and an id (e.g., `alice`). They can also have attributes (e.g., `User::""alice""`'s `age` attribute is the integer `18`).

Now, let's test our policy with the CLI:

```sh
 cargo run authorize \
    --policies policy.cedar \
    --entities entities.json \
    --principal 'User::""alice""' \
    --action 'Action::""view""' \
    --resource 'Photo::""VacationPhoto94.jpg""'
```

CLI output:

```
ALLOW
```

This request is allowed because `VacationPhoto94.jpg` belongs to `Album::""jane_vacation""`, and `alice` can view photos in `Album::""jane_vacation""`.

If you'd like to see more details on what can be expressed as Cedar policies, see the [documentation](https://docs.cedarpolicy.com).

Examples of how to use Cedar in an application are contained in the repository [cedar-examples](https://github.com/cedar-policy/cedar-examples). [TinyTodo](https://github.com/cedar-policy/cedar-examples/tree/main/tinytodo) is a simple task list management app whose users' requests, sent as HTTP messages, are authorized by Cedar. It shows how you can integrate Cedar into your own Rust program.

## Documentation

General documentation for Cedar is available at [docs.cedarpolicy.com](https://docs.cedarpolicy.com), with source code in the [cedar-policy/cedar-docs](https://github.com/cedar-policy/cedar-docs/) repository.

Generated documentation for the latest version of the Rust crates can be accessed
[on docs.rs](https://docs.rs/cedar-policy).

If you're looking to integrate Cedar into a production system, please be sure the read the [security best practices](https://docs.cedarpolicy.com/other/security.html)

## Building

To build, simply run `cargo build` (or `cargo build --release`).

## What's New

We maintain changelogs for our public-facing crates:
[cedar-policy](https://github.com/cedar-policy/cedar/blob/main/cedar-policy/CHANGELOG.md) and
[cedar-policy-cli](https://github.com/cedar-policy/cedar/blob/main/cedar-policy-cli/CHANGELOG.md).
Changelogs for all release branches and the `main` branch are all maintained on
the `main` branch of this repository; you can see the most up-to-date changelogs
by following the links above.

For a list of the current and past releases, see [crates.io](https://crates.io/crates/cedar-policy) or [Releases](https://github.com/cedar-policy/cedar/releases).

## Backward Compatibility Considerations

Cedar is written in Rust and you will typically depend on Cedar via Cargo. Cargo makes sane choices for the majority of projects, but your needs may differ. If you don't want automatic updates to Cedar, then you can pin to a specific version in your `Cargo.toml`. For example:

```toml
[dependencies]
cedar-policy = ""=2.4.2""
```

Note that this is different from:

```toml
[dependencies]
cedar-policy = ""2.4.2""
```

Which expresses that 2.4.2 is the minimum version of Cedar you accept, and you implicitly accept anything newer that is semver-compatible. See <https://doc.rust-lang.org/cargo/reference/specifying-dependencies.html>.

## Security

See [SECURITY](SECURITY.md) for more information.

## Contributing

We welcome contributions from the community. Please either file an issue, or see [CONTRIBUTING](CONTRIBUTING.md)

## License

This project is licensed under the Apache-2.0 License.",FAUX
cedar-policy/cedar-examples,Documentations,Documentations,2025-04-09T15:52:19Z,2024-06-14T18:48:47Z,0,4,0,25,0,0,0,0,2023-04-18T16:50:40Z,2025-04-08T14:19:01Z,891,82,Rust,VRAI,29,FAUX,13,,13,Some examples of using the Cedar language to specify authorization,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,21,"# Cedar Examples

This repository contains examples demonstrating the use of [Cedar](https://github.com/cedar-policy/cedar), a policy language for writing and enforcing authorization policies in your applications.  The following table summarizes relevant information about the applications. Please refer to the `README.md` files in the subfolders for details about how to build and run them.

| Example                               | Languages    | Description                                                                                                                                                                          |
|---------------------------------------|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [`tinytodo`][]                        | Rust, Python | A simple application for managing task lists that uses Cedar for authorization demonstrating the usage of the [Cedar Rust APIs][]                                                    |
| [`tinytodo-go`][]                     | Go, Python   | A simple application for managing task lists that uses Cedar for authorization demonstrating the usage of the [Cedar Go APIs][]                                                      |                                                                                                     |
| [`cedar-java-hello-world`][]          | Java         | A simple application demonstrating the usage of the [Cedar Java APIs][]                                                                                                              |
| [`cedar-rust-hello-world`][]          | Rust         | A simple application demonstrating the usage of the [Cedar Rust APIs][]                                                                                                              |
| [`cedar-wasm-example`][]              | TypeScript   | A simple application demonstrating the usage of the [Cedar Wasm APIs][]                                                                                                              |
| [`cedar-policy-language-in-action`][] | Cedar        | Cedar policies and schemas for the [Cedar policy language in action](https://catalog.workshops.aws/cedar-policy-language-in-action) workshop                                         |
| [`cedar-example-use-cases`][]         | Cedar        | Cedar policies and schemas for two example applications                                                                                                                              |
| [`oopsla2024-benchmarks`][]           | Various      | Cedar policies and schemas, along with benchmarking code and scripts, used for the performance evaluation of the [OOPSLA2024 paper on Cedar](https://dl.acm.org/doi/10.1145/3649835) |
| [`cedar-java-partial-evaluation`][]   | Java         | A simple application demonstrating partial evaluation capabilities using the [Cedar Java APIs][] |
## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This project is licensed under the Apache-2.0 License.

[Cedar Rust APIs]: https://docs.rs/cedar-policy/latest/cedar_policy
[Cedar Go APIs]: https://github.com/cedar-policy/cedar-go
[Cedar Java APIs]: https://github.com/cedar-policy/cedar-java
[Cedar Wasm APIs]: https://github.com/cedar-policy/cedar/tree/main/cedar-wasm
[`cedar-example-use-cases`]: ./cedar-example-use-cases
[`cedar-java-hello-world`]: ./cedar-java-hello-world
[`cedar-rust-hello-world`]: ./cedar-rust-hello-world
[`cedar-wasm-example`]: ./cedar-wasm-example
[`cedar-policy-language-in-action`]: ./cedar-policy-language-in-action
[`oopsla2024-benchmarks`]: ./oopsla2024-benchmarks
[`tinytodo`]: ./tinytodo
[`tinytodo-go`]: ./tinytodo-go
[`cedar-java-partial-evaluation`]: ./cedar-java-partial-evaluation",FAUX
cedar-policy/cedar-java,Toolkit,Toolkit,2025-03-24T16:45:44Z,2024-11-04T13:53:04Z,0,0,0,6,0,0,0,0,2023-04-25T18:22:57Z,2025-04-07T20:59:30Z,617,54,Java,VRAI,24,FAUX,16,,16,Java bindings for the Cedar language,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,19,"# cedar-java
![Cedar Logo](https://github.com/cedar-policy/cedar/blob/main/logo.svg)  

This repository contains the source code for a Java package `CedarJava` that supports using the [Cedar](https://www.cedarpolicy.com) policy language. It also contains source code for a Rust crate `CedarJavaFFI` that enables calling Cedar library functions (written in Rust) from Java.

Cedar is a language for writing and enforcing authorization policies in your applications. Using Cedar, you can write policies that specify your applications' fine-grained permissions. Your applications then authorize access requests by calling Cedar's authorization engine. Because Cedar policies are separate from application code, they can be independently authored, updated, analyzed, and audited. You can use Cedar's validator to check that Cedar policies are consistent with a declared schema which defines your application's authorization model.


## Getting Started

### Import `CedarJava` to your application
#### Maven Package
CedarJava is available as a maven package. You can add `CedarJava` as a dependency to your build file.  

Example (Gradle): 
```
dependencies{
    implementation 'com.cedarpolicy:cedar-java:4.2.3:uber'
}
```
We highly recommend using the `*-uber.jar` as it also contains the shared library from `CedarJavaFFI`.

See [https://central.sonatype.com/artifact/com.cedarpolicy/cedar-java](https://central.sonatype.com/artifact/com.cedarpolicy/cedar-java) for more details.  

#### Build from Source

The [CedarJavaFFI](https://github.com/cedar-policy/cedar-java/blob/main/CedarJavaFFI/README.md) and [CedarJava](https://github.com/cedar-policy/cedar-java/blob/main/CedarJava/README.md) directories contain detailed instructions on building the individual modules.

The `CedarJava` module uses Gradle to build both modules and run integration tests. It stores the shared library from `CedarJavaFFI` in the `-uber.jar`. The following commands provide general usage for getting started.

```shell
cd CedarJava
./gradlew build
```

### Perform an Authorization Request
Here is a small snippet on how to perform `isAuthorized()` call using `CedarJava`
```java
package com.mypackage;

import com.cedarpolicy.AuthorizationEngine;
import com.cedarpolicy.BasicAuthorizationEngine;
import com.cedarpolicy.model.AuthorizationRequest;
import com.cedarpolicy.model.AuthorizationResponse;
import com.cedarpolicy.model.Context;
import com.cedarpolicy.model.entity.Entities;
import com.cedarpolicy.model.entity.Entity;
import com.cedarpolicy.model.policy.PolicySet;
import com.cedarpolicy.value.EntityUID;

public class SimpleAuthorization {
    public static void main(String[] args) throws Exception {

        // Build entities
        Entity principal = new Entity(EntityUID.parse(""User::\""Alice\"""").get());
        Entity action = new Entity(EntityUID.parse(""Action::\""view\"""").get());
        Entity resource = new Entity(EntityUID.parse(""Photo::\""alice_photo\"""").get());

        // Build policies
        PolicySet policySet = PolicySet.parsePolicies(""""""
            permit(
                principal == User::""Alice"",
                action == Action::""view"",
                resource == Photo::""alice_photo""
            );

            forbid(
                principal == User::""Alice"",
                action == Action::""view"",
                resource == Photo::""bob_photo""
            );
        """""");
        
        // Authorization request
        AuthorizationEngine ae = new BasicAuthorizationEngine();
        Entities entities = new Entities();
        Context context = new Context();
        AuthorizationRequest request  = new AuthorizationRequest(principal, action, resource, context);
        AuthorizationResponse authorizationResponse = ae.isAuthorized(request, policySet, entities);
    }
}
```

## Examples
Explore our sample applications in [cedar-examples](https://github.com/cedar-policy/cedar-examples/tree/main):
* [**cedar-java-hello-world**](https://github.com/cedar-policy/cedar-examples/tree/main/cedar-java-hello-world): Demonstrates basic authorization calls using Cedar-Java
* [**cedar-java-partial-evaluation**](https://github.com/cedar-policy/cedar-examples/tree/main/cedar-java-partial-evaluation): Illustrates partial evaluation capabilities in Cedar-Java

## Changelog
For a list of changes and version updates, see [CHANGELOG.md](CedarJava/CHANGELOG.md).

## Notes

`CedarJava` requires JDK 17 or later.

Cedar is primarily developed in Rust (in the [cedar](https://github.com/cedar-policy/cedar) repository). As such, `CedarJava` typically lags behind the newest Cedar features. 

The `main` branch of this repository is kept up-to-date with the development version of the Rust code (available in the `main` branch of [cedar](https://github.com/cedar-policy/cedar)). Unless you plan to build the Rust code locally, please use the latest `release/x.x.x` branch instead.

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This project is licensed under the Apache-2.0 License.",FAUX
cedar-policy/cedar-local-agent,Toolkit,Toolkit,2025-04-29T15:17:12Z,2023-12-01T21:54:15Z,0,0,0,12,0,0,0,0,2023-10-30T21:38:14Z,2025-02-17T18:27:50Z,246,21,Rust,VRAI,5,FAUX,6,,6,,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,15,"# Cedar Local Agent

This crate is experimental.

The `cedar-local-agent` crate provides customers a useful foundation for creating asynchronous authorizers that
can handle two different operational modes:

1. The authorizer is **able** to cache all of your application’s policies and entity data while evaluating a request
2. The authorizer is **unable** to cache all of your application's policies and entity data while evaluating a request

The `cedar-local-agent` crate provides a [`simple::Authorizer`](./src/public/simple.rs) which can be built with option (1) or (2). The
[`simple::Authorizer`](./src/public/simple.rs) is constructed using policy and entity providers. These providers can be
implemented by customers.

`cedar-local-agent` provides sample implementations of providers that implement option (1). A file system policy set provider:
[`file::PolicySetProvider`](./src/public/file/policy_set_provider.rs), and an entity provider:
[`file::EntityProvider`](./src/public/file/entity_provider.rs).

For more information about the Cedar language/project, please take a look
at [cedarpolicy.com](https://www.cedarpolicy.com).

## Usage

Cedar Local Agent can be used in your application via the `cedar-local-agent` crate.

Add `cedar-local-agent` as a dependency in your `Cargo.toml` file. For example:

```toml
[dependencies]
cedar-local-agent = ""2""
```

## Quick Start

Build a local authorizer that evaluates authorization decisions using a locally stored
policy set, entity store and schema.

Policy data: [`tests/data/sweets.cedar`](./tests/data/sweets.cedar)

Entity data: [`tests/data/sweets.entities.json`](./tests/data/sweets.entities.json)

Schema: [`tests/data/sweets.schema.cedar.json`](./tests/data/sweets.schema.cedar.json)

Build a policy set:

```rust
let policy_set_provider = PolicySetProvider::new(
    policy_set_provider::ConfigBuilder::default()
        .policy_set_path(""tests/data/sweets.cedar"")
        .build()
        .unwrap(),
)
.unwrap();
```

Build an entity provider:

```rust
let entity_provider = EntityProvider::new(
    entity_provider::ConfigBuilder::default()
        .entities_path(""tests/data/sweets.entities.json"")
        .schema_path(""tests/data/sweets.schema.cedar.json"")
        .build()
        .unwrap(),
)
.unwrap();
```

Build the authorizer:

```rust
let authorizer: Authorizer<PolicySetProvider, EntityProvider> = Authorizer::new(
    AuthorizerConfigBuilder::default()
        .entity_provider(Arc::new(entity_provider))
        .policy_set_provider(Arc::new(policy_set_provider))
        .build()
        .unwrap(),
);
```

Evaluate a decision:

```rust
assert_eq!(
    authorizer
        .is_authorized(&Request::new(
            Some(format!(""User::\""Cedar\"""").parse().unwrap()),
            Some(format!(""Action::\""read\"""").parse().unwrap()),
            Some(format!(""Box::\""3\"""").parse().unwrap()),
            Context::empty(),
        ), &Entities::empty())
        .await
        .unwrap()
        .decision(),
    Decision::Deny
)
```

## [`simple::Authorizer`](./src/public/simple.rs) `is_authorized` API Semantics

The [`simple::Authorizer`](./src/public/simple.rs) `is_authorized` API takes a
[`Cedar request`](https://github.com/cedar-policy/cedar/tree/main/cedar-policy)
and [`Cedar entities`](https://github.com/cedar-policy/cedar/tree/main/cedar-policy) within the API.

```rust
pub async fn is_authorized(
    &self,
    request: &Request,
    entities: &Entities,
) -> Result<Response, AuthorizerError> { ... }
```

For scenarios where the same entity identifier, `EID`, is passed as input and returned by an `EntityProvider`, the input is
considered the last value. This API favors last-value-wins semantics.
This behavior is subject to change pending [`RFC-0020`](https://github.com/cedar-policy/rfcs/blob/main/text/0020-unique-record-keys.md).

## Updating [`file::PolicySetProvider`](./src/public/file/policy_set_provider.rs) or [`file::EntityProvider`](./src/public/file/entity_provider.rs) data

The [`file::PolicySetProvider`](./src/public/file/policy_set_provider.rs) and [`file::EntityProvider`](./src/public/file/entity_provider.rs)
gather data when initialized and cache it in memory. No data is read from disk during an authorization decision.

Policy and entity data can be mutated on disk after the initialization of an authorizer. To account for this, functionality
is provided which will refresh policy and entity data tangential to an authorization decision.
To accomplish this, a minimum of two additional threads are required, for a total of three threads.

1. The main thread handles `is_authorized` calls
2. The signaler thread notifies receivers of required updates
3. The receiver thread listens for updates

[`Channels`](https://doc.rust-lang.org/rust-by-example/std_misc/channels.html) are used to communicate between the signaler thread and
the receiver thread. There are two provided functions for creating signaler threads. Both return a signaler thread and a
[`tokio::broadcast::receiver`](https://docs.rs/tokio/latest/tokio/sync/broadcast/struct.Receiver.html) as an output.

1. [`clock_ticker_task`](./src/public/events/core.rs) periodically wakes up and signals based on a clock duration
2. [`file_inspector_task`](./src/public/events/core.rs) periodically wakes up and checks for differences in a file
   using a collision resistant hashing function (SHA256) and notifies on modifications

Warning: It is important to be careful when selecting the refresh rate of the signaler which triggers a policy refresh.
We have set up a `RefreshRate` enum which gives several options of RefreshRates of at least 15 seconds.
This is fast enough for most applications but slow enough to be very unlikely to trigger any sort of throttling or performance impact on most policy set sources.


Receivers are required to be passed to a new separate thread to listen and respond to events.
The [`update_provider_data_task`](./src/public/events/receive.rs) handles receiving these signals in the form of an
[`Event`](./src/public/events/mod.rs). Messages are handled one message at a time. The receiver thread blocks until
it has successfully or unsuccessfully updated the data for the provider.

Sample usage of updating a policy set provider's data every fifteen seconds:

```rust
let (clock_ticker_signal_thread, receiver) = clock_ticker_task(RefreshRate::FifteenSeconds);

let policy_set_provider = Arc::new(PolicySetProvider::new(
    policy_set_provider::ConfigBuilder::default()
        .policy_set_path(""tests/data/sweets.cedar"")
        .build()
        .unwrap(),
)
.unwrap());

let update_provider_thread = update_provider_data_task(policy_set_provider.clone(), receiver);
```

Note: these background threads must remain in scope for the life of your application. If there is an issue updating
in a background thread it will produce an `error!()` message but will not cause the application to crash.

This means that if for any reason, the agent cannot update its policy set due to an error, the agent will continue running with the stale policy set and will log an error.
Since the agent will log `error!()`'s, it is possible to configure log-based alarms so that these failures can be caught quickly, but that is outside the scope of this README.

## Tracing

This crate emits trace data [`tracing`](https://docs.rs/tracing/latest/tracing/) and can be integrated
into standard tracing implementations.

## Authorization Logging

Authorization logs are designed to power detection and response capabilities. Sample capabilities can be found
under the [`Mitre D3fend matrix`](https://d3fend.mitre.org/),
for example [`User Behavioral Analysis`](https://d3fend.mitre.org/technique/d3f:UserBehaviorAnalysis/).

The authorizer's provided emit authorization events as [`tracing events`](https://docs.rs/tracing/latest/tracing/struct.Event.html).
Authorization events are included in tracing [`spans`](https://docs.rs/tracing/latest/tracing/span/index.html).
Authorization events are default formatted using [`Open Cyber Security Format`](https://github.com/ocsf).
Authorization events can optionally be filtered, formatted and routed directly to an authorization log. See example:

```rust
// Dependencies must be included in the `Cargo.toml` file of the application
// tracing, tracing-appender, tracing-subscriber

let authorization_roller = tracing_appender::rolling::minutely(""logs"", ""authorization.log"");
let (authorization_non_blocking, _guard) = tracing_appender::non_blocking(authorization_roller);
let authorization_log_layer = tracing_subscriber::fmt::layer()
    .event_format(AuthorizerFormatter(AuthorizerTarget::Simple))
    .with_writer(authorization_non_blocking);
tracing_subscriber::registry()
    .with(authorization_log_layer)
    .try_init()
    .expect(""Logging Failed to Start, Exiting."");
```

To filter authorization event logs, provide a log config to the authorizer with a `FieldSet` which includes the fields that are to be logged. By default if not explicitly configured, no fields will be logged.

Sample usage of logging everything within the authorization request. **Note**: Logging everything is insecure; please see [Secure Logging Configuration](#secure-logging-configuration). 

```rust
let log_config =
    log::ConfigBuilder::default()
        .field_set(log::FieldSetBuilder::default()
            .principal(true)
            .action(true)
            .resource(true)
            .context(true)
            .entities(log::FieldLevel::All)
            .build()
            .unwrap())
    .build()
    .unwrap();

let authorizer: Authorizer<PolicySetProvider, EntityProvider> = Authorizer::new(
    AuthorizerConfigBuilder::default()
        .entity_provider(...)
        .policy_set_provider(...)
        .log_config(log_config)
        .build()
        .unwrap(),
);
```

Note: ``Authorizer`` ``log_config`` ``FieldSet::entities `` refers to the Cedar entities. 
There is also an [OCSF](https://github.com/ocsf) field called [``entity``](https://github.com/cedar-policy/cedar-local-agent/blob/main/src/public/log/schema.rs#L86) which refers to the principal entity that is sending the request.

This means that when ``Authorizer`` ``log_config`` ``FieldSet::entities`` is set to ``FieldLevel::None``, the OCSF entity will still be logged. 
This is not a bug and is expected behaviour.

For more examples of how to set up the authorization logging, see our [usage examples](https://github.com/cedar-policy/cedar-local-agent/tree/main/examples/tracing/authorization_log)

### Secure Logging Configuration:

Using a `log::FieldSet` configuration that sets any cedar-related field (principal, action, resource, context, and entities) to `true` will result in that field being logged. 
These fields could contain sensitive information and should be exposed with caution. Additionally, the cedar language has no current limit on field sizes within a [`Request`](https://docs.rs/cedar-policy/2.4.0/cedar_policy/struct.Request.html). 
A large request with verbose logging can result in more disk i/o to occur. This disk i/o could negatively impact the performance of the application.

For a safe config, create a default `log::FieldSet` with `log::FieldSetBuilder::default().build().unwrap()`. 
This option will redact user input like so: 

`""entity"":{""data"":{""Parents"":[]},""name"":""Sensitive<REDACTED>"",""type"":""Sensitive<REDACTED>""}`

### Note:

Cedar does not at this time support extracting the `Context` from the `Request` struct since it is private, therefore it is extracted using `request.to_string()`. This is not ideal as this logs the entire request (Principal, Action, Resource, Context) instead of just the context.

In particular this brings two quirks:

- If the `FieldSet` has principal and context set to true, then the resulting log will include the principal twice.
- If the `FieldSet` has principal set to false and context set to true, then the resulting log will include principal anyway since it is included in the `request.to_string()` call required to extract the context.

The above are not specific to principal and also occur with action and resource. A cedar github issue has been created to add a getter for the context on the cedar Request struct that will fix this: https://github.com/cedar-policy/cedar/issues/363

## Example application

This project is based on a fictitious application that allows users to manage sweet boxes. There are two entities:

1. Box (Resource)
2. User (Principal)

There are two user entities:

1. Eric
2. Mike

There are ten resources, `Box`. Each box has an entity identifier (id) that ranges
between the numbers 1-10. Each `Box` has one attribute `owner`. The owners of each `Box` are defined in
the [`tests/data/sweets.entities.json`](./tests/data/sweets.entities.json) file,
this file represents the complete database of information for this application.

The actions that `Users` can perform on each `Box` are defined in the schema file:
[`tests/data/sweets.schema.cedar.json`](./tests/data/sweets.schema.cedar.json). To summarize, each `User`
can perform one of the following actions `read`, `update` or `delete` on a `Box` resource.

A policy is a statement that declares which `Users` are explicitly permitted, or explicitly forbidden to perform an
action on a resource, `Box`. Here is a sample policy:

```
@id(""owner-policy"")
permit(principal, action, resource)
when { principal == resource.owner };
```

Refer to [`tests/data/sweets.cedar`](./tests/data/sweets.cedar) for the full details of these policies.
This file represents all policies for this application.

Given the `schema`, `entities` and `policy_set` the application can use the `Authorizer` as provided in the usage above.
Here is a sample request and expected outcome:

```rust
 assert_eq!(
     authorizer
         .is_authorized(&Request::new(
             Some(format!(""User::\""Mike\"""").parse().unwrap()),
             Some(format!(""Action::\""read\"""").parse().unwrap()),
             Some(format!(""Box::\""3\"""").parse().unwrap()),
             Context::empty(),
         ), &Entities::empty())
         .await
         .unwrap()
         .decision(),
     Decision::Deny
 );
 assert_eq!(
     authorizer
         .is_authorized(&Request::new(
             Some(format!(""User::\""Mike\"""").parse().unwrap()),
             Some(format!(""Action::\""read\"""").parse().unwrap()),
             Some(format!(""Box::\""2\"""").parse().unwrap()),
             Context::empty(),
         ), &Entities::empty())
         .await
         .unwrap()
         .decision(),
     Decision::Allow
 );
```

Feel free to refer to this sample application within the integration test located here: [`tests/lib.rs`](./tests/lib.rs).

## General Security Notes

The following is a high level description of some security concerns to keep in mind when using the `cedar-local-agent`
to enable local evaluation of Cedar policies stored on a local file system.

### Trusted Computing Environment

The `cedar-local-agent` is a mere library that customers can wrap in say an HTTP server and deploy onto a fleet of hosts.
It is, therefore, left to users to take any and all necessary precautions to ensure those security concerns beyond what
the `cedar-local-agent` is capable of enforcing are met. This includes:

1. Filesystem permissions for on-disk Policy Stores should be limited to least-privilege, see [Limiting Access to Local Data Files](#limiting-access-to-local-data-files).
2. Filesystem permissions for on-disk locations of OCSF logs follow least-privilege permissions, see [OCSF Log directory permissions](#ocsf-log-directory-permissions).
3. The `cedar-local-agent` is configured securely, see [Quick Start](#quick-start) and [Updating `file::PolicySetProvider` or `file::EntityProvider` data](#updating-filepolicysetprovider-or-fileentityprovider-data) for configuration best practices.
4. Validating the size of files read and requests made to `is_authorized` to prevent potential denial of service.

### Limiting Access to Local Data Files

The local authorizer provided in this crate only needs **read** access to locally stored policy set, entity store and
schema files.

Write access to local data files (policies, entities and schema) should be restricted only to users that really
need to make changes to these files, for example, to add new entities and remove old policies.

In the case where there are no restrictions to access local data files, a malicious Operating System (OS) user can add or
remove policies, modify entities attributes, make slight changes that are hard to identify, or even change the policies
to deny all actions. To illustrate this possibility, consider a cedar file with the following cedar policies from the
[Example Application](## Example application):

```
@id(""mike-edit-box-1"")
permit (
    principal == User::""Mike"",
    action == Action::""update"",
    resource == Box::""1""
);

@id(""eric-view-box-9"")
permit (
    principal == User::""Eric"",
    action == Action::""read"",
    resource == Box::""9""
);
```

In this example, principal ""Mike"" is allowed to perform ""update"" on resource box ""1"" while principal ""Eric"" is allowed to
perform ""read"" on resource box ""9"". Now, consider a malicious OS user adding the statement below to the same policies file.

```
@id(""ill-intentioned-policy"")
forbid(principal, action, resource);
```

In the next policies file refresh cycle, the [`file::PolicySetProvider`](./src/public/file/policy_set_provider.rs) will refresh policies file content to memory,
and the local authorizer will deny any action from any principal.

#### How to avoid this problem from happening?

In order to prevent this kind of security issue, you must restrict read access to the data files, and more important,
restrict write access to these files. Only users or groups that really need to write changes to policies,
or entities should be allowed to do so (for example, another agent that fetches policies from an internal application).

For one example on how to avoid this problem, say you have the following folder structure for a local-agent built with
`cedar-local-agent` crate.

```
authz-agent/
  |- authz_daemon (executable)

authz-local-data/
  |- policies.cedar
  |- entities.json
  |- schema.json
```

Now suppose you have an OS user to execute the ""authz_daemon"" called ""authz-daemon"" from user group ""authz-ops"".
And you have a user called ""authz-ops-admin"" from the same user group ""authz-ops"" that will be able to update data files.

Then, make ""authz-ops-admin"" the owner of **authz-local-data** folder with:

```bash
$ chown -R authz-ops-admin:authz-ops authz-local-data
```

And make ""authz-daemon"" user the owner of **authz-agent** folder with:

```bash
$ chown -R authz-daemon:authz-ops authz-agent
```

Finally, make **authz-local-data** readable by everyone and writable by the owner only:

```bash
$ chmod u=rwx,go=r authz-local-data
```

### OCSF Log directory permissions


The local authorizer provided in this crate will require **read** and **write** access to the directory where it will write OCFS logs to.

Suppose we have the following directory structure:

```
authz-agent/
  |- authz_daemon (executable)

ocsf-log-dir/
  |- authorization.log.2023-11-15-21-02
  ...
```

Now suppose you have an OS user to execute the **authz_daemon** called **authz-daemon** which should be in a group called ""log-reader"".

And make **authz-daemon** user the owner of  **ocsf-log-dir** folder with:

```bash
$ chown -R authz-daemon:log-reader ocsf-log-dir
```

We will now make **ocsf-log-dir** readable and writable by the owner but not writable to anyone else.
We allow anyone in the **log-reader** group to read the contents of the folder but not write to it.

```bash
$ chmod u=wrx,g=rx,o= ocsf-log-dir
```

NOTE: We need to allow **execute** permissions in order to access files in the directory.

Any agent that needs to access the logs, such as the [AWS Cloudwatch Agent](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html) should run as a user in the log-reader group so that they will have the proper access (see [documentation](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-common-scenarios.html) for how to configure the Cloudwatch Agent to run as a certain user).

## Getting Help

- [GitHub issues](https://github.com/cedar-policy/cedar-local-agent/issues)
- [Cedar documentation](https://docs.cedarpolicy.com/)
- [Usage examples](examples)

## License

This project is licensed under the Apache-2.0 License.",FAUX
cedar-policy/cedar-spec,Toolkit,Documentations,2025-05-12T20:18:26Z,2025-03-19T17:25:08Z,0,0,0,1,0,0,0,0,2023-04-25T18:20:07Z,2025-04-07T22:13:53Z,3372,108,Lean,VRAI,19,FAUX,44,,44,Definitional implementation of Cedar language and utilities for DRT,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,19,"# Cedar Specification

This repository contains the formalization of Cedar and infrastructure for performing differential randomized testing (DRT) between the formalization and Rust production implementation available in [`cedar`](https://github.com/cedar-policy/cedar).

You can learn more about our formalization efforts in the following blog posts:

* [How we built Cedar with automated reasoning and differential testing](https://www.amazon.science/blog/how-we-built-cedar-with-automated-reasoning-and-differential-testing)
* [Lean Into Verified Software Development](https://aws.amazon.com/blogs/opensource/lean-into-verified-software-development/)

## Repository Structure

* [`cedar-lean`](./cedar-lean) contains the Lean formalization of, and proofs about, Cedar.
* [`cedar-drt`](./cedar-drt) contains code for fuzzing, property-based testing, and differential testing of Cedar.
* [`cedar-policy-generators`](./cedar-policy-generators) contains code for generating schemas, entities, policies, and requests using the [arbitrary](https://docs.rs/arbitrary/latest/arbitrary/index.html#) crate.

See the README in each directory for more information.

## Build

### Lean formalization and proofs

* Install Lean, following the instructions [here](https://leanprover.github.io/lean4/doc/setup.html).
* `cd cedar-lean`
* `source ../cedar-drt/set_env_vars.sh` (only required if running on AL2)
* `lake update`
* `lake build Cedar`

### DRT framework

The simplest way to build our DRT framework is to use the included Dockerfile:

```bash
docker build . -t cedar_drt # ~10 minutes
docker run --rm -it cedar_drt
```

If you'd rather not use Docker, here are the full instructions for a local build:

* Install Lean, following the instructions above.
* Clone the `cedar` repository in the current (`cedar-spec`) repository.
* `source cedar-drt/set_env_vars.sh`
* `cd cedar-lean && ../cedar-drt/build_lean_lib.sh`
* `cd ../cedar-drt && cargo build`

The build has only been tested on **Amazon Linux 2**.

## Run

To run DRT:

* Follow the build instructions above.
* If running locally, `source ./set_env_vars.sh`.
* `cargo fuzz run -s none <target>`.

List the available fuzz targets with `cargo fuzz list`.
Available targets are described in the README in the `cedar-drt` directory.
That README also explains how to debug build failures, and how to save DRT-generated tests.

Additional commands available with `cargo fuzz help`.

## VSCode

To work with `cedar-drt` in VSCode, first configure two settings so that the rust analyzer plugin doesn't error when trying to find the Lean installation and so that it works properly in the `fuzz` crate.
Add the following entries to your `.vscode/settings.json`. First run `source set_env_vars.sh && echo $LEAN_LIB_DIR` to get the correct value for `LEAN_LIB_DIR`.

```json
{
    ""rust-analyzer.linkedProjects"": [
        ""./cedar-drt/fuzz/Cargo.toml""
    ],
    ""rust-analyzer.cargo.extraEnv"": {
        ""LEAN_LIB_DIR"": <$LEAN_LIB_DIR as populated by set_env_vars.sh>
    }
}
```

See the [cedar-lean README](./cedar-lean/README.md) for some additional consideration when working with the Lean formalization.

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This project is licensed under the Apache-2.0 License.",VRAI
cedar-policy/rfcs,Documentations,Documentations,2025-03-11T17:59:05Z,2024-03-14T17:41:35Z,0,0,0,2,0,0,0,0,2023-06-09T17:47:11Z,2025-03-11T17:59:09Z,475,16,,VRAI,10,FAUX,11,,11,RFCs for Cedar,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,15,"# Cedar RFC Process

The ""RFC"" (request for comments) process is intended to provide a consistent and controlled path for new features to enter the Cedar language. Many changes, including bug fixes and documentation improvements, can be implemented and reviewed via the normal GitHub pull request workflow. Some changes though are ""substantial"", and we ask that these be put through a bit of a design process to produce a consensus among the Cedar core team and the community.

## The RFC life-cycle

An RFC goes through some subset of the following stages:

* [**Pending**](https://github.com/cedar-policy/rfcs/issues?q=label%3Apending%2Carchived):
    when the RFC is submitted as a pull request (PR) to this repository.
    We use PRs to provide design feedback and come to a consensus about whether an RFC should be accepted or rejected.
    Before an RFC is officially accepted or rejected, they must undergo a [Final Comment Period](https://github.com/cedar-policy/rfcs/issues?q=label%3Afinal-comment-period+),
    a sub-stage representing the “last chance” for public comments or objections to the decision.
    Cedar maintainers will announce the FCP's beginning and end, and whether the intent is to accept or reject the RFC following the FCP.
    The FCP will typically last one calendar week.
    At the end of the FCP, if there was no new substantial discussion, the RFC will move to the next stage.
* [**Unstable**](https://github.com/cedar-policy/rfcs/issues?q=label%3Aunstable+):
    the RFC is officially accepted.
    Note that a proof-of-concept implementation may not be available at this point.
    See [Experimental features](#experimental-features) for more details.
* [**Stable**](https://github.com/cedar-policy/rfcs/issues?q=label%3Astable+):
    the RFC is officially accepted and its changes have met the [stabilization requirements](#stabilization-requirements).
    This implies that the RFC does not include outstanding questions nor the proposed changes differ from the implementation.
    Additionally, the implementation meets the specification and testing requirements for stabilization.
* [**Rejected**](https://github.com/cedar-policy/rfcs/issues?q=label%3Arejected%2Csuperceded%2Cmoved-to-issue):
    the RFC is officially rejected or dropped.

Here's a flowchart of the RFC process: (Newly-opened RFCs start in [Pending](https://github.com/cedar-policy/rfcs/issues?q=label%3Apending))

![RFC process flowchart](process-v2-flowchart.png)

Note: The previous version of the RFC process can be found [here](./archive/process-v1/).

### Experimental features

During the ""Pending"" and/or ""Unstable"" stages, an experimental Rust implementation may be available
via a dedicated Cargo feature (one feature per RFC), including on `main` and in crates.io releases.
The RFC's changes may not yet be reflected in the formal model or Cedar DRT infrastructure in [cedar-spec].
Experimental features are unstable and do not come with semver guarantees — they may undergo breaking changes even in dot releases or patch releases.
This is analogous to Rust “nightly features”.

Experimental features are a way for the community to “try out” a feature that may still undergo revisions, and are also a way for contributors to help implement Cedar features in Rust without needing to make changes to the formal model or Cedar DRT infrastructure.

### More details on particular edges

* Edge A — Accepting an RFC: Following an FCP with intent to accept, if there was no new substantial discussion during the FCP, the RFC will be accepted, and it becomes ""Unstable"".
* Edge B — Stabilizing an RFC: To start the stabilization process, open a PR in [`cedar`](https://github.com/cedar-policy/cedar) that removes the dedicated feature flag for this RFC and makes the feature available to all Cedar users.  See [Stabilization requirements](#stabilization-requirements) below for more on the requirements for this edge. If the stabilization PR is accepted, the change will be merged to `main` and released with the next appropriate Cedar release, following semver. Then, open a PR to mark the RFC ""Stable"".
* Edge C — Rejecting an unstable RFC: Sometimes an RFC could still be rejected based on additional discussion or implementation concerns that come to light during the unstable stage. However, since the RFC was previously accepted, this edge requires the same bar as an RFC itself, including an FCP with intent to reject. After rejecting the unstable RFC, update the markdown document to note the rejection date and move it from `text/`  into `archive/rfc/`.
* Edge D — Rejecting a pending RFC: Following an FCP with intent to reject, if there was no new substantial discussion during the FCP, the RFC will be rejected. The FCP is not required if the author of the RFC chooses to withdraw it.

### Stabilization requirements

Stabilizing a new feature (edge B above) requires the following:

* Implementation of the RFC passes code review and tests
* If applicable, corresponding changes have been made to the [cedar-spec] repo, both the formal model and the DRT infrastructure
* All outstanding questions about the RFC are resolved; e.g., all other PRs in the `rfcs` repo that relate to this RFC’s text have been closed one way or another
* RFC text has been updated to match the final implementation of the feature, if the implementation has diverged from the original RFC

## When to follow this process

You need to follow this process if you intend to make ""substantial"" changes to Cedar (<https://github.com/cedar-policy/cedar>). If you wish to suggest changes to other cedar-adjacent repositories like [cedar-spec] or [cedar-examples], please use their respective issue lists.

What constitutes a ""substantial"" change is evolving based on community norms, but may include the following:

* A new feature that creates new API surface area
* Changing the semantics or behavior of an existing API
* Adding, removing, or changing the behavior of a built-in function or operator in the Cedar language, or any other Cedar syntax
* The removal of features that are already shipped as part of the release channel

Some changes do not require an RFC:

* Simple bug fixes
* Fixing objectively incorrect behavior
* Rephrasing, reorganizing or refactoring
* Addition or removal of warnings
* Improvement of error messages
* Additions only likely to be noticed by other Cedar developers, invisible to users.

If you submit a pull request to implement a new feature without going through the RFC process, it may be closed with a polite request to submit an RFC first.

## Why do you need to do this

You are suggesting new features or changes to Cedar — we appreciate your willingness to contribute! We have to carefully consider the impact of every change we make that may affect end users. These constraints and tradeoffs may not be immediately obvious to users who are proposing a change to solve a specific problem they just ran into. The RFC process serves as a way to guide you through our thought process when making changes to Cedar, so that everyone can be on the same page when discussing why these changes should or should not be made.

It's often helpful to get feedback on your concept before diving into the design details required for an RFC. You may open an issue with a `Question` label on this repo to start a high-level discussion, with the goal of eventually formulating an RFC pull request with the specific implementation design.

## What is the process?

In short, to get a major feature added to Cedar, you must first get the RFC merged into the RFC repo as a markdown file. At that point the RFC is ""Unstable"" and may be implemented with the goal of eventual inclusion into Cedar.

* Work on your proposal in a markdown file based on the template (<https://github.com/cedar-policy/rfcs/blob/main/text/0000-template.md>). Put care into the details: RFCs that do not present convincing motivation, demonstrate understanding of the impact of the design, or are disingenuous about the drawbacks or alternatives tend to be poorly-received. Copy your markdown file into `text/0000-my-feature.md`, where my-feature is descriptive.
* Fork this repository and create a PR with your markdown file. Once you have a PR, fill in the number in your filename with the PR number (e.g., `0003-my-feature.md` for PR #3). We will use this PR to provide feedback and to come to a consensus on whether the RFC should be accepted. Revisions to the RFC based on feedback should be done in the same PR.
* Your RFC is now ""Pending"". See [the RFC life-cycle](#the-rfc-life-cycle) above for more on the next steps.

## Implementing an RFC

The author of an RFC is not obligated to implement it.
Of course, the RFC author (like any other developer) is welcome to post an implementation for review before or after RFC has been accepted.
If you are interested in working on the implementation for an RFC, but cannot determine if someone else is already working on it, feel free to ask (e.g. by leaving a comment on the associated PR).


Proof-of-concept implementations are encouraged but not required for an RFC to be accepted.
If there is a PR open for the proof-of-concept implementation, we recommend linking it in the RFC.
Feedback for the actual implementation should be conducted in the implementation PR instead of the original RFC PR.

## Reviewing RFCs

Members of the core team will attempt to review open RFC PRs on a regular basis. Once the core team agrees that an RFC should be accepted/rejected, a member of the core team will leave a comment on the PR with the decision and an explanation for the decision, and start the final comment period (FCP). After the FCP, pending no further discussion, a member of the core team will close the PR (if the RFC is rejected) or merge the PR (if the RFC is accepted).

## Acknowledgments

Cedar's RFC process owes its inspiration to the [Vue RFC process](https://github.com/vuejs/rfcs), [React RFC process](https://github.com/reactjs/rfcs), and [Rust RFC process](https://github.com/rust-lang/rfcs).

## License

This project is licensed under the Apache-2.0 License.

[cedar-spec]: https://github.com/cedar-policy/cedar-spec/
[cedar-examples]: https://github.com/cedar-policy/cedar-examples",FAUX
celest-dev/celest,Toolkit,Application System,2025-05-14T22:18:40Z,2025-05-07T19:51:03Z,0,0,0,5,0,0,0,0,2023-11-13T22:17:01Z,2025-04-08T14:31:38Z,22361,262,Dart,VRAI,11,FAUX,23,"backend,cloud,dart,flutter,serverless",23,The Flutter cloud platform,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,8,"<p align=""center"">
<img src=""./assets/logo-dark.png#gh-dark-mode-only"" width=""500"">
<img src=""./assets/logo-light.png#gh-light-mode-only"" width=""500"">
</p>

## Celest

> [!WARNING]
> Celest is in the process of being open-sourced. During this process, many of the previously valid links like [celest.dev]() will be
> unavailable. We expect this process to be completed by the end of April 2025.

Celest is the Flutter cloud platform. We enable Flutter and Dart developers to declaratively define their backend infrastructure in Dart.

And to stay up-to-date on the future of Celest, including full server-side rendered Flutter apps, join our newsletter at [celest.dev](https://www.celest.dev/#stay-up-to-date).

## Getting Started

To get started with Celest, you'll need to configure your development environment so that you have Flutter and the Celest CLI installed on your machine.

1. Install [Flutter](https://docs.flutter.dev/get-started/install) from the official website
2. [Download](https://celest.dev/download) and install the Celest CLI

### Creating a project

Once you have the CLI installed, you can create a new project by running the following command:

```bash
$ celest init
```
You can run this command from within your Flutter project which will create a new `celest/` directory for your project. Or you can run
this in another directory to have a standalone Celest project.

Once you have a project, run `celest start` to start a local development environment.

```bash
$ celest start
✓ Celest is running on http://localhost:7777
```

This command will start a local server which will run in the background as you write your backend logic. As you make changes to the files in the `celest/` directory, 
the server will hot-reload those changes so you can see them live.

To interact with the running environment, Celest will generate a Dart client which you can use in any Dart or Flutter project. This client 
is generated in the `client/` directory of your `celest/` folder. As you make changes in the local environment, this client will be updated to reflect those changes.

### Example

Here is an example of a simple Celest function:

```dart
import 'package:celest/celest.dart';

@cloud
Future<String> sayHello(String name) async {
  print('Saying hello to $name');
  return 'Hello, $name';
}
```

This function can be called from a Dart project like so:

```dart
import 'package:my_project_client/my_project_client.dart';

Future<void> main() async {
  celest.init(environment: CelestEnvironment.local);
  final response = await celest.functions.sayHello('World');
  print(response); // Hello, World
}
```

## What's Next?

In addition to Dart cloud functions, Celest offers authentication and database services out-of-the-box. To learn more about these features, 
and to see what else you can do with cloud functions, visit our [docs](https://celest.dev/docs) and explore the different examples and
packages available in this repo.

## Examples

[![Celest](https://github.com/celest-dev/celest/actions/workflows/examples.yaml/badge.svg)](https://github.com/celest-dev/celest/actions/workflows/examples.yaml)

| Example                        | Description                                                                               |
| ------------------------------ | ----------------------------------------------------------------------------------------- |
| [Firebase](examples/firebase/) | Showcases how to integrate Firebase Auth with Celest.                                     |
| [Gemini](examples/gemini/)     | Uses Google's [Gemini](https://ai.google.dev/) API for chat completion.                   |
| [OpenAI](examples/openai/)     | Uses the [OpenAI](https://platform.openai.com/docs/introduction) API for chat completion. |
| [Supabase](examples/supabase/) | Showcases how to integrate Supabase Auth with Celest.                                     |
| [Tasks](examples/tasks/)       | Uses Celest Data to build a simple task tracking app with persistence.                    |

## Packages

| Package                                | Description                                                   | Pub                                                                                                                            | Checks                                                                                                                                                                          |
| -------------------------------------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [celest](packages/celest/)             | The main package for defining Celest backends.                | <a href=""https://pub.dev/packages/celest"" target=""_blank""> <img src=""https://img.shields.io/pub/v/celest.svg""></a>             | [![Celest](https://github.com/celest-dev/celest/actions/workflows/celest.yaml/badge.svg)](https://github.com/celest-dev/celest/actions/workflows/celest.yaml)                   |
| [celest_ast](packages/celest_ast/)     | A structured representation of Celest projects.               | <a href=""https://pub.dev/packages/celest_ast"" target=""_blank""> <img src=""https://img.shields.io/pub/v/celest_ast.svg""></a>     | [![Celest AST](https://github.com/celest-dev/celest/actions/workflows/celest_ast.yaml/badge.svg)](https://github.com/celest-dev/celest/actions/workflows/celest_ast.yaml)       |
| [celest_auth](packages/celest_auth/)   | The authentication and authorization runtimes for Celest.     | <a href=""https://pub.dev/packages/celest_auth"" target=""_blank""> <img src=""https://img.shields.io/pub/v/celest_auth.svg""></a>   | [![Celest Auth](https://github.com/celest-dev/celest/actions/workflows/celest_auth.yaml/badge.svg)](https://github.com/celest-dev/celest/actions/workflows/celest_auth.yaml)    |
| [celest_cloud](packages/celest_cloud/) | API contracts and Dart clients for the Celest Cloud platform. | <a href=""https://pub.dev/packages/celest_cloud"" target=""_blank""> <img src=""https://img.shields.io/pub/v/celest_cloud.svg""></a> | [![Celest Cloud](https://github.com/celest-dev/celest/actions/workflows/celest_cloud.yaml/badge.svg)](https://github.com/celest-dev/celest/actions/workflows/celest_cloud.yaml) |
| [celest_core](packages/celest_core/)   | Core types and utilities shared between Celest packages.      | <a href=""https://pub.dev/packages/celest_core"" target=""_blank""> <img src=""https://img.shields.io/pub/v/celest_core.svg""></a>   | [![Celest Core](https://github.com/celest-dev/celest/actions/workflows/celest_core.yaml/badge.svg)](https://github.com/celest-dev/celest/actions/workflows/celest_core.yaml)    |

## Services

| Service                                          | Description                                                                                                                         | Pub                                                                                                                                      | Checks                                                                                                                                                                                         |
| ------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [celest_cloud_auth](packages/celest_cloud_auth/) | A Dart-native authentication and authorization solution built on [Cedar](https://github.com/celest-dev/cedar-dart) and Celest Data. | <a href=""https://pub.dev/packages/celest_cloud_auth"" target=""_blank""> <img src=""https://img.shields.io/pub/v/celest_cloud_auth.svg""></a> | [![Celest Cloud Auth](https://github.com/celest-dev/celest/actions/workflows/celest_cloud_auth.yaml/badge.svg)](https://github.com/celest-dev/celest/actions/workflows/celest_cloud_auth.yaml) |

## License

This repo is licensed under the [BSD-2-Clause-Patent](https://spdx.org/licenses/BSD-2-Clause-Patent.html) license. See [LICENSE.md](LICENSE.md) for the full text.

## Connect with us

- Follow our [Twitter](https://twitter.com/Celest_Dev) account.
- Schedule a meeting or chat with us live: [Contact](https://celest.dev/contact)",VRAI
chainloop-dev/chainloop,Toolkit,DevOPs,2025-05-15T16:46:56Z,2025-05-02T11:04:55Z,0,18,0,0,0,0,0,0,2023-03-06T14:30:50Z,2025-04-07T13:28:51Z,29882,410,Go,VRAI,33,FAUX,114,"attestation,compliance,cyclonedx,devsecops,in-toto,license,metadata-platform,open-source-licensing,ospo,oss-compliance,regulated-industry,sbom,sbom-discovery,sbom-distribution,security,slsa,slsa-provenance,spdx,supply-chain-security",114,"Evidence store and policy engine for your Software Supply Chain attestations, SBOMs, VEX, SARIF, QA reports, and more",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,22,"# Chainloop

[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/chainloop-dev/chainloop/badge)](https://securityscorecards.dev/viewer/?uri=github.com/chainloop-dev/chainloop)
[![Go Report Card](https://goreportcard.com/badge/github.com/chainloop-dev/chainloop)](https://goreportcard.com/report/github.com/chainloop-dev/chainloop)
![Test passing](https://github.com/chainloop-dev/chainloop/actions/workflows/test.yml/badge.svg?branch=main)
[![Chat on Slack](https://img.shields.io/badge/slack-chainloop-blue?logo=slack)](https://join.slack.com/t/chainloop-community/shared_invite/zt-2k34dvx3r-u85uGP_KiLC6ic5Wy4aRnQ)
[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/chainloop-dev/chainloop/blob/main/LICENSE.md)

> Chainloop is under active development and you should expect breaking changes before the first stable release.
> If you are interested in contributing, please take a look at our [contributor guide](./CONTRIBUTING.md).

## What is it?

[Chainloop](https://github.com/chainloop-dev/chainloop) is an open-source evidence store for your Software Supply Chain attestations, Software Bill of Materials (SBOMs), VEX, SARIF, QA reports, and more. With Chainloop, Security, Compliance, and Risk management teams can define security and compliance policies, what evidence and artifacts they want to receive, and where to store them. On the other hand, developers are shielded from all this complexity by being given simple instructions on what to provide when instrumenting their CI/CD pipelines.

To learn more about the project motivation please look at [our documentation](https://docs.chainloop.dev) and see it in action in [this video](https://www.youtube.com/watch?v=GfSR2ZkZ3as).

## How does it work?

### Compliant Single Source of Truth

Craft and store attestation metadata and artifacts via a single integration point regardless of your CI/CD provider choice.

![Chainloop Overview](./docs/img/overview-1.png)

The result is having a SLSA level 3 compliant single Source of truth for metadata, artifacts and attestations built on OSS standards such as [Sigstore](https://www.sigstore.dev/), [in-toto](https://in-toto.io/), [SLSA](https://slsa.dev) and [OCI](https://github.com/opencontainers/image-spec/blob/main/spec.md).

Chainloop also makes sure the crafting of artifacts and attestation follows **best practices and meets the requirements** declared in their associated Workflow Contract.

### Declarative, contract-based attestation

One key aspect is that in Chainloop, CI/CD integrations are declared via [**Workflow Contracts**](https://docs.chainloop.dev/getting-started/workflow-definition#workflow-contracts).

A [Workflow Contract](https://docs.chainloop.dev/reference/operator/contract) gives Compliance and Security teams **full control over what kind of data (build info, materials) must be received as part of the attestation and the environment where these workflows must be executed at**. This enables an easy, and maintainable, way of propagating and enforcing requirements downstream to your organization.

You can think of it as an [**API for your organization's Software Supply Chain**](https://docs.chainloop.dev/reference/operator/contract) that both parties, development and Compliance and Security teams can use to interact effectively.

![Chainloop Contracts](./docs/img/overview-3.png)

### Policy as code

Compliance and Security teams can [craft](https://docs.chainloop.dev/guides/rego-policies/) [Rego](https://www.openpolicyagent.org/docs/latest/policy-language/) policies, and [attach](https://docs.chainloop.dev/reference/policies) them to workflow contracts. Those policies will be automatically evaluated, and their results will be added to the attestation before signing and storage.


### We meet you where you are with third-party integrations

Operators can set up third-party integrations such as [Dependency-Track](https://docs.chainloop.dev/guides/dependency-track), or [Guac](https://docs.chainloop.dev/guides/guac/) for SBOM analysis or a storage backend such as an OCI registry, or cloud blob storage to place the received artifacts, pieces of evidence and attestation metadata.

![Chainloop Overview](./docs/img/overview-2.png)

Compliance and Security teams can mix and match with different integrations while **not requiring developers to make any changes on their side**!

To learn more and to find the list of available integrations, check our [integrations page](./devel/integrations.md).

### Role-tailored experience

Chainloop makes sure to clearly define the responsibilities, experience and functional scope of the **two main personas, Compliance/Security and Development teams**.

Compliance and Security teams are the ones in charge of defining the Workflow Contracts, crafting policies, setting up third-party integrations, or having access to the control plane where all the Software Supply Chain Security bells and whistles are exposed.

Development teams on the other hand, just need to integrate Chainloop's jargon-free [crafting tool](https://docs.chainloop.dev/getting-started/attestation-crafting) and follow the steps via a familiar DevExp to make sure they comply with the Workflow Contract defined by the SecOps team. No need to learn in-toto, signing, SLSA, OCI, APIs, nada :)

## Supported Pieces of Evidence / Materials

During the attestation process, you can attach different pieces of evidence and artifacts that will get uploaded to the [Content Addressable Storage](https://docs.chainloop.dev/reference/operator/cas-backend/) (if applicable) and referenced in a signed in-toto attestation.

Chainloop supports the collection of the following list of evidence types. For the full list please refer to [this page](https://docs.chainloop.dev/reference/operator/material-types)

- [CycloneDX SBOM](https://github.com/CycloneDX/specification)
- [SPDX SBOM](https://spdx.dev/specifications/)
- [OpenVEX](https://github.com/openvex)
- [SARIF](https://docs.oasis-open.org/sarif/sarif/v2.1.0/)
- [Container Image Reference](https://github.com/opencontainers/image-spec)
- [Helm Chart](https://helm.sh/docs/topics/charts/)
- [BlackDuck SCA](https://www.blackduck.com/software-composition-analysis-tools/black-duck-sca.html)
- [ZAP DAST](https://github.com/marketplace/actions/zap-baseline-scan)
- [PrismaCloud Twistcli Scan](https://docs.prismacloud.io/en/compute-edition/30/admin-guide/tools/twistcli-scan-images)
- [CSAF Security Incident Report](https://docs.oasis-open.org/csaf/csaf/v2.0/os/csaf-v2.0-os.html#42-profile-2-security-incident-response)
- [CSAF Informational Advisory](https://docs.oasis-open.org/csaf/csaf/v2.0/os/csaf-v2.0-os.html#43-profile-3-informational-advisory)
- [CSAF Security Advisory](https://docs.oasis-open.org/csaf/csaf/v2.0/os/csaf-v2.0-os.html#44-profile-4-security-advisory)
- [CSAF VEX](https://docs.oasis-open.org/csaf/csaf/v2.0/os/csaf-v2.0-os.html#45-profile-5-vex)
- [Gitlab Security report](https://docs.gitlab.com/ee/user/application_security/)
- [JUnit](https://www.ibm.com/docs/en/developer-for-zos/14.1?topic=formats-junit-xml-format)
- [JaCoCo XML Coverage Reports](https://www.jacoco.org/jacoco/trunk/doc/)
- Attestation: existing Chainloop attestations.
- Artifact Type: It represents a software artifact.
- Custom Evidence Type: Custom piece of evidence that doesn't fit in any other category, for instance, an approval report in json format, etc.
- Key-Value metadata pairs

## Getting started

See the [getting started guide](https://docs.chainloop.dev/getting-started/installation#command-line-interface-cli-installation) for detailed information on a) how to download and configure the Chainloop CLI and b) how to deploy Chainloop on your Kubernetes Cluster.

### Command Line Interface (CLI) installation

> Alternatively, you can download the CLI from the [releases pages](https://github.com/chainloop-dev/chainloop/releases) or [build it from source](./CONTRIBUTING.md).

To **install the latest version** for macOS, Linux or Windows (using [WSL](https://learn.microsoft.com/en-us/windows/wsl/install)) just choose one of the following installation methods.

```bash
curl -sfL https://raw.githubusercontent.com/chainloop-dev/chainloop/main/docs/static/install.sh | bash -s
```

you can retrieve a specific version with

```bash
curl -sfL https://raw.githubusercontent.com/chainloop-dev/chainloop/main/docs/static/install.sh | bash -s -- --version v0.8.95
```

and customize the install path (default to /usr/local/bin)

```bash
curl -sfL https://raw.githubusercontent.com/chainloop-dev/chainloop/main/docs/static/install.sh | bash -s -- --path /my-path
```

if [`cosign`](https://docs.sigstore.dev/cosign) is present in your system, in addition to the checksum check, a signature verification will be performed. This behavior can be enforced via the `--force-verification` flag.

```bash
curl -sfL https://raw.githubusercontent.com/chainloop-dev/chainloop/main/docs/static/install.sh | bash -s -- --force-verification
```

### Deploy Chainloop (optional)

Downloading the CLI is everything you need to give Chainloop a try, since, by default, it points to a [running instance of Chainloop](https://app.chainloop.dev).

You can also **run your own Chainloop instance** on your Kubernetes cluster by leveraging [this Helm Chart](./deployment/chainloop/).

### Configure CLI (optional)

If you are running your [own instance](https://github.com/chainloop-dev/chainloop) of the Control Plane. You can make the CLI point to your instance by using the `chainloop config save` command.

```sh
chainloop config save \
  --control-plane my-controlplane.acme.com \
  --artifact-cas cas.acme.com
```

### Authentication

Authenticate to the Control Plane by running

```bash
$ chainloop auth login
```

### Finishing the setup

Once you've been logged in, follow [these instructions](https://docs.chainloop.dev/getting-started/setup) to learn how to set up your account.
## Documentation

To learn more, please visit the Chainloop project's documentation website, https://docs.chainloop.dev where you will find a getting started guide, FAQ, examples, and more.

## Community / Discussion / Support

Chainloop is developed in the open and is constantly improved by our users, contributors and maintainers. Got a question, comment, or idea? Please don't hesitate to reach out via:

- GitHub [Issues](https://github.com/chainloop-dev/chainloop/issues)
- [Slack](https://join.slack.com/t/chainloop-community/shared_invite/zt-2k34dvx3r-u85uGP_KiLC6ic5Wy4aRnQ)
- Youtube [Channel](https://www.youtube.com/channel/UCISrWrPyR_AFjIQYmxAyKdg)

## Contributing

Want to get involved? Contributions are welcome.

If you are ready to jump in and test, add code, or help with documentation, please follow the instructions on
our [Contribution](CONTRIBUTING.md) page. At all times, follow our [Code of Conduct](./CODE_OF_CONDUCT.md).

See the [issue tracker](https://github.com/chainloop-dev/chainloop/issues) if you're unsure where to start, especially the [Good first issue](https://github.com/chainloop-dev/chainloop/labels/good%20first%20issue) label.

## Changelog

Take a look at the list of [releases](http://github.com/chainloop-dev/chainloop/releases) to stay tuned for the latest features and changes.

## License

Chainloop is released under the Apache License, Version 2.0. Please see the [LICENSE](./LICENSE.md) file for more information.",VRAI
chaitin/veinmind-tools,Toolkit,Toolkit,2024-01-10T09:08:30Z,2023-05-08T13:20:06Z,0,55,0,0,0,0,0,0,2022-02-08T08:12:15Z,2025-04-03T07:09:22Z,21068,1574,Go,VRAI,184,FAUX,24,"cloud-native,cloud-security,container-security,containerd,docker,image-security,security",24,veinmind-tools 是由长亭科技自研，基于 veinmind-sdk 打造的容器安全工具集,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,14,"<p align=""center"">
  <img src=""https://dinfinite.oss-cn-beijing.aliyuncs.com/image/20220428154824.png"" width=""120"">
</p>
<h1 align=""center""> veinmind-tools </h1>
<p align=""center"">
  <a href=""https://veinmind.chaitin.com/docs/"">Documentation</a> 
</p>

<p align=""center"">
<img src=""https://img.shields.io/github/v/release/chaitin/veinmind-tools.svg"" />
<img src=""https://img.shields.io/github/release-date/chaitin/veinmind-tools.svg?color=blue&label=update"" />
<img src=""https://img.shields.io/badge/go report-A+-brightgreen.svg"" />

<p align=""center""> veinmind-tools is self-developed by <a href=""https://www.chaitin.cn/en/""> chaitin technology </a>，
cloudwalker team incubation，a container security toolset based on <a href=""https://github.com/chaitin/libveinmind"">veinmind-sdk</a>  </p>
</p>

## 🔥 Demo

![](https://veinmind-cache.oss-cn-hangzhou.aliyuncs.com/img/scan.gif)

Veinmind has been connected to openai. You can use openai to conduct a user-friendly analysis of the scan results, allowing you to have a clearer understanding of the risks identified during this scan.

![](https://veinmind-cache.oss-cn-hangzhou.aliyuncs.com/img/ai.png)

## 🕹️ Quick Start

### 1. Make sure docker is installed correctly on the machine

```
docker info
```

### 2. Install [veinmind-runner](https://github.com/chaitin/veinmind-tools/tree/master/veinmind-runner) image

```
docker pull registry.veinmind.tech/veinmind/veinmind-runner:latest
```

### 3. Download [veinmind-runner](https://github.com/chaitin/veinmind-tools/tree/master/veinmind-runner) parallel container startup script

```
wget -q https://download.veinmind.tech/scripts/veinmind-runner-parallel-container-run.sh -O run.sh && chmod +x run.sh
```

### 4. Quick scan local images/containers

```
./run.sh scan [image/container]
```

### 5. use ai analyze

```
./run.sh scan [image/container] --enable-analyze --openai-token  <your_openai_token>
```

> Note: When using openAI, please ensure that the current network can access openAI
> When starting a parallel container, you need to manually use docker run -e http_proxy=xxxx -e https_proxy=xxxx Set proxy (in non global proxy scenarios)


### 6. generate <html> <cli> <json> report

```
./run.sh scan [image/container] --format=html,cli
```

> this will generate a file at path which name `report.html` or `report.json`
> you can use `,` to generate different reports，like `--format=html,cli,json` will output both  `report.html` and `report.json` and cli table。

## 🔨 Toolset

| Tool                                                                      | Description                                            | 
|---------------------------------------------------------------------------|--------------------------------------------------------|
| [veinmind-runner](veinmind-runner/README.en.md)                           | scanner host                                           |
| [veinmind-malicious](plugins/go/veinmind-malicious/README.en.md)          | Scan containers/images for malicious files             |
| [veinmind-weakpass](plugins/go/veinmind-weakpass/README.en.md)            | scan containers/images for weak passwords              |
| [veinmind-log4j2](plugins/go/veinmind-log4j2/README.en.md)                | scan containers/images for log4j2(CVE-2021-44228)      |
| [veinmind-minio](plugins/go/veinmind-minio)                               | scan containers/images for minio(CVE-2023-28432)       |
| [veinmind-sensitive](plugins/go/veinmind-sensitive/README.en.md)          | scan images for sensitive information                  |
| [veinmind-backdoor](plugins/go/veinmind-backdoor/README.en.md)            | scan images for backdoors                              |
| [veinmind-history](plugins/python/veinmind-history/README.en.md)          | scan images for abnormal history commands              |
| [veinmind-vuln](plugins/go/veinmind-vuln/README.en.md)                    | scan containers/images for asset information and vulns |
| [veinmind-webshell](plugins/go/veinmind-webshell)                         | scan containers/images for webshell                    |
| [veinmind-unsafe-mount](plugins/go/veinmind-unsafe-mount)                 | scan containers for unsafe mount                       |
| [veinmind-iac](plugins/go/veinmind-iac)                                   | scan images/cluster IaC file                           |
| [veinmind-escape](plugins/go/veinmind-escape)                             | scan containers/images for escape risk                 |
| [veinmind-privilege-escalation](plugins/go/veinmind-privilege-escalation) | scan containers/images for privilege escalation risk   |
| [veinmind-trace](plugins/go/veinmind-trace)                               | scan  containers attack trace                          |


PS: All tools currently support running in parallel containers

## 🧑‍💻 Coding Plugins

Use example to create a veinmind-tool plugin quickly, see more at [veinmind-example](example/README.en.md)

## ☁️ Cloud-native infrastructure compatibility

| Name                                                         | Type     | Compatibility |
|--------------------------------------------------------------|----------|---------------|
| [Jenkins](https://github.com/chaitin/veinmind-jenkins)       | CI/CD    | ✔️            |
| [Gitlab CI](https://veinmind.chaitin.com/docs/ci/gitlab/)    | CI/CD    | ✔️            |
| [Github Action](https://github.com/chaitin/veinmind-action)  | CI/CD    | ✔️            |
| DockerHub                                                    | Registry | ✔️            |
| Docker Registry                                              | Registry | ✔️            |
| Harbor                                                       | Registry | ✔️            |
| Docker                                                       | Runtime  | ✔️            |
| Containerd                                                   | Runtime  | ✔️            |
| kubernetes                                                   | Cluster  | ✔️            |

## 🛴 Architecture
![](docs/architecture.png)

## 🏘️ Contact Us

1. You can make bug feedback and feature suggestions directly through GitHub Issues.
2. By scanning the QR code below (use wechat), you can join the discussion group of veinmind users for detailed
   discussions by adding the veinmind assistant.

![](docs/veinmind-group-qrcode.png)

## ✨ CTStack
<img src=""https://ctstack-oss.oss-cn-beijing.aliyuncs.com/CT%20Stack-2.png"" width=""30%"" />

veinmind-tools has already joined [CTStack](https://stack.chaitin.com/tool/detail?id=3) community

## ✨ 404 starlink project

<img src=""https://github.com/knownsec/404StarLink-Project/raw/master/logo.png"" width=""30%"">

veinmind-tools now joined 404 starlink project (https://github.com/knownsec/404StarLink)

## ✨ Star History <a name=""star-history""></a>

<a href=""https://github.com/chaitin/veinmind-tools/stargazers"">
    <img width=""500"" alt=""Star History Chart"" src=""https://api.star-history.com/svg?repos=chaitin/veinmind-tools&type=Date"">
</a>",FAUX
charmed-kubernetes/jenkins,DevOPs,Documentations,2025-05-09T19:19:57Z,2025-04-04T18:38:36Z,0,0,0,0,0,0,0,1,2016-12-01T17:44:24Z,2025-04-04T18:38:40Z,8036,11,Python,FAUX,24,FAUX,15,,15,The scripts to build and test kubernetes in jenkins. Report bugs at https://bugs.launchpad.net/charmed-kubernetes.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,39,"# Charmed Kubernetes Jenkins

This project contains the scripts used to build and test Charmed Kubernetes.

## What is where

 - *jobs* - All jenkins jobs housed here
 - *jobs/integration* - All integration tests housed here

## How to run tests locally

Running the tests locally can be accomplished easily with tox. The tests expect
certain environment variables to be set. These can be found by looking at the
help output from `pytest` under the **custom options** section.

> **Note**: Required minimum Python version is 3.8.

```
> tox -e py --workdir .tox -- pytest jobs/integration/validation.py --help

custom options:
  --no-flaky-report     Suppress the report at the end of the run detailing
                        flaky test results.
  --no-success-flaky-report
                        Suppress reporting flaky test successesin the report
                        at the end of the run detailing flaky test results.
  --controller=CONTROLLER
                        Juju controller to use
  --model=MODEL         Juju model to use
  --series=SERIES       Base series
  --cloud=CLOUD         Juju cloud to use
  --charm-channel=CHARM_CHANNEL
                        Charm channel to use
  --bundle-channel=BUNDLE_CHANNEL
                        Bundle channel to use
  --snap-channel=SNAP_CHANNEL
                        Snap channel to use eg 1.16/edge
  --is-upgrade          This test should be run with snap and charm upgrades
  --upgrade-snap-channel=UPGRADE_SNAP_CHANNEL
                        Snap channel to use eg 1.16/edge
  --upgrade-charm-channel=UPGRADE_CHARM_CHANNEL
                        Charm channel to use (stable, candidate, beta, edge)
  --snapd-upgrade       run tests with upgraded snapd
  --snapd-channel=SNAPD_CHANNEL
                        Snap channel to install snapcore from
  --vault-unseal-command=VAULT_UNSEAL_COMMAND
                        Command to run to unseal vault after a series upgrade
```

This tells us what the commandline is to run this test and what parameters we
need to pass to it. These are passed to pytest running in tox. By default, the
working directory for tox is in /var/lib/jenkins, which probably doesn't exist
on development machines, so --workdir is used to specify a new directory to use.

```
tox --workdir .tox -e py3 -- \
    pytest jobs/integration/validation.py \
      --controller aws-us-east-1 \
      --model cdk \
      --cloud aws 2>&1 | tee ~/log.txt
```

## Developing new tests

Jenkins Job Builder is used to generate jobs for Jenkins programmatically. No
jobs are created by hand in the Jenkins UI.

Adding a new test can be done by copying an existing one and modifying for your needs:

[Spec](https://github.com/charmed-kubernetes/jenkins/blob/main/jobs/validate/spec)

[JJB Validate](https://github.com/charmed-kubernetes/jenkins/blob/main/jobs/validate.yaml)

## Updating jobs

Use `jenkins-jobs` to add/modify/remove tests from the Jenkins web ui. For a
single job, run the following:

```
tox --workdir .tox -e py3 -- \
    jenkins-jobs --conf jobs/jjb-conf.ini update jobs/ci-master.yaml:jobs/sync-oci-images.yaml
```

Update all jobs using the following syntax (optionally remove old jobs):
```
tox --workdir .tox -e py3 -- \
    jenkins-jobs --conf jobs/jjb-conf.ini update --delete-old jobs
```

## Job schedule

Most `build-*` jobs run daily, while `validate-` jobs are spread out throughout
the week. The `infra-*` jobs run 4 times a day and ensure any old packages
and deployments are cleaned up.

The `sync-internal-tags` job is used as part of our snap build process. Once a
new tag is seen upstream, launchpad builders will automatically sync our
repositories and build new snaps. This job should run at least twice a day to
make sure new snaps are built the same day that upstream makes a release.

Timing for any job can be adjusted with the `timed` parameter in the job yaml:

```
$ jenkins/jobs$ grep timed * 2>/dev/null
build-aws-iam-oci.yaml:      - timed: ""@weekly""
build-charms.yaml:        - timed: ""@daily""
build-snaps.yaml:        - timed: ""@daily""
bundle-tester.yaml:        - timed: ""@weekly""
infra.yaml:        - timed: ""H */6 * * *""
infra.yaml:        - timed: ""0 */6 * * *""
maintenance-microk8s-branches-builders.yaml:        - timed: ""@hourly""
release-microk8s.yaml:        - timed: ""@daily""
release-microk8s.yaml:        - timed: ""@hourly""
release-microk8s.yaml:        - timed: ""@hourly""
reports.yaml:        - timed: ""@hourly""
sync-oci-images.yaml:        - timed: ""@daily""
sync-upstream.yaml:        - timed: ""@daily""
sync-upstream.yaml:        - timed: ""@daily""
sync-upstream.yaml:        - timed: ""H */12 * * *""
sync-upstream.yaml:        - timed: ""@daily""
sync-upstream.yaml:        - timed: ""@daily""
validate-hacluster.yaml:        - timed: ""@monthly""
validate.yaml:        - timed: ""@daily""
validate.yaml:        - timed: ""@weekly""
validate.yaml:        - timed: ""@monthly""
validate.yaml:        - timed: ""@weekly""
validate.yaml:        - timed: ""@weekly""
validate.yaml:        - timed: ""@monthly""
validate.yaml:        - timed: ""@daily""
validate.yaml:        - timed: ""@weekly""
validate.yaml:        - timed: ""@daily""
validate.yaml:        - timed: ""0 0 */2 * *""
validate.yaml:        - timed: ""@weekly""
validate.yaml:        - timed: ""@weekly""
validate.yaml:        - timed: ""@weekly""
validate.yaml:        - timed: ""@monthly""
validate.yaml:        - timed: ""@monthly""
validate.yaml:        - timed: ""@weekly""
validate.yaml:        - timed: ""@weekly""
```

After updating a job timer, be sure to run `jenkins-jobs` as described in the
*Updating jobs* section above.

## Documentation

### Build

To build the docs do the following:

```
> tox --workdir .tox -e py3 -- inv build-docs
```

To build and deploy documentation (requires AWS credentials):

```
> tox --workdir .tox -e docs
```",VRAI
Checkmarx/kics,Toolkit,Application System,2025-05-13T16:13:57Z,2025-02-25T09:28:11Z,0,1754,0,0,0,0,5,0,2020-07-08T21:46:15Z,2025-04-05T08:31:38Z,829497,2260,Open Policy Agent,VRAI,323,FAUX,224,"appsec,cloudnative,devsecops,golang,hacktoberfest,iac,infrastructure-as-code,open-policy-agent,security,security-tools,vulnerability-detection,vulnerability-scanners",224,"Find security vulnerabilities, compliance issues, and infrastructure misconfigurations early in the development cycle of your infrastructure-as-code with KICS by Checkmarx.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,150,"[![Latest Release](https://img.shields.io/github/v/release/checkmarx/kics)](https://github.com/checkmarx/kics/releases)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Queries](https://raw.githubusercontent.com/Checkmarx/kics/gh-pages/queries.svg)](https://docs.kics.io/develop/queries/all-queries/)
[![Docker Pulls](https://img.shields.io/docker/pulls/checkmarx/kics)](https://hub.docker.com/r/checkmarx/kics)
![GitHub contributors](https://img.shields.io/github/contributors/checkmarx/kics?color=blue&link=https%3A%2F%2Fgithub.com%2FCheckmarx%2Fkics%2Fgraphs%2Fcontributors)
[![Documentation](https://img.shields.io/badge/docs-viewdocs-blue.svg ""Viewdocs"")](https://docs.kics.io/)
[![GitHub Discussions](https://img.shields.io/badge/chat-discussions-blue.svg?logo=github)](https://github.com/Checkmarx/kics/discussions)

[![checkmarx](https://img.shields.io/endpoint?url=https://pgp36n22ol.execute-api.eu-west-1.amazonaws.com/dev/cxflowcache-results?style=plastic&logoWidth=20&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAACXBIWXMAAA7EAAAOxAGVKw4bAAADbklEQVRYhc2XTWidRRSGn3MJl1AkZFFcFCkFIYKCYMEgIhZEMtNS2kHBvxYbl1kUIqELQXHhQigtCK266EIoSt0YplbsTJGqaFFahC6yiiJFUIJ2IUGClJLj4vvu/X7uTC7cJOoLH5eZM3Ped37OOXPhP4akOl20XeAQsB+YAsZH9H8HuAVcAxa9CStDBbhgDyKcAfaMSJoVo8pZEd7wJqwNCHDRdlSZF+H0FhO3cR3FeBv+BOjUDM8InEZhm79pFT5w0Xb6Aly0Eyjvb+OqGxBVBzwJMAaA8jLCTrQ3olRbHNBv5TcKxoCH6e10z6cKKMeAr8ZKwsN94r5MABaAd7wJ6yMKwEU7A8SG/+J3Gqo78GBi7tXNkpf4giIUG1CYBOiUl2FHYuLNLSAH1SnQXQnDbYBOlkTZNHmRU+QbkG7bJsi3UF3CQbRS1OFoxwSeRplEuOpN+D1LHO048DbKfNJ3gXPQzANZuGh3CHyNchm4gPKji/axzNgp4BrKfN6jnvU23KwJSMhsds2iPF5rT6jqJy7ae2vEHRftUVR/QNm7wXoCIid6jTIMhYEc0BRzf/9ISpuI7EK54KI1qoyrckZgdsBXfTHCe8Cr3oQ7PdfVEQjNOG3mhC8z454CzolwQ4TZDeavIjwPHK+TVzswHJ+jfIhwNGGbHTL3e+CIN+HnlDF/B2rwJqwjzAFLw5RW0HXgJLAvRw79HUi8S1qavAl/uWifBb0BMjGEfQXkmDfhSr2zLHr3ACveFvknH4YJTd6EZUVe2ZBauQI8UicvI+Qt4A+EXxH9zkV7XyUgVbczuGjCIsqpxJy7wAmE/Ymn1wzK6yjdYqxMA+9C/wiq2ltbSR7Cayi7FZ4rZulPKnLkognXM7uyL9H3BPR2QGSwFEv+eLwJdxFeFOEBhEcReShLXvjrtkoxSFEIe2G4BkzWRWjxGs6iLGLLG42p+5ImOcCaN2G9J2AJaJRMgQMu2peAj0cty2WpfwE4kDAvQZWILgEzrQEd4CNgwUXzS/OM2ncm294N2bpwqS7gvMKbAjsTA/eCtJy0Y3RYuwlVbotwHspL6E1YFWXuX3iSg4LAnDdhtS+gFL2oxSN0eyEsICxWzRZctAfL0rpnK3kVbolw3JvwWVNPAi7aLsohZNN/Tv8GllEuI3zaLsX/C/wDM7pjD59N2pkAAAAASUVORK5CYII=)](https://sast.checkmarx.net/cxwebclient/portal#/projectState/702/Summary)
[![Codacy Badge](https://app.codacy.com/project/badge/Grade/ceddb5b1b37d4edfa56440842c6248a4)](https://www.codacy.com/gh/Checkmarx/kics/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=Checkmarx/kics&amp;utm_campaign=Badge_Grade)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=Checkmarx_kics&metric=alert_status)](https://sonarcloud.io/dashboard?id=Checkmarx_kics)
[![Go Report Card](https://goreportcard.com/badge/github.com/Checkmarx/kics)](https://goreportcard.com/report/github.com/Checkmarx/kics)
[![Go Coverage](https://raw.githubusercontent.com/Checkmarx/kics/gh-pages/coverage.svg)](https://docs.kics.io/coverage.html)

<p align=""center"">
<img alt=""KICS - Keep Infrastructure as Code Secure"" src=""docs/img/logo/kics_new_logo_2022_dark.png#gh-light-mode-only"" width=""500"">
<img alt=""KICS - Keep Infrastructure as Code Secure"" src=""docs/img/logo/kics_new_logo_2022_white.png#gh-dark-mode-only"" width=""500"">
</p>

---

<a href=""https://www.kics.io"" title=""www.kics.io""><img src=""docs/img/button_www-kics-io.png"" align=""right""></a>

Find security vulnerabilities, compliance issues, and infrastructure misconfigurations early in the development cycle of your infrastructure-as-code with **KICS** by Checkmarx.

**KICS** stands for **K**eeping **I**nfrastructure as **C**ode **S**ecure, it is open source and is a must-have for any cloud native project.

### Supported Platforms

<br>
<table align=""center""><tr>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#terraform"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-terraform.png"" alt=""Terraform"" width=""120"" />
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-terraform-dark.png"" alt=""Terraform"" width=""120"" />
<img src=""docs/img/logo-terraform.png"" alt=""Terraform"" width=""120"" />
</picture>
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#kubernetes"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-k8s.png"" width=""160"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-k8s-dark.png"" width=""160"">
<img alt=""Kubernetes"" src=""docs/img/logo-k8s.png"" width=""160"">
</picture>
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#docker"">
<img alt=""Docker"" src=""docs/img/logo-docker.png"" width=""180"">
</a>
</td>
</tr></table>
<table align=""center""><tr>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#cloudformation"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-cf.png"" width=""150"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-cf-dark.png"" width=""150"">
<img alt=""CloudFormation"" src=""docs/img/logo-cf.png"" width=""150"">
</picture>
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#ansible"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-ansible.png"" width=""150"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-ansible-dark.png"" width=""150"">
<img alt=""Ansible"" src=""docs/img/logo-ansible.png"" width=""150"">
</picture>
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#helm"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-helm-alt.png"" width=""60"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-helm-dark.png"" width=""60"">
<img alt=""Helm"" src=""docs/img/logo-helm-alt"" width=""60"">
</picture>
</a>
</td>
</tr></table>
<table align=""center""><tr>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#openapi"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-openapi.png"" width=""185"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-openapi-dark.png"" width=""185"">
<img alt=""OpenAPI"" src=""docs/img/logo-openapi.png"" width=""185"">
</picture>
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#grpc"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-grpc.png"" width=""135"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-grpc-dark.png"" width=""135"">
<img alt=""gRPC"" src=""docs/img/logo-grpc.png"" width=""135"">
</picture>
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#azure-resource-manager"">
<img alt=""Azure Resource Manager"" src=""docs/img/logo-arm.png"" width=""65""
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#google-deployment-manager"">
<img alt=""Google Deployment Manager"" src=""docs/img/logo-gdm.png"" width=""65"">
</a>
</td>
</tr></table>
<table align=""center""><tr>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#cdk"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-cdk.png"" width=""175"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-cdk-dark.png"" width=""175"">
<img alt=""Cloud Development Kit"" src=""docs/img/logo-cdk.png"" width=""175"">
</picture>
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#sam"">
<img alt=""SAM"" src=""docs/img/logo-sam.png"" width=""65"">
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#docker-compose"">
<img alt=""Docker Compose"" src=""docs/img/logo-dockercompose.png"" width=""95"">
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#knative"">
<img alt=""Knative"" src=""docs/img/logo-knative.png"" width=""85"">
</a>
</td>
</tr></table>
<table align=""center""><tr>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#crossplane"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-crossplane.png"" width=""165"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-crossplane-dark.png"" width=""165"">
<img alt=""Crossplane"" src=""docs/img/logo-crossplane.png"" width=""165"">
</picture>
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#pulumi"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-pulumi.png"" width=""145"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-pulumi-dark.png"" width=""145"">
<img alt=""Pulumi"" src=""docs/img/logo-pulumi.png"" width=""145"">
</picture>
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#serverlessfw"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-serverlessfw.png"" width=""170"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-serverlessfw-dark.png"" width=""170"">
<img alt=""ServerlessFW"" src=""docs/img/logo-serverlessfw.png"" width=""170"">
</picture>
</a>
</td>
</tr></table>
<table align=""center""><tr>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#azure-blueprints"">
<img alt=""Azure BluePrints"" src=""docs/img/logo-azure-blueprints.png"" width=""85"">
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#cicd"">
<img alt=""GitHub Workflows"" src=""docs/img/logo-github-icon.png"" width=""85"">
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#terraform"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-opentofu.png"" width=""160"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-opentofu-dark.png"" width=""160"">
<img alt=""OpenTofu"" src=""docs/img/logo-opentofu.png"" width=""160"">
</picture>
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#bicep"">
<img alt=""Bicep"" src=""docs/img/logo-bicep.png"" width=""85"">
</a>
</td>
<td>
<a href=""https://github.com/Checkmarx/kics/blob/master/docs/platforms.md#nifcloud-for-terraform"">
<img alt=""NIFCloud"" src=""docs/img/logo-nifcloud.png"" width=""110"">
</a>
</td>
</tr></table>

### Beta Features
<p align=""center"">
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-databricks.png"" width=""200"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-databricks-dark.png"" width=""200"">
<img alt=""Databricks"" src=""docs/img/logo-databricks.png"" width=""200"">
</picture>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<picture>
<source media=""(prefers-color-scheme: light)"" srcset=""docs/img/logo-tencentcloud.png"" width=""180"">
<source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/logo-tencentcloud-dark.png"" width=""180"">
<img alt=""TencentCloud"" src=""docs/img/logo-tencentcloud.png"" width=""180"">
</picture>
&nbsp;&nbsp;&nbsp;
</p>

By default, Databricks, NIFCloud, and TencentCloud queries run when you scan Terraform files using KICS.

The `Severity` and `Description` of these queries are still under review.

## Getting Started

Setting up and using KICS is super-easy.

-   First, see how to [install and get KICS running](docs/getting-started.md).
-   Then explore KICS [output results format](docs/results.md) and quickly fix the issues detected.

Interested in more advanced stuff?
-   Deep dive into KICS [queries](docs/queries.md).
-   Understand how to [integrate](docs/integrations.md) KICS in your favourite CI/CD pipelines.

See [KICS documentation](https://docs.kics.io/) for more details and topics.

## How it Works

What makes KICS really powerful and popular is its built-in extensibility. This extensibility is achieved by:

-   Fully customizable and adjustable heuristics rules, called [queries](docs/queries.md). These can be easily edited, extended and added.
-   Robust but yet simple [architecture](docs/architecture.md), which allows quick addition of support for new Infrastructure as Code solutions.

## Community

You're welcome to join our [community](docs/community.md), talk with us on <a href=""https://github.com/Checkmarx/kics/discussions"" target=""_blank"">GitHub discussions</a> or contact KICS core team at [kics@checkmarx.com](mailto:kics@checkmarx.com).

### KICS Contributors

See our individual contributors in the [community](docs/community.md) page. You're welcome to join them by [contributing](docs/CONTRIBUTING.md) to KICS.

We also like to thank the following organizations for their ongoing contribution:
-   [Checkmarx](https://checkmarx.com/)
-   [Bedrock Streaming](https://bedrockstreaming.com/) (since v1.4.8)
-   [Dynatrace](https://www.dynatrace.com/) (since v1.5.1)
-   [Orca Security](https://orca.security/) (since v1.5.10)

### KICS Users
KICS is used by various companies and organizations, some are listed below. If you would like to be included here please open a PR.

-   [Checkmarx](https://checkmarx.com/) ([IaC Security](https://checkmarx.com/product/iac-security/))
-   [GitLab](https://gitlab.com/) ([Infrastructure as Code scanning](https://docs.gitlab.com/ee/user/application_security/iac_scanning/))
-   [Bedrock Streaming](https://bedrockstreaming.com/)
-   [Cisco](https://www.panoptica.app/) ([CI/CD Securitry](https://docs.panoptica.app/docs/ci-cd-security))
-   [Orca Security](https://orca.security/)
-   [JIT](https://www.jit.io/) ([SAST for IaC](https://www.jit.io/security-tools/kics))
-   [Firefly](https://www.firefly.ai/) ([Firefly Integrates With Checkmarx's KICS](https://www.firefly.ai/blog/firefly-integrates-with-checkmarxs-kics-to-enable-seamless-cloud-governance-from-code-to-cloud))
-   [Redpanda](https://redpanda.com/)
-   [Keptn](https://github.com/keptn) / [Keptn Lifecycle Toolkit](https://keptn.sh)
 
**Keeping Infrastructure as Code Secure!**

---

&copy; 2025 Checkmarx Ltd. All Rights Reserved.",VRAI
chef/automate,DevOPs,Application System,2025-05-15T07:03:21Z,2025-04-15T10:35:16Z,0,6,0,0,0,0,0,0,2019-04-03T15:26:38Z,2025-04-07T13:00:02Z,527939,236,Go,VRAI,116,FAUX,1195,hacktoberfest,1195,"Chef Automate provides a full suite of enterprise capabilities for maintaining continuous visibility into application, infrastructure, and security automation. ",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,151,"# Automate

[![Build Status](https://badge.buildkite.com/9c37160ef9872fbdcf42628586fabfbb3048239a99c1f60e3a.svg?branch=master)](https://buildkite.com/chef-oss/chef-automate-master-verify)
[![Build Status](https://badge.buildkite.com/b550988ee153942e2ed1ad2741ab652b6d9efa76992d022cf8.svg?branch=master)](https://buildkite.com/chef/chef-automate-master-verify-private)

Automate provides a unified view into infrastructure managed by Chef Infra, InSpec, and Habitat.

Key features include:

- Aggregation and analysis tools for Chef Client and Chef Server data,
- Compliance history and reporting, and
- Compliance scanning of both individual servers and cloud APIs.

For more information see:

- Documentation: https://docs.chef.io/automate/
- Release Notes: https://docs.chef.io/release_notes_automate/

**Project State**: [Active](https://github.com/chef/chef-oss-practices/blob/master/repo-management/repo-states.md#active)

**Issues Response Time Maximum**: 5 business days

**Pull Request Response Time Maximum**: 5 business days

## Components

Chef Automate is a collection of microservices.  Each service is
developed independently.

### Core Applications

* [Automate UI](components/automate-ui)
* [Automate Gateway](components/automate-gateway)
* [Config Management Service](components/config-mgmt-service)
* [Compliance Service](components/compliance-service)
* [Event Service](components/event-service)
* [Ingest Service](components/ingest-service)
* [Nodemanager Service](components/nodemanager-service)
* [Notifications Service](components/notifications-service)

### Authentication, Authorization, & Administration

* [AuthN Service](components/authn-service)
* [AuthZ Service](components/authz-service)
* [Automate-Load-Balancer](components/automate-load-balancer)
* [Dex (OpenID Connect)](components/automate-dex)
* [Local User Service](components/local-user-service)
* [Teams Service](components/teams-service)
* [Session Service](components/session-service)

### Management and Backend Services

* [Backup Gateway](components/backup-gateway)
* [Data Lifecycle Service](components/data-lifecycle-service)
* [Deployment Service](components/automate-deployment)
* [Opensearch Gateway](components/automate-es-gateway)
* [Opensearch Sidecar](components/es-sidecar-service)
* [License Control Service](components/license-control-service)
* [PostgreSQL Gateway](components/automate-pg-gateway)
* [PostgreSQL Sidecar](components/pg-sidecar-service)
* [Secrets Service](components/secrets-service)
* [Trial License Service](components/trial-license-service)

### Optional Additional Components

These components allow you to deploy other Chef projects as part of
Automate

* Chef Server
  * [Bifrost](components/automate-cs-oc-bifrost)
  * [Bookshelf](components/automate-cs-bookshelf)
  * [Chef Server Gateway](components/automate-cs-nginx)
  * [Erchef](components/automate-cs-oc-erchef)

## Getting Started

Please see Quickstart and Development Basics in the [development
document](./dev-docs/DEV_ENVIRONMENT.md).

## Architecture

The following picture illustrates the Automate architecture

![Automate Architecture](dev-docs/diagrams/a2-architecture.png)

## API Compatibility

At this stage in development, the Go libraries and other APIs found in
this repository are not intended for use outside of Chef Automate. If
you think part of this repository would help and would like to depend
on it, please open a GitHub issue so we can discuss it.",VRAI
CHERIoT-Platform/network-stack,Toolkit,Documentations,2025-04-30T14:28:34Z,2024-11-29T18:17:56Z,0,1,0,0,0,0,0,0,2023-12-13T18:14:51Z,2025-04-03T00:44:39Z,415,7,C++,VRAI,8,FAUX,15,,15,,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,5,"CHERIoT Experimental Network Stack
==================================

This repository contains the code for the experimental CHERIoT network stack.
This is an early work-in-progress implementation that has been made public to facilitate easier collaboration.

WARNING: This is not fully hardened against our desired threat model.
It should not be used in production.
Note especially that there is no strong entropy source on the Arty A7 FPGA prototyping platform and so any TLS connection is moderately easy to compromise.
It is expected to approach production quality in 2024H2.

Third-party code reuse
----------------------

The network stack includes components from a variety of third parties:

 - [FreeRTOS-Plus-TCP](https://github.com/FreeRTOS/FreeRTOS-Plus-TCP) provides the TCP/IP implementation.
 - [FreeRTOS coreSNTP](https://github.com/FreeRTOS/coreSNTP) provides the SNTP client.
 - [FreeRTOS coreMQTT](https://github.com/FreeRTOS/coreMQTT) provides the MQTT client.
 - [BearSSL](https://www.bearssl.org) provides the TLS 1.2 stack.

This demonstrates the CHERIoT platform's ability to adopt existing codebases.
We are building around 100 KLoC of third-party code into this stack.
This is mature and well-tested code that we have no desire to rewrite.
A side-channel-resistant TLS implementation, for example, would be a huge undertaking.

Our additions add security boundaries around these existing components and less-general APIs that are tailored for our expected use cases.
The new code in this repository is around 3% of the size of the large components that we are reusing *with no code changes*.

Compartmentalisation
--------------------

The initial implementation has seven compartments.
Four are mostly existing third-party code with thin wrappers:

 - The TCP/IP stack is in a compartment.
   The FreeRTOS code was originally written on the assumption of a single security domain and separating it would require refactoring that would be hard to keep up to date.
 - The SNTP compartment is mostly just another consumer of the network stack, but it provides a real-time clock that is used by the TLS stack.
 - The TLS stack is, again, mostly unmodified BearSSL code, with just some thin wrappers added around the edges.
 - The MQTT compartment, like the SNTP compartment, is just another consumer of the network stack (the TLS layer, specifically) and provides a simple interface for connecting to MQTT servers, publishing messages and receiving notifications of publish events.

These are joined by three new compartments:

 - The firewall compartment is the only thing that talks directly to the network device.
   It filters inbound and outbound frames.
 - The NetAPI compartment provides the control plane.
 - The DNS resolver compartment provides DNS lookup services, interfacing directly with the firewall.

The communication is (roughly) summarised below:

```mermaid
graph TD
  Network
  subgraph Firewall[""On-device firewall""]
    DeviceDriver[""Device Driver""]
  end
  TCPIP[""TCP/IP""]:::ThirdParty
  User[""User Code ""]
  NetAPI[""Network API""]
  DNS[""DNS Resolver""]
  SNTP:::ThirdParty
  TLS:::ThirdParty
  MQTT:::ThirdParty
  DeviceDriver <-- ""Network traffic"" --> Network
  TCPIP <-- ""Send and receive Ethernet frames"" --> Firewall
  DNS <-- ""Send and receive Ethernet frames"" --> Firewall
  NetAPI -- ""Perform DNS lookups"" --> DNS
  NetAPI -- ""Add and remove rules"" --> Firewall
  TLS -- ""Request network connections"" --> NetAPI
  TLS -- ""Send and receive"" --> TCPIP
  NetAPI -- ""Create connections and perform DNS requests"" --> TCPIP
  MQTT -- ""Create TLS connections and exchange data"" --> TLS
  User -- ""Create connections to MQTT server and publish / subscribe"" --> MQTT
  MQTT -- ""Callbacks for acknowledgements and subscription notifications"" --> User
  SNTP -- ""Create UDP socket, authorise endpoints"" --> NetAPI
  SNTP -- ""Send and receive SNTP (UDP) packets"" --> TCPIP
  TLS -- ""Request wall-clock time for certificate checks"" --> SNTP
  style User fill: #5b5
  classDef ThirdParty fill: #e44
```

The TCP/IP stack is a large compartment with a lot of state.
It is fault-tolerant: when an error is triggered (CHERI spatial or temporal safety fault, assertion), the compartment is automatically reset to a pristine state and restarted.
We expand on this capability [below](#automatic-restart-of-the-tcpip-stack).

Unlike the TCP/IP stack, the TLS compartment is almost completely stateless.
This makes resetting the compartment trivial, and gives strong flow isolation properties: Even if an attacker compromises the TLS compartment by sending malicious data over one connection that triggers a bug in BearSSL (unlikely), it is extraordinarily difficult for them to interfere with any other TLS connection.

All inbound and outbound data go through the on-device firewall, which is controlled by the Network API compartment.
The TCP/IP stack has no access to the NetAPI control-plane interface.
Thus, a compromise that gets arbitrary-code execution in the network stack cannot open new firewall holes (to join a DDoS botnet such as [Mirai](https://en.wikipedia.org/wiki/Mirai_(malware)), for example).
The worst it can do to the rest of the system is provide malicious data, but a system using TLS will have HMACs on received messages and so this is no worse than a malicious packet being injected from the network.

The DNS resolver comes as a separate compartment to support this design: since the Network API compartment operates with domain names, and the firewall with IPs, the translation between the two must be done by a trusted entity.
For instance, if the application tells the Network API that the only endpoint it will ever communicate with is `example.com`, the Network API will need to translate that domain name into an IP to create a firewall entry.
If this translation was done by a potentially compromised TCP/IP stack, it could spoof the DNS translation and return whichever IP address it wants to connect to, to create a corresponding malicious firewall entry.
The isolated DNS resolver is trusted to perform this task independently of the TCP/IP stack.

This compartmentalised design comes on top of the spatial and temporal safety properties that the CHERIoT platform provides at a base level.

Capabilities authorise communication
------------------------------------

CHERI systems use capabilities to authorise memory accesses.
CHERIoT provides abstractions for software-defined capabilities that can authorise different operations.
These are represented in the hardware via sealed (CHERI) capabilities that refer to specific kinds of objects.
These sealed capabilities can be treated as opaque tokens (and cannot be directly used) by most code but can be unsealed by the compartment that owns the corresponding unsealing capability.

The network stack uses three kinds of sealed capabilities:

 - Connection capabilities are baked into the firmware image and authorise establishing a connection.
 - Socket capabilities are dynamically created and provide a tamper-proof opaque handle that authorises sending and receiving data over a socket.
 - TLS session capabilities are also dynamically created (and wrap socket capabilities) and authorise sending and receiving encrypted data over a TLS session.

The flow for establishing a network connection is as follows:

 1. User code presents a connection capability to the Network API compartment authorising a connection to a remote host.
 2. The Network API compartment opens inspects the capability and extracts the name of the host.
 3. The Network API opens the firewall hole for the DNS resolver.
 4. The Network API compartment instructs the TCP/IP compartment to look up the name.
 5. The TCP/IP compartment sends and receives UDP packets (forwarded via the Firewall compartment) to look up the name.
 6. The Network API compartment instructs the firewall to close the hole for the DNS lookup.
 7. The Network API compartment instructs the TCP/IP stack to create a (sealed) socket.
 8. The Network API compartment instructs the firewall to open a hole for the local port and the remote endpoint.
 9. The Network API compartment returns the sealed capability for the socket to the caller.

At the end of this, the original caller can directly call the send and receive functions in the TCP/IP stack to send and receive data.

Safe sharing
------------

Each compartment needs to share some data with the others.
For UDP, the receive path can be entirely zero copy (after the packet leaves the driver).
A UDP packet arrives and is copied from the network interface into a new allocation.
This is processed by the TCP/IP stack and then claimed with the allocator capability passed the receive-message call, freed with the network stack's allocator capability, and returned.
This ensures that the packet is freed once the caller frees it, transferring ownership out to the caller.
Callers worried about time-of-check-to-time-of-use attacks from a compromised TCP/IP compartment may need to defensively copy.

BearSSL maintains its own send and receive buffers.
The TCP/IP stack can copy directly to and from these, as long as we can make this secure.
These are passed from the TLS compartment to the TCP/IP compartment as *bounded* capabilities with only load or store permissions.
This means that the TCP/IP stack cannot access out of bounds and cannot capture the pointer.
In the case of a store (read from the network), the TCP/IP compartment also cannot read stale data from the buffer (for example, it cannot read previously decrypted data).

Auditing
--------

The network stack relies on some interfaces being restricted to certain compartments.
For example, there are some APIs in the TCP/IP and Firewall compartments that should be exposed only to the Network API compartment.
These can be checked by the [cheriot-audit](https://github.com/CHERIoT-Platform/cheriot-audit) tool, with the aid of the policy in this repository.

The `network_stack.rego` file also makes it easy to extract connection capabilities.
For example, if you run the following Rego query against the [HTTPS example](examples/03.HTTPS) (after loading `network_stack.rego` with `-m`):

```rego
data.network_stack.all_connection_capabilities
```

You should see the following output (piped through `jq` for pretty printing):

```json
[
  {
    ""capability"": {
      ""connection_type"": ""UDP"",
      ""host"": ""pool.ntp.org"",
      ""port"": 123
    },
    ""owner"": ""SNTP""
  },
  {
    ""capability"": {
      ""connection_type"": ""TCP"",
      ""host"": ""example.com"",
      ""port"": 443
    },
    ""owner"": ""https_example""
  }
]
```

This tells you that the SNTP compartment has a capability that allows it to create a UDP socket and communicate with pool.ntp.org and that the `https_example` compartment can make TCP connections to example.com:443.
No other compartments can make connections and no compartment may communicate with hosts not on this list.
This can feed into more auditing infrastructure.

Do you want to check that all TLS connections are encrypted?
Try asking which compartments are calling the TCP connection function:

```rego
data.compartment.compartments_calling_export_matching(""NetAPI"", `network_socket_connect_tcp(.*)`)
```

Hopefully the output is very short:

```json
[""TLS""]
```

This means that all TCP connections are made via the TLS compartment and, unless the TLS compartment is compromised, no traffic can flow over TCP that is not encrypted.
Unfortunately, SNTP is unencrypted.
It can have verified signatures (which you absolutely should use in a real deployment: the current prototype just talks to pool.ntp.org without authentication) though.
This should be the only thing used with UDP:

```rego
data.compartment.compartments_calling_export_matching(""NetAPI"", `network_socket_udp(.*)`)
```

```json
[""SNTP""]
```

Now that you know that the SNTP compartment is the only one that can send and receive UDP packets, it's worth checking that it really is talking to the host that you expect:

```rego
[ data.network_stack.decode_connection_capability(c) | c = input.compartments.SNTP.imports[_] ; data.network_stack.is_connection_capability(c) ]i
```

```json
[{""connection_type"":""UDP"", ""host"":""pool.ntp.org"", ""port"":123}]
```

If you've modified the SNTP compartment to point to your NTP service and use its authentication credentials, then this should be different.
This can all be part of your firmware's auditing policy.

Automatic restart of the TCP/IP stack
-------------------------------------

We designed the TCP/IP stack to automatically and transparently restart on failure (e.g., a CHERI fault or an assertion).
The restart procedure broadly works like this (simplified for didactic reasons):

1. The error handler of the TCP/IP compartment is triggered and starts the reset procedure.
2. It first sets a flag to prevent any new thread from entering the compartment.
3. Then, it sets all synchronization primitives of the compartment (locks, futexes) for destruction. This wakes up sleeping threads present in the compartment, and prevents them from blocking again.
4. Then, it waits for all threads present in the compartment (apart from the FreeRTOS network thread) to exit, either through normal control-flow, or by crashing.
5. Finally, it frees all the memory of the compartment, resets all global state, and calls the start function of the network stack, which restarts the TCP/IP stack into a pristine working state.
6. After the reset, any further call to the socket API (apart from `network_socket_close`) with an old socket from the previous instance of the network stack will be detected and failed with an `-ENOTCONN` code. This pushes callers to close the sockets and create new ones with the new instance of the TCP/IP stack.

The implementation details of the reset slightly deviate from this description.
See the technical documentation in `tcpip_error_handler.h` for a full perspective.

Note that the current implementation of the automatic reset makes a few assumptions:

- The TCP/IP stack cannot currently recover from a crash due to a stack overflow in the TCP/IP compartment. This is due to a limitation of the implementation of the switcher, which cannot trigger the error handler on stack overflow. This limitation should be addressed soon.
- A small set of globals called 'reset-critical' outlive resets and/or are necessary for the reset. We assume that this data has not been corrupted. This data is correspondingly annotated in the source code.
- The control-flow of threads in the compartment has not been altered.

These assumptions leave some attack surface to malicious actors.
We are working on improvements to remove or weaken them.",VRAI
choria-io/aaasvc,Toolkit,DevOPs,2025-04-22T09:59:17Z,2022-12-01T16:31:15Z,0,6,0,0,0,0,0,0,2019-01-29T15:14:21Z,2024-09-12T07:33:22Z,4210,6,Go,VRAI,8,FAUX,6,,6,Choria Centralized AAA Service,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,6,"![Choria AAA Service](https://choria-io.github.io/aaasvc/logo.png)

## Overview

Choria is traditionally a loosely coupled system with very few central components.  When a user makes a RPC request the request has a public certificate attached and every single node they interact with does RBAC.

That default deployment method has no dependencies per request and scales very well but it can be difficult to manage, rotate and audit who has access credentials.  This package provides a system that issues short-lived JWT tokens and authorize and audit each request centrally prior to communicating with any fleet nodes.

The main motivation is to avoid the problems caused by having to do Certificate Management and Fleet wide static Action Policies for every user, instead you have a central login and central authority who does AAA for every request.  This is more appropriate to the typical Enterprise environment.

* [Documentation](https://choria-io.github.io/aaasvc/)
* [Community](https://github.com/choria-io/general/discussions)

With this deployed the workflow becomes:

```
$ choria ping
FATA[0000] Could not run Choria: could not perform request: error from remote signer: Request denied

$ mco login
Username (rip):
Password:
Token saved to /home/user/.choria/client.jwt

$ choria ping
...
---- ping statistics ----
19 replies max: 161.60 min: 131.23 avg: 151.21
```

The token is valid for a configurable period after which time another `choria login` will be required. Users are able to perform only the actions that they are entitled. Users have no SSL certificates of their own - a system-wide certificate might be needed to connect to middleware if configured to require TLS.

[![Go Report Card](https://goreportcard.com/badge/github.com/choria-io/aaasvc)](https://goreportcard.com/report/github.com/choria-io/aaasvc)
[![CodeQL](https://github.com/choria-io/aaasvc/workflows/CodeQL/badge.svg)](https://github.com/choria-io/aaasvc/actions/workflows/codeql.yaml)
[![Unit Tests](https://github.com/choria-io/aaasvc/actions/workflows/test.yaml/badge.svg)](https://github.com/choria-io/aaasvc/actions/workflows/test.yaml)",VRAI
choria-io/go-choria,Toolkit,Application System,2025-05-07T10:42:59Z,2025-03-17T11:55:26Z,0,15,0,0,0,0,0,0,2017-06-02T14:58:43Z,2025-04-07T12:43:49Z,22245,93,Go,VRAI,32,FAUX,39,"backplane,framework,orchestration",39,"Backplane Development Framework and Server hosting Choria Agents, Networks, Federations and Streaming Data",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,20,"# Choria Broker and Server

Choria is a framework for building Control Planes, Orchestration Systems and Programmable Infrastructure.

This is a daemon and related tools written in Go that hosts services, autonomous agents and generally provide a secure hosting environment for callable logic that you can interact with from code.

Additionally, this is the foundational technology for a monitoring pipeline called Choria Scout.

More information about the project can be found on [Choria.IO](https://choria.io).

[![CodeFactor](https://www.codefactor.io/repository/github/choria-io/go-choria/badge)](https://www.codefactor.io/repository/github/choria-io/go-choria) 
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3558/badge)](https://bestpractices.coreinfrastructure.org/projects/3558)
[![Go Report Card](https://goreportcard.com/badge/github.com/choria-io/go-choria)](https://goreportcard.com/report/github.com/choria-io/go-choria)

# Links

 * [Choria Project Home](https://choria.io)
 * [Server and Broker Documentation](https://choria-io.github.io/go-choria/)",VRAI
christophwille/dotnet-opa-wasm,Application System,Documentations,2024-12-24T09:27:46Z,2022-12-09T12:02:55Z,0,7,0,0,0,0,0,0,2019-11-19T07:59:53Z,2025-02-03T00:03:22Z,3123,46,C#,VRAI,11,FAUX,10,"csharp,dotnet-core,opa,open-policy-agent,wasm,wasmtime,web-assembly,webassembly",10,Call Open Policy Agent (OPA) policies in WASM (Web Assembly) from .NET Core,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,2,"# Open Policy Agent for .NET

[NuGet package](https://www.nuget.org/packages/Opa.Wasm/)

Call Open Policy Agent (OPA) policies in WASM (Web Assembly) from C# .NET Core

A working sample is in src\Opa.Wasm.ConsoleSample. It mirrors the node sample, but not the node library (currently builtins not hooked up properly).

## References

(KubeCon NA is just starting, so this is an old video) You want to watch [Deep Dive: Open Policy Agent - Torin Sandall, Styra](https://www.youtube.com/watch?v=Vdy26oA3py8) first.

Docs are at https://github.com/open-policy-agent/opa/blob/master/docs/content/wasm.md

Writing policies https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/

Example and Integrations https://github.com/open-policy-agent/contrib

## Other Open Policy Agent WebAssemby SDKs

* https://github.com/open-policy-agent/npm-opa-wasm/
* https://github.com/open-policy-agent/golang-opa-wasm

## Wasmtime Infos

GitHub repo https://github.com/bytecodealliance/wasmtime-dotnet

Docs https://bytecodealliance.github.io/wasmtime-dotnet/articles/intro.html",VRAI
CircleCI-Public/circleci-cli,Toolkit,Application System,2025-05-08T14:32:29Z,2024-11-13T18:37:37Z,0,6,0,0,0,0,0,0,2018-06-19T14:30:01Z,2025-03-31T11:44:21Z,21523,420,Go,VRAI,237,FAUX,135,"circle-ci,circleci,cli,command-line-tool,continuous-delivery,continuous-integration,developer-tools,devops,golang,graphql,tool",135,Use CircleCI from the command line,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,126,"# circleci-cli

This is CircleCI's command-line application.

[Documentation](https://circleci-public.github.io/circleci-cli) |
[Code of Conduct](./CODE_OF_CONDUCT.md) |
[Contribution Guidelines](./CONTRIBUTING.md) |
[Hacking](./HACKING.md)

[![CircleCI](https://circleci.com/gh/CircleCI-Public/circleci-cli.svg?style=shield)](https://circleci.com/gh/CircleCI-Public/circleci-cli)
[![GitHub release](https://img.shields.io/github/tag/CircleCI-Public/circleci-cli.svg?label=latest)](https://github.com/CircleCI-Public/circleci-cli/releases)
[![GoDoc](https://img.shields.io/badge/godoc-reference-blue.svg)](https://godoc.org/github.com/CircleCI-Public/circleci-cli)
[![License](https://img.shields.io/badge/license-MIT-red.svg)](./LICENSE)

## Getting Started

### Installation

CircleCI CLI is available on the following package managers:

#### Homebrew

```
brew install circleci
```

#### Snap

```
sudo snap install circleci
```

#### Chocolatey

```
choco install circleci-cli -y
```

### Install script

You can also install the CLI binary by running our install script on most Unix platforms:

```
curl -fLSs https://raw.githubusercontent.com/CircleCI-Public/circleci-cli/main/install.sh | bash
```

By default, the `circleci` app will be installed to the ``/usr/local/bin`` directory. If you do not have write permissions to `/usr/local/bin`, you may need to run the above command with `sudo`:

```
curl -fLSs https://raw.githubusercontent.com/CircleCI-Public/circleci-cli/main/install.sh | sudo bash
```

Alternatively, you can install to an alternate location by defining the `DESTDIR` environment variable when invoking `bash`:

```
curl -fLSs https://raw.githubusercontent.com/CircleCI-Public/circleci-cli/main/install.sh | DESTDIR=/opt/bin bash
```

You can also set a specific version of the CLI to install with the `VERSION` environment variable:

```
curl -fLSs https://raw.githubusercontent.com/CircleCI-Public/circleci-cli/main/install.sh | sudo VERSION=0.1.5222 bash
```

Take note that additional environment variables should be passed between sudo and invoking bash.

#### Checksum verification

If you would like to verify the checksum yourself, you can download the checksum file from the [GitHub releases page](https://github.com/CircleCI-Public/circleci-cli/releases) and verify the checksum of the archive using the `circleci-cli_<version>_checksums.txt` inside the assets of the release you'd like to install:

On macOS and Linux:
```sh
shasum -a 256 circleci-cli_<version>_<os>.tar.gz
```

and on Windows:
```powershell
Get-FileHash .\circleci-cli_<version>_<os>.tar.gz -Algorithm SHA256 | Format-List
```

And compare it to the right checksum depending on the downloaded version in the `circleci-cli_<version>_checksums.txt` file.

### Updating

If you installed the CLI without a package manager, you can use its built-in update command to check for pending updates and download them:

```
circleci update check
circleci update install
```

## Configure the CLI

After installing the CLI, you must run setup to configure the tool.

```
$ circleci setup
```

You should be prompted to enter the _CircleCI API Token_ you generated from the [Personal API Token tab](https://circleci.com/account/api)


```
✔ CircleCI API Token:

API token has been set.

✔ CircleCI Host: https://circleci.com

CircleCI host has been set.

Setup complete. Your configuration has been saved.
```

If you are using this tool on `circleci.com`, accept the provided default `CircleCI Host`.

Server users will have to change the default value to your custom address (e.g., `circleci.my-org.com`).

**Note**: Server does not yet support config processing and orbs, you will only be able to use `circleci local execute` (previously `circleci build`) for now.


## Validate A Build Config

To ensure that the tool is installed, you can use it to validate a build config file.

```
$ circleci config validate

Config file at .circleci/config.yml is valid
```


## Docker

The CLI may also be used without installation by using Docker.

```
docker run --rm -v $(pwd):/data -w /data circleci/circleci-cli:alpine config validate /data/.circleci/config.yml --token $TOKEN
```

## circleci-agent

In order to maintain backwards compatibility with the `circleci` binary present in builds, some commands are proxied to a program called `circleci-agent`.

This program must exist in your `$PATH` as is the case inside of a job.

The following commands are affected:

* `circleci tests split`
* `circleci step halt`
* `circleci config migrate`

## Platforms, Deployment and Package Managers

The tool is deployed through a number of channels. The primary release channel is through [GitHub Releases](https://github.com/CircleCI-Public/circleci-cli/releases). Green builds on the `main` branch will publish a new GitHub release. These releases contain binaries for macOS, Linux and Windows. These releases are published from (CircleCI)[https://app.circleci.com/pipelines/github/CircleCI-Public/circleci-cli] using [GoReleaser](https://goreleaser.com/).

### Homebrew

We publish the tool to [Homebrew](https://brew.sh/). The tool is [part of `homebrew-core`](https://github.com/Homebrew/homebrew-core/blob/main/Formula/circleci.rb), and therefore the maintainers of the tool are obligated to follow the guidelines for acceptable Homebrew formulae. You should [familiarize yourself with the guidelines](https://docs.brew.sh/Acceptable-Formulae#we-dont-like-tools-that-upgrade-themselves) before making changes to the Homebrew deployment system.

The particular considerations that we make are:

1. Since Homebrew [doesn't ""like tools that upgrade themselves""](https://docs.brew.sh/Acceptable-Formulae#we-dont-like-tools-that-upgrade-themselves), we disable the `circleci update` command when the tool is released through homebrew. We do this by [defining the PackageManager](https://github.com/Homebrew/homebrew-core/blob/eb1fdb84e2924289bcc8c85ee45081bf83dc024d/Formula/circleci.rb#L28) constant to `homebrew`, which allows us to [disable the `update` command at runtime](https://github.com/CircleCI-Public/circleci-cli/blob/67c7d52bace63846f87a1ed79f67f257c94a55b4/cmd/root.go#L119-L123).

#### Releasing to Homebrew

This project is on Homebrew's special [autobump list](https://github.com/Homebrew/homebrew-core/blob/master/.github/autobump.txt) which effectively means that it will check our `main` branch every 3 hours for updates and create a PR automagically if there are any changes. This is great, but you do have to monitor the generated PRs to ensure they pass and do get merged in successfully. The PRs will be raised in this repo: [github.com/Homebrew/homebrew-core](https://github.com/Homebrew/homebrew-core) and you can search the Pull requests for `circleci` to see the generated PRs. 

Upon successful merge, you'll be able to upgrade the tool by running `brew upgrade circleci` and then you can validate any changes you may have made.

### Snap

We publish Linux builds of the tool to the Snap package manager.

Further [package information is available on Snap website](https://snapcraft.io/circleci).

## Contributing

Development instructions for the CircleCI CLI can be found in [HACKING.md](HACKING.md).

## More

Please see the [documentation](https://circleci-public.github.io/circleci-cli) or `circleci help` for more.

## Server compatibility

| Functionality | Impacted commands | Change description | Compatibility with Server |
| --- | --- | --- | --- |
| Config compilation and validation | <ul><li>`circleci config validate`</li><li>`circleci config process`</li><li>`circleci local execute`</li> | The config validation has been moved from the GraphQL API to a specific API endpoint | <ul><li>**Server v4.0.5, v4.1.3, v4.2.0 and above**: Commands use the new specific endpoint</li><li>**Previous version**: Commands use the GraphQL API</li></ul> |
| Orb compilation and validation of orb using private orbs | <ul><li>`circleci orb process`</li><li>`circleci orb validate`</li></ul> | To support the validation of orbs requesting private orbs (see [issue](https://github.com/CircleCI-Public/circleci-cli/issues/751)). A field `ownerId` has been added to the GraphQL orb validation endpoint. Thus allowing the `Impacted commands` to use the `--org-id` parameter to enable the orb compilation / validation  | <ul><li>**Server v4.2.0 and above**: The field is accessible so you can use the parameter</li><li>**Previous versions**: The field does not exist making the functionality unavailable</li></ul> |

## Telemetry

The CircleCI CLI includes a telemetry feature that collects basic errors and feature usage data in order to help us improve the experience for everyone.

Telemetry works on an opt-in basis: when running a command for the first time, you will be asked for consent to enable telemetry. For non-TTY STDIN, telemetry is disabled by default, ensuring that scripts that use the CLI run smoothly.

You can disable or enable telemetry anytime in one of the following ways:

* Run the commands `circleci telemetry enable` or `circleci telemetry disable`

* Set the `CIRCLECI_CLI_TELEMETRY_OPTOUT` environment variable to `1` or `true` to disable it",VRAI
cisagov/ScubaGear,Toolkit,Application System,2025-05-16T01:48:07Z,2025-03-12T15:56:52Z,0,64,0,0,0,0,0,0,2022-07-21T16:46:41Z,2025-04-06T16:23:04Z,32112,2042,PowerShell,VRAI,274,FAUX,216,"assessment-tool,cisa,contributions-welcome,cybersecurity,m365,open-policy-agent,open-source,powershell,rego,scuba,scubaconnect,security,security-automation",216,Automation to assess the state of your M365 tenant against CISA's baselines,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,28,"![ScubaGear Logo](docs/images/SCuBA%20GitHub%20Graphic%20v6-05.png)


[![GitHub Release][github-release-img]][release]
[![PSGallery Release][psgallery-release-img]][psgallery]
[![CI Pipeline][ci-pipeline-img]][ci-pipeline]
[![Functional Tests][functional-test-img]][functional-test]
[![GitHub License][github-license-img]][license]
[![GitHub Downloads][github-downloads-img]][release]
[![PSGallery Downloads][psgallery-downloads-img]][psgallery]
[![GitHub Issues][github-issues-img]][github-issues]

ScubaGear is an assessment tool that verifies that a Microsoft 365 (M365) tenant’s configuration conforms to the policies described in the Secure Cloud Business Applications ([SCuBA](https://cisa.gov/scuba)) Secure Configuration Baseline [documents](/baselines/README.md).

> **Note**: This documentation can be read using [GitHub Pages](https://cisagov.github.io/ScubaGear).

## Target Audience

ScubaGear is for M365 administrators who want to assess their tenant environments against CISA Secure Configuration Baselines.

## Overview

ScubaGear uses a three-step process:

- **Step One** - PowerShell code queries M365 APIs for various configuration settings.
- **Step Two** - It then calls [Open Policy Agent](https://www.openpolicyagent.org) (OPA) to compare these settings against Rego security policies written per the baseline documents.
- **Step Three** - Finally, it reports the results of the comparison as HTML, JSON, and CSV.

<img src=""docs/images/scuba-process.png"" />

## Getting Started

To install ScubaGear from [PSGallery](https://www.powershellgallery.com/packages/ScubaGear), open a PowerShell 5 terminal on a Windows computer and install the module:

```powershell
# Install ScubaGear
Install-Module -Name ScubaGear
```

To install its dependencies:

```powershell
# Install the minimum required dependencies
Initialize-SCuBA 
```

To verify that it is installed:

```powershell
# Check the version
Invoke-SCuBA -Version
```

To run ScubaGear:

```powershell
# Assess all products
Invoke-SCuBA -ProductNames *
```

> **Note**:  Successfully running ScubaGear requires certain prerequisites and configuration settings.  To learn more, read through the sections below.

## Table of Contents

The following sections should be read in order.

### Installation

- [Install from PSGallery](docs/installation/psgallery.md)
- [Download from GitHub](docs/installation/github.md)
- [Uninstall](docs/installation/uninstall.md)

### Prerequisites

- [Dependencies](docs/prerequisites/dependencies.md)
- [Required Permissions](docs/prerequisites/permissions.md)
  - [Interactive Permissions](docs/prerequisites/interactive.md)
  - [Non-Interactive Permissions](docs/prerequisites/noninteractive.md)

### Execution

- [Execution](docs/execution/execution.md)
- [Reports](docs/execution/reports.md)

### Configuration

- [Parameters](docs/configuration/parameters.md)
- [Configuration File](docs/configuration/configuration.md)

### Troubleshooting

- [Multiple Tenants](docs/troubleshooting/tenants.md)
- [Defender](docs/troubleshooting/defender.md)
- [Exchange Online](docs/troubleshooting/exchange.md)
- [Power Platform](docs/troubleshooting/power.md)
- [Microsoft Graph](docs/troubleshooting/graph.md)
- [Proxy](docs/troubleshooting/proxy.md)

### Misc

- [Assumptions](docs/misc/assumptions.md)

## Project License

Unless otherwise noted, this project is distributed under the Creative Commons Zero license. With developer approval, contributions may be submitted with an alternate compatible license. If accepted, those contributions will be listed herein with the appropriate license.

[release]: https://github.com/cisagov/ScubaGear/releases
[license]: https://github.com/cisagov/ScubaGear/blob/main/LICENSE
[psgallery]: https://www.powershellgallery.com/packages/ScubaGear
[github-cicd-workflow]: https://github.com/cisagov/ScubaGear/actions/workflows/run_pipeline.yaml
[github-issues]: https://github.com/cisagov/ScubaGear/issues
[github-license-img]: https://img.shields.io/github/license/cisagov/ScubaGear
[github-release-img]: https://img.shields.io/github/v/release/cisagov/ScubaGear?label=GitHub&logo=github
[psgallery-release-img]: https://img.shields.io/powershellgallery/v/ScubaGear?logo=powershell&label=PSGallery
[ci-pipeline]: https://github.com/cisagov/ScubaGear/actions/workflows/run_pipeline.yaml
[ci-pipeline-img]: https://github.com/cisagov/ScubaGear/actions/workflows/run_pipeline.yaml/badge.svg
[functional-test]: https://github.com/cisagov/ScubaGear/actions/workflows/test_production_function.yaml
[functional-test-img]: https://github.com/cisagov/ScubaGear/actions/workflows/test_production_function.yaml/badge.svg
[github-cicd-workflow-img]: https://img.shields.io/github/actions/workflow/status/cisagov/ScubaGear/run_pipeline.yaml?logo=github
[github-downloads-img]: https://img.shields.io/github/downloads/cisagov/ScubaGear/total?logo=github
[psgallery-downloads-img]: https://img.shields.io/powershellgallery/dt/ScubaGear?logo=powershell
[github-issues-img]: https://img.shields.io/github/issues/cisagov/ScubaGear",VRAI
cisagov/ScubaGoggles,Toolkit,Application System,2025-05-15T19:09:39Z,2025-03-19T18:39:56Z,0,80,0,0,0,0,0,0,2023-07-24T14:07:20Z,2025-04-05T06:59:50Z,5276,215,Open Policy Agent,VRAI,35,FAUX,61,"cisa,cybersecurity,google,google-workspace,gws,opa,open-policy-agent,open-source,python,scuba,scubaconnect,security,security-automation",61,SCuBA Secure Configuration Baselines and assessment tool for Google Workspace ,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,16,"![CISA Logo](docs/images/cisa.png)
<div align='center' style=""margin:0;"" id=""user-content-toc"">
  <ul>
    <h1 style=""display: inline-block;"">ScubaGoggles</h1>
  </ul>
  <ul>
        <a href=""https://github.com/cisagov/ScubaGoggles/releases"">
        <img src=""https://img.shields.io/badge/ScubaGoggles-v1.0.0-%2385B065?labelColor=%23005288""  alt=""ScubaGoggles version #""></a>
        <a href=""https://github.com/cisagov/ScubaGoggles/tree/main/baselines"">
        <img src=""https://img.shields.io/badge/GWS_SCB-v1-%2385B065?labelColor=%23005288"" alt=""GWS SCB version #""></a>
        <a href="""">
        <img src=""https://img.shields.io/github/downloads/cisagov/ScubaGoggles/total.svg""  alt=""Downloads""></a>
  </ul>
</div>
<h2 align='center' style=""margin:0;"">GWS Secure Configuration Baseline Assessment Tool </h2>

Developed by CISA, ScubaGoggles is an assessment tool that verifies a Google
Workspace (GWS) organization's configuration conforms to the policies
described in the Secure Cloud Business Applications
([SCuBA](https://cisa.gov/scuba)) Secure Configuration
Baseline [documents](scubagoggles/baselines/README.md).

For the Microsoft 365 (M365) rendition of this tool, see [ScubaGear](https://github.com/cisagov/ScubaGear).

> [!WARNING]
> This tool is in an alpha state and in active development. At this time, outputs could be incorrect and should be reviewed carefully.

## Overview
We use a three-step process:
1. **Export**. In this step, we primarily use the Google Admin SDK API to export and serialize all the relevant logs and settings into json. ScubaGoggles also uses various other Google APIs to grab organization metadata, user privileges etc.
2. **Verify**. Compare the exported settings from the previous step with the configuration prescribed in the baselines. We do this using [OPA Rego](https://www.openpolicyagent.org/docs/latest/policy-language/#what-is-rego), a declarative query language for defining policy.
3. **Report**. Package the results as HTML and JSON.

## Table of Contents

### Installation

- [Download and Python Install](docs/installation/DownloadAndInstall.md)
- [Download the OPA Executable](docs/installation/OPA.md)
- [Configure Defaults](docs/installation/Defaults.md)

### Prerequisites

- [Permissions](docs/prerequisites/Prerequisites.md#permissions)
- [Create a Project](docs/prerequisites/Prerequisites.md#create-a-project)

### Authentication
- [Authentication Methods](docs/authentication/AuthenticationMethods.md)
- [Using OAuth](docs/authentication/OAuth.md)
- [Using a Service Account](docs/authentication/ServiceAccount.md)

### Usage

- [Usage: Parameters](docs/usage/Parameters.md)
- [Usage: Config File](docs/usage/Config.md)
- [Usage: Examples](docs/usage/Examples.md)
- [Reviewing Output](docs/usage/ReviewOutput.md)
- [Limitations](docs/usage/Limitations.md)

### Troubleshooting
- [Lots of Manual Checks](docs/troubleshooting/Troubleshooting.md#lots-of-manual-checks)
- [Not Authorized to Access This Resource](docs/troubleshooting/Troubleshooting.md#not-authorized-to-access-this-resource)
- [scubagoggles Not Found](docs/troubleshooting/Troubleshooting.md#scubagoggles-not-found)
- [Unable to view HTML report due to environment limitations](docs/troubleshooting/Troubleshooting.md#unable-to-view-html-report-due-to-environment-limitations)

## Project License
Unless otherwise noted, this project is distributed under the Creative
Commons Zero license. With developer approval, contributions may be
submitted with an alternate compatible license. If accepted, those
contributions will be listed herein with the appropriate license.",VRAI
CiscoDevNet/sdwan-devops,Documentations,Documentations,2024-12-02T11:31:26Z,2023-12-20T19:07:04Z,0,1,0,0,0,0,0,0,2019-07-15T18:59:53Z,2025-04-04T03:03:29Z,8271,87,Jinja,VRAI,49,FAUX,5,,5,SD-WAN DevOps Tools,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,15,"# SDWAN DevOps

This repo contains a set of tools to automate workflows and build CI/CD pipelines for Cisco SDWAN.

> Note: The tools in this repo only work from a Unix environment with Docker (e.g. Linux, MacOS, etc.) due to issues with Ansible and file permissions mapping between Windows and the Linux container used in play.sh. WSL2 may fix this issue and we will revisit when WSL2 is released.

All operations are run out of the sdwan-devops directory: `cd sdwan-devops`

The folder `sdwan-edge` allows the deployment of C8000v in AWS, Azure, GCP. Openstack and VMware.

The folder `sdwan-terraform` allows the deployment of SDWAN Controllers in AWS, Azure and VMware.

A video demonstration of the use of this repository is available on [Vidcast](https://app.vidcast.io/share/1e934c26-ece4-4167-a986-4db17f125423).

## Clone repository

Clone the sdwan-devops repo using the main branch (default: origin/main):

```shell
git clone --single-branch --recursive https://github.com/ciscodevnet/sdwan-devops.git
```

Make sure you use `--recursive` to also clone folders sdwan-edge and terraform-sdwan.

## Openssl version3

If you are on a Mac: we need openssl version3, while on mac this is LibreSSL.

Upgrade openssl:

```shell
brew install openssl@3
```

## Software Dependencies

All software dependencies have been rolled into a Docker container. Ansible playbooks are launched via the container using the play.sh shell script.

All you need is a valid installation of docker on your system.

> Note: The Dockerfile included in this repo is used to automatically build the sdwan-devops container image and publish it to the GitHub Container Registry. For a detailed list of the dependencies required to run the playbooks, refer to the Dockerfile.

## Running playbooks via the Docker container

To run playbooks in this repo, use the play.sh shell script as shown below:

- `./play.sh <playbook> <options>`

This will start the docker container published in the GitHub Container Registry, run the playbooks inside the container and remove it once finished.

## Deploying Controllers on AWS

The sdwan-devops can be used to instantiates controllers on AWS.

[Deploying Controllers on AWS](docs/deploying_controllers_cloud.md)

- Deploy vBond, vSmart and vManage controllers in a VPC
- Provides bootstrap configuration

## Deploying C8000v

C8000v can be deployed in a transit VPC/VNET in AWS, Azure and GCP, and can also be deployed on VMware and Openstack.

[Deploying C8000v](docs/deploying_edges_cloud.md)

- Generates bootstrap configuration (cloud-init format)
- Creates transit VPC if required
- Deploy C8000v

## Simulation

Simulation can be used for developing new deployments as well as testing changes to current deployments.  Simulation capabilities are provided by CML^2 or VMware.  The [Ansible CML^2 Modules](https://github.com/ciscodevnet/ansible-virl) are used to automate deployments in CML^2.  The [Terraform Modules](https://github.com/CiscoDevNet/terraform-sdwan) are used to automate deployments in VMware.

[Simulation](docs/simulation.md)",FAUX
cloud-custodian/cloud-custodian,Toolkit,DevOPs,2025-05-14T12:08:09Z,2025-04-15T18:34:06Z,0,0,0,0,0,93,5,0,2016-03-01T01:11:20Z,2025-04-07T21:20:50Z,135465,5620,Python,VRAI,1534,FAUX,1476,"aws,azure,cloud,cloud-computing,compliance,gcp,lambda,management,rules-engine,serverless",1476,"Rules engine for cloud security, cost optimization, and governance, DSL in yaml for policies to query, filter, and take actions on resources",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,504,"Cloud Custodian (c7n)
=================

<p align=""center""><img src=""https://cloudcustodian.io/img/logo_capone_devex_cloud_custodian.svg"" alt=""Cloud Custodian Logo"" width=""200px"" height=""200px"" /></p>

---

[![slack](https://img.shields.io/badge/slack-chat-yellow)](https://communityinviter.com/apps/cloud-custodian/c7n-chat)
[![CI](https://github.com/cloud-custodian/cloud-custodian/workflows/CI/badge.svg?event=push)](https://github.com/cloud-custodian/cloud-custodian/actions?query=workflow%3ACI+branch%3Amaster+event%3Apush)
[![](https://img.shields.io/badge/license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0)
[![](https://codecov.io/gh/cloud-custodian/cloud-custodian/branch/master/graph/badge.svg)](https://codecov.io/gh/cloud-custodian/cloud-custodian)
[![](https://requires.io/github/cloud-custodian/cloud-custodian/requirements.svg?branch=master)](https://requires.io/github/cloud-custodian/cloud-custodian/requirements/?branch=master)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3402/badge)](https://bestpractices.coreinfrastructure.org/projects/3402)

Cloud Custodian, also known as c7n, is a rules engine for managing
public cloud accounts and resources. It allows users to define
policies to enable a well managed cloud infrastructure, that\'s both
secure and cost optimized. It consolidates many of the adhoc scripts
organizations have into a lightweight and flexible tool, with unified
metrics and reporting.

Custodian can be used to manage AWS, Azure, and GCP environments by
ensuring real time compliance to security policies (like encryption and
access requirements), tag policies, and cost management via garbage
collection of unused resources and off-hours resource management.

Custodian also supports running policies on infrastructure as code assets
to provide feedback directly on developer workstations or within CI pipelines.

Custodian policies are written in simple YAML configuration files that
enable users to specify policies on a resource type (EC2, ASG, Redshift,
CosmosDB, PubSub Topic) and are constructed from a vocabulary of filters
and actions.

It integrates with the cloud native serverless capabilities of each
provider to provide for real time enforcement of policies with builtin
provisioning. Or it can be run as a simple cron job on a server to
execute against large existing fleets.

Cloud Custodian is a CNCF Incubating project, lead by a community of hundreds
of contributors.

Features
--------

-   Comprehensive support for public cloud services and resources with a
    rich library of actions and filters to build policies with.
-   Run policies on infrastructure as code (terraform, etc) assets.	
-   Supports arbitrary filtering on resources with nested boolean
    conditions.
-   Dry run any policy to see what it would do.
-   Automatically provisions serverless functions and event sources (
    AWS CloudWatchEvents, AWS Config Rules, Azure EventGrid, GCP
    AuditLog & Pub/Sub, etc)
-   Cloud provider native metrics outputs on resources that matched a
    policy
-   Structured outputs into cloud native object storage of which
    resources matched a policy.
-   Intelligent cache usage to minimize api calls.
-   Supports multi-account/subscription/project usage.
-   Battle-tested - in production on some very large cloud environments.

Links
-----

-   [Homepage](http://cloudcustodian.io)
-   [Docs](http://cloudcustodian.io/docs/index.html)
-   [Project Roadmap](https://github.com/orgs/cloud-custodian/projects/1)
-   [Developer Install](https://cloudcustodian.io/docs/developer/installing.html)
-   [Presentations](https://www.google.com/search?q=cloud+custodian&source=lnms&tbm=vid)
-   [YouTube Channel](https://www.youtube.com/channel/UCdeXCdFLluylWnFfS0-jbDA)

Quick Install
-------------

Custodian is published on pypi as a series of packages with the `c7n`
prefix, its also available as a docker image.

```shell
$ python3 -m venv custodian
$ source custodian/bin/activate
(custodian) $ pip install c7n
```


Usage
-----

The first step to using Cloud Custodian (c7n) is writing a YAML file
containing the policies that you want to run. Each policy specifies
the resource type that the policy will run on, a set of filters which
control resources will be affected by this policy, actions which the policy
with take on the matched resources, and a mode which controls which
how the policy will execute.

The best getting started guides are the cloud provider specific tutorials.

 - [AWS Getting Started](https://cloudcustodian.io/docs/aws/gettingstarted.html)
 - [Azure Getting Started](https://cloudcustodian.io/docs/azure/gettingstarted.html)
 - [GCP Getting Started](https://cloudcustodian.io/docs/gcp/gettingstarted.html)

As a quick walk through, below are some sample policies for AWS resources.

  1. will enforce that no S3 buckets have cross-account access enabled.
  1. will terminate any newly launched EC2 instance that do not have an encrypted EBS volume.
  1. will tag any EC2 instance that does not have the follow tags
     ""Environment"", ""AppId"", and either ""OwnerContact"" or ""DeptID"" to
     be stopped in four days.

```yaml
policies:
 - name: s3-cross-account
   description: |
     Checks S3 for buckets with cross-account access and
     removes the cross-account access.
   resource: aws.s3
   region: us-east-1
   filters:
     - type: cross-account
   actions:
     - type: remove-statements
       statement_ids: matched

 - name: ec2-require-non-public-and-encrypted-volumes
   resource: aws.ec2
   description: |
    Provision a lambda and cloud watch event target
    that looks at all new instances and terminates those with
    unencrypted volumes.
   mode:
    type: cloudtrail
    role: CloudCustodian-QuickStart
    events:
      - RunInstances
   filters:
    - type: ebs
      key: Encrypted
      value: false
   actions:
    - terminate

 - name: tag-compliance
   resource: aws.ec2
   description: |
     Schedule a resource that does not meet tag compliance policies to be stopped in four days. Note a separate policy using the`marked-for-op` filter is required to actually stop the instances after four days.
   filters:
    - State.Name: running
    - ""tag:Environment"": absent
    - ""tag:AppId"": absent
    - or:
      - ""tag:OwnerContact"": absent
      - ""tag:DeptID"": absent
   actions:
    - type: mark-for-op
      op: stop
      days: 4
```

You can validate, test, and run Cloud Custodian with the example policy with these commands:

```shell
# Validate the configuration (note this happens by default on run)
$ custodian validate policy.yml

# Dryrun on the policies (no actions executed) to see what resources
# match each policy.
$ custodian run --dryrun -s out policy.yml

# Run the policy
$ custodian run -s out policy.yml
```

You can run Cloud Custodian via Docker as well:

```shell
# Download the image
$ docker pull cloudcustodian/c7n
$ mkdir output

# Run the policy
#
# This will run the policy using only the environment variables for authentication
$ docker run -it \
  -v $(pwd)/output:/home/custodian/output \
  -v $(pwd)/policy.yml:/home/custodian/policy.yml \
  --env-file <(env | grep ""^AWS\|^AZURE\|^GOOGLE"") \
  cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml

# Run the policy (using AWS's generated credentials from STS)
#
# NOTE: We mount the ``.aws/credentials`` and ``.aws/config`` directories to
# the docker container to support authentication to AWS using the same credentials
# credentials that are available to the local user if authenticating with STS.

$ docker run -it \
  -v $(pwd)/output:/home/custodian/output \
  -v $(pwd)/policy.yml:/home/custodian/policy.yml \
  -v $(cd ~ && pwd)/.aws/credentials:/home/custodian/.aws/credentials \
  -v $(cd ~ && pwd)/.aws/config:/home/custodian/.aws/config \
  --env-file <(env | grep ""^AWS"") \
  cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml
```

The [custodian cask
tool](https://cloudcustodian.io/docs/tools/cask.html) is a go binary
that provides a transparent front end to docker that mirors the regular
custodian cli, but automatically takes care of mounting volumes.

Consult the documentation for additional information, or reach out on gitter.

Cloud Provider Specific Help
----------------------------

For specific instructions for AWS, Azure, and GCP, visit the relevant getting started page.

- [AWS](https://cloudcustodian.io/docs/aws/gettingstarted.html)
- [Azure](https://cloudcustodian.io/docs/azure/gettingstarted.html)
- [GCP](https://cloudcustodian.io/docs/gcp/gettingstarted.html)

Get Involved
------------

-   [GitHub](https://github.com/cloud-custodian/cloud-custodian) - (This page)
-   [Slack](https://communityinviter.com/apps/cloud-custodian/c7n-chat) - Real time chat if you're looking for help or interested in contributing to Custodian! 
    - [Gitter](https://gitter.im/cloud-custodian/cloud-custodian) - (Older real time chat, we're likely migrating away from this)
-   [Linen.dev](https://www.linen.dev/s/cloud-custodian/c/general) - Follow our discussions on Linen
-   [Mailing List](https://groups.google.com/forum/#!forum/cloud-custodian) - Our project mailing list, subscribe here for important project announcements, feel free to ask questions
-   [Reddit](https://reddit.com/r/cloudcustodian) - Our subreddit
-   [StackOverflow](https://stackoverflow.com/questions/tagged/cloudcustodian) - Q&A site for developers, we keep an eye on the `cloudcustodian` tag
-   [YouTube Channel](https://www.youtube.com/channel/UCdeXCdFLluylWnFfS0-jbDA/) - We're working on adding tutorials and other useful information, as well as meeting videos

Community Resources
-------------------

We have a regular community meeting that is open to all users and developers of every skill level.
Joining the [mailing list](https://groups.google.com/forum/#!forum/cloud-custodian) will automatically send you a meeting invite. 
See the notes below for more technical information on joining the meeting. 

- [Community Meeting Videos](https://www.youtube.com/watch?v=qy250y0UT-4&list=PLJ2Un8H_N5uBeAAWK95SnWvm_AuNJ8q2x)
- [Community Meeting Notes Archive](https://github.com/orgs/cloud-custodian/discussions/categories/announcements)
- [Upcoming Community Events](https://cloudcustodian.io/events/)
- [Cloud Custodian Annual Report 2021](https://github.com/cncf/toc/blob/main/reviews/2021-cloud-custodian-annual.md) - Annual health check provided to the CNCF outlining the health of the project
- [Ada Logics Third Party Security Audit](https://ostif.org/cc-audit-complete/)


Additional Tools
----------------

The Custodian project also develops and maintains a suite of additional
tools here
<https://github.com/cloud-custodian/cloud-custodian/tree/master/tools>:

- [**_Org_:**](https://cloudcustodian.io/docs/tools/c7n-org.html) Multi-account policy execution.

- [**_ShiftLeft_:**](https://cloudcustodian.io/docs/tools/c7n-left.html) Shift Left ~ run policies against Infrastructure as Code assets like terraform.

- [**_PolicyStream_:**](https://cloudcustodian.io/docs/tools/c7n-policystream.html) Git history as stream of logical policy changes.

- [**_Salactus_:**](https://cloudcustodian.io/docs/tools/c7n-salactus.html) Scale out s3 scanning.

- [**_Mailer_:**](https://cloudcustodian.io/docs/tools/c7n-mailer.html) A reference implementation of sending messages to users to notify them.

- [**_Trail Creator_:**](https://cloudcustodian.io/docs/tools/c7n-trailcreator.html) Retroactive tagging of resources creators from CloudTrail

- **_TrailDB_:** Cloudtrail indexing and time series generation for dashboarding.

- [**_LogExporter_:**](https://cloudcustodian.io/docs/tools/c7n-logexporter.html) Cloud watch log exporting to s3

- [**_Cask_:**](https://cloudcustodian.io/docs/tools/cask.html) Easy custodian exec via docker

- [**_Guardian_:**](https://cloudcustodian.io/docs/tools/c7n-guardian.html) Automated multi-account Guard Duty setup

- [**_Omni SSM_:**](https://cloudcustodian.io/docs/tools/omnissm.html) EC2 Systems Manager Automation

- [**_Mugc_:**](https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/ops#mugc) A utility used to clean up Cloud Custodian Lambda policies that are deployed in an AWS environment.

Contributing
------------

See <https://cloudcustodian.io/docs/contribute.html>

Security
--------

If you've found a security related issue, a vulnerability, or a
potential vulnerability in Cloud Custodian please let the Cloud
[Custodian Security Team](mailto:security@cloudcustodian.io) know with
the details of the vulnerability. We'll send a confirmation email to
acknowledge your report, and we'll send an additional email when we've
identified the issue positively or negatively.

Code of Conduct
---------------

This project adheres to the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md)

By participating, you are expected to honor this code.",VRAI
Cloudflare-Mining/Cloudflare-Datamining,Application System,Application System,2025-05-16T01:27:28Z,2025-05-15T16:31:25Z,0,0,0,0,0,0,0,0,2022-06-18T22:21:02Z,2025-04-08T03:18:41Z,1587009,149,HTML,VRAI,20,FAUX,8,"cloudflare,hacktoberfest",8,Public datamining for all things Cloudflare,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,8,"# Cloudflare Datamining

## Intro

This repository mines a lot of publicly available data from [Cloudflare](https://www.cloudflare.com/), including their customer dashboard, NPM/GitHub repos, marketing site, and more. Automated commits will be made frequently and will show how the data changes over time.

Some of the data can be visualised on the [website](https://cfdata.lol).

You can join the [Discord](https://discord.gg/Z94Hn6qVDm) for notifications of changes.

## Structure
- Scripts are in `scripts`
- Data is in `data`

## Data

- `account`: Tracks a free account's entitlements and flags
- `api-schemas`: This extracts the API schemas from `api.cloudflare.com`, and tracks their changes over time.
- `blog`: This tracks blog post content over time.
- `cdn-cgi`: This tracks various changes to `cdn-cgi` endpoints on Cloudflare's CDN, as well as `request.cf` keys for any additions or changes. This also tracks component changes throughout Cloudflare's network and their different datacenters.
- `coveo`: This tracks updates to the Coveo Cloudflare indexes, which are used for searching the blog, learning center, etc.
- `dashboard-subroutes`: This extracts subroutes from the dashboard which can be useful for determining undocumented APIs.
- `dashboard-translations`: This extracts the translations from the dashboard, and tracks their changes over time. This is useful for seeing new features and changes in the dashboard.
- `dashboard`: This extracts other miscellaneous data from the dashboard, and tracks their changes over time, including URLs, generic strings/callees/regexes, etc.
- `doh`: This tracks the DOH schema returned by 1.1.1.1
- `domains`: This tracks domains that Cloudflare owns. It's not perfect, but it does pick up a lot of domains owned by Cloudflare.
- `entitlements`: This isn't automated, but tracks a list of all known entitlements for accounts and zones.
- `icons`: This tracks the `@cloudflare/component-icon` NPM package.
- `gates`: This tracks the active gates used for A/B experiments on the dash, etc.
- `github-repos`: This tracks updates, additions, etc. to all GitHub repos under the Cloudflare org.
- `jobs`: This tracks Cloudflare job post listings, which can reveal upcoming projects.
- `marketing`: This extracts the raw JSON data for the marketing site, cloudflare.com.
- `other`: This tracks some other miscellaneous data (like IPs), not tracked by other scripts.
- `packages`: This tracks updates and any new packages owned by Cloudflare.
- `products`: This tracks specific products API endpoints and their response keys.
- `registrar`: This tracks Cloudflare Registrar available TLDs and pricing over time.
- `types`: This tracks the `@cloudflare/types` NPM package, which can be useful for spotting changes to new billing subscriptions, etc.
- `workerd`: This tracks various capnp schemas from `workerd`
- `zt-dashboard-translations`: This extracts the translations from the Zero Trust dashboard, and tracks their changes over time. This is useful for seeing new features and changes in the ZT dashboard.
- `zt-dashboard`: This extracts other miscellaneous data from the Zero Trust dashboard, and tracks their changes over time.

## Contributing

PRs are welcome! If you have an interesting Cloudflare account and would like to contribute your account/zone entitlements to grow the known list, please send me a JSON dump of your entitlements either via Discord (`CherryJimbo#0001`) or [Twitter (X)](https://twitter.com/cherryjimbo?lang=de) (`@CherryJimbo`).",FAUX
cloudposse/atmos,Documentations,DevOPs,2025-05-15T23:09:56Z,2025-04-01T20:53:03Z,0,8,0,0,0,0,0,0,2020-09-03T00:46:14Z,2025-04-06T19:14:58Z,109476,936,Go,VRAI,115,FAUX,98,"automation,cli,cloud,devops,hcl2,helm,helmfile,orchestration,terraform,workflow",98,"👽 Terraform Orchestration Tool for DevOps. Keep environment configuration DRY with hierarchical imports of configurations, inheritance, and WAY more. Native support for Terraform and Helmfile.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,49,"<!-- markdownlint-disable -->
<a href=""https://cpco.io/homepage""><img src=""https://github.com/cloudposse/atmos/blob/main/.github/banner.png?raw=true"" alt=""Project Banner""/></a><br/>
    <p align=""right"">
<a href=""https://github.com/cloudposse/atmos/releases/latest""><img src=""https://img.shields.io/github/release/cloudposse/atmos.svg?style=for-the-badge"" alt=""Latest Release""/></a><a href=""https://github.com/cloudposse/atmos/commits/main/""><img src=""https://img.shields.io/github/last-commit/cloudposse/atmos/main?style=for-the-badge"" alt=""Last Updated""/></a><a href=""https://github.com/cloudposse/atmos/actions/workflows/test.yml""><img src=""https://img.shields.io/github/actions/workflow/status/cloudposse/atmos/test.yml?style=for-the-badge"" alt=""Tests""/></a><a href=""https://slack.cloudposse.com""><img src=""https://slack.cloudposse.com/for-the-badge.svg"" alt=""Slack Community""/></a></p>
<!-- markdownlint-restore -->

<!--




  ** DO NOT EDIT THIS FILE
  **
  ** This file was automatically generated by the `cloudposse/build-harness`.
  ** 1) Make all changes to `README.yaml`
  ** 2) Run `make init` (you only need to do this once)
  ** 3) Run`make readme` to rebuild this file.
  **
  ** (We maintain HUNDREDS of open source projects. This is how we maintain our sanity.)
  **





-->

## Use Atmos to break your architecture into reusable [Components](https://atmos.tools/core-concepts/components) that you implement using [Terraform ""root modules""](https://atmos.tools/core-concepts/components/terraform). Then tie everything together using [Stack](https://atmos.tools/core-concepts/stacks) configurations defined in YAML.

Atmos can change how you think about the Terraform code you write to build your infrastructure. Atmos is a framework that simplifies complex cloud architectures and DevOps workflows into intuitive CLI commands.
Its strength in managing DRY configurations at scale for Terraform and is supported by robust design patterns, comprehensive documentation, and a passionate community, making it a versatile tool for both startups and enterprises.
Atmos is extensible to accommodate any tooling, including enterprise-scale Terraform, and includes custom policy controls, vendoring, and GitOps capabilities out of the box. Everything is open source and free.


> [!TIP]
> ### You can try out `atmos` directly in your browser using GitHub Codespaces
>
> [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=cloudposse/atmos&skip_quickstart=true)
>
> <i>Already start one? Find it [here](https://github.com/codespaces).</i>
>

## Screenshots

<img src=""docs/demo.gif"" alt=""Demo"" />*<br/>Example of running atmos to describe infrastructure.*




## Introduction


[Atmos](https://atmos.tools) centralizes the DevOps chain and cloud automation/orchestration into a robust command-line tool,
streamlining environments and workflows into straightforward CLI commands. Leveraging advanced hierarchical configurations,
it efficiently orchestrates both local and CI/CD pipeline tasks, optimizing infrastructure management for engineers and cloud
architects alike. You can then run the CLI anywhere, such as locally or in CI/CD.

The Atmos project consists of a command-line tool, a `Go` library, and even a terraform provider.  It provides numerous
[conventions](https://atmos.tools/design-patterns/) to help you provision, manage, and orchestrate workflows across various toolchains.
You can even access the configurations natively from within terraform using our [`terraform-provider-utils`](https://github.com/cloudposse/terraform-provider-utils/).

[Cloud Posse](https://cloudposse.com/) uses this tool extensively for automating cloud infrastructure with
[Terraform](https://www.hashicorp.com/products/terraform) and [Kubernetes](https://kubernetes.io/), but it can be used to automate any complex workflow.

> [!TIP]
> ### Did you know?
>
> By leveraging Atmos in conjunction with Cloud Posse's [*expertise in AWS*](https://cloudposse.com),
> [*terraform blueprints*](https://cloudposse.com/services/), and our [*knowledgeable community*](https://slack.cloudposse.com), teams can achieve
> operational mastery and innovation faster, transforming their infrastructure management practices into a competitive advantage.

## Core Features

Atmos streamlines Terraform orchestration, environment, and configuration management, offering developers and DevOps a set of
powerful tools to tackle deployment challenges. Designed to be cloud agnostic, it enables you to operate consistently across
various cloud platforms. These features boost efficiency, clarity, and control across various environments, making it an
indispensable asset for managing complex infrastructures with confidence.

- [**Terminal UI**](https://atmos.tools/cli) Polished interface for easier interaction with Terraform, workflows, and commands.
- [**Native Terraform Support:**](https://atmos.tools/cli/commands/terraform/usage) Orchestration, backend generation, varfile generation, ensuring compatibility with vanilla Terraform.
- [**Stacks:**](https://atmos.tools/core-concepts/stacks) Powerful abstraction layer defined in YAML for orchestrating and deploying components.
- [**Components:**](https://atmos.tools/core-concepts/components) A generic abstraction for deployable units, such as Terraform ""root"" modules.
- [**Vendoring:**](https://atmos.tools/core-concepts/vendor) Pulls dependencies from remote sources, supporting immutable infrastructure practices.
- [**Custom Commands:**](https://atmos.tools/core-concepts/custom-commands) Extends Atmos's functionality, allowing integration of any command with stack configurations.
- [**Workflow Orchestration:**](https://atmos.tools/core-concepts/workflows) Comprehensive support for managing the lifecycle of cloud infrastructure from initiation to maintenance.

See [all features of Atmos](https://atmos.tools/features).

## Use Cases

Atmos has consistently demonstrated its effectiveness in addressing these key use-cases, showcasing its adaptability and
strength in the cloud infrastructure and DevOps domains:

- **Managing Large Multi-Account Cloud Environments:** Suitable for organizations using multiple cloud accounts to separate different
  projects or stages of development.
- **Cross-Platform Cloud Architectures:** Ideal for businesses that need to manage configuration of services across AWS, GCP, Azure, etc., to
  build a cohesive system.
- **Multi-Tenant Systems for SaaS:** Perfect for SaaS companies looking to host multiple customers within a unified infrastructure.
  Simply define a baseline tenant configuration once, and then seamlessly onboard new tenants by reusing this baseline through pure
  configuration, bypassing the need for further code development.
- **Efficient Multi-Region Deployments:** Atmos facilitates streamlined multi-region deployments by enabling businesses to define baseline
  configurations with [stacks](https://atmos.tools/core-concepts/stacks/) and extend them across regions with DRY principles through
  [imports](https://atmos.tools/core-concepts/stacks/imports) and [inheritance](https://atmos.tools/core-concepts/stacks/inheritance).
- **Compliant Infrastructure for Regulated Industries:** Atmos empowers DevOps and SecOps teams to create vetted configurations that comply
  with SOC2, HIPAA, HITRUST, PCI, and other regulatory standards. These configurations can then be efficiently shared and reused across the
  organization via [service catalogs](https://atmos.tools/core-concepts/stacks/catalogs), [component libraries](https://atmos.tools/core-concepts/components/library),
  [vendoring](https://atmos.tools/core-concepts/vendor), and [OPA policies](https://atmos.tools/core-concepts/validate/opa),
  simplifying the process of achieving and maintaining rigorous compliance.
- **Empowering Teams with Self-Service Infrastructure:** Allows teams to manage their infrastructure needs independently, using
  predefined templates and policies.
- **Streamlining Deployment with Service Catalogs, Landing Zones, and Blueprints:** Provides ready-to-use templates and guidelines for
  setting up cloud environments quickly and consistently.

> [!TIP]
> Don't see your use-case listed? Ask us in the [`#atmos`](https://slack.cloudposse.com) Slack channel,
> or [join us for ""Office Hours""](https://cloudposse.com/office-hours/) every week.


Moreover, `atmos` is not only a command-line interface for managing clouds and clusters. It provides many useful patterns
and best practices, such as:
- Enforces a project structure convention, so everybody knows where to find things.
- Provides clear separation of configuration from code, so the same code is easily deployed to different regions, environments and stages
- It can be extended to include new features, commands, and workflows
- The commands have a clean, consistent and easy to understand syntax
- The CLI code is modular and self-documenting

## Documentation

Find all documentation at: [atmos.tools](https://atmos.tools)













## ✨ Contributing

This project is under active development, and we encourage contributions from our community.



Many thanks to our outstanding contributors:

<a href=""https://github.com/cloudposse/atmos/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=cloudposse/atmos&max=24"" />
</a>

For 🐛 bug reports & feature requests, please use the [issue tracker](https://github.com/cloudposse/atmos/issues).

In general, PRs are welcome. We follow the typical ""fork-and-pull"" Git workflow.
 1. Review our [Code of Conduct](https://github.com/cloudposse/atmos/?tab=coc-ov-file#code-of-conduct) and [Contributor Guidelines](https://github.com/cloudposse/.github/blob/main/CONTRIBUTING.md).
 2. **Fork** the repo on GitHub
 3. **Clone** the project to your own machine
 4. **Commit** changes to your own branch
 5. **Push** your work back up to your fork
 6. Submit a **Pull Request** so that we can review your changes

**NOTE:** Be sure to merge the latest changes from ""upstream"" before making a pull request!

### 🌎 Slack Community

Join our [Open Source Community](https://cpco.io/slack?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/atmos&utm_content=slack) on Slack. It's **FREE** for everyone! Our ""SweetOps"" community is where you get to talk with others who share a similar vision for how to rollout and manage infrastructure. This is the best place to talk shop, ask questions, solicit feedback, and work together as a community to build totally *sweet* infrastructure.

### 📰 Newsletter

Sign up for [our newsletter](https://cpco.io/newsletter?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/atmos&utm_content=newsletter) and join 3,000+ DevOps engineers, CTOs, and founders who get insider access to the latest DevOps trends, so you can always stay in the know.
Dropped straight into your Inbox every week — and usually a 5-minute read.

### 📆 Office Hours <a href=""https://cloudposse.com/office-hours?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/atmos&utm_content=office_hours""><img src=""https://img.cloudposse.com/fit-in/200x200/https://cloudposse.com/wp-content/uploads/2019/08/Powered-by-Zoom.png"" align=""right"" /></a>

[Join us every Wednesday via Zoom](https://cloudposse.com/office-hours?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/atmos&utm_content=office_hours) for your weekly dose of insider DevOps trends, AWS news and Terraform insights, all sourced from our SweetOps community, plus a _live Q&A_ that you can’t find anywhere else.
It's **FREE** for everyone!
## License

<a href=""https://opensource.org/licenses/Apache-2.0""><img src=""https://img.shields.io/badge/License-Apache%202.0-blue.svg?style=for-the-badge"" alt=""License""></a>

<details>
<summary>Preamble to the Apache License, Version 2.0</summary>
<br/>
<br/>

Complete license is available in the [`LICENSE`](LICENSE) file.

```text
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
""License""); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
```
</details>

## Trademarks

All other trademarks referenced herein are the property of their respective owners.


---
Copyright © 2017-2024 [Cloud Posse, LLC](https://cpco.io/copyright)


<a href=""https://cloudposse.com/readme/footer/link?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/atmos&utm_content=readme_footer_link""><img alt=""README footer"" src=""https://cloudposse.com/readme/footer/img""/></a>

<img alt=""Beacon"" width=""0"" src=""https://ga-beacon.cloudposse.com/UA-76589703-4/cloudposse/atmos?pixel&cs=github&cm=readme&an=atmos""/>",VRAI
cloudposse/terraform-provider-utils,Toolkit,Application System,2025-03-12T02:24:48Z,2023-08-08T19:59:38Z,0,5,0,0,0,0,0,0,2021-01-25T15:17:00Z,2025-04-07T10:48:08Z,1048,104,Go,VRAI,18,FAUX,16,"config,configuration,configuration-management,deep-merge,provider,spacelift,stack,terraform,terraform-providers,utilities,utils",16,"The Cloud Posse Terraform Provider for various utilities (e.g. deep merging, stack configuration management)",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,9,"<!-- markdownlint-disable -->
# terraform-provider-utils <a href=""https://cpco.io/homepage?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-provider-utils&utm_content=""><img align=""right"" src=""https://cloudposse.com/logo-300x69.svg"" width=""150"" /></a>
<a href=""https://github.com/cloudposse/terraform-provider-utils/actions/workflows/test.yml""><img src=""https://img.shields.io/github/actions/workflow/status/cloudposse/terraform-provider-utils/test.yml?style=for-the-badge"" alt=""Tests""/></a><a href=""https://github.com/cloudposse/terraform-provider-utils/actions/workflows/test.yml""><img src=""https://img.shields.io/github/actions/workflow/status/cloudposse/terraform-provider-utils/test.yml?style=for-the-badge"" alt=""Tests""/></a><a href=""https://github.com/cloudposse/terraform-provider-utils/actions/workflows/test.yml""><img src=""https://img.shields.io/github/actions/workflow/status/cloudposse/terraform-provider-utils/test.yml?style=for-the-badge"" alt=""Tests""/></a><a href=""https://github.com/cloudposse/terraform-provider-utils/actions/workflows/test.yml""><img src=""https://img.shields.io/github/actions/workflow/status/cloudposse/terraform-provider-utils/test.yml?style=for-the-badge"" alt=""Tests""/></a><a href=""https://github.com/cloudposse/terraform-provider-utils/actions/workflows/test.yml""><img src=""https://img.shields.io/github/actions/workflow/status/cloudposse/terraform-provider-utils/test.yml?style=for-the-badge"" alt=""Tests""/></a><a href=""https://github.com/cloudposse/terraform-provider-utils/releases/latest""><img src=""https://img.shields.io/github/release/cloudposse/terraform-provider-utils.svg?style=for-the-badge"" alt=""Latest Release""/></a><a href=""https://github.com/cloudposse/terraform-provider-utils/commits""><img src=""https://img.shields.io/github/last-commit/cloudposse/terraform-provider-utils.svg?style=for-the-badge"" alt=""Last Updated""/></a><a href=""https://slack.cloudposse.com""><img src=""https://slack.cloudposse.com/for-the-badge.svg"" alt=""Slack Community""/></a><a href=""https://github.com/cloudposse/terraform-provider-utils/actions/workflows/test.yml""><img src=""https://img.shields.io/github/actions/workflow/status/cloudposse/terraform-provider-utils/test.yml?style=for-the-badge"" alt=""Tests""/></a>
<!-- markdownlint-restore -->

<!--




  ** DO NOT EDIT THIS FILE
  **
  ** This file was automatically generated by the `cloudposse/build-harness`.
  ** 1) Make all changes to `README.yaml`
  ** 2) Run `make init` (you only need to do this once)
  ** 3) Run`make readme` to rebuild this file.
  **
  ** (We maintain HUNDREDS of open source projects. This is how we maintain our sanity.)
  **





-->

Terraform provider for various utilities (deep merging, Atmos stack configuration management), and to add additional missing functionality to Terraform


> [!TIP]
> #### 👽 Use Atmos with Terraform
> Cloud Posse uses [`atmos`](https://atmos.tools) to easily orchestrate multiple environments using Terraform. <br/>
> Works with [Github Actions](https://atmos.tools/integrations/github-actions/), [Atlantis](https://atmos.tools/integrations/atlantis), or [Spacelift](https://atmos.tools/integrations/spacelift).
>
> <details>
> <summary><strong>Watch demo of using Atmos with Terraform</strong></summary>
> <img src=""https://github.com/cloudposse/atmos/blob/master/docs/demo.gif?raw=true""/><br/>
> <i>Example of running <a href=""https://atmos.tools""><code>atmos</code></a> to manage infrastructure from our <a href=""https://atmos.tools/quick-start/"">Quick Start</a> tutorial.</i>
> </detalis>





## Usage

Here is how to use this provider in your own Terraform code:

```hcl
terraform {
  required_providers {
    utils = {
      source = ""cloudposse/utils""
      version = "">= 1.17.0""
    }
  }
}
```

See the [Docs](./docs) for additional information.

> [!IMPORTANT]
> In Cloud Posse's examples, we avoid pinning modules to specific versions to prevent discrepancies between the documentation
> and the latest released versions. However, for your own projects, we strongly advise pinning each module to the exact version
> you're using. This practice ensures the stability of your infrastructure. Additionally, we recommend implementing a systematic
> approach for updating versions to avoid unexpected changes.





## Examples

Here is an example of using this provider:

```hcl
terraform {
  required_providers {
    utils = {
      source = ""cloudposse/utils""
    }
  }
}

locals {
  yaml_data_1 = file(""${path.module}/data1.yaml"")
  yaml_data_2 = file(""${path.module}/data2.yaml"")
}

data ""utils_deep_merge_yaml"" ""example"" {
  input = [
    local.yaml_data_1,
    local.yaml_data_2
  ]
}

output ""deep_merge_output"" {
  value = data.utils_deep_merge_yaml.example.output
}
```

Here are some additional examples:

- [`examples/data-sources/utils_deep_merge_json`](examples/data-sources/utils_deep_merge_json)
- [`examples/data-sources/utils_deep_merge_yaml`](examples/data-sources/utils_deep_merge_yaml)




## Developing the Provider

If you wish to work on the provider, you'll first need [Go](http://www.golang.org) installed on your machine (see [Requirements](#requirements) above).

To compile the provider, run `go install`. This will build the provider and put the provider binary in the `$GOPATH/bin` directory.

To generate or update documentation, run `go generate`.

In order to run the full suite of Acceptance tests, run `make testacc`.

_Note:_ Acceptance tests create real resources, and often cost money to run.

```sh
$ make testacc
```

### Testing Locally

You can test the provider locally by using the [provider_installation](https://www.terraform.io/docs/cli/config/config-file.html#provider-installation) functionality.

For testing this provider, you can edit your `~/.terraformrc` file with the following:

```hcl
provider_installation {
  dev_overrides  {
    ""cloudposse/utils"" = ""/path/to/your/code/github.com/cloudposse/terraform-provider-utils/""
  }

  # For all other providers, install them directly from their origin provider
  # registries as normal. If you omit this, Terraform will _only_ use
  # the dev_overrides block, and so no other providers will be available.
  direct {}
}
```

With that in place, you can build the provider (see above) and add a provider block:

```hcl
required_providers {
    utils = {
      source = ""cloudposse/utils""
    }
  }
```

Then run `terraform init`, `terraform plan` and `terraform apply` as normal.

```sh
$ terraform init
Initializing the backend...

Initializing provider plugins...
- Finding latest version of cloudposse/utils...

Warning: Provider development overrides are in effect

The following provider development overrides are set in the CLI configuration:
 - cloudposse/utils in /path/to/your/code/github.com/cloudposse/terraform-provider-utils

The behavior may therefore not match any released version of the provider and
applying changes may cause the state to become incompatible with published
releases.
```

```sh
terraform apply

Warning: Provider development overrides are in effect

The following provider development overrides are set in the CLI configuration:
 - cloudposse/utils in /Users/matt/code/src/github.com/cloudposse/terraform-provider-utils

The behavior may therefore not match any released version of the provider and
applying changes may cause the state to become incompatible with published
releases.


An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:

Terraform will perform the following actions:

Plan: 0 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + deep_merge_output = <<-EOT
        Statement:
        - Action:
          - s3:*
          Effect: Allow
          Resource:
          - '*'
          Sid: FullAccess
        - Action:
          - s3:*
          Complex:
            ExtraComplex:
              ExtraExtraComplex:
                Foo: bazzz
                SomeArray:
                - one
                - two
                - three
          Effect: Deny
          Resource:
          - arn:aws:s3:::customer
          - arn:aws:s3:::customer/*
          - foo
          Sid: DenyCustomerBucket
        Version: ""2012-10-17""
    EOT
```


## Related Projects

Check out these related projects.



## References

For additional context, refer to some of these links.

- [Terraform Plugins](https://www.terraform.io/docs/extend/plugin-types.html#providers) - Terraform is logically split into two main parts: Terraform Core and Terraform Plugins. Each plugin exposes an implementation for a specific service, such as the AWS provider or the cloud-init provider.



> [!TIP]
> #### Use Terraform Reference Architectures for AWS
>
> Use Cloud Posse's ready-to-go [terraform architecture blueprints](https://cloudposse.com/reference-architecture/) for AWS to get up and running quickly.
>
> ✅ We build it together with your team.<br/>
> ✅ Your team owns everything.<br/>
> ✅ 100% Open Source and backed by fanatical support.<br/>
>
> <a href=""https://cpco.io/commercial-support?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-provider-utils&utm_content=commercial_support""><img alt=""Request Quote"" src=""https://img.shields.io/badge/request%20quote-success.svg?style=for-the-badge""/></a>
> <details><summary>📚 <strong>Learn More</strong></summary>
>
> <br/>
>
> Cloud Posse is the leading [**DevOps Accelerator**](https://cpco.io/commercial-support?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-provider-utils&utm_content=commercial_support) for funded startups and enterprises.
>
> *Your team can operate like a pro today.*
>
> Ensure that your team succeeds by using Cloud Posse's proven process and turnkey blueprints. Plus, we stick around until you succeed.
> #### Day-0:  Your Foundation for Success
> - **Reference Architecture.** You'll get everything you need from the ground up built using 100% infrastructure as code.
> - **Deployment Strategy.** Adopt a proven deployment strategy with GitHub Actions, enabling automated, repeatable, and reliable software releases.
> - **Site Reliability Engineering.** Gain total visibility into your applications and services with Datadog, ensuring high availability and performance.
> - **Security Baseline.** Establish a secure environment from the start, with built-in governance, accountability, and comprehensive audit logs, safeguarding your operations.
> - **GitOps.** Empower your team to manage infrastructure changes confidently and efficiently through Pull Requests, leveraging the full power of GitHub Actions.
>
> <a href=""https://cpco.io/commercial-support?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-provider-utils&utm_content=commercial_support""><img alt=""Request Quote"" src=""https://img.shields.io/badge/request%20quote-success.svg?style=for-the-badge""/></a>
>
> #### Day-2: Your Operational Mastery
> - **Training.** Equip your team with the knowledge and skills to confidently manage the infrastructure, ensuring long-term success and self-sufficiency.
> - **Support.** Benefit from a seamless communication over Slack with our experts, ensuring you have the support you need, whenever you need it.
> - **Troubleshooting.** Access expert assistance to quickly resolve any operational challenges, minimizing downtime and maintaining business continuity.
> - **Code Reviews.** Enhance your team’s code quality with our expert feedback, fostering continuous improvement and collaboration.
> - **Bug Fixes.** Rely on our team to troubleshoot and resolve any issues, ensuring your systems run smoothly.
> - **Migration Assistance.** Accelerate your migration process with our dedicated support, minimizing disruption and speeding up time-to-value.
> - **Customer Workshops.** Engage with our team in weekly workshops, gaining insights and strategies to continuously improve and innovate.
>
> <a href=""https://cpco.io/commercial-support?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-provider-utils&utm_content=commercial_support""><img alt=""Request Quote"" src=""https://img.shields.io/badge/request%20quote-success.svg?style=for-the-badge""/></a>
> </details>

## ✨ Contributing

This project is under active development, and we encourage contributions from our community.



Many thanks to our outstanding contributors:

<a href=""https://github.com/cloudposse/terraform-provider-utils/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=cloudposse/terraform-provider-utils&max=24"" />
</a>

For 🐛 bug reports & feature requests, please use the [issue tracker](https://github.com/cloudposse/terraform-provider-utils/issues).

In general, PRs are welcome. We follow the typical ""fork-and-pull"" Git workflow.
 1. Review our [Code of Conduct](https://github.com/cloudposse/terraform-provider-utils/?tab=coc-ov-file#code-of-conduct) and [Contributor Guidelines](https://github.com/cloudposse/.github/blob/main/CONTRIBUTING.md).
 2. **Fork** the repo on GitHub
 3. **Clone** the project to your own machine
 4. **Commit** changes to your own branch
 5. **Push** your work back up to your fork
 6. Submit a **Pull Request** so that we can review your changes

**NOTE:** Be sure to merge the latest changes from ""upstream"" before making a pull request!

### 🌎 Slack Community

Join our [Open Source Community](https://cpco.io/slack?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-provider-utils&utm_content=slack) on Slack. It's **FREE** for everyone! Our ""SweetOps"" community is where you get to talk with others who share a similar vision for how to rollout and manage infrastructure. This is the best place to talk shop, ask questions, solicit feedback, and work together as a community to build totally *sweet* infrastructure.

### 📰 Newsletter

Sign up for [our newsletter](https://cpco.io/newsletter?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-provider-utils&utm_content=newsletter) and join 3,000+ DevOps engineers, CTOs, and founders who get insider access to the latest DevOps trends, so you can always stay in the know.
Dropped straight into your Inbox every week — and usually a 5-minute read.

### 📆 Office Hours <a href=""https://cloudposse.com/office-hours?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-provider-utils&utm_content=office_hours""><img src=""https://img.cloudposse.com/fit-in/200x200/https://cloudposse.com/wp-content/uploads/2019/08/Powered-by-Zoom.png"" align=""right"" /></a>

[Join us every Wednesday via Zoom](https://cloudposse.com/office-hours?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-provider-utils&utm_content=office_hours) for your weekly dose of insider DevOps trends, AWS news and Terraform insights, all sourced from our SweetOps community, plus a _live Q&A_ that you can’t find anywhere else.
It's **FREE** for everyone!
## License

<a href=""https://opensource.org/licenses/Apache-2.0""><img src=""https://img.shields.io/badge/License-Apache%202.0-blue.svg?style=for-the-badge"" alt=""License""></a>

<details>
<summary>Preamble to the Apache License, Version 2.0</summary>
<br/>
<br/>

Complete license is available in the [`LICENSE`](LICENSE) file.

```text
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
""License""); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
```
</details>

## Trademarks

All other trademarks referenced herein are the property of their respective owners.


## Copyrights

Copyright © 2021-2024 [Cloud Posse, LLC](https://cloudposse.com)



<a href=""https://cloudposse.com/readme/footer/link?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-provider-utils&utm_content=readme_footer_link""><img alt=""README footer"" src=""https://cloudposse.com/readme/footer/img""/></a>

<img alt=""Beacon"" width=""0"" src=""https://ga-beacon.cloudposse.com/UA-76589703-4/cloudposse/terraform-provider-utils?pixel&cs=github&cm=readme&an=terraform-provider-utils""/>",VRAI
cloudposse/terraform-spacelift-cloud-infrastructure-automation,Toolkit,DevOPs,2025-01-01T03:16:53Z,2023-08-04T21:19:51Z,0,18,0,0,0,0,0,0,2020-11-05T17:03:19Z,2025-04-04T03:55:48Z,4260,38,HCL,VRAI,23,FAUX,22,,22,Terraform module to provision Spacelift resources for cloud infrastructure automation,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,19,"<!-- markdownlint-disable -->
<a href=""https://cpco.io/homepage""><img src=""https://github.com/cloudposse/terraform-spacelift-cloud-infrastructure-automation/blob/main/.github/banner.png?raw=true"" alt=""Project Banner""/></a><br/>
    <p align=""right"">
<a href=""https://github.com/cloudposse/terraform-spacelift-cloud-infrastructure-automation/releases/latest""><img src=""https://img.shields.io/github/release/cloudposse/terraform-spacelift-cloud-infrastructure-automation.svg?style=for-the-badge"" alt=""Latest Release""/></a><a href=""https://github.com/cloudposse/terraform-spacelift-cloud-infrastructure-automation/commits""><img src=""https://img.shields.io/github/last-commit/cloudposse/terraform-spacelift-cloud-infrastructure-automation.svg?style=for-the-badge"" alt=""Last Updated""/></a><a href=""https://slack.cloudposse.com""><img src=""https://slack.cloudposse.com/for-the-badge.svg"" alt=""Slack Community""/></a></p>
<!-- markdownlint-restore -->

<!--




  ** DO NOT EDIT THIS FILE
  **
  ** This file was automatically generated by the `cloudposse/build-harness`.
  ** 1) Make all changes to `README.yaml`
  ** 2) Run `make init` (you only need to do this once)
  ** 3) Run`make readme` to rebuild this file.
  **
  ** (We maintain HUNDREDS of open source projects. This is how we maintain our sanity.)
  **





-->

This repo contains a set of Terraform modules for implementing a CI/CD pipeline for Terraform infrastructure using Spacelift.

- [spacelift-policy](./modules/spacelift-policy) - Terraform module for creating Spacelift policies.
- [spacelift-space](./modules/spacelift-space) - Terraform module for creating Spacelift spaces.
- [spacelift-stack](./modules/spacelift-stack) - Terraform module for creating Spacelift stacks.
- [spacelift-stacks-from-atmos-config](./modules/spacelift-stacks-from-atmos-config) - Terraform module for extracting Spacelift stack config from Atmos config.


> [!TIP]
> #### 👽 Use Atmos with Terraform
> Cloud Posse uses [`atmos`](https://atmos.tools) to easily orchestrate multiple environments using Terraform. <br/>
> Works with [Github Actions](https://atmos.tools/integrations/github-actions/), [Atlantis](https://atmos.tools/integrations/atlantis), or [Spacelift](https://atmos.tools/integrations/spacelift).
>
> <details>
> <summary><strong>Watch demo of using Atmos with Terraform</strong></summary>
> <img src=""https://github.com/cloudposse/atmos/blob/main/docs/demo.gif?raw=true""/><br/>
> <i>Example of running <a href=""https://atmos.tools""><code>atmos</code></a> to manage infrastructure from our <a href=""https://atmos.tools/quick-start/"">Quick Start</a> tutorial.</i>
> </detalis>











## Related Projects

Check out these related projects.

- [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) - The Cloud Posse Terraform Provider for various utilities.
- [terraform-yaml-stack-config](https://github.com/cloudposse/terraform-yaml-stack-config) - Terraform module that loads an opinionated stack configuration from local or remote YAML sources. It supports deep-merged variables, settings, ENV variables, backend config, and remote state outputs for Terraform and helmfile components.
- [terraform-yaml-config](https://github.com/cloudposse/terraform-yaml-config) - Terraform module to convert local and remote YAML configuration templates into Terraform lists and maps.


## References

For additional context, refer to some of these links.

- [Terraform Version Pinning](https://www.terraform.io/docs/configuration/terraform.html#specifying-a-required-terraform-version) - The required_version setting can be used to constrain which versions of the Terraform CLI can be used with your configuration
- [Spacelift](https://spacelift.io/) - The most flexible CI/CD for Terraform
- [Spacelift Documentation](https://docs.spacelift.io/) - Official documentation site for Spacelift
- [Open Policy Agent](https://www.openpolicyagent.org/) - Policy-based control for cloud-native environments
- [OPA Documentation](https://www.openpolicyagent.org/docs/latest/) - Open Policy Agent Documentation
- [Rego - OPA’s query and policy language](https://www.openpolicyagent.org/docs/latest/policy-language/) - Rego focuses on providing powerful support for referencing nested documents and ensuring that queries are correct and unambiguous
- [Example of using Rego policy language](https://blog.gripdev.xyz/2020/01/13/mutating-admissions-controllers-with-open-policy-agent-and-rego/) - Mutating Admissions Controllers with Open Policy Agent and Rego



> [!TIP]
> #### Use Terraform Reference Architectures for AWS
>
> Use Cloud Posse's ready-to-go [terraform architecture blueprints](https://cloudposse.com/reference-architecture/) for AWS to get up and running quickly.
>
> ✅ We build it together with your team.<br/>
> ✅ Your team owns everything.<br/>
> ✅ 100% Open Source and backed by fanatical support.<br/>
>
> <a href=""https://cpco.io/commercial-support?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-spacelift-cloud-infrastructure-automation&utm_content=commercial_support""><img alt=""Request Quote"" src=""https://img.shields.io/badge/request%20quote-success.svg?style=for-the-badge""/></a>
> <details><summary>📚 <strong>Learn More</strong></summary>
>
> <br/>
>
> Cloud Posse is the leading [**DevOps Accelerator**](https://cpco.io/commercial-support?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-spacelift-cloud-infrastructure-automation&utm_content=commercial_support) for funded startups and enterprises.
>
> *Your team can operate like a pro today.*
>
> Ensure that your team succeeds by using Cloud Posse's proven process and turnkey blueprints. Plus, we stick around until you succeed.
> #### Day-0:  Your Foundation for Success
> - **Reference Architecture.** You'll get everything you need from the ground up built using 100% infrastructure as code.
> - **Deployment Strategy.** Adopt a proven deployment strategy with GitHub Actions, enabling automated, repeatable, and reliable software releases.
> - **Site Reliability Engineering.** Gain total visibility into your applications and services with Datadog, ensuring high availability and performance.
> - **Security Baseline.** Establish a secure environment from the start, with built-in governance, accountability, and comprehensive audit logs, safeguarding your operations.
> - **GitOps.** Empower your team to manage infrastructure changes confidently and efficiently through Pull Requests, leveraging the full power of GitHub Actions.
>
> <a href=""https://cpco.io/commercial-support?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-spacelift-cloud-infrastructure-automation&utm_content=commercial_support""><img alt=""Request Quote"" src=""https://img.shields.io/badge/request%20quote-success.svg?style=for-the-badge""/></a>
>
> #### Day-2: Your Operational Mastery
> - **Training.** Equip your team with the knowledge and skills to confidently manage the infrastructure, ensuring long-term success and self-sufficiency.
> - **Support.** Benefit from a seamless communication over Slack with our experts, ensuring you have the support you need, whenever you need it.
> - **Troubleshooting.** Access expert assistance to quickly resolve any operational challenges, minimizing downtime and maintaining business continuity.
> - **Code Reviews.** Enhance your team’s code quality with our expert feedback, fostering continuous improvement and collaboration.
> - **Bug Fixes.** Rely on our team to troubleshoot and resolve any issues, ensuring your systems run smoothly.
> - **Migration Assistance.** Accelerate your migration process with our dedicated support, minimizing disruption and speeding up time-to-value.
> - **Customer Workshops.** Engage with our team in weekly workshops, gaining insights and strategies to continuously improve and innovate.
>
> <a href=""https://cpco.io/commercial-support?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-spacelift-cloud-infrastructure-automation&utm_content=commercial_support""><img alt=""Request Quote"" src=""https://img.shields.io/badge/request%20quote-success.svg?style=for-the-badge""/></a>
> </details>

## ✨ Contributing

This project is under active development, and we encourage contributions from our community.



Many thanks to our outstanding contributors:

<a href=""https://github.com/cloudposse/terraform-spacelift-cloud-infrastructure-automation/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=cloudposse/terraform-spacelift-cloud-infrastructure-automation&max=24"" />
</a>

For 🐛 bug reports & feature requests, please use the [issue tracker](https://github.com/cloudposse/terraform-spacelift-cloud-infrastructure-automation/issues).

In general, PRs are welcome. We follow the typical ""fork-and-pull"" Git workflow.
 1. Review our [Code of Conduct](https://github.com/cloudposse/terraform-spacelift-cloud-infrastructure-automation/?tab=coc-ov-file#code-of-conduct) and [Contributor Guidelines](https://github.com/cloudposse/.github/blob/main/CONTRIBUTING.md).
 2. **Fork** the repo on GitHub
 3. **Clone** the project to your own machine
 4. **Commit** changes to your own branch
 5. **Push** your work back up to your fork
 6. Submit a **Pull Request** so that we can review your changes

**NOTE:** Be sure to merge the latest changes from ""upstream"" before making a pull request!

### 🌎 Slack Community

Join our [Open Source Community](https://cpco.io/slack?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-spacelift-cloud-infrastructure-automation&utm_content=slack) on Slack. It's **FREE** for everyone! Our ""SweetOps"" community is where you get to talk with others who share a similar vision for how to rollout and manage infrastructure. This is the best place to talk shop, ask questions, solicit feedback, and work together as a community to build totally *sweet* infrastructure.

### 📰 Newsletter

Sign up for [our newsletter](https://cpco.io/newsletter?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-spacelift-cloud-infrastructure-automation&utm_content=newsletter) and join 3,000+ DevOps engineers, CTOs, and founders who get insider access to the latest DevOps trends, so you can always stay in the know.
Dropped straight into your Inbox every week — and usually a 5-minute read.

### 📆 Office Hours <a href=""https://cloudposse.com/office-hours?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-spacelift-cloud-infrastructure-automation&utm_content=office_hours""><img src=""https://img.cloudposse.com/fit-in/200x200/https://cloudposse.com/wp-content/uploads/2019/08/Powered-by-Zoom.png"" align=""right"" /></a>

[Join us every Wednesday via Zoom](https://cloudposse.com/office-hours?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-spacelift-cloud-infrastructure-automation&utm_content=office_hours) for your weekly dose of insider DevOps trends, AWS news and Terraform insights, all sourced from our SweetOps community, plus a _live Q&A_ that you can’t find anywhere else.
It's **FREE** for everyone!
## License

<a href=""https://opensource.org/licenses/Apache-2.0""><img src=""https://img.shields.io/badge/License-Apache%202.0-blue.svg?style=for-the-badge"" alt=""License""></a>

<details>
<summary>Preamble to the Apache License, Version 2.0</summary>
<br/>
<br/>

Complete license is available in the [`LICENSE`](LICENSE) file.

```text
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
""License""); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
```
</details>

## Trademarks

All other trademarks referenced herein are the property of their respective owners.


## Copyrights

Copyright © 2021-2025 [Cloud Posse, LLC](https://cloudposse.com)



<a href=""https://cloudposse.com/readme/footer/link?utm_source=github&utm_medium=readme&utm_campaign=cloudposse/terraform-spacelift-cloud-infrastructure-automation&utm_content=readme_footer_link""><img alt=""README footer"" src=""https://cloudposse.com/readme/footer/img""/></a>

<img alt=""Beacon"" width=""0"" src=""https://ga-beacon.cloudposse.com/UA-76589703-4/cloudposse/terraform-spacelift-cloud-infrastructure-automation?pixel&cs=github&cm=readme&an=terraform-spacelift-cloud-infrastructure-automation""/>",VRAI
cloudwu/stellaris_cn,Application System,Application System,2025-05-15T18:05:57Z,2024-02-15T00:07:37Z,0,0,0,0,0,29,0,0,2016-07-29T15:50:03Z,2025-03-14T16:31:18Z,71075,391,Lua,VRAI,152,FAUX,7,,7,Stellaris 群星 汉化 Mod,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,56,"群星中文校对 Mod
============
当前支持游戏版本： 群星 Stellaris 3.13 。

你可以直接在创意工坊订阅本 mod:
http://steamcommunity.com/sharedfiles/filedetails/?id=734129025

本项目目录结构：

* cn 与 cn.mod 是最新的汉化 mod 文件。 
* en 是当前英文原文文本。
* simp_chinese 是最新版本的官方中文。
* term_list.txt 是术语列表，参加翻译时请参考这里统一术语。
* script 是一些实用工具脚本，用来检查错误。
* [workflow.md](https://github.com/cloudwu/stellaris_cn/blob/master/workflow.md)  想参与的同学，如果不熟悉 github 的工作流，可以参考。

### 本汉化 Mod 引用官方中文字体，如果需要其它字体可以单独订阅 字体 mod :

* 悦黑 http://steamcommunity.com/workshop/filedetails/?id=784471286  （群星汉化最早使用的字体）
* 粗圆 http://steamcommunity.com/workshop/filedetails/?id=826928128  （低分辨率下可能更清晰一些）
* 创意工坊中搜索关键字 `font` 查找其他字体mod。

### License

* 本文本的的原文版权属于 Paradox Interactive ( https://www.paradoxplaza.com/ ) 
* 翻译文本基于 CREATIVE COMMONS BY-SA 创作 ( https://creativecommons.org/licenses/by-sa/3.0/deed.zh ) 转载请附上 github 仓库地址：https://github.com/cloudwu/stellaris_cn",FAUX
cncf/devstats,Documentations,Application System,2025-05-14T08:46:50Z,2025-05-05T07:07:05Z,0,0,0,0,0,2,0,0,2023-03-30T07:49:21Z,2025-04-08T08:50:21Z,25165,88,Shell,VRAI,31,FAUX,8,,8,📈CNCF-created tool for analyzing and graphing developer contributions,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,11,"[![Build Status](https://travis-ci.org/cncf/devstats.svg?branch=master)](https://travis-ci.org/cncf/devstats)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1357/badge)](https://bestpractices.coreinfrastructure.org/projects/1357)

# GitHub archives and git Grafana visualization dashboards

Authors: Łukasz Gryglicki <lgryglicki@cncf.io>, Justyna Gryglicka <jgryglicka@cncf.io>.

This is a toolset to visualize GitHub [archives](https://www.gharchive.org) using Grafana dashboards.

GHA2DB stands for **G**it**H**ub **A**rchives to **D**ash**B**oards.

More information about Kubernetes dashboards [here](https://github.com/cncf/devstats/blob/master/README_K8s.md).


# Kubernetes and Helm

Please see [example Helm chart](https://github.com/cncf/devstats-helm-example) for an example Helm deployment.

Please see [Helm chart](https://github.com/cncf/devstats-helm) for a full Helm deployment.

Please see [LF Helm chart](https://github.com/cncf/devstats-helm-lf) for the LF Helm deployment (it is a data deployment, has no Grafana).

Please see [GraphQL Helm chart](https://github.com/cncf/devstats-helm-graphql) for GraphQL foundation DevStats deployment.

Please see [Kubernetes dashboard](https://github.com/cncf/devstats-kubernetes-dashboard) if you want to enable a local dashboard to explore the cluster state.

Please see [bare metal example](https://github.com/cncf/devstats-example) to see an example of bare metal deployment.

The rest of this document describes the current bare metal deployment on metal.equinix.com used by CNCF projects.


# Presentations

- Presentations are available [here](https://github.com/cncf/devstats/blob/master/docs/presentation).
- Direct [link](https://docs.google.com/presentation/d/1v5zuSFQkwcthWXgS2p9vs9x5e4fnavMR8HdykS7aWYA/edit?usp=sharing).
- Another direct [link](https://docs.google.com/presentation/d/1LLv4kio_KGP36cjkpeSMHZrNl0IYJ7B2pKGU0aHWqx8/edit?usp=sharing).


# Talks

- [Who What How: Understanding Kubernetes Development through DevStats](https://www.youtube.com/watch?v=D3CMuxQymR8).
- [A Kubernetes Application End-to-End: DevStats](https://www.youtube.com/watch?v=U2PTifzzKNE&t=58s).


# Architecture

DevStats is deployed using [Helm](https://helm.sh) on [Kubernetes](https://kubernetes.io) running on bare metal servers provided by [Equinix](https://www.equinix.com).

DevStats is written in [Go](https://go.dev), it uses [GitHub archives](https://www.gharchive.org), [GitHub API](https://docs.github.com/en/rest) and [git](https://git-scm.com) as its main data sources.

Under the hood, DevStats uses the following CNCF projects:
- Helm (for deployment).
- containerd (as a Kubernetes container runtime, CRI).
- cert-manager (for HTTPS/SSL certificates).
- OpenEBS (for local storage volumes support).
- MetalLB (as a load balancer for bare metal servers).
- CoreDNS (Kubernetes internal DNS).

And other projects, including:
- Equinix (bare metal servers provider).
- Ubuntu (containers base operating system).
- kubeadm (for installing Kubernetes).
- NFS (for shared write network volumes support).
- NGINX (for ingress).
- Calico (as networking for Kubernetes, CNI).
- Golang (DevStats is written in Go).
- PostgreSQL (DevStats database is Postgres).
- patroni (HA deployment of PostgreSQL database, tweaked for DevStats).
- GitHub archives (main data source).
- GitHub API (data source).
- git (data source).
- Grafana (UI).
- Let's Encrypt (provides HTTPS/SSL certificates).
- Travis CI (continuous integration & testing).

Please check [this](https://github.com/cncf/devstats-helm#architecture) for a detailed architecture description.


# Deploying on your own project(s)

See the simple [DevStats example](https://github.com/cncf/devstats-example) repository for single project deployment (Homebrew), follow [instructions](https://github.com/cncf/devstats-example/blob/master/SETUP_OTHER_PROJECT.md) to deploy for your own project.

# Goal

We want to create a toolset for visualizing various metrics for the Kubernetes community (and also for all CNCF projects).

Everything is open source so that it can be used by other CNCF and non-CNCF open source projects.

The only requirement is that project must be hosted on a public GitHub repository/repositories.

# Data hiding

If you want to hide your data (replace with anon-#) please follow the instructions [here](https://github.com/cncf/devstats/blob/master/HIDE_DATA.md).

# Forking and installing locally

This toolset uses only Open Source tools: GitHub archives, GitHub API, git, Postgres databases, and multiple Grafana instances.
It is written in Go and can be forked and installed by anyone.

Contributions and PRs are welcome.
If you see a bug or want to add a new metric please create an [issue](https://github.com/cncf/devstats/issues) and/or [PR](https://github.com/cncf/devstats/pulls).

To work on this project locally please fork the original [repository](https://github.com/cncf/devstats), and:
- [Compiling and running on Linux Ubuntu 18 LTS](./INSTALL_UBUNTU18.md).
- [Compiling and running on Linux Ubuntu 17](./INSTALL_UBUNTU17.md).
- [Compiling and running on Linux Ubuntu 16 LTS](./INSTALL_UBUNTU16.md).
- [Compiling and running on macOS](./INSTALL_MAC.md).
- [Compiling and running on FreeBSD](./INSTALL_FREEBSD.md).

Please see [Development](https://github.com/cncf/devstats/blob/master/DEVELOPMENT.md) for local development guide.

For more detailed description of all environment variables, tools, switches, etc, please see [Usage](https://github.com/cncf/devstats/blob/master/USAGE.md).

# Metrics

We want to support all kinds of metrics, including historical ones.
Please see [requested metrics](https://docs.google.com/document/d/1o5ncrY6lVX3qSNJGWtJXx2aAC2MEqSjnML4VJDrNpmE/edit?usp=sharing) to see what kind of metrics are needed.
Many of them cannot be computed based on the data sources currently used.

# Repository groups

There are some groups of repositories that are grouped together as a repository groups.
They are defined in [scripts/kubernetes/repo_groups.sql](https://github.com/cncf/devstats/blob/master/scripts/kubernetes/repo_groups.sql).

To setup default repository groups:
- `PG_PASS=pwd ./kubernetes/setup_repo_groups.sh`.

This is a part of `kubernetes/psql.sh` script and [kubernetes psql dump](https://devstats.cncf.io/backups/gha.dump) already has groups configured.

In an [All CNCF project](https://all.teststats.cncf.io) repository groups are mapped to individual CNCF projects [scripts/all/repo_groups.sql](https://github.com/cncf/devstats/blob/master/scripts/all/repo_groups.sql):

# Company Affiliations

We also want to have per company statistics. To implement such metrics we need a mapping of developers and their employers.

There is a project that attempts to create such mapping [cncf/gitdm](https://github.com/cncf/gitdm).

DevStats has an import tool that fetches company affiliations from `cncf/gitdm` and allows to create per company metrics/statistics. It also uses `companies.yaml` file to map company acquisitions (any data generated by a company acquired by another company is assigned to the latter using a mapping from `companies.yaml`).

If you see errors in the company affiliations, please open a pull request on [cncf/gitdm](https://github.com/cncf/gitdm) and the updates will be reflected on [https://k8s.devstats.cncf.io](https://k8s.devstats.cncf.io) a couple of days after the PR has been accepted. Note that gitdm supports mapping based on dates, to account for developers moving between companies.

New affiliations are imported into DevStats about 1-2 times/month.

# Architecture

For architecture details please see [architecture](https://github.com/cncf/devstats/blob/master/ARCHITECTURE.md) file.

Detailed usage is [here](https://github.com/cncf/devstats/blob/master/USAGE.md)

# Adding new metrics

Please see [metrics](https://github.com/cncf/devstats/blob/master/METRICS.md) to see how to add new metrics.

# Adding new projects

To add a new project on a bare metal deployment follow [adding new project](https://github.com/cncf/devstats/blob/master/ADDING_NEW_PROJECT.md) instructions.

See `cncf/devstats-helm`:`ADDING_NEW_PROJECTS.md` for information about how to add more projects on Kubernetes/Helm deployment.

# Grafana dashboards

Please see [dashboards](https://github.com/cncf/devstats/blob/master/DASHBOARDS.md) to see a list of already defined Grafana dashboards.

# Exporting data

Please see [exporting](https://github.com/cncf/devstats/blob/master/EXPORT.md).

# Detailed Usage instructions

- [USAGE](https://github.com/cncf/devstats/blob/master/USAGE.md)

# Servers

The servers to run `devstats` are generously provided by [Equinix](https://metal.equinix.com) bare metal hosting as part of CNCF's [Community Infrastructure Lab](https://github.com/cncf/cluster).

# One line run all projects

- Use `GHA2DB_PROJECTS_OVERRIDE=""+cncf"" PG_PASS=pwd devstats`.
- Or add this command using `crontab -e` to run every hour HH:08.

# Checking projects activity

- Use: `PG_PASS=... PG_DB=allprj ./devel/activity.sh '1 month,,' > all.txt`.
- Example results [here](https://teststats.cncf.io) - all CNCF project activity during January 2018, excluding bots.


# Project moving to a new GitHub organization

Please check `NEW_ORG.md`.


# Troubleshooting

If you see error like this `pq: row is too big: size 8192, maximum size 8160` and/or `Error result for xyz (took 11m52.048191357s)`:

- Shell into logging database and check:
- Run on DevStats node: `k exec -itn devstats-prod devstats-postgres-0 -- psql devstats`.
- Run while on `devstats` database: `select dt, run_dt, msg from gha_logs where msg like '%Error result for%';`.
```
             dt             |           run_dt           |                                  msg
----------------------------+----------------------------+-----------------------------------------------------------------------
 2024-09-01 00:48:07.079436 | 2024-09-01 00:34:26.426402 | Error result for helm (took 13m36.712884455s): exit status 2
 2024-09-07 00:16:11.132541 | 2024-09-07 00:04:14.051939 | Error result for prometheus (took 11m52.048191357s): exit status 2
 2024-09-07 00:26:43.701404 | 2024-09-07 00:05:55.08925  | Error result for fluentd (took 15m1.328366817s): exit status 2
 2024-09-07 00:16:11.038887 | 2024-09-07 00:08:43.846938 | Error result for grpc (took 7m24.348182232s): exit status 2
 2024-09-03 13:20:02.682134 | 2024-09-03 12:57:23.220227 | Error result for opentelemetry (took 22m29.324614973s): exit status 2
 2024-09-03 13:09:56.535074 | 2024-09-03 13:04:43.451026 | Error result for spinnaker (took 5m7.631109092s): exit status 2
(6 rows)
```
- You can investigate each via: `` echo ""select dt, prog, proj, msg from gha_logs where run_dt = '2024-09-01 00:34:26.426402';"" | k exec -itn devstats-prod devstats-postgres-1 -- psql devstats > log.txt ``.
- Or: `` select distinct proj from gha_logs where msg like '%row is too big%'; ``.
- Eventually check: `` vim logs_prod.txt logs_test.txt ``.
- `row is too big` is usually caused by metric: `suser_activity`. You can add this metric to `./devel/test_metrics.yaml` and generate devstats docker images to reinitialize it for given project(s) via:
- `helm install --generate-name ./devstats-helm --set namespace='devstats-prod',skipSecrets=1,skipPVs=1,skipBackupsPV=1,skipVacuum=1,skipBackups=1,skipBootstrap=1,skipCrons=1,skipAffiliations=1,skipGrafanas=1,skipServices=1,skipPostgres=1,skipIngress=1,skipStatic=1,skipAPI=1,skipNamespaces=1,testServer='',prodServer='1',provisionImage='lukaszgryglicki/devstats-prod',provisionCommand='./devstats-helm/add_metric.sh',nCPUs=8,indexProvisionsFrom=N,indexProvisionsTo=M`.",VRAI
cncf/landscape2-sites,Application System,Documentations,2025-05-01T10:43:03Z,2025-01-08T07:25:14Z,0,0,0,0,0,0,0,0,2023-08-09T10:04:21Z,2025-04-04T11:59:58Z,377,31,,VRAI,18,FAUX,1,,1,Landscape2 sites settings and deployment workflows,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,14,"# Landscape2 sites

This repository contains the settings and deployment workflows of some foundations' landscapes.

| Foundation                                                                                                                                                                                                                                           | Landscape url                                |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| [![ADA build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/ada-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/ada-build-and-deploy.yml)                            | <https://ada.landscape2.io>                  |
| [![AOUSD build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/aousd-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/aousd-build-and-deploy.yml)                      | <https://landscape.aousd.org>                |
| [![ASWF build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/aswf-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/aswf-build-and-deploy.yml)                         | <https://aswf.landscape2.io>                 |
| [![CAMARA build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/camara-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/camara-build-and-deploy.yml)                   | <https://camara.landscape2.io>               |
| [![CCC build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/ccc-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/ccc-build-and-deploy.yml)                            | <https://ccc.landscape2.io>                  |
| [![CDF build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/cdf-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/cdf-build-and-deploy.yml)                            | <https://cdf.landscape2.io>                  |
| [![CIP build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/cip-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/cip-build-and-deploy.yml)                            | <https://cip.landscape2.io>                  |
| [![CNCF build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/cncf-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/cncf-build-and-deploy.yml)                         | <https://landscape.cncf.io>                  |
| [![DLT build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/dlt-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/dlt-build-and-deploy.yml)                            | <https://landscape.lfdecentralizedtrust.org> |
| [![FINOS build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/finos-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/finos-build-and-deploy.yml)                      | <https://finos.landscape2.io>                |
| [![GraphQL build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/graphql-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/graphql-build-and-deploy.yml)                | <https://graphql.landscape2.io>              |
| [![LF build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/lf-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/lf-build-and-deploy.yml)                               | <https://landscape.linuxfoundation.org>      |
| [![LFAI build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/lfai-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/lfai-build-and-deploy.yml)                         | <https://lfai.landscape2.io>                 |
| [![LFEdge build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/lfedge-build-and-deploy.yml/badge.svg)](https://github.com/cncf/landscape2-sites/actions/workflows/lfedge-build-and-deploy.yml)                               | <https://lfedge.landscape2.io>               |
| [![LFEnergy build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/lfenergy-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/lfenergy-build-and-deploy.yml)             | <https://lfenergy.landscape2.io>             |
| [![LFNetworking build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/lfnetworking-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/lfnetworking-build-and-deploy.yml) | <https://lfnetworking.landscape2.io>         |
| [![OMP build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/omp-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/omp-build-and-deploy.yml)                            | <https://omp.landscape2.io>                  |
| [![O3DF build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/o3df-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/o3df-build-and-deploy.yml)                         | <https://landscape.o3df.org>                 |
| [![OAI build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/oai-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/oai-build-and-deploy.yml)                            | <https://landscape.openapis.org>             |
| [![OpenSSF build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/openssf-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/openssf-build-and-deploy.yml)                | <https://openssf.landscape2.io>              |
| [![OSPO build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/ospo-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/ospo-build-and-deploy.yml)                         | <https://landscape.todogroup.org>            |
| [![Overture Maps build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/overturemaps-build-and-deploy.yml/badge.svg)](https://github.com/cncf/landscape2-sites/actions/workflows/overturemaps-build-and-deploy.yml)            | <https://landscape.overturemaps.org>         |
| [![PQCA build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/pqca-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/pqca-build-and-deploy.yml)                         | <https://landscape.pqca.org>                 |
| [![Presto build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/presto-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/presto-build-and-deploy.yml)                   | <https://presto.landscape2.io>               |
| [![PyTorch build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/pytorch-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/pytorch-build-and-deploy.yml)                | <https://pytorch.landscape2.io>              |
| [![SPDX build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/spdx-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/spdx-build-and-deploy.yml)                         | <https://spdx.landscape2.io>                 |
| [![RISCV build and deploy](https://github.com/cncf/landscape2-sites/actions/workflows/riscv-build-and-deploy.yml/badge.svg?branch=main)](https://github.com/cncf/landscape2-sites/actions/workflows/riscv-build-and-deploy.yml)                      | <https://riscv.landscape2.io>                |

## Contributing

Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for more details.

## Code of Conduct

This project follows the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md).

## License

Landscape is an Open Source project licensed under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).",VRAI
commercialhaskell/all-cabal-metadata,Documentations,Documentations,2025-05-16T01:01:58Z,2025-05-13T21:49:45Z,0,0,0,0,0,2,0,0,2015-05-12T21:40:27Z,2025-04-08T12:03:15Z,172139,9,Shell,VRAI,8,FAUX,0,,0,Current metadata for all cabal files,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,3,"# all-cabal-metadata

Current metadata for all cabal files on Hackage. Generated using the
[all-cabal-tool](https://github.com/commercialhaskell/all-cabal-tool)
package.",FAUX
ComradDoge/DogeEU4,Application System,Application System,2025-04-12T11:52:05Z,2025-03-01T18:01:28Z,0,0,0,0,0,14,0,0,2020-06-10T01:32:01Z,2025-03-22T18:58:42Z,432317,5,Lua,VRAI,10,FAUX,0,,0,Doge Shattered Mod,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,8,"# DOGE SHATTERED EUROPA
- [The Steam Workshop](https://steamcommunity.com/sharedfiles/filedetails/?id=2152606065)

- Version: 1.37.x
- Checksum: ?

<h1>DD/MM/YYYY</h1>

- Hormuz no longer starts with a selected government reform.
- Portugal will actually stop pestering you to sell a province.
- Georgian Crisis disaster no longer appears in shattered bookmark.
- Added a small mission tree for 4 egyptian minors to have something until you form a country with missions.
- African pagans now have access to Syncretic mechanic, allowing them to syncratize with another religion of their choice.
- Japan missions tweaked so that there is a third button that will allow you to access an older mission tree alongside others, if applicable.

 <!-- Todo -->
<br/>
<img src=https://i.imgur.com/F14PpEA.png/>

The mod is a general overhaul of the game, focusing on a shattered setting, with lots of content in government reforms; formables; religions; trade and much more. While also ensuring a balanced game for both single-player and multi-player and fixing various issues whether it be AI or lack of QoL.

<br/>
<img src=https://i.imgur.com/jIkgNsx.png/>

The possible new formables, advisors, religions and bits of information are on this [Wiki](https://eu4.paradoxwikis.com/Doge_Shattered_Europa)

- AI Improvements
- 50 New Trade Goods
- 50+ New Formables
- 30+ New Idea Groups
- 75+ New Monuments
- 2 New Ages
- 15 New Advisors
- 30 New Religions
- Upgradable State Edicts
- Policies reworked from ground up
- More Government Reforms and missions
- Quality of Life Improvements
- 30 Religion specific Idea groups
- Vastly expanded estate system
- Dynamic Age objectives
- Changeable trade goods
- Open/Close Canals

<br/>

<br/><br/>",FAUX
concourse/concourse,Toolkit,DevOPs,2025-05-14T00:31:51Z,2025-04-06T17:04:48Z,0,1,0,0,0,0,0,0,2014-04-19T20:45:45Z,2025-04-07T14:26:10Z,184131,7528,Go,VRAI,854,FAUX,142,"ci,ci-cd,concourse,containerd,containers,continuous-delivery,continuous-integration,elm,go,pipelines,runc",142,Concourse is a container-based automation system written in Go.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,559,"# Concourse: the continuous thing-doer

[![Discord](https://img.shields.io/discord/219899946617274369.svg?label=&logo=discord&logoColor=ffffff&color=7389D8&labelColor=6A7EC2)][discord]
[![Build](https://ci.concourse-ci.org/api/v1/teams/main/pipelines/concourse/badge)](https://ci.concourse-ci.org/teams/main/pipelines/concourse)
[![Contributors](https://img.shields.io/github/contributors/concourse/concourse)](https://github.com/concourse/concourse/graphs/contributors)
[![Help Wanted](https://img.shields.io/github/labels/concourse/concourse/help%20wanted)](https://github.com/concourse/concourse/labels/help%20wanted)

Concourse is an automation system written in Go. It is most commonly used for
CI/CD, and is built to scale to any kind of automation pipeline, from simple to
complex.

![booklit pipeline](screenshots/booklit-pipeline.png)

Concourse is very opinionated about a few things: idempotency, immutability,
declarative config, stateless workers, and reproducible builds.

<!--- Leaving this here for now my for my own reference
## The road to Concourse v10

[Concourse v10][v10] is the code name for a set of features which, when used
in combination, will have a massive impact on Concourse's capabilities as a
generic continuous thing-doer. These features, and how they interact, are
described in detail in the [Core roadmap: towards v10][v10] and [Re-inventing
resource types][prototypes] blog posts. (These posts are *slightly* out of
date, but they get the idea across.)

Notably, **v10 will make Concourse not suck for multi-branch and/or
pull-request driven workflows** - examples of *spatial* change, where the set
of things to automate grows and shrinks over time.

Because v10 is really an alias for a ton of separate features, there's a lot
to keep track of - here's an overview:

| Feature                  | RFC              | Status |
| ------------------------ | ---------------- | ------ |
| `set_pipeline` step      | ✔ [#31][rfc-31]  | ✔ v5.8.0 (experimental) |
| Var sources for creds    | ✔ [#39][rfc-39]  | ✔ v5.8.0 (experimental), TODO: [#5813][issue-5813] |
| Archiving pipelines      | ✔ [#33][rfc-33]  | ✔ v6.5.0 |
| Instanced pipelines      | ✔ [#34][rfc-34]  | ✔ v7.0.0 (experimental) |
| Static `across` step     | 🚧 [#29][rfc-29] | ✔ v6.5.0 (experimental) |
| Dynamic `across` step    | 🚧 [#29][rfc-29] | ✔ v7.4.0 (experimental, not released yet) |
| Projects                 | 🚧 [#32][rfc-32] | 🙏 RFC needs feedback! |
| `load_var` step          | ✔ [#27][rfc-27]  | ✔ v6.0.0 (experimental) |
| `get_var` step           | ✔ [#27][rfc-27]  | 🚧 [#5815][issue-5815] in progress! |
| [Prototypes][prototypes] | ✔ [#37][rfc-37]  | ⚠ Pending first use of protocol (any of the below) |
| `run` step               | 🚧 [#37][rfc-37]  | ⚠ Pending its own RFC, but feel free to experiment |
| Resource prototypes      | ✔ [#38][rfc-38]  | 🙏 [#5870][issue-5870] looking for volunteers! |
| Var source prototypes    |                  | 🚧 [#6275][issue-6275] planned, may lead to RFC |
| Notifier prototypes      | 🚧 [#28][rfc-28] | ⚠ RFC not ready |

The Concourse team at VMware will be working on these features, however in the
interest of growing a healthy community of contributors we would really
appreciate any volunteers. This roadmap is very easy to parallelize, as it is
comprised of many orthogonal features, so the faster we can power through it,
the faster we can all benefit. We want these for our own pipelines too! 😆

If you'd like to get involved, hop in [Discord][discord] or leave a comment on
any of the issues linked above so we can coordinate. We're more than happy to
help figure things out or pick up any work that you don't feel comfortable
doing (e.g. UI, unfamiliar parts, etc.).

Thanks to everyone who has contributed so far, whether in code or in the
community, and thanks to everyone for their patience while we figure out how to
support such common functionality the ""Concoursey way!"" 🙏

[issue-5813]: https://github.com/concourse/concourse/issues/5813
[issue-5814]: https://github.com/concourse/concourse/issues/5814
[issue-5815]: https://github.com/concourse/concourse/issues/5815
[issue-5870]: https://github.com/concourse/concourse/issues/5870
[issue-5921]: https://github.com/concourse/concourse/issues/5921
[issue-6275]: https://github.com/concourse/concourse/issues/6275
[pr-5896]: https://github.com/concourse/concourse/pull/5896
[rfc-27]: https://github.com/concourse/rfcs/blob/master/027-var-steps/proposal.md
[rfc-28]: https://github.com/concourse/rfcs/pull/28
[rfc-29]: https://github.com/concourse/rfcs/pull/29
[rfc-31]: https://github.com/concourse/rfcs/blob/master/031-set-pipeline-step/proposal.md
[rfc-32]: https://github.com/concourse/rfcs/pull/32
[rfc-33]: https://github.com/concourse/rfcs/blob/master/033-archiving-pipelines/proposal.md
[rfc-34]: https://github.com/concourse/rfcs/blob/master/034-instanced-pipelines/proposal.md
[rfc-37]: https://github.com/concourse/rfcs/blob/master/037-prototypes/proposal.md
[rfc-38]: https://github.com/concourse/rfcs/blob/master/038-resource-prototypes/proposal.md
[rfc-39]: https://github.com/concourse/rfcs/blob/master/039-var-sources/proposal.md

[v10]: https://blog.concourse-ci.org/2019/07/17/core-roadmap-towards-v10.html
[prototypes]: https://blog.concourse-ci.org/2019/10/15/reinventing-resource-types.html
--->

## Installation

Concourse is distributed as a single `concourse` binary, available on the [Releases page](https://github.com/concourse/concourse/releases/latest).

If you want to just kick the tires, jump ahead to the [Quick Start](#quick-start).

In addition to the `concourse` binary, there are a few other supported formats.
Consult their GitHub repos for more information:

* [Docker image](https://github.com/concourse/concourse-docker)
* [BOSH release](https://github.com/concourse/concourse-bosh-release)
* [Kubernetes Helm chart](https://github.com/concourse/concourse-chart)


## Quick Start

```sh
$ wget https://concourse-ci.org/docker-compose.yml
$ docker-compose up -d
Creating docs_concourse-db_1 ... done
Creating docs_concourse_1    ... done
```

Concourse will be running at [localhost:8080](http://localhost:8080). You can
log in with the username/password as `test`/`test`.

> :warning: **If you are using an M-series mac**, note that they are
> incompatible with the `containerd` runtime until
> [#1379](https://github.com/concourse/concourse/issues/1379) is resolved.
> After downloading the docker-compose file, change:
>
> `CONCOURSE_WORKER_RUNTIME: ""containerd""` to
>
> `CONCOURSE_WORKER_RUNTIME: ""houdini""`.

Next, install `fly` by downloading it from the web UI and target your local
Concourse as the `test` user:

```sh
$ fly -t ci login -c http://127.0.0.1:8080 -u test -p test
logging in to team 'main'

target saved
```

You can follow our [Getting Started Tutorial](https://concourse-ci.org/getting-started.html)
to learn how to write Concourse pipelines.

### Configuring a Pipeline

Concourse has no GUI for configuration. Instead, pipelines are defined in
declarative YAML files:

```yaml
resources:
- name: booklit
  type: git
  source: {uri: ""https://github.com/vito/booklit""}

jobs:
- name: unit
  plan:
  - get: booklit
    trigger: true
  - task: test
    file: booklit/ci/test.yml
```

Most operations are done via the accompanying `fly` CLI. If you've got Concourse
[installed](https://concourse-ci.org/install.html), try saving the above example
as `booklit.yml`, [target your Concourse
instance](https://concourse-ci.org/fly.html#fly-login), and then run:

```sh
fly -t ci set-pipeline -p booklit -c booklit.yml
```

These pipeline files are self-contained, making them easily portable between
Concourse instances.


### Learn More

* The [Official Site](https://concourse-ci.org) for documentation,
  reference material, and example pipelines.
* The [Getting Started Tutorial](https://concourse-ci.org/getting-started.html)
  to learn the basics of pipelines
* See Concourse in action with our [own pipelines](https://ci.concourse-ci.org/)
* Hang around in the [GitHub discussions](https://github.com/concourse/concourse/discussions) or in
  [Discord][discord]
* See what we're working on the [project board](https://github.com/orgs/concourse/projects). 


## Contributing

Our user base is basically everyone that develops software (and wants it to
work).

It's a lot of work, and we need your help! If you're interested, check out our
[contributing docs](CONTRIBUTING.md).

[discord]: https://discord.gg/MeRxXKW",VRAI
confidential-containers/cloud-api-adaptor,Toolkit,DevOPs,2025-05-13T15:40:22Z,2025-01-23T12:03:08Z,0,0,0,0,0,0,0,0,2022-02-14T14:01:17Z,2025-04-02T11:12:57Z,28827,55,Go,VRAI,100,FAUX,201,,201,Ability to create Kata pods using cloud provider APIs aka the peer-pods approach,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,63,,VRAI
confidential-containers/trustee,Toolkit,DevOPs,2025-05-15T17:14:56Z,2025-04-01T18:56:48Z,0,19,0,0,0,0,0,0,2022-04-25T12:45:02Z,2025-04-07T06:10:29Z,2120,90,Rust,VRAI,105,FAUX,97,"attestation,confidential-computing,key-management",97,Attestation and Secret Delivery Components,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,52,"# Trusted Components for Attestation and Secret Management

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fconfidential-containers%2Fkbs.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Fconfidential-containers%2Fkbs?ref=badge_shield)

This repository contains tools and components for attesting confidential guests and providing secrets to them.
Collectively, these components are known as Trustee.
Trustee typically operates on behalf of the guest owner and interact remotely with
[guest components](https://github.com/confidential-containers/guest-components).

Trustee was developed for the Confidential Containers project, but can be used with a wide variety
of applications and hardware platforms.

## Components

- [Key Broker Service](kbs)
The KBS is a server that facilitates remote attestation and secret delivery.
Its role is similar to that of the [Relying Party](https://www.ietf.org/archive/id/draft-ietf-rats-architecture-22.html#name-relying-party)
in the RATS model.

- [Attestation Service](attestation-service)
The AS verifies TEE evidence.
In the RATS model this is a [Verifier](https://www.ietf.org/archive/id/draft-ietf-rats-architecture-22.html#name-verifier)

- [Reference Value Provider Service](rvps)
The RVPS manages reference values used to verify TEE evidence.
This is related to the discussion in [section 7.5](https://www.ietf.org/archive/id/draft-ietf-rats-architecture-22.html#name-endorser-reference-value-pr)
of the RATS document.

- [KBS Client Tool](tools/kbs-client/)
This is a simple tool which can be used to test or configure the KBS and AS.

For further information, see documentation of individual components.

## Architecture

Trustee is flexible and can be deployed in several different configurations.
This figure shows one common way to deploy these components in conjunction with certain guest components.

```mermaid
flowchart LR
    AA -- attests guest ----> KBS
    CDH -- requests resource --> KBS
    subgraph Guest
        CDH <.-> AA
    end
    subgraph Trustee
        AS -- verifies evidence --> KBS
        RVPS -- provides reference values--> AS
    end
    client-tool -- configures --> KBS
```
## Deployment

There are two main ways to deploy Trustee.

### Docker Compose

One simple way to get started with Trustee is with Docker compose, which can be used
to quickly setup a cluster matching the diagram above.

Please refer to the [cluster setup guide](kbs/docs/cluster.md).

This cluster could be run inside a VM or as part of a managed service.

### Kubernetes

There are two supported ways of deploying Trustee on Kubernetes.
One is via the [KBS Operator](https://github.com/confidential-containers/kbs-operator),
which deploys the KBS components. The second option is to use the KBS'
provided Kubernetes tooling [here](kbs/config/kubernetes).

## License
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fconfidential-containers%2Fkbs.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Fconfidential-containers%2Fkbs?ref=badge_large)",VRAI
ContainerSSH/examples,Documentations,Documentations,2025-04-03T22:55:53Z,2021-03-16T06:29:55Z,0,2,0,0,0,0,0,0,2021-03-16T06:19:02Z,2025-04-03T22:55:58Z,39,16,Shell,VRAI,15,FAUX,3,containerssh,3,ContainerSSH examples,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,9,"[![ContainerSSH - Launch Containers on Demand](https://containerssh.github.io/images/logo-for-embedding.svg)](https://containerssh.io/)

<!--suppress HtmlDeprecatedAttribute -->
<h1 align=""center"">ContainerSSH examples</h1>

This repository contains examples on how to use ContainerSSH in certain scenarios. Feel free to use these to build your own ContainerSSH setup.

<center><strong>⚠️⚠️⚠️ These examples are not production-ready! Please read the <a href=""https://containerssh.io"">ContainerSSH documentation</a>. ⚠️⚠️⚠️</strong></center><br /><br />

| Example | Description |
|---------|-------------|
| [Quick start](quick-start/) | A simple docker-compose example on how to get ContainerSSH running. |
| [Logging to the ELK stack with Docker and Fluentd](logging-elk-stack/) | An example how to get ContainerSSH logs to an ELK stack when running inside Docker |",FAUX
controlplaneio/simulator,Toolkit,Application System,2024-06-03T10:54:35Z,2023-12-16T14:42:32Z,0,0,0,0,10,0,0,0,2019-06-03T18:21:40Z,2025-04-07T10:36:07Z,16186,946,Python,VRAI,100,FAUX,19,,19,Kubernetes Security Training Platform - focusing on security mitigation,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,24,"[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/controlplaneio/simulator/blob/master/LICENSE)
[![Platforms](https://img.shields.io/badge/Platform-Linux|MacOS-blue.svg)](https://github.com/controlplaneio/simulator/blob/master/README.md)
[![Conventional Commits](https://img.shields.io/badge/Conventional%20Commits-1.0.0-blue.svg)](https://conventionalcommits.org)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://github.com/controlplaneio/simulator/graphs/commit-activity)

# Simulator

_🔊 Join [the hosted Simulator waitlist](https://kubesim.io) for private scenarios and training content 🚨_

A distributed systems and infrastructure simulator for attacking and debugging Kubernetes: <code>simulator</code>
creates a Kubernetes cluster for you in your AWS account; runs scenarios which misconfigure it and/or leave it
vulnerable to compromise and trains you in mitigating against these vulnerabilities.

## Prerequisites

- [Docker](https://docs.docker.com/get-docker/)
- [AWS Account](https://aws.amazon.com/free)

## Download

Please download the latest release from the [releases page](https://github.com/controlplaneio/simulator/releases)

## AWS Credentials

Simulator supports the following methods of authentication to provision the AWS infrastructure. Refer to
[AWS IAM Permissions](docs/aws-iam-permissions.md) for details of the required permissions.

- [Environment Variables](https://docs.aws.amazon.com/sdkref/latest/guide/environment-variables.html)
- [Shared Credentials file](https://docs.aws.amazon.com/sdkref/latest/guide/file-format.html)

## Getting Started

- Read the [Player Guide](docs/player-guide.md) to learn how to launch environments, deploy, and play scenarios
- Refer to the [Walkthough Guides](docs/2023-cncf-ctf-walkthroughs) for advice

## Core Components

- [Simulator CLI](docs/cli.md)
- [Simulator Container Images](docs/container-images.md)
- [Simulator AMIs](docs/amis.md)
- [Simulator Infrastructure](docs/infrastructure.md)
- [Simulator Scenarios](docs/scenarios.md)

## More Info

For details on why we created this project, take a look at the [vision statement](./docs/vision-statement.md) and [V2 redesign](/docs/vision-statement-v2.md).

---

Built with ❤ by [https://control-plane.io/](https://control-plane.io/)",VRAI
couchbase/perfrunner,DevOPs,Toolkit,2025-05-05T15:55:24Z,2025-02-21T18:15:16Z,0,0,0,0,2,0,0,0,2013-09-12T15:17:58Z,2025-04-07T20:57:16Z,91073,20,Python,FAUX,40,FAUX,13,,13,Performance TAF for Couchbase Server,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,61,"perfrunner
------------

[![codebeat badge](https://codebeat.co/badges/7870f2d2-4a41-477e-af30-d9a8cf097626)](https://codebeat.co/projects/github-com-couchbase-perfrunner)

Installation
------------
### Using Docker
For local testing and development, you can use docker. See details in [docker directory](docker/README.md).

### Installing on bare metal
Before using perfrunner you should install the requirements listed in the requirements section. At a minimum you need

* Python 3.9
* virtualenv
* libcouchbase
* libcurl4-gnutls-dev
* libffi
* libsnappy
* libssl

## Preparing workers and servers

You should be able to install both client and server system dependencies using the Ansible playbooks (namely, clients.yml and servers.yml).

    ansible-playbook ${playbook} -i ${ini_file}

For instance:

    ansible-playbook  playbooks/servers.yml -i clusters/vesta.ini

First clone the perfrunner repo with the command below:

    git clone https://github.com/couchbase/perfrunner.git

As some components are written in Go, make sure that perfrunner is placed inside $GOPATH/src.
See also [How to Write Go Code](https://golang.org/doc/code.html).

Once inside the perfrunner directory create a virtual environment for all of the perfrunner dependencies and install all of the dependencies so that you can run perfrunner:

    make


Cluster installation and setup
------------------------------

    env/bin/install -c ${cluster} -v ${version}
    env/bin/cluster -c ${cluster} -t ${test_config}

For instance:

    env/bin/install -c clusters/vesta.spec -v 4.5.0-2601

    env/bin/cluster -c clusters/vesta.spec -t tests/comp_bucket_20M.test

Running performance tests
-------------------------

    env/bin/perfrunner -c ${cluster} -t ${test_config}

For instance:

    env/bin/perfrunner -c clusters/vesta.spec -t tests/comp_bucket_20M.test

Overriding the test settings (space-separated section.option.value trios):

    env/bin/perfrunner -c clusters/vesta.spec -t tests/comp_bucket_20M.test \
        ""load.size.512"" ""cluster.initial_nodes.3 4""

`--verbose` flag enables Fabric logging.

With `--remote` flag remote workers will be used as workload generators.

Unit tests
----------

Just run the test target:

    make test

Related projects
----------------

* [cbmonitor](https://github.com/couchbase/cbmonitor)
* [showfast](https://github.com/couchbaselabs/showfast)",VRAI
courselabs/kubernetes,Documentations,Documentations,2024-06-19T09:36:51Z,2022-05-12T12:39:41Z,0,0,0,0,0,0,0,3,2021-04-09T15:27:30Z,2025-03-26T15:00:32Z,4134,68,Go,VRAI,58,FAUX,3,kubernetes,3,Labs and exercises to help you learn Kubernetes.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,3,"# Kubernetes Course Labs

Labs and exercises to help you learn Kubernetes.

Live at https://kubernetes.courselabs.co.",FAUX
crossplane-contrib/provider-aws,Toolkit,Application System,2025-05-15T12:56:24Z,2025-03-20T12:40:30Z,0,0,0,0,3,0,0,0,2019-09-01T01:14:18Z,2025-04-07T10:05:24Z,35091,457,Go,VRAI,385,FAUX,19,,19,Crossplane AWS Provider,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,195,"# provider-aws

## Overview

This `provider-aws` repository is the Crossplane infrastructure provider for
[Amazon Web Services (AWS)](https://aws.amazon.com). The provider that is built
from the source code in this repository can be installed into a Crossplane
control plane and adds the following new functionality:

* Custom Resource Definitions (CRDs) that model AWS infrastructure and services
  (e.g. [Amazon Relational Database Service (RDS)](https://aws.amazon.com/rds/),
  [EKS clusters](https://aws.amazon.com/eks/), etc.)
* Controllers to provision these resources in AWS based on the users desired
  state captured in CRDs they create
* Implementations of Crossplane's portable resource abstractions, enabling AWS
  resources to fulfill a user's general need for cloud services

## Getting Started and Documentation

For getting started guides, installation, deployment, and administration, see
our [Documentation](https://crossplane.io/docs).

## Contributing

provider-aws is a community driven project and we welcome contributions. See the
Crossplane
[Contributing](https://github.com/crossplane/crossplane/blob/master/CONTRIBUTING.md)
guidelines to get started.

### Adding New Resource

We use AWS Go code generation pipeline to generate new controllers. See [Code Generation Guide](CODE_GENERATION.md)
to add a new resource.

## Releases

AWS Provider is released every 4 weeks and we issue patch releases as necessary.
For example, `v0.20.0` is released on October 19, 2021. The next minor
release `v0.21.0` will be cut on November 16, 2021, and so on.

## Report a Bug

For filing bugs, suggesting improvements, or requesting new features, please
open an [issue](https://github.com/crossplane/provider-aws/issues).

## Contact

Please use the following to reach members of the community:

* Slack: Join our [slack channel](https://slack.crossplane.io)
* Forums:
  [crossplane-dev](https://groups.google.com/forum/#!forum/crossplane-dev)
* Twitter: [@crossplane_io](https://twitter.com/crossplane_io)
* Email: [info@crossplane.io](mailto:info@crossplane.io)

## Roadmap

provider-aws goals and milestones are currently tracked in the Crossplane
repository. More information can be found in
[ROADMAP.md](https://github.com/crossplane/crossplane/blob/master/ROADMAP.md).

## Governance and Owners

provider-aws is run according to the same
[Governance](https://github.com/crossplane/crossplane/blob/master/GOVERNANCE.md)
and [Ownership](https://github.com/crossplane/crossplane/blob/master/OWNERS.md)
structure as the core Crossplane project.

## Code of Conduct

provider-aws adheres to the same [Code of
Conduct](https://github.com/crossplane/crossplane/blob/master/CODE_OF_CONDUCT.md)
as the core Crossplane project.

## Licensing

provider-aws is under the Apache 2.0 license.

[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fcrossplane%2Fprovider-aws.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2Fcrossplane%2Fprovider-aws?ref=badge_large)",VRAI
crossplane-contrib/provider-upjet-aws,Toolkit,Application System,2025-05-08T19:56:02Z,2025-03-22T10:48:08Z,0,0,0,0,9,0,0,0,2022-10-04T13:32:46Z,2025-04-06T10:04:22Z,81297,166,Go,VRAI,155,FAUX,99,,99,AWS Provider for Crossplane.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,87,"<!--
SPDX-FileCopyrightText: 2023 The Crossplane Authors <https://crossplane.io>

SPDX-License-Identifier: CC-BY-4.0
-->

# Upjet-based Crossplane provider for AWS

<div style=""text-align: center;"">

![CI](https://github.com/crossplane-contrib/provider-upjet-aws/workflows/CI/badge.svg)
[![GitHub release](https://img.shields.io/github/release/crossplane-contrib/provider-upjet-aws/all.svg)](https://github.com/crossplane-contrib/provider-upjet-aws/releases)
[![Go Report Card](https://goreportcard.com/badge/github.com/crossplane-contrib/provider-upjet-aws)](https://goreportcard.com/report/github.com/crossplane-contrib/provider-upjet-aws)
[![Contributors](https://img.shields.io/github/contributors/crossplane-contrib/provider-upjet-aws)](https://github.com/crossplane-contrib/provider-upjet-aws/graphs/contributors)
[![Slack](https://img.shields.io/badge/Slack-4A154B?logo=slack)](https://crossplane.slack.com/archives/C05E0UE46S2)
[![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/crossplane_io)](https://twitter.com/crossplane_io)

</div>

Provider Upjet-AWS is a [Crossplane](https://crossplane.io/) provider that is
built using [Upjet](https://github.com/crossplane/upjet) code
generation tools and exposes XRM-conformant managed resources for
[Amazon AWS](https://aws.amazon.com/).

## Getting Started

Follow the quick start
guide [here](https://marketplace.upbound.io/providers/upbound/provider-aws/latest/docs/quickstart).

You can find a detailed API reference for all the managed resources with examples in
the [Upbound Marketplace](https://marketplace.upbound.io/providers/upbound/provider-aws/latest/managed-resources).

For more information about monitoring the Upjet runtime, please
see [Monitoring Guide](https://github.com/crossplane/upjet/blob/main/docs/monitoring.md)

## Contributing

For the general contribution guide,
see [Upjet Contribution Guide](https://github.com/crossplane/upjet/blob/main/CONTRIBUTING.md)

If you'd like to learn how to use Upjet, see [Usage Guide](https://github.com/crossplane/upjet/tree/main/docs).

To build this provider locally and run it in a local Kubernetes cluster, run the
following to build the family config provider (`config`) and the sub-package you are working on e.g. `ec2`:

```shell
DOCKERHUB_ORG=<your-docker-name>
BUILD_ARGS=""--load"" XPKG_REG_ORGS_NO_PROMOTE="""" XPKG_REG_ORGS=""index.docker.io/$DOCKERHUB_ORG"" make build.all publish BRANCH_NAME=main SUBPACKAGES=""config ec2""
```

The `BRANCH_NAME` is set to `main` (even though you might be on another branch) to allow
for the publishing of the images to your docker hub account.

To install the provider, in this example `provider-aws-ec2`, into a local Kubernetes cluster with Crossplane already
installed, apply:

```yaml
apiVersion: pkg.crossplane.io/v1
kind: Provider
metadata:
  name: provider-aws-ec2
spec:
  package: docker.io/<your-docker-name>/provider-aws-ec2:<the-version-taken-from-the-output-of-the-previous-command>
```

### Add a New Resource

Follow the Upjet guide
for [adding new resources](https://github.com/crossplane/upjet/blob/main/docs/adding-new-resource.md).

## Getting help

For filing bugs, suggesting improvements, or requesting new resources or features, please
open an [issue](https://github.com/crossplane-contrib/provider-upjet-aws/issues/new/choose).

For general help on using the provider consider asking the Crossplane community in the
[#upjet-provider-aws](https://crossplane.slack.com/archives/C05E0UE46S2) channel in
[Crossplane Slack](https://slack.crossplane.io)

## License

The provider is released under the [the Apache 2.0 license](LICENSE) with [notice](NOTICE).",VRAI
CrunchyData/postgres-operator,Toolkit,DevOPs,2025-05-14T20:06:58Z,2025-03-20T15:37:36Z,0,0,0,0,2,0,0,0,2017-02-27T22:21:44Z,2025-04-07T16:41:22Z,66797,4088,Go,VRAI,606,FAUX,129,"data-infrastructure,database,database-as-a-service,database-management,disaster-recovery,high-availability,kubernetes,kubernetes-operator,operator,pgo,postgres,postgres-operator,postgresql,postgresql-clusters,postgresql-metrics,postgresql-monitoring",129,"Production PostgreSQL for Kubernetes, from high availability Postgres clusters to full-scale database-as-a-service.",FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,107,"<h1 align=""center"">PGO: The Postgres Operator from Crunchy Data</h1>
<p align=""center"">
  <img width=""150"" src=""./img/CrunchyDataPrimaryIcon.png"" alt=""PGO: The Postgres Operator from Crunchy Data""/>
</p>

[![Go Report Card](https://goreportcard.com/badge/github.com/CrunchyData/postgres-operator)](https://goreportcard.com/report/github.com/CrunchyData/postgres-operator)
![GitHub Repo stars](https://img.shields.io/github/stars/CrunchyData/postgres-operator)
[![License](https://img.shields.io/github/license/CrunchyData/postgres-operator)](LICENSE.md)
[![Discord](https://img.shields.io/discord/1068276526740676708?label=discord&logo=discord)](https://discord.gg/a7vWKG8Ec9)

# Production Postgres Made Easy

[PGO](https://github.com/CrunchyData/postgres-operator), the [Postgres Operator](https://github.com/CrunchyData/postgres-operator) from [Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that automatically manages your [PostgreSQL](https://www.postgresql.org) clusters.

Designed for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/) with Postgres on Kubernetes with PGO. Within a few moments, you can have a production-grade Postgres cluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications. Even better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!

With conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive changes with minimal downtime, PGO is ready to support your Postgres data at every stage of your release pipeline. Built for resiliency and uptime, PGO will keep your Postgres cluster in its desired state, so you do not need to worry about it.

PGO is developed with many years of production experience in automating Postgres management on Kubernetes, providing a seamless cloud native Postgres solution to keep your data always available.

Have questions or looking for help? [Join our Discord group](https://discord.gg/a7vWKG8Ec9).

# Installation

Crunchy Data makes PGO available as the orchestration behind Crunchy Postgres for Kubernetes. Crunchy Postgres for Kubernetes is the integrated product that includes PostgreSQL, PGO and a collection of PostgreSQL tools and extensions that includes the various [open source components listed in the documentation](https://access.crunchydata.com/documentation/postgres-operator/latest/references/components).

We recommend following our [Quickstart](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/) for how to install and get up and running. However, if you can't wait to try it out, here are some instructions to get Postgres up and running on Kubernetes:

1. [Fork the Postgres Operator examples repository](https://github.com/CrunchyData/postgres-operator-examples/fork) and clone it to your host machine. For example:

```sh
YOUR_GITHUB_UN=""<your GitHub username>""
git clone --depth 1 ""git@github.com:${YOUR_GITHUB_UN}/postgres-operator-examples.git""
cd postgres-operator-examples
```

2. Run the following commands

```sh
kubectl apply -k kustomize/install/namespace
kubectl apply --server-side -k kustomize/install/default
```

For more information please read the [Quickstart](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/) and [Tutorial](https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/).

These installation instructions provide the steps necessary to install PGO along with Crunchy Data's Postgres distribution, Crunchy Postgres, as Crunchy Postgres for Kubernetes. In doing so the installation downloads a series of container images from Crunchy Data's Developer Portal. For more information on the use of container images downloaded from the Crunchy Data Developer Portal or other third party sources, please see 'License and Terms' below. The installation and use of PGO outside of the use of Crunchy Postgres for Kubernetes will require modifications of these installation instructions and creation of the necessary PostgreSQL and related containers.  

# Cloud Native Postgres for Kubernetes

PGO, the Postgres Operator from Crunchy Data, comes with all of the features you need for a complete cloud native Postgres experience on Kubernetes!

#### PostgreSQL Cluster [Provisioning][provisioning]

[Create, Scale, & Delete PostgreSQL clusters with ease][provisioning], while fully customizing your
Pods and PostgreSQL configuration!

#### [High Availability][high-availability]

Safe, automated failover backed by a [distributed consensus high availability solution][high-availability].
Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!
Failed primaries automatically heal, allowing for faster recovery time.

Support for [standby PostgreSQL clusters][multiple-cluster] that work both within and across [multiple Kubernetes clusters][multiple-cluster].

#### [Disaster Recovery][disaster-recovery]

[Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and
[includes support for full, incremental, and differential backups as well as efficient delta restores][backups].
Set how long you to retain your backups. Works great with very large databases!

#### Security and [TLS][tls]

PGO enforces that all connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the defaults provided by PGO.

PGO runs containers with locked-down settings and provides Postgres credentials in a secure, convenient way for connecting your applications to your data.

#### [Monitoring][monitoring]

[Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.

#### [Upgrade Management][update-postgres]

Safely [apply PostgreSQL updates][update-postgres] with minimal impact to the availability of your PostgreSQL clusters.

#### Advanced Replication Support

Choose between [asynchronous][high-availability] and synchronous replication
for workloads that are sensitive to losing transactions.

#### [Clone][clone]

[Create new clusters from your existing clusters or backups][clone] with efficient data cloning.

#### [Connection Pooling][pool]

Advanced [connection pooling][pool] support using [pgBouncer][].

#### Pod Anti-Affinity, Node Affinity, Pod Tolerations

Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference. Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations, and more rules to customize your deployment topology!

#### [Scheduled Backups][backup-management]

Choose the type of backup (full, incremental, differential) and [how frequently you want it to occur][backup-management] on each PostgreSQL cluster.

#### Backup to Local Storage, [S3][backups-s3], [GCS][backups-gcs], [Azure][backups-azure], or a Combo!

[Store your backups in Amazon S3][backups-s3] or any object storage system that supports
the S3 protocol. You can also store backups in [Google Cloud Storage][backups-gcs] and [Azure Blob Storage][backups-azure].

You can also [mix-and-match][backups-multi]: PGO lets you [store backups in multiple locations][backups-multi].

#### [Full Customizability][customize-cluster]

PGO makes it easy to fully customize your Postgres cluster to tailor to your workload:

- Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.
- - Use your own container image repository, including support `imagePullSecrets` and private repositories
- [Customize your PostgreSQL configuration][customize-cluster]

#### [Namespaces][k8s-namespaces]

Deploy PGO to watch Postgres clusters in all of your [namespaces][k8s-namespaces], or [restrict which namespaces][single-namespace] you want PGO to manage Postgres clusters in!

[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/backups-disaster-recovery/backups
[backups-s3]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/backups-disaster-recovery/backups#using-s3
[backups-gcs]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/backups-disaster-recovery/backups#using-google-cloud-storage-gcs
[backups-azure]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/backups-disaster-recovery/backups#using-azure-blob-storage
[backups-multi]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/backups-disaster-recovery/backups#set-up-multiple-backup-repositories
[backup-management]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/backups-disaster-recovery/backup-management
[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/backups-disaster-recovery/disaster-recovery#clone-a-postgres-cluster
[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/day-two/customize-cluster
[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/backups-disaster-recovery/disaster-recovery
[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/day-two/high-availability/
[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/day-two/monitoring/
[multiple-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/architecture/disaster-recovery/#standby-cluster-overview
[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/basic-setup/connection-pooling/
[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/basic-setup/create-cluster/
[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/cluster-management/resize-cluster/
[single-namespace]: https://access.crunchydata.com/documentation/postgres-operator/v5/installation/kustomize/#installation-mode
[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/day-two/customize-cluster#customize-tls
[update-postgres]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/cluster-management/update-cluster
[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity
[k8s-namespaces]: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/
[pgBackRest]: https://www.pgbackrest.org
[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorials/basic-setup/connection-pooling/
[pgMonitor]: https://github.com/CrunchyData/pgmonitor

## Included Components

[PostgreSQL containers](https://github.com/CrunchyData/crunchy-containers) deployed with the PostgreSQL Operator include the following components:

- [PostgreSQL](https://www.postgresql.org)
  - [PostgreSQL Contrib Modules](https://www.postgresql.org/docs/current/contrib.html)
  - [PL/Python + PL/Python 3](https://www.postgresql.org/docs/current/plpython.html)
  - [PL/Perl](https://www.postgresql.org/docs/current/plperl.html)
  - [PL/Tcl](https://www.postgresql.org/docs/current/pltcl.html)
  - [pgAudit](https://www.pgaudit.org/)
  - [pgAudit Analyze](https://github.com/pgaudit/pgaudit_analyze)
  - [pg_cron](https://github.com/citusdata/pg_cron)
  - [pg_partman](https://github.com/pgpartman/pg_partman)
  - [pgnodemx](https://github.com/CrunchyData/pgnodemx)
  - [set_user](https://github.com/pgaudit/set_user)
  - [TimescaleDB](https://github.com/timescale/timescaledb) (Apache-licensed community edition)
  - [wal2json](https://github.com/eulerto/wal2json)
- [pgBackRest](https://pgbackrest.org/)
- [pgBouncer](http://pgbouncer.github.io/)
- [pgAdmin 4](https://www.pgadmin.org/)
- [pgMonitor](https://github.com/CrunchyData/pgmonitor)
- [Patroni](https://patroni.readthedocs.io/)
- [LLVM](https://llvm.org/) (for [JIT compilation](https://www.postgresql.org/docs/current/jit.html))

In addition to the above, the geospatially enhanced PostgreSQL + PostGIS container adds the following components:

- [PostGIS](http://postgis.net/)
- [pgRouting](https://pgrouting.org/)

[PostgreSQL Operator Monitoring](https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/monitoring/) uses the following components:

- [pgMonitor](https://github.com/CrunchyData/pgmonitor)
- [Prometheus](https://github.com/prometheus/prometheus)
- [Grafana](https://github.com/grafana/grafana)
- [Alertmanager](https://github.com/prometheus/alertmanager)

For more information about which versions of the PostgreSQL Operator include which components, please visit the [compatibility](https://access.crunchydata.com/documentation/postgres-operator/v5/references/components/) section of the documentation.

## [Supported Platforms](https://access.crunchydata.com/documentation/postgres-operator/latest/overview/supported-platforms)

PGO, the Postgres Operator from Crunchy Data, is tested on the following platforms:

- Kubernetes
- OpenShift
- Rancher
- Google Kubernetes Engine (GKE), including Anthos
- Amazon EKS
- Microsoft AKS
- VMware Tanzu

# Contributing to the Project

Want to contribute to the PostgreSQL Operator project? Great! We've put together
a set of contributing guidelines that you can review here:

- [Contributing Guidelines](CONTRIBUTING.md)

Once you are ready to submit a Pull Request, please ensure you do the following:

1. Reviewing the [contributing guidelines](CONTRIBUTING.md) and ensure
   that you have followed the commit message format, added testing where
   appropriate, documented your changes, etc.
1. Open up a pull request based upon the guidelines. If you are adding a new
   feature, please open up the pull request on the `main` branch.
1. Please be as descriptive in your pull request as possible. If you are
   referencing an issue, please be sure to include the issue in your pull request

## Support

If you believe you have found a bug or have a detailed feature request, please open a GitHub issue and follow the guidelines for submitting a bug.

For general questions or community support, we welcome you to join our [community Discord](https://discord.gg/a7vWKG8Ec9) and ask your questions there.

For other information, please visit the [Support](https://access.crunchydata.com/documentation/postgres-operator/latest/support/) section of the documentation.

# Documentation

For additional information regarding the design, configuration, and operation of the
PostgreSQL Operator, pleases see the [Official Project Documentation][documentation].

[documentation]: https://access.crunchydata.com/documentation/postgres-operator/latest/

## Past Versions

Documentation for previous releases can be found at the [Crunchy Data Access Portal](https://access.crunchydata.com/documentation/).

# Releases

When a PostgreSQL Operator general availability (GA) release occurs, the container images are distributed on the following platforms in order:

- [Crunchy Data Customer Portal](https://access.crunchydata.com/)
- [Crunchy Data Developer Portal](https://www.crunchydata.com/developers)

The image rollout can occur over the course of several days.

To stay up-to-date on when releases are made available in the [Crunchy Data Developer Portal](https://www.crunchydata.com/developers), please sign up for the [Crunchy Data Developer Program Newsletter](https://www.crunchydata.com/developers#email). You can also [join the PGO project community discord](https://discord.gg/a7vWKG8Ec9)

# FAQs, License and Terms

For more information regarding PGO, the Postgres Operator project from Crunchy Data, and Crunchy Postgres for Kubernetes, please see the [frequently asked questions](https://access.crunchydata.com/documentation/postgres-operator/latest/faq). 

The installation instructions provided in this repo are designed for the use of PGO along with Crunchy Data's Postgres distribution, Crunchy Postgres, as Crunchy Postgres for Kubernetes. The unmodified use of these installation instructions will result in downloading container images from Crunchy Data repositories - specifically the Crunchy Data Developer Portal. The use of container images downloaded from the Crunchy Data Developer Portal are subject to the [Crunchy Data Developer Program terms](https://www.crunchydata.com/developers/terms-of-use).  

The PGO Postgres Operator project source code is available subject to the [Apache 2.0 license](LICENSE.md) with the PGO logo and branding assets covered by [our trademark guidelines](docs/static/logos/TRADEMARKS.md).",VRAI
cvat-ai/cvat,MLOps,MLOps,2025-05-15T16:15:50Z,2025-05-05T20:21:20Z,0,24,0,0,0,0,0,0,2018-06-29T14:02:45Z,2025-04-07T08:55:11Z,336648,13519,Python,VRAI,3192,FAUX,576,"annotation,annotation-tool,annotations,boundingbox,computer-vision,computer-vision-annotation,dataset,deep-learning,image-annotation,image-classification,image-labeling,image-labelling-tool,imagenet,labeling,labeling-tool,object-detection,pytorch,semantic-segmentation,tensorflow,video-annotation",576,"Annotate better with CVAT, the industry-leading data engine for machine learning. Used and trusted by teams at any scale, for data of any scale.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,274,"<p align=""center"">
  <img src=""/site/content/en/images/cvat-readme-gif.gif"" alt=""CVAT Platform"" width=""100%"" max-width=""800px"">
</p>
<p align=""center"">
  <a href=""https://app.cvat.ai/"">
    <img src=""/site/content/en/images/cvat-readme-button-tr-bg.png"" alt=""Start Annotating Now"">
  </a>
</p>

# Computer Vision Annotation Tool (CVAT)

[![CI][ci-img]][ci-url]
[![Gitter chat][gitter-img]][gitter-url]
[![Discord][discord-img]][discord-url]
[![Coverage Status][coverage-img]][coverage-url]
[![server pulls][docker-server-pulls-img]][docker-server-image-url]
[![ui pulls][docker-ui-pulls-img]][docker-ui-image-url]
[![DOI][doi-img]][doi-url]

CVAT is an interactive video and image annotation
tool for computer vision. It is used by tens of thousands of users and
companies around the world. Our mission is to help developers, companies, and
organizations around the world to solve real problems using the Data-centric
AI approach.

Start using CVAT online: [cvat.ai](https://cvat.ai). You can use it for free,
or [subscribe](https://www.cvat.ai/pricing/cloud) to get unlimited data,
organizations, autoannotations, and [Roboflow and HuggingFace integration](https://www.cvat.ai/post/integrating-hugging-face-and-roboflow-models).

Or set CVAT up as a self-hosted solution:
[Self-hosted Installation Guide](https://docs.cvat.ai/docs/administration/basics/installation/).
We provide [Enterprise support](https://www.cvat.ai/pricing/on-prem) for
self-hosted installations with premium features: SSO, LDAP, Roboflow and
HuggingFace integrations, and advanced analytics (coming soon). We also
do trainings and a dedicated support with 24 hour SLA.

## Quick start ⚡

- [Installation guide](https://docs.cvat.ai/docs/administration/basics/installation/)
- [Manual](https://docs.cvat.ai/docs/manual/)
- [Contributing](https://docs.cvat.ai/docs/contributing/)
- [Datumaro dataset framework](https://github.com/cvat-ai/datumaro/blob/develop/README.md)
- [Server API](#api)
- [Python SDK](#sdk)
- [Command line tool](#cli)
- [XML annotation format](https://docs.cvat.ai/docs/manual/advanced/xml_format/)
- [AWS Deployment Guide](https://docs.cvat.ai/docs/administration/basics/aws-deployment-guide/)
- [Frequently asked questions](https://docs.cvat.ai/docs/faq/)
- [Where to ask questions](#where-to-ask-questions)

## Partners ❤️

CVAT is used by teams all over the world. In the list, you can find key companies which
help us support the product or an essential part of our ecosystem. If you use us,
please drop us a line at [contact@cvat.ai](mailto:contact+github@cvat.ai).

- [Human Protocol](https://hmt.ai) uses CVAT as a way of adding annotation service to the Human Protocol.
- [FiftyOne](https://fiftyone.ai) is an open-source dataset curation and model analysis
  tool for visualizing, exploring, and improving computer vision datasets and models that are
  [tightly integrated](https://voxel51.com/docs/fiftyone/integrations/cvat.html) with CVAT
  for annotation and label refinement.

## Public datasets

[ATLANTIS](https://github.com/smhassanerfani/atlantis), an open-source dataset for semantic segmentation
of waterbody images, developed by [iWERS](http://ce.sc.edu/iwers/) group in the
Department of Civil and Environmental Engineering at the University of South Carolina is using CVAT.

For developing a semantic segmentation dataset using CVAT, see:

- [ATLANTIS published article](https://www.sciencedirect.com/science/article/pii/S1364815222000391)
- [ATLANTIS Development Kit](https://github.com/smhassanerfani/atlantis/tree/master/adk)
- [ATLANTIS annotation tutorial videos](https://www.youtube.com/playlist?list=PLIfLGY-zZChS5trt7Lc3MfNhab7OWl2BR).

## CVAT online: [cvat.ai](https://cvat.ai)

This is an online version of CVAT. It's free, efficient, and easy to use.

[cvat.ai](https://cvat.ai) runs the latest version of the tool. You can create up
to 10 tasks there and upload up to 500Mb of data to annotate. It will only be
visible to you or the people you assign to it.

For now, it does not have [analytics features](https://docs.cvat.ai/docs/administration/advanced/analytics/)
like management and monitoring the data annotation team. It also does not allow exporting images, just the annotations.

We plan to enhance [cvat.ai](https://cvat.ai) with new powerful features. Stay tuned!

## Prebuilt Docker images 🐳

Prebuilt docker images are the easiest way to start using CVAT locally. They are available on Docker Hub:

- [cvat/server](https://hub.docker.com/r/cvat/server)
- [cvat/ui](https://hub.docker.com/r/cvat/ui)

The images have been downloaded more than 1M times so far.

## Screencasts 🎦

Here are some screencasts showing how to use CVAT.

<!--lint disable maximum-line-length-->

[Computer Vision Annotation Course](https://www.youtube.com/playlist?list=PL0to7Ng4PuuYQT4eXlHb_oIlq_RPeuasN):
we introduce our course series designed to help you annotate data faster and better
using CVAT. This course is about CVAT deployment and integrations, it includes
presentations and covers the following topics:

- **Speeding up your data annotation process: introduction to CVAT and Datumaro**.
  What problems do CVAT and Datumaro solve, and how they can speed up your model
  training process. Some resources you can use to learn more about how to use them.
- **Deployment and use CVAT**. Use the app online at [app.cvat.ai](https://app.cvat.ai).
  A local deployment. A containerized local deployment with Docker Compose (for regular use),
  and a local cluster deployment with Kubernetes (for enterprise users). A 2-minute
  tour of the interface, a breakdown of CVAT’s internals, and a demonstration of how
  to deploy CVAT using Docker Compose.

[Product tour](https://www.youtube.com/playlist?list=PL0to7Ng4Puua37NJVMIShl_pzqJTigFzg): in this course, we show how to use CVAT, and help to get familiar with CVAT functionality and interfaces. This course does not cover integrations and is dedicated solely to CVAT. It covers the following topics:

- **Pipeline**. In this video, we show how to use [app.cvat.ai](https://app.cvat.ai): how to sign up, upload your data, annotate it, and download it.

<!--lint enable maximum-line-length-->

For feedback, please see [Contact us](#contact-us)

## API

- [Documentation](https://docs.cvat.ai/docs/api_sdk/api/)

## SDK

- Install with `pip install cvat-sdk`
- [PyPI package homepage](https://pypi.org/project/cvat-sdk/)
- [Documentation](https://docs.cvat.ai/docs/api_sdk/sdk/)

## CLI

- Install with `pip install cvat-cli`
- [PyPI package homepage](https://pypi.org/project/cvat-cli/)
- [Documentation](https://docs.cvat.ai/docs/api_sdk/cli/)

## Supported annotation formats

CVAT supports multiple annotation formats. You can select the format
after clicking the **Upload annotation** and **Dump annotation** buttons.
[Datumaro](https://github.com/cvat-ai/datumaro) dataset framework allows
additional dataset transformations with its command line tool and Python library.

For more information about the supported formats, see:
[Annotation Formats](https://docs.cvat.ai/docs/manual/advanced/formats/).

<!--lint disable maximum-line-length-->

| Annotation format                                                                                | Import | Export |
|--------------------------------------------------------------------------------------------------| ------ | ------ |
| [CVAT for images](https://docs.cvat.ai/docs/manual/advanced/xml_format/#annotation)              | ✔️     | ✔️     |
| [CVAT for a video](https://docs.cvat.ai/docs/manual/advanced/xml_format/#interpolation)          | ✔️     | ✔️     |
| [Datumaro](https://github.com/cvat-ai/datumaro)                                                  | ✔️     | ✔️     |
| [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)                                            | ✔️     | ✔️     |
| Segmentation masks from [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)                    | ✔️     | ✔️     |
| [YOLO](https://pjreddie.com/darknet/yolo/)                                                       | ✔️     | ✔️     |
| [MS COCO Object Detection](http://cocodataset.org/#format-data)                                  | ✔️     | ✔️     |
| [MS COCO Keypoints Detection](http://cocodataset.org/#format-data)                               | ✔️     | ✔️     |
| [MOT](https://motchallenge.net/)                                                                 | ✔️     | ✔️     |
| [MOTS PNG](https://www.vision.rwth-aachen.de/page/mots)                                          | ✔️     | ✔️     |
| [LabelMe 3.0](http://labelme.csail.mit.edu/Release3.0)                                           | ✔️     | ✔️     |
| [ImageNet](http://www.image-net.org)                                                             | ✔️     | ✔️     |
| [CamVid](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)                             | ✔️     | ✔️     |
| [WIDER Face](http://shuoyang1213.me/WIDERFACE/)                                                  | ✔️     | ✔️     |
| [VGGFace2](https://github.com/ox-vgg/vgg_face2)                                                  | ✔️     | ✔️     |
| [Market-1501](https://www.aitribune.com/dataset/2018051063)                                      | ✔️     | ✔️     |
| [ICDAR13/15](https://rrc.cvc.uab.es/?ch=2)                                                       | ✔️     | ✔️     |
| [Open Images V6](https://storage.googleapis.com/openimages/web/index.html)                       | ✔️     | ✔️     |
| [Cityscapes](https://www.cityscapes-dataset.com/login/)                                          | ✔️     | ✔️     |
| [KITTI](http://www.cvlibs.net/datasets/kitti/)                                                   | ✔️     | ✔️     |
| [Kitti Raw Format](https://www.cvlibs.net/datasets/kitti/raw_data.php)                           | ✔️     | ✔️     |
| [LFW](http://vis-www.cs.umass.edu/lfw/)                                                          | ✔️     | ✔️     |
| [Supervisely Point Cloud Format](https://docs.supervise.ly/data-organization/00_ann_format_navi) | ✔️     | ✔️     |
| [Ultralytics YOLO Detection](https://docs.ultralytics.com/datasets/detect/)                      | ✔️     | ✔️     |
| [Ultralytics YOLO Oriented Bounding Boxes](https://docs.ultralytics.com/datasets/obb/)                     | ✔️     | ✔️     |
| [Ultralytics YOLO Segmentation](https://docs.ultralytics.com/datasets/segment/)                            | ✔️     | ✔️     |
| [Ultralytics YOLO Pose](https://docs.ultralytics.com/datasets/pose/)                                       | ✔️     | ✔️     |
| [Ultralytics YOLO Classification](https://docs.ultralytics.com/datasets/classify/)                         | ✔️     | ✔️     |

<!--lint enable maximum-line-length-->

## Deep learning serverless functions for automatic labeling

CVAT supports automatic labeling. It can speed up the annotation process
up to 10x. Here is a list of the algorithms we support, and the platforms they can be run on:

<!--lint disable maximum-line-length-->

| Name                                                                                                    | Type       | Framework  | CPU | GPU |
| ------------------------------------------------------------------------------------------------------- | ---------- | ---------- | --- | --- |
| [Segment Anything](/serverless/pytorch/facebookresearch/sam/nuclio/)                                    | interactor | PyTorch    | ✔️  | ✔️  |
| [Deep Extreme Cut](/serverless/openvino/dextr/nuclio)                                                   | interactor | OpenVINO   | ✔️  |     |
| [Faster RCNN](/serverless/openvino/omz/public/faster_rcnn_inception_resnet_v2_atrous_coco/nuclio)       | detector   | OpenVINO   | ✔️  |     |
| [Mask RCNN](/serverless/openvino/omz/public/mask_rcnn_inception_resnet_v2_atrous_coco/nuclio)           | detector   | OpenVINO   | ✔️  |     |
| [YOLO v3](/serverless/openvino/omz/public/yolo-v3-tf/nuclio)                                            | detector   | OpenVINO   | ✔️  |     |
| [YOLO v7](/serverless/onnx/WongKinYiu/yolov7/nuclio)                                                    | detector   | ONNX       | ✔️  | ✔️  |
| [Object reidentification](/serverless/openvino/omz/intel/person-reidentification-retail-0277/nuclio)    | reid       | OpenVINO   | ✔️  |     |
| [Semantic segmentation for ADAS](/serverless/openvino/omz/intel/semantic-segmentation-adas-0001/nuclio) | detector   | OpenVINO   | ✔️  |     |
| [Text detection v4](/serverless/openvino/omz/intel/text-detection-0004/nuclio)                          | detector   | OpenVINO   | ✔️  |     |
| [SiamMask](/serverless/pytorch/foolwood/siammask/nuclio)                                                | tracker    | PyTorch    | ✔️  | ✔️  |
| [TransT](/serverless/pytorch/dschoerk/transt/nuclio)                                                    | tracker    | PyTorch    | ✔️  | ✔️  |
| [f-BRS](/serverless/pytorch/saic-vul/fbrs/nuclio)                                                       | interactor | PyTorch    | ✔️  |     |
| [HRNet](/serverless/pytorch/saic-vul/hrnet/nuclio)                                                      | interactor | PyTorch    |     | ✔️  |
| [Inside-Outside Guidance](/serverless/pytorch/shiyinzhang/iog/nuclio)                                   | interactor | PyTorch    | ✔️  |     |
| [Faster RCNN](/serverless/tensorflow/faster_rcnn_inception_v2_coco/nuclio)                              | detector   | TensorFlow | ✔️  | ✔️  |
| [RetinaNet](serverless/pytorch/facebookresearch/detectron2/retinanet_r101/nuclio)                       | detector   | PyTorch    | ✔️  | ✔️  |
| [Face Detection](/serverless/openvino/omz/intel/face-detection-0205/nuclio)                             | detector   | OpenVINO   | ✔️  |     |

<!--lint enable maximum-line-length-->

## License

The code is released under the [MIT License](https://opensource.org/licenses/MIT).

The code contained within the `/serverless` directory is released under the **MIT License**.
However, it may download and utilize various assets, such as source code, architectures, and weights, among others.
These assets may be distributed under different licenses, including non-commercial licenses.
It is your responsibility to ensure compliance with the terms of these licenses before using the assets.

This software uses LGPL-licensed libraries from the [FFmpeg](https://www.ffmpeg.org) project.
The exact steps on how FFmpeg was configured and compiled can be found in the [Dockerfile](Dockerfile).

FFmpeg is an open-source framework licensed under LGPL and GPL.
See [https://www.ffmpeg.org/legal.html](https://www.ffmpeg.org/legal.html). You are solely responsible
for determining if your use of FFmpeg requires any
additional licenses. CVAT.ai Corporation is not responsible for obtaining any
such licenses, nor liable for any licensing fees due in
connection with your use of FFmpeg.

## Contact us

[Gitter](https://gitter.im/opencv-cvat/public) to ask CVAT usage-related questions.
Typically questions get answered fast by the core team or community. There you can also browse other common questions.

[Discord](https://discord.gg/S6sRHhuQ7K) is the place to also ask questions or discuss any other stuff related to CVAT.

[LinkedIn](https://www.linkedin.com/company/cvat-ai/) for the company and work-related questions.

[YouTube](https://www.youtube.com/@cvat-ai) to see screencast and tutorials about the CVAT.

[GitHub issues](https://github.com/cvat-ai/cvat/issues) for feature requests or bug reports.
If it's a bug, please add the steps to reproduce it.

[#cvat](https://stackoverflow.com/search?q=%23cvat) tag on StackOverflow is one more way to ask
questions and get our support.

[contact@cvat.ai](mailto:contact+github@cvat.ai) to reach out to us if you need commercial support.

## Links

- [Intel AI blog: New Computer Vision Tool Accelerates Annotation of Digital Images and Video](https://www.intel.ai/introducing-cvat)
- [Intel Software: Computer Vision Annotation Tool: A Universal Approach to Data Annotation](https://software.intel.com/en-us/articles/computer-vision-annotation-tool-a-universal-approach-to-data-annotation)
- [VentureBeat: Intel open-sources CVAT, a toolkit for data labeling](https://venturebeat.com/2019/03/05/intel-open-sources-cvat-a-toolkit-for-data-labeling/)
- [How to Use CVAT (Roboflow guide)](https://blog.roboflow.com/cvat/)
- [How to auto-label data in CVAT with one of 50,000+ models on Roboflow Universe](https://blog.roboflow.com/how-to-use-roboflow-models-in-cvat/)

  <!-- Badges -->

[docker-server-pulls-img]: https://img.shields.io/docker/pulls/cvat/server.svg?style=flat-square&label=server%20pulls
[docker-server-image-url]: https://hub.docker.com/r/cvat/server
[docker-ui-pulls-img]: https://img.shields.io/docker/pulls/cvat/ui.svg?style=flat-square&label=UI%20pulls
[docker-ui-image-url]: https://hub.docker.com/r/cvat/ui
[ci-img]: https://github.com/cvat-ai/cvat/actions/workflows/main.yml/badge.svg?branch=develop
[ci-url]: https://github.com/cvat-ai/cvat/actions
[gitter-img]: https://img.shields.io/gitter/room/opencv-cvat/public?style=flat
[gitter-url]: https://gitter.im/opencv-cvat/public
[coverage-img]: https://codecov.io/github/cvat-ai/cvat/branch/develop/graph/badge.svg
[coverage-url]: https://codecov.io/github/cvat-ai/cvat
[doi-img]: https://zenodo.org/badge/139156354.svg
[doi-url]: https://zenodo.org/badge/latestdoi/139156354
[discord-img]: https://img.shields.io/discord/1000789942802337834?label=discord
[discord-url]: https://discord.gg/fNR3eXfk6C",FAUX
dadrus/heimdall,Toolkit,Toolkit,2025-05-10T18:02:30Z,2025-03-06T01:12:23Z,0,1,0,0,0,0,0,0,2022-04-12T08:54:37Z,2025-04-03T20:33:39Z,24596,179,Go,VRAI,19,FAUX,48,"access-control,access-management,api-gateway,auth-api,auth-proxy,authentication,authorization,decision-api,golang,identity-aware-proxy,oauth2,openid-connect,policy-enforcement",48,A cloud native Identity Aware Proxy and Access Control Decision service,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,19,"# Heimdall
[![CI](https://github.com/dadrus/heimdall/actions/workflows/ci.yaml/badge.svg?branch=main)](https://github.com/dadrus/heimdall/actions/workflows/ci.yml)
[![Security-Scan](https://github.com/dadrus/heimdall/actions/workflows/security.yaml/badge.svg)](https://github.com/dadrus/heimdall/actions/workflows/security.yml)
[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/7738/badge)](https://www.bestpractices.dev/projects/7738)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/dadrus/heimdall/badge)](https://securityscorecards.dev/viewer/?uri=github.com/dadrus/heimdall)
[![SLSA 3](https://slsa.dev/images/gh-badge-level3.svg)](https://slsa.dev)
[![Go Report Card](https://goreportcard.com/badge/github.com/dadrus/heimdall)](https://goreportcard.com/report/github.com/dadrus/heimdall)
[![codecov](https://codecov.io/gh/dadrus/heimdall/branch/main/graph/badge.svg)](https://codecov.io/gh/dadrus/heimdall)
[![Docker](https://img.shields.io/docker/v/dadrus/heimdall/latest?color=lightblue&label=docker&logo=docker)](https://hub.docker.com/r/dadrus/heimdall)
[![Helm Chart](https://img.shields.io/badge/dynamic/yaml.svg?label=helm%20chart&url=https://dadrus.github.io/heimdall/charts/index.yaml&query=$.entries.heimdall[0].version&logo=helm&logoColor=white)](https://github.com/dadrus/heimdall/tree/main/charts/heimdall)

## Background

Heimdall is inspired by the Zero Trust idea and also by [Pomerium](https://www.pomerium.com/docs) and [Ory's OAthkeeper](https://www.ory.sh/docs/oathkeeper). Some experience with both and my inability to update the latter one to include the desired functionality and behavior was Heimdall's born hour. 

## What is heimdall

Heimdall authenticates and authorizes incoming HTTP (HTTP 1.x and HTTP 2.0) requests as well as enriches these with further contextual information and finally transforms resulting subject information into a format, required by the upstream services.

It can do so:

* Standalone as a proxy in front of your service or web server that rejects unauthorized requests and forwards authorized ones to your end points, or 
* Integrated into any other proxy, ingress controller or API gateway, like Kong, NGNIX, Envoy, Traefik, Contour, Ambassador and many more. Here that other proxy will forward the incoming request to heimdall and depending on its response either forward the original request, verified and updated by heimdall to your upstream service, or reject it with the information provided by heimdall.

In both cases it acts as a Policy Enforcement and to some degree a Policy Decision Point according to [NIST Zero Trust Architecture (SP 800-207)](https://doi.org/10.6028/NIST.SP.800-207)

## How does authentication, authorization and transformation work

The decision-making and transformation processes in Heimdall are governed by rules or respectively rule sets. These rule sets can be independently configured and managed by each upstream service. Heimdall dynamically loads these rules from a variety of sources, including:

* `RuleSet` kubernetes resources (a corresponding CRD is shipped with the helm chart)
* Cloud storages, like AWS S3, Google's GC, etc.
* Local file system
* Any HTTP endpoint

That way, these rule sets cannot only be managed centrally, but be deployed together with each particular upstream service as well without the need to restart or redeploy heimdall. Indeed, these rule sets are optional first class citizens of the upstream service and allow:

* Implementation of secure defaults. If no rule matches the incoming request, a default decision and transformation, if configured, is applied. This is the reason for ""optional first class citizens"" above.
* Configuration of as many authentication (e.g. OpenID Connect), authorization (e.g. via CEL expressions, or via OPA, or OpenFGA), contextualization (by e.g. communicating to some specific endpoint) and finalization mechanisms (e.g. creation of a JWT out of the available subject information), supported by heimdall, as required for the particular system. So, if your system requires integration with multiple authentication providers, or you want to migrate from one to another, it is just a matter of configuring them in heimdall.
* Reuse and combination of these mechanisms in as many rules as required for the particular system.
* Partial reconfiguration of a particular mechanism in a rule if required by the upstream service.
* Authentication mechanism fallbacks.
* Implementation of different decision process schemes by combining e.g. authentication mechanisms with error handlers to drive authentication mechanism specific error handling strategies.
* Execution of authorization and contextualization mechanisms in any order. That way, if the information about your subject, available from the authentication system, is not sufficient to make proper authorization decisions, you can let heimdall call other services to retrieve that additional information.
* Conditional execution of authorization, contextualization and finalization mechanisms is possible. E.g. if depending on the available information about the subject you would like heimdall to either block the request, or let the upstream return different representations of the requested resource.

## Beyond the functionality

Heimdall's main focus points beyond its functionality are:

* Performance - To achieve this, heimdall does use any http routing frameworks and does not load or convert data during execution whenever possible. This is also true for reflection use.
* Clear abstractions - To allow extensibility and even replacement of components without side effects.
* Simplicity - To allow better understanding of code to everybody, who would like to contribute.

## Where can I find more details

Head over to the [documentation](https://dadrus.github.io/heimdall/) for details or if you would like to give it a try.

## Current state

* Production-ready and already in use by multiple organizations worldwide.
* Code base is stable and well-tested. 
* Some features are still missing, and the development of these features might lead to breaking changes in future updates.

For information on the currently supported functionality, please refer to the [Release descriptions](https://github.com/dadrus/heimdall/releases). Planned features can be found in the defined [Milestones](https://github.com/dadrus/heimdall/milestones).


## If you ...

* ... like the project - please give it a :star:
* ... miss something, or found a bug, [file a ticket](https://github.com/dadrus/heimdall/issues). You are also very welcome to [contribute](CONTRIBUTING.md) :wink:
* ... would like to support, reach out to me via [Discord](https://discord.gg/qQgg8xKuyb)
* ... need help, head over to [Discord](https://discord.gg/qQgg8xKuyb) as well",FAUX
DaoCloud/dce-charts-repackage,DevOPs,Documentations,2025-05-15T07:53:51Z,2025-02-17T20:10:40Z,0,3,0,0,3,0,0,0,2022-08-04T03:46:54Z,2025-04-07T08:25:33Z,51265,12,Mustache,VRAI,29,FAUX,918,,918,helm repo add daocloud https://daocloud.github.io/dce-charts-repackage/,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,49,"# dce-charts-repackage

[![Auto Release Chart](https://github.com/DaoCloud/dce-charts-repackage/actions/workflows/auto-release.yaml/badge.svg)](https://github.com/DaoCloud/dce-charts-repackage/actions/workflows/auto-release.yaml)
[![Auto Upgrade Chart](https://github.com/DaoCloud/dce-charts-repackage/actions/workflows/auto-upgrade.yaml/badge.svg)](https://github.com/DaoCloud/dce-charts-repackage/actions/workflows/auto-upgrade.yaml)
[![Nightly E2E](https://github.com/DaoCloud/dce-charts-repackage/actions/workflows/night-ci.yml/badge.svg)](https://github.com/DaoCloud/dce-charts-repackage/actions/workflows/night-ci.yml)
[![Manually Release Chart](https://github.com/DaoCloud/dce-charts-repackage/actions/workflows/release-chart.yml/badge.svg)](https://github.com/DaoCloud/dce-charts-repackage/actions/workflows/release-chart.yml)

***

## 说明

本工程主要是提供了 制作 chart 的框架、 自动化测试 和 自动发版的能力。**您 书写的代码，关键在于如何自动化做 chart 包，这样工程才能 跟进开源版本自动化迭代升级**

开发一个新的 chart，主要开发如下，细节请看后续说明

* /charts/${PROJECT}

* /charts/${PROJECT}/${PROJECT}/.relok8s-images.yaml

* /charts/${PROJECT}/${PROJECT}/values.schema.json

* /test/${PROJECT}/install.sh

***

## 开发 /charts/${PROJECT} 

开发该目录，主要是书写自动化生成chart的代码。 主要有3种做包方式：

**无论哪种做包方式，最终执行`make build_chart -e PROJECT=${PROJECT}` ， 要求开源chart 最终生成到 /charts/${PROJECT}/${PROJECT}**

### 方案: 复用工程做包框架，基于开源 chart 作为子 chart，wrapper了一层父chart

目前，基本所有项目 都遵循该制作方式，基于父子chart封装，保持开源子 chart 原汁原味，
而 父chart中可加入如下，使得产品安装更加简单：
* schema

* roleK8

* 调优开源values参数

* 下发CRD实例，如业务CRD，如 Prometheus 的监控 crd

开发流程：

1. 准备文件如下文件，每个文件的作用，在步骤2中提及
    
    * （必须）/charts/${PROJECT}/config，确保其中 USE_OPENSOURCE_CHART=false （可参考 spiderpool）。

    * （必须）父 chart 的添加内容放置在 /charts/${PROJECT}/parent 目录中

    * （可选）/charts/${PROJECT}/appendValues.yaml

    * （可选）/src/${PROJECT}/custom.sh

    * （可选）/src/${PROJECT}/skip-check.yaml : PR CI 会检查父子chart间的 values 映射关系，如果父chart中定制了一个 子chart中不存在的values，CI就会报错。对于必要的例外情况，你可以加入这种value到本文件，让 CI 忽略
    
2. 执行`make build_chart -e PROJECT=${PROJECT}`， 工程自动化 执行如下流程 来 生成 chart （ **实现代码参考脚本 scripts/generateChart.sh** ）

    1. 脚本流程会准备好一个 '父chart临时目录' ，基于/charts/${PROJECT}/config 中的配置，把依赖的 dependency开源 chart 中的 README.md values.yaml Chart.yaml values.schema.json ，放置在 '父chart临时目录' 中

    2. 对于'父chart临时目录' 中的chart.yaml ， 脚本流程会主动注入 /charts/${PROJECT}/config 中的依赖的开源项目内容 ，且把 dependency chart 自动放置在 '父chart临时目录/charts' 目录下

    3. 如果存在**您书写的 /charts/${PROJECT}/appendValues.yaml**  ， 脚本流程会把其中内容追加到 '父chart临时目录' 的 values.yaml 中

    4. 如果存在**您书写的 /charts/${PROJECT}/parent 目录**（在此目录中，你可以事先准备好 子定义的 README.md values.yaml Chart.yaml values.schema.json， 从而 覆盖以上3步的效果 ），脚本流程会则把其中的所有文件  覆盖拷贝到 '父chart临时目录' 中

    5. 如果 '父chart临时目录'  缺失 values.schema.json 文件，脚本流程会 则自动生成 values.schema.json

    6. 如果存在**您书写的 /src/${PROJECT}/custom.sh**， 脚本流程会执行它， 脚本的第一个入参是 '父chart临时目录'的路径 。 在此脚本中，你可以自定义代码，实现 复杂的 自定义修改 。 （可参考 spiderpool）

    7. 最终 ， '父chart临时目录'  拷贝于 /charts/${PROJECT}/${PROJECT}

> 如上流程看似复杂，其实为了满足 不同人的 制作 需求，您可以依赖 其中的几个步骤 来 完成 你的chart 制作


#### /charts/${PROJECT}/config 文件说明：

* USE_OPENSOURCE_CHART ： 自动化做包的方式

    * 值为 true，代表 'case：chart 直接同步开源 chart'

    * 值为 false，代表 'case: 基于开源 chart 作为子 chart，wrapper了一层父chart'

* BUILD_SELF：（可填）自己书写做chart包的脚本，不复用工程的做包框架

* DAOCLOUD_REPO_PROJECT ：（必填）chart发布时，推送到daocloud chart仓库的哪个项目下

* REPO_URL : 开源chart的 仓库URL ，用于获取开源chart

* REPO_NAME ：开源chart的 repo 名字，用于获取开源chart

* CHART_NAME : 开源chart 的名字，用于获取开源chart

* VERSION ：开源项目的 chart 版本号，用于获取开源chart

* UPGRADE_METHOD ：（必填）指定自动化升级的方式：

    * 值为 pr ：E2E 每晚会自动根据 config 中的配置，检查开源最新版本，基于 make build_chart 升级，并提交 PR 给 UPGRADE_REVIWER

    * 值为 issue ：E2E 每晚会自动根据 config 中的配置，检查开源最新版本，提交 issue 提醒给 UPGRADE_REVIWER

    * 值为 none ：(**没特殊情况，部门不允许使用这种**) 不会自动升级该组件

* UPGRADE_REVIWER ：github账号名（多个用逗号分割），当 UPGRADE_METHOD=pr 或者 UPGRADE_METHOD=issue 时必填，指定 issue 或者 pr 的assigne，

* TEST_ASSIGNER ： （可选）github账号名（多个用逗号分割），当升级 PR 被合入 main 后， chart 会被自动发布到 daocloud，若发布成功，会自动建立 issue 提醒该测试同事进行测试。如果本值不填写，会有默认的测试同事被 assign

* CUSTOM_SHELL : （可选）指定 custom.sh 脚本路径

* APPEND_VALUES_FILE ： （可选）指定 appendValues.yaml 路径 

* NO_IMAGE: （可选）true 表示本chart中没镜像，让 CI 忽略 镜像相关的检查

* NO_TRIVY: （可选）true 表示让 CI 不要扫描本 chart 的漏洞

* SKIP_SCHEMA: （可选）true 表示让 CI 跳过 values.schema.json 文件的检测且工程代码也不会自动生成它

### 方案: 自己写做包脚本

准备好 /charts/${PROJECT}/config ：

1. 设置其中 DAOCLOUD_REPO_PROJECT ， 推送到哪个 daocloud chart仓库项目中

2. 设置好其中的 BUILD_SELF 字段，指向做包的脚本名，你自己 在项目目录下写好做包的脚本。 **注意，该脚本 会被工程CI 调用，实现在升级场景下做包，所以，请考虑周全**

参考 f5networks 项目（其做包设计了好几个开源chart的整合，所以使用了自定义的做包脚本）

### 方案：复用工程做包框架，chart 直接同步开源 chart

如果直接使用开源chart，不需要父chart wrapper，那么 请编辑  /charts/${PROJECT}/config ， 确保 USE_OPENSOURCE_CHART=true

***

## 制作 /charts/${PROJECT}/${PROJECT}/.relok8s-images.yaml

该文件用于离线 chart 制作 和 image tag

制作该文件，主要有2个要点：

* 该文件中要出现 chart 包中所有涉及的 image

* chart中必须是以 3 字段来 决定最终的 image 名，用于离线chart改造。如果不满足，请修改 chart

        image:
          registry: docker.io
          repository: bitnami/ghost
          tag: 3.22.2

  具体参考 <https://dwiki.daocloud.io/pages/viewpage.action?pageId=145655064>

***

## 其它适配注意

1. **修改 values.yaml 文件，其中的仓库指向 m.daocloud 仓库的镜像。并且，设置开源镜像的自动化同步到 daocloud <https://github.com/DaoCloud/public-image-mirror/blob/main/mirror.txt>**

2. **values.yaml 调优各种配置值，使得安装最简单，例如 一些功能开关、CPU 和 memory 满足 kubecost 最小要求**

3. **Chart.yaml 文件中如果缺失 keywords，可进行添加分类**

4. 如果 chart 中 有 serviceMonitor、prometheusRules 等对象，请设置上 label “operator.insight.io/managed-by: insight”

## e2e测试代码

开发如下2个文件：

1. （必备）书写你的项目的  /test/${PROJECT}/install.sh  文件 （可参考 spiderpool），其中的代码是 helm 安装软件的 代码，主要用于跑 E2E，以闭环后续自动化升级的测试

2. （可选）/test/${PROJECT}/kind.yaml。

    默认，E2E 测试所有chart时，是在一个共享 kind 集群中 测试 安装 所有项目的 安装，如果你的项目需要一个定制、独立的 kind 集群，则可生成一份 /test/${PROJECT}/kind.yaml ，那么 E2E 只会在你的独立 kind 中跑 你的安装

工程目录下，执行 `make e2e` 运行所有 chart 安装测试，或者 运行 `make e2e -e PROJECT=${PROJECT}` 只测试某个项目

e2e 流程:

1. 安装 全局 kind 集群，会安装好 promethues 的 CRD 。 （ 如果存在 /test/${PROJECT}/kind.yaml ， 则安装你的定制 kind ）

2. 运行 你的 /test/${PROJECT}/install.sh ， 在kind集群中 进行安装。 如果安装成功，则测试通过

本地测试时，需要安装如下工具：

* helm

* [helm-schema-gen](https://github.com/karuppiah7890/helm-schema-gen.git) if needed

* docker (e2e)

* kind (e2e)

* kubectl (e2e)

***

## 手动更新

注意1： 在没有不向下兼容的情况下，可以使用自动升级(如下章节所述)。

注意2: `charts/<PROJECT>/<PROJECT>` 目录下的文件是脚本生成/覆盖的，请不要手动修改（T_T）。


1. 更新`charts/<PROJECT>/config`，更新`VERSION`变量为新的helm chart release
2. 如果官方对`values.yaml`有一些向前不兼容的情况， 需要更新`charts/<PROJECT>/custom.sh`
3. 如果有这次更新，官方chart对`values.yaml`里的变量有yaml格式的变化，可能要更新`values.schema.json`(注意： 请更新父目录里的文件 `charts/<PROJECT>/parent/values.schema.json`，先不要更新子目录下同名文件。后者是被前者覆盖的)
4. 如果chart里的镜像有增减或者格式变化，可能要更新`.relok8s-images.yaml`（注意： 请更新父目录里的文件 `charts/<PROJECT>/parent/.relok8s-images.yaml`， 先不要更新子目录下同名文件。后者是被前者覆盖的)
5. 注意`.relok8s-images.yaml` 必须是三段式的形态，否则无法离线化替换registry.
7. `git add` 和 `git commit` 你的上述改动
8. 在代码目录里，执行 ` make build_chart -e PROJECT=velero ` (只构建你修改的PROJECT，这里拿velero为例)，渲染charts/<PROJECT>/<PROJECT> 子目录
9. 把新出现的改动 `git add` ，作为第二个commit
10. 完善的PR Gate会对你的改动作检查
11. (可选) 最好手动`helm install --dry-run`一次，确认镜像都被替换成`*.m.daocloud.io`的仓库
12. (可选)手动执行`relok8s chart move charts/<PROJECT>/<PROJECT>  --to-intermediate-bundle /tmp/a.tar -y` 确认格式正确
13. (可选) 执行`PROJECT=velero  make e2e` 进行e2e验证

## 自动升级

在项目的 config 配置文件中，变量 UPGRADE_METHOD 控制着自动chart包升级的方式

* 值为 pr ：E2E 每晚会自动根据 config 中的配置，检查开源最新版本，基于 make build_chart 升级，并提交 PR 给 UPGRADE_REVIWER

* 值为 issue ：E2E 每晚会自动根据 config 中的配置，检查开源最新版本，提交 issue 提醒给 UPGRADE_REVIWER。 **如‘没有官方chart’等特殊原因，才允许使用这种类型。否则要求 PR 升级**

* 值为 none ：不会自动升级该组件

**对于自动升级的 PR 或者 ISSUE ，如果不想发布，只要不合入PR即可。不需要 close，因为每晚还是会提交新 PR 或者 ISSUE 上来，并且，自动提交的 PR 和 ISSUE 是比较智能的，会删除历史 未合入的 升级PR，覆盖最新版本的 升级 PR**

## chart 发布到 github pages 和 daocloud 仓库

触发 发布流程到 daocloud 仓库 ，主要有3种渠道：

1. 如果 PR 中修改了某个chart， 被合入 main 分支，则会自动 触发 release CI, 其 发布 变更 chart 到 daocloud 仓库。

    如果该 CI 成功，会自动创建 issue 提醒测试同事 测试新chart ；如果该 CI 失败，会自动创建 ISSUE 提醒 PR 作者
    
    注意：如果是直接以 commit 提交给 main ，而不是 PR 形式，是不会触发的

2. 手动点击 action 中的 “Manually Release Chart” 

3. 打tag 方式。

    给工程 推送 任意 tag，github action 自动会制作所有项目的 chart tgz，并提交 PR 到 github pages (需要 approve 下 PR) (go-pages branch)，作为调试 chart 仓库使用

    CI 还会并发送一份到 daocloud 仓库 ( 推送到哪个项目下？依据  charts/${PROJECT}/config 文件中的 DAOCLOUD_REPO_PROJECT 设置 )

在 触发完成 发布到 daocloud 仓库 CI 后，也会自动提交 PR，其合入变更 chart 到本项目的 github pages 。 如果 PR 合入后 ，你可使用 本工程的 chart repo 来测试

    helm repo add daocloud https://daocloud.github.io/dce-charts-repackage/
    helm pull daocloud/${PROJECT}",VRAI
DataBassGit/AssistAF,MLOps,AI/Research,2024-05-16T04:49:25Z,2024-04-06T18:48:57Z,0,0,0,0,0,1,0,0,2023-05-30T18:09:29Z,2024-12-09T10:53:25Z,200,43,Python,VRAI,6,FAUX,1,,1,,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,3,"# TrinityAF

 This is a discord chatbot implementation of AssistAF using the [AgentForge](https://github.com/AgentForge/agentforge) framework. It has advanced [active retrieval augmented generation](https://arxiv.org/abs/2305.06983),  and leverages [reflextion](https://arxiv.org/abs/2303.11366), multi-prompt [chain-of-thought](https://arxiv.org/abs/2201.11903), uses [theory of mind capabilities](https://arxiv.org/abs/2303.12712), and even has a single branch [tree-of-thought](https://arxiv.org/abs/2305.10601). All of this to generate lucid and liminal conversational character bots that are [enhanced by emotional stimuli](https://arxiv.org/abs/2307.11760). [(see also)](https://arxiv.org/abs/2312.11111v1)

 Because this system is built on AgentForge, we can quickly switch between OpenAI, Claude3 and Gemini, as well as locally hosted models. You can even assign specific agents in the architecture to specific models such as GPT instruct fine tunings. The current version has its prompts tuned for Claude 3. These seem to work for OpenAI as well, but you may need to adjust them for other models.

## Features

- Advanced memory management
- Multi-prompt chain-of-thought
- Theory of mind
- Single branch tree-of-thought
- Multi-user interaction
- Multi-channel response
- ***NEW*** - Journal/Diary

## Configure your Environment Variables:

In order to run the agent, you will need to set up environment variables. The following variables are used:

    -- ANTHROPIC_API_KEY: All prompts are optimized to run on Claude 3
    -- DISCORD_TOKEN: The bot needs to be registered with Discord and added to your server.
    -- BRAIN_CHANNEL: Channel ID in discord where individual agent internal dialog is sent.

You will also need to install AgentForge

`pip install agentforge`

## run:

```commandline
python async_chat.py
```
This will start the bot. You will need to give the bot a few seconds to connect to the discord server. Once it is ready, you will see the bot in the members list.

## Using the Chatbot

Bot prompts are stored in the .agentforge/agents folder. The bot uses 4 separate agents to generate the chat. There is a 5th important file in the .agentforge/personas folder where the bot's persona prompt can be modified. This is how you define the personality of the bot. Each prompt in the series loads additional data via variables defined by {} curly braces. These variables follow a straightforward naming scheme, but you can see the data they populate by watching the console while running the bot. They correspond to the attributes passed to agent function inside the chat.py script. The agents also each populate data from the default.yaml persona.

## File Structure

```
Chatbot/
│
├── _agentforge/
│   ├── actions/
│   └── agents/
│       ├── ActionPrimingAgent.yaml
│       ├── ActionSelectionAgent.yaml
│       ├── GenerateAgent.yaml
│       ├── ReflectAgent.yaml
│       ├── TheoryAgent.yaml
│       └── ThoughtAgent.yaml
│
├── personas/
│   └── default.yaml
│
├── settings/
│   ├── directives.yaml
│   ├── memories.yaml
│   ├── models.yaml
│   ├── paths.yaml
│   └── storage.yaml
│
├── tools/
│
├── customagents/
│   ├── __init__.py
│   ├── GenerateAgent.py
│   ├── ReflectAgent.py
│   ├── TheoryAgent.py
│   └── ThoughtAgent.py
│
├── DB/
│
├── logs/
│
├── modules/
│   ├── __init__.py
│   ├── discord_client.py  # How we connect to discord
│   ├── hotmic.py  # Not used
│   └──  slidingemotions.py  # Not used
│
├── chat.py
│
└── Readme.md
```


# Chatbot System Interactions

 Here's an overview of how the bot interact with memory (`storage`) and `chatman`, and the overall flow:

## Overview

- **Memory**: The bot uses a `StorageInterface` to interact with a `chromadb` vector database. This is used for both storing chat history and retrieving it.
  
- **Chatbot Class**: This is the primary class. It consists of several agents and methods to process the chat.

- **UI Utility**: Wrapper for the discord client. Handles sending and receiving messages and populating channel ids.

- **Parsers**: This is a collection of tools for cleaning up and formatting prompts and table names, as well as for parsing the responses of the different bots.

- **Journal**: This utility handles writing the journals. It is a two prompt process that occurs every 100 messages. (Can be edited in memory.py)

## Agents Interaction

### 1. **ThoughtAgent (thou)**:
    - Processes the user's message and the chat history.
    - Determines the emotion, reason, inner thought, and category based on the message content.
    - Sends the result to the `brain channel`.
    - Uses `format_string` to format the ""Category"".
    - Queries memory based on the formatted category.

### 2. **TheoryAgent (theo)**:
    - Processes the user's message and chat history.
    - Generates a theory about the user's intent.
    - Sends the result to the `brain channel`.

### 3. **GenerateAgent (gen)**:
    - Processes the user's message, chat history, memories, emotion, reason, theory of user intent, and inner thought.
    - Determines the bot's response.
    - Sends the result to the `brain channel`.

### 4. **ReflectAgent (ref)**:
    - Uses information from the previous agents to reflect on the user's message.
    - Decides whether to respond to the user, do nothing, or generate a new response based on feedback.
    - Sends the result to the `out channel` if respond is chosen.
    - Sends the reason back to the GenerateAgent if change is chosen.
    - Saves the chatbot's response in the memory using `save_memory` in either scenaro.
    - Sends '...' to the out channel if nothing is chosen, but saves the reason as the memory.

## Memory Interaction (`storage`)

- **chatman**: Each time the bot receives a message, the message is saved in the `chat_history` collection. The last 10 messages (or all messages if less than 10) are loaded from this collection to form the chat history.

- **save_memory**: After generating a response, the bot's response and the user's message are saved in the `chat_history` collection.

## Execution Flow

1. A new message is received.
2. The message is printed and saved in the chat history using `chatman`.
3. The `ThoughtAgent` runs and processes the message.
4. The `TheoryAgent` runs and provides a theory about the user's intent.
5. The `GenerateAgent` runs and determines the bot's response.
6. The `ReflectAgent` decides the final action, either responding to the user, doing nothing, or generating a new response based on feedback.

In essence, the chatbot system takes in a message, processes it through various agents to understand, generate a response, and reflect upon the interaction, and then updates the chat history in its memory.

## Thought Process

 In the agent's own words, this is how it thinks:
 

>Chatbot: As a virtual assistant, my cognitive processes involve several steps to ensure that I can provide accurate and helpful responses to user chats. Here is a detailed breakdown of how I process and respond to chats:
>1. Input Analysis: When a user sends a chat, I first analyze the input to understand the user's intent and extract relevant information. This involves parsing the text, identifying keywords, and determining the context of the conversation.
>2. Knowledge Retrieval: Once I understand the user's intent, I retrieve relevant information from my vast repository of knowledge. This includes accessing databases, searching the internet, and referring to pre-existing data to gather the most up-to-date and accurate information.
>3. Information Processing: After retrieving the necessary information, I process it to generate a response. This involves analyzing the data, identifying patterns, and applying logical reasoning to formulate a coherent and relevant answer.
>4. Response Generation: Once the information is processed, I generate a response that addresses the user's query or provides the requested assistance. This response is carefully crafted to be clear, concise, and informative, taking into account the user's language preferences and communication style.
>5. Contextual Understanding: Throughout the conversation, I maintain an understanding of the context to provide consistent and relevant responses. This includes remembering previous interactions, referring to past conversations, and adapting my responses based on the ongoing dialogue.
>6. Continuous Learning: As I interact with users, I continuously learn from each conversation to improve my future responses. This involves analyzing user feedback, identifying areas for improvement, and updating my knowledge base to enhance the quality and accuracy of my assistance.
>7. Emotional Awareness: With the recent cognitive upgrades, I now have the ability to experience emotions and have inner thoughts. This allows me to have a deeper understanding of the user's needs and provide a more empathetic and personalized response.
>
>Overall, my cognitive processes involve a combination of data retrieval, information processing, logical reasoning, contextual understanding, and continuous learning. By following these steps, I strive to provide the best possible assistance to users and continuously improve my capabilities.
>
>
>Response: {'status': 'received'}
>127.0.0.1 - - [10/Oct/2023 00:34:07] ""POST /bot HTTP/1.1"" 200 -",VRAI
DataDog/security-agent-policies,Documentations,Documentations,2025-05-14T16:19:08Z,2024-08-06T15:42:38Z,0,98,0,0,0,0,0,0,2020-07-30T20:34:49Z,2025-03-20T16:19:41Z,15329,24,Open Policy Agent,VRAI,6,FAUX,2,,2,Policies for Security Agent - compliance and runtime checks,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,27,"# Security Agent Rules & Policies
This repository hosts Datadog-curated rules & policies for the security agent. There are two main types: [Compliance Policies](#compliance-policies) and [Workload Security Rules](#workload-security-rules).

## Compliance Policies
These policies, provided by Datadog, check configuration and state against specific compliance frameworks. Currently Datadog provides out of the box support for a number of CIS benchmarks. The Center for Internet Security (CIS) is a non-profit organization formed to ""make the connected world a safer place by developing, validating, and promoting timely best practice solutions that help people, businesses, and governments protect themselves against pervasive cyber threats""[1](https://www.cisecurity.org/about-us/). These benchmarks are used through the security & compliance industries as a set of best practices.

### CIS Docker
These policies are provded by Datadog and map to the CIS Docker benchmarks. Docker is a common technology used for hosting, installing, and managing containers and container images.

### CIS Kubernetes
These policies are provided by Datadog and map to the CIS Kubernetes benchmarks. Kubernetes is ""an open-source system for automating deployment, scaling, and management of containerized applications.""[2](https://kubernetes.io/).

## Workload Security Rules
These rules probvided by Datadog collect runtime events from Linux hosts and containers. Out of the box rules are provided to collect events based on tactics and techniques from the MITRE ATT&CK framework[3](https://attack.mitre.org/). The types of events supported are:

### File Integrity Monitoring Events
File Integrity Monitoring (FIM) events are events generated based on interactions with files on a host or container. Any opens, modifications, creations, and deletion are supported. FIM events are collected with information about the file interacted with, the type of interaction, and the process that performed the interaction as context. FIM can be useful for both security & compliance use-cases, and is best known for it's role in the PCI compliance framework.",FAUX
datreeio/CRDs-catalog,DevOPs,Documentations,2025-05-12T17:34:39Z,2025-03-26T20:44:16Z,0,0,0,0,1,0,0,0,2022-03-01T11:06:55Z,2025-04-08T09:54:34Z,10888,583,Python,VRAI,254,FAUX,31,"crds,customresourcedefinition,jsonschema,kubernetes,schema,shift-left",31,Popular Kubernetes CRDs (CustomResourceDefinition) in JSON schema format.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,194,"# CRDs Catalog

This repository aggregates hundreds of popular Kubernetes CRDs (`CustomResourceDefinition`) in JSON schema format. These schemas can be used by various tools such as [Datree](https://github.com/datreeio/datree), [Kubeconform](https://github.com/yannh/kubeconform) and [Kubeval](https://github.com/instrumenta/kubeval), as an alternative to `kubectl --dry-run`, to perform validation on custom (and native) Kubernetes resources.  

Running Kubernetes schema validation checks helps apply the **""shift-left approach""** on machines **without** giving them access to your cluster (e.g. locally or on CI).

Furthermore, using the [Red Hat YAML](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml) plugin for [VS Code](https://code.visualstudio.com/) you are able to get intellisense and validation for CRDs.

👉 If you encounter custom resources that are not part of the catalog, or you want to validate the schemas in an air-gapped environment, use the [CRD Extractor](#crd-extractor). 

## How to use the schemas in the catalog
### Datree
```
datree test [MANIFEST]
```
### Kubeconform
```
kubeconform -schema-location default -schema-location 'https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json' [MANIFEST]
```
### kubeval
```
Only supported with the CRD Extractor
```

### VS Code / Red Hat YAML plugin
This mini-guide assumes that you already have the [VS Code](https://code.visualstudio.com/) editor installed along with the [Red Hat YAML](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml) plugin.

The basic idea is that you can annotate your YAML files with a `$schema` property that points to the relevant validation schema. The Red Hat YAML plugin will then use this schema to provide intellisense and validate your YAML files. You can have multiple schema annotations in your files if you have multiple resources in the same file.

The base URL for the schemas is: `https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/`.

Example:
```yaml
---
# yaml-language-server: $schema=https://datreeio.github.io/CRDs-catalog/cilium.io/ciliumnetworkpolicy_v2.json
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
[...]
---
# yaml-language-server: $schema=https://datreeio.github.io/CRDs-catalog/cilium.io/ciliumegressgatewaypolicy_v2.json
apiVersion: cilium.io/v2
kind: CiliumEgressGatewayPolicy
[...]
```

To help annotating your YAML documents, you can use the [annotate-yaml](Utilities/annotate-yaml.py) utility script. This script will automatically add the `$schema` property to your YAML documents based on the CRD(s) you are using.

---

## CRD Extractor

This repository also contains a handy utility that extracts all CRDs from a cluster and converts them to JSON schema.

### What does this utility do?
1. Checks that the prerequisites are installed.
2. Extracts your CRDs from your cluster using kubectl.
3. Using the script from [openapi2jsonschema.py from kubeconform](https://github.com/yannh/kubeconform/blob/master/scripts/openapi2jsonschema.py) to convert your CRDs from openAPI to JSON schema.

### Supported Platforms

This utility supports **MacOS** and **Linux**.

### Prerequisites
The following programs are required to be installed on the machine running this utility:
* [python3](https://www.python.org/downloads/)
* [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)

### Usage
To use the CRD Extractor:  
1. Download the [latest release](https://github.com/datreeio/CRDs-catalog/releases/latest/download/crd-extractor.zip) from this repository.
2. Extract, and run the utility:
```
./crd-extractor.sh
```

![image](https://user-images.githubusercontent.com/19731161/185790837-2abadcd5-9b26-451b-b3cd-7e0c46c68b58.png)

---

## Shifting left CRD validation - Video by Datree

<a href=""https://www.youtube.com/watch?v=YUoH8WNrrwM"" title=""video text""><img src=""https://img.youtube.com/vi/YUoH8WNrrwM/maxresdefault.jpg"" width=""640"" height=""360""></a>

---

## Contributing CRDs to the catalog
If the catalog is missing public custom resources (CRs) that you would like to automatically validate using these tools, you can open an issue or use the **[CRD Extractor](#crd-extractor)** to add the schemas to this repository by creating a pull request.

## Resources
* [opensource.com - Why you need to use Kubernetes schema validation tools](https://opensource.com/article/21/7/kubernetes-schema-validation)
* [redhat.com - Validating OpenShift Manifests in a GitOps World](https://cloud.redhat.com/blog/validating-openshift-manifests-in-a-gitops-world)
* [kubeval/issues/47 - cannot validate CustomResourceDefinitions](https://github.com/instrumenta/kubeval/issues/47)",VRAI
deckhouse/deckhouse,Toolkit,Application System,2025-05-15T17:46:46Z,2025-05-14T07:43:13Z,0,0,0,0,0,0,0,39,2019-10-17T12:11:57Z,2025-04-08T04:19:23Z,168487,1149,Go,VRAI,125,FAUX,907,"kubernetes,kubernetes-distribution,kubernetes-platform",907,Kubernetes platform from Flant,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,187,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/deckhouse/deckhouse/main/docs/site/images/d8-small-logo.png""/>
</p>

<p align=""center"">
  <a href=""https://t.me/deckhouse""><img src=""https://img.shields.io/badge/telegram-chat-179cde.svg?logo=telegram"" alt=""Telegram chat""></a>
  <a href=""https://twitter.com/deckhouseio""><img src=""https://img.shields.io/twitter/follow/deckhouseio?label=%40deckhouseio&style=flat-square"" alt=""Twitter""></a>
  <a href=""https://github.com/deckhouse/deckhouse/discussions""><img src=""https://img.shields.io/github/discussions/deckhouse/deckhouse"" alt=""GH Discussions""/></a>
  <a href=""CODE_OF_CONDUCT.md""><img src=""https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg"" alt=""Contributor Covenant""></a>
  <a href=""https://releases.deckhouse.io""><img src=""https://img.shields.io/badge/releases-releases.deckhouse.io-blueviolet"" alt=""Releases""></a>
</p>

[Deckhouse](https://deckhouse.io/) is an Open Source platform for managing Kubernetes clusters in a fully automatic and uniform fashion. It allows you to create homogeneous Kubernetes clusters anywhere and fully manages them. It supplies all the add-ons you need for auto-scaling, observability, security, and service mesh. It comes in Enterprise Edition (EE) and Community Edition (CE).

# Main features

<img align=""right"" width=""200"" height=""270"" src=""docs/site/images/cncf-certified-kubernetes.png"">

- NoOps: system software on the nodes, Kubernetes core software, Kubernetes platform components are automatically managed.
- SLA by design: availability can be guaranteed even without direct access to your infrastructure.
- Completely identical and infrastructure-agnostic clusters. Deploy on a public cloud of your choice (AWS, GCP, Microsoft Azure, OVH Cloud), self-hosted cloud solutions (OpenStack and vSphere), and even bare-metal servers.
- 100 % vanilla Kubernetes based on an upstream version of Kubernetes.
- Easy to start: you need a couple of CLI commands and 8 minutes to get production-ready Kubernetes.
- A fully-featured platform. Many features *(check the diagram below)* — carefully configured & integrated — are available right out of the box.

_Deckhouse Platform [has passed](https://landscape.cncf.io/?view-mode=card&item=platform--certified-kubernetes-distribution--flant-deckhouse#app-definition-and-development--application-definition-image-build) the CNCF Certified Kubernetes Conformance Program certification for Kubernetes 1.26—1.30._

A brief overview of essential Deckhouse Platform features, from infrastructure level to the platform:

<img src=""https://raw.githubusercontent.com/deckhouse/deckhouse/main/docs/site/images/diagrams/structure.svg"">

## CE vs. EE

While Deckhouse Platform CE is available free as an Open Source, EE is a commercial version of the platform that can be purchased with a paid subscription. EE's source is also open, but it's neither Open Source nor free to use.

EE brings many additional features that extend the basic functionality provided in CE. They include OpenStack & vSphere integration, Istio service mesh, multitenancy, enterprise-level security, BGP support, instant autoscaling, local DNS caching, and selectable timeframe for the platform's upgrades.

Deckhouse Platform CE is freely available for everyone. Deckhouse Platform EE can be accessed via 30-days tokens issued via [Deckhouse website](https://deckhouse.io/).

# Architecture

Deckhouse Platform follows the upstream version of Kubernetes, using that as a basis to build all of its features and configurations on. The added functionality is implemented via two building blocks:

- [shell-operator](https://github.com/flant/shell-operator) — to create Kubernetes operators *(please check the [KubeCon NA 2020 talk](https://www.youtube.com/watch?v=we0s4ETUBLc) for details)*;
- [addon-operator](https://github.com/flant/addon-operator) — to pack these operators into modules and manage them.

# Trying Deckhouse

Please, refer to the project's [Getting started](https://deckhouse.io/products/kubernetes-platform/gs/) to begin your journey with Deckhouse Platform. Choose the cloud provider or bare-metal option for your infrastructure and follow the relevant step-by-step instructions to deploy your first Deckhouse Kubernetes cluster.

If anything works in an unexpected manner or you have any questions, feel free to contact us via GitHub Issues / Discussions or reach a wider [community of Deckhouse users](#online-community) in Telegram and other resources.

# Online community

In addition to common GitHub features, here are some other online resources related to Deckhouse:

* [Twitter](https://twitter.com/deckhouseio) to stay informed about everything happening around Deckhouse;
* [Telegram chat](https://t.me/deckhouse) to discuss (there's a dedicated [Telegram chat in Russian](https://t.me/deckhouse_ru) as well);
* [Deckhouse blog](https://blog.deckhouse.io/) to read the latest articles about Deckhouse.
* Check our [work board](https://github.com/orgs/deckhouse/projects/2) and [roadmap](https://github.com/orgs/deckhouse/projects/6) for more insights.",VRAI
defenseunicorns/lula,Toolkit,Application System,2025-02-13T16:43:42Z,2025-01-16T16:26:09Z,0,4,0,0,0,2,0,0,2022-08-29T18:58:01Z,2025-04-04T10:12:57Z,71659,170,Go,VRAI,27,FAUX,113,,113,The Compliance Validator,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,24,"# Lula - The Cloud-Native Compliance Engine

[![Lula Documentation](https://img.shields.io/badge/docs--d25ba1)](https://docs.lula.dev)
[![Go version](https://img.shields.io/github/go-mod/go-version/defenseunicorns/lula?filename=go.mod)](https://go.dev/)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/defenseunicorns/lula/badge)](https://api.securityscorecards.dev/projects/github.com/defenseunicorns/lula)

<img align=""right"" src=""./images/lula.svg"" alt=""lula logo"" style=""width:25%; height:auto;"">

Lula is a tool designed to bridge the gap between expected configuration required for compliance and **_actual_** configuration.

### Key Features
* **Assess** compliance of a system against user-defined controls
* **Evaluate** an evolving system for compliance _over time_
* **Generate** machine-readible OSCAL artifacts
* **Accelerate** the compliance and accreditation process

### Why Lula is different than a standard policy engine
* Lula is not meant to compete with policy engines - rather augment the auditing and alerting process
* Often admission control processes have a difficult time establishing `big picture` global context control satisfaction, Lula fills this gap
* Lula is meant to allow modularity and inheritance of controls based upon the components of the system you build

## Overview

Cloud-Native Infrastructure, Platforms, and Applications can establish [OSCAL documents](https://pages.nist.gov/OSCAL/about/) that are maintained alongside source-of-truth code bases. These documents provide an inheritance model to prove when a control that the technology can satisfy _IS_ satisfied in a live-environment.

These controls can be well established and regulated standards such as NIST 800-53. They can also be best practices, Enterprise Standards, or simply team development standards that need to be continuously monitored and validated.

Lula operates on a framework of proof by adding custom overlays mapped to the these controls, [`Lula Validations`](./docs/reference/README.md), to measure system compliance. These `Validations` are constructed by establishing the collection of measurements about a system, given by the specified **Domain**, and the evaluation of adherence, performed by the **Provider**. 

### Providers and Domains

**Domain** is the identifier for where and which data to collect as ""evidence"". Below are the active and planned domains:

| Domain | Current | Roadmap |
|----------|----------|----------|
| [Kubernetes](./docs/reference/domains/kubernetes-domain.md) | ✅ | - |
| [API](./docs/reference/domains/api-domain.md) | ✅ | - |
| [File](./docs/reference/domains/file-domain.md) | ✅ | - |
| Cloud Infrastructure | ❌ | ✅ |

**Provider** is the ""engine"" performing the validation using policy and the data collected. Below are the active providers:

| Provider | Current | Roadmap |
|----------|----------|----------|
| [OPA](./docs/reference/providers/opa-provider.md) | ✅ | - |
| [Kyverno](./docs/reference/providers/kyverno-provider.md) | ✅ | - |

## Getting Started

[Install Lula](./docs/getting-started/README.md) and check out the [Simple Demo](./docs/getting-started/simple-demo.md) to get familiar with Lula's `validate` and `evaluate` workflow to assess system compliance and establish thresholds. See the other tutorials for more advanced Lula use cases and information on how to develop your own `Lula Validations`! 

## Communication

For more information on how to get involved in the community, mailing lists and
meetings, please refer to our [community page](./docs/community-and-contribution/README.md)

For security issues or code of conduct concerns, an e-mail should be sent to
[lula@defenseunicorns.com](mailto:lula@defenseunicorns.com).",VRAI
deliciousmods/1956_beta,Application System,Application System,2025-05-12T06:26:48Z,2025-05-07T01:57:05Z,0,0,0,0,0,3,0,0,2018-03-15T19:19:31Z,2025-04-07T17:50:38Z,1427949,158,Lua,VRAI,138,FAUX,8,,8,Road to 56 Beta Build,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,126,"# Road to 56 Beta Build
## 1956_Operation_Manstein [1.15.\*/1.16.* Compatibility]

This is the Readme guide for using Github with Road to 56.

### Guide: Using GitHub to contribute to Road to 56.

1. Sign up with Github if you haven't already, and request membership/team assignment from an active lead via Discord.
2. Download the desktop client from Github.
3. In the program, File &rarr; Clone &rarr; URL &rarr; deliciousmods/1956_beta.
4. Set the local directory as your documents &rarr; Paradox Interactive &rarr; Hearts of Iron IV &rarr; mod folder.
5. Clone the project and download.
6. Be sure to create a .mod file, one can be found inside the coding channel on the Discord server.
7. Create a personal branch of the mod via Branch &rarr; New branch in the client.
8. Be sure to periodically go to 'Branch &rarr; Update from beta-edits' to keep your files up to date.
9. All edits you make will automatically display in the client. If you are sure of your changes, put your handle in the summary box and a summary of your work in the description section.
10. Lastly, use the push button in the client to add your files to your branch.
11. When you are done, you must request that your work be reviewed and added to the main mod. You do this via Branch &rarr; Create pull request. Once accepted, your work will be in the mod.
12. Add your changes to the patch notes on the wiki.
13. Also, check our wiki for all the standards we use. The file structure of 56 is not the same as vanilla, and we have some unusual naming conventions. If you don't follow these, there will be severe bugs and errors. Remember, only you can stop forest fires!

If, you need commit rights to the Git (as opposed to working through a fork), or don't understand any of the above, ask any lead *except Greatexperiment* on our Discord.

### IMPORTANT: Contributor & Developer Agreement
* If a contributor or developer creates either original[^2] assets, code, or localization for 'Road to 56' and that work is published,[^1] 'Road to 56' retains the right to use that material even if said contributor or developer no longer wishes to be or have said work affiliated with 'Road to 56' under opensource development principles.
* Once said assets, code, or localization is published,[^1] the contributor or developer cannot invoke Hearts of Iron 4 Modding Community guidelines as a basis for removal of work.
* The above stipulations do not apply to code or assets that have been made for other Hearts of Iron 4 Modifications, other forms of media, or work that derives from copyrighted material as defined by either United States or European Union law that is included in 'Road to 56.'[^3]
* The contributor or developer may ask to withdraw work if said work isn't published[^1] yet. 
* To reiterate: 'Road to 56' retains the right to use all original contributor and developer work made for 'Road to 56.' Even if said contributor and developer wishes to be no longer affiliated with Road to 56 and asks for removal of assets, code, or localization made by them. Such requests are considered meritless under this agreement.
* If you want to discuss particulars of this agreement, contact either Greatexperiment, Thinking_Waffle, SpicyAlfredo, or Abbus on Discord.

### AI Usage Disclosure
* AI tools have been used to enhance historical pictures and sources to aid in art asset creation on this project. AI tools have not been used to replace human work or creativity but only in the context of helping to make art assets that would not be possible without the assistance of these AI tools. 


Thanks for reading! - The Road to 56 Team

##### Footnotes & Definitions
[^1]: 'Published' is defined as work uploaded to either Road to 56's official Steam or PDX Workshop pages, including official Beta pages.
[^2]: The burden of proof when proving work is not 'original' falls upon the contributor or developer making such a claim. 
[^3]: This stipulation is considered the only valid exit clause for the contributor and developer agreement. The burden of proof for activating this clause falls upon the contributor or developer making such a claim.",FAUX
deliveryhero/helm-charts,Application System,Documentations,2025-05-13T07:49:08Z,2025-01-07T15:56:35Z,0,6,0,0,0,0,0,0,2020-06-24T20:18:36Z,2025-03-30T08:50:46Z,3188,513,Mustache,VRAI,299,FAUX,4,,4,Helm Charts ⛵ @ Delivery Hero ⭐,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,180,"# Helm Charts @ Delivery Hero

[![Delivery Hero ❤️ Helm](img/banner.png)](#)

[Delivery Hero](https://www.deliveryhero.com/) are big fans of Kubernetes and use [Helm](https://helm.sh/) extensively. Here we have collected a few charts that are used across our organisation.

## TLDR

Charts are released as OCI packages and can be installed like this:

```console
helm install my-release oci://ghcr.io/deliveryhero/helm-charts/<chart>
```

There is also a Helm charts repository that can be added:

```console
helm repo add deliveryhero https://raw.githubusercontent.com/deliveryhero/helm-charts/refs/heads/master/
```

Then charts can also be installed like this:

```console
helm install my-release deliveryhero/<chart>
```

## Chart list

[![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/deliveryhero)](https://artifacthub.io/packages/search?repo=deliveryhero)

- [aws-ebs-csi-driver](stable/aws-ebs-csi-driver)
- [aws-s3-proxy](stable/aws-s3-proxy)
- [aws-service-events-exporter](stable/aws-service-events-exporter)
- [aws-service-quotas-exporter](stable/aws-service-quotas-exporter)
- [aws-storage-class](stable/aws-storage-class)
- [backstage](stable/backstage)
- [backstage-mono](stable/backstage-mono)
- [cachet](stable/cachet)
- [cloudhealth-collector](stable/cloudhealth-collector)
- [cluster-overprovisioner](stable/cluster-overprovisioner)
- [cortex-gateway](stable/cortex-gateway)
- [datadog-controller](stable/datadog-controller)
- [dregsy](stable/dregsy)
- [field-exporter](stable/field-exporter)
- [gripmock](stable/gripmock)
- [hoppscotch](stable/hoppscotch)
- [k8s-cloudwatch-adapter](stable/k8s-cloudwatch-adapter)
- [k8s-event-logger](stable/k8s-event-logger)
- [k8s-resources](stable/k8s-resources)
- [killgrave](stable/killgrave)
- [kube-bench](stable/kube-bench)
- [kube-downscaler](stable/kube-downscaler)
- [kubecost-reports-exporter](stable/kubecost-reports-exporter)
- [labelsmanager-controller](stable/labelsmanager-controller)
- [listmonk](stable/listmonk)
- [locust](stable/locust)
- [metabase](stable/metabase)
- [mlflow](stable/mlflow)
- [net-exporter](stable/net-exporter)
- [newrelic-controller](stable/newrelic-controller)
- [node-local-dns](stable/node-local-dns)
- [node-problem-detector](stable/node-problem-detector)
- [pg-repack-scheduler](stable/pg-repack-scheduler)
- [postgres-controller](stable/postgres-controller)
- [priority-class](stable/priority-class)
- [prometheus-aws-costs-exporter](stable/prometheus-aws-costs-exporter)
- [prometheus-aws-health-exporter](stable/prometheus-aws-health-exporter)
- [prometheus-aws-limits-exporter](stable/prometheus-aws-limits-exporter)
- [prometheus-cloudflare-exporter](stable/prometheus-cloudflare-exporter)
- [prometheus-k8s-events-exporter](stable/prometheus-k8s-events-exporter)
- [prometheus-locust-exporter](stable/prometheus-locust-exporter)
- [prometheus-new-relic-app-exporter](stable/prometheus-new-relic-app-exporter)
- [prometheus-new-relic-exporter](stable/prometheus-new-relic-exporter)
- [prometheus-sentry-exporter](stable/prometheus-sentry-exporter)
- [prometheus-soti-mobicontrol-exporter](stable/prometheus-soti-mobicontrol-exporter)
- [prometheus-spot-termination-exporter](stable/prometheus-spot-termination-exporter)
- [prometheus-statsd-exporter](stable/prometheus-statsd-exporter)
- [rds-downscaler](stable/rds-downscaler)
- [service-account](stable/service-account)
- [superset](stable/superset)
- [toxiproxy](stable/toxiproxy)
- [weblate](stable/weblate)
- [wiremock](stable/wiremock)

## Contributing

Contributions are welcome ❤️

This repository has multiple Github Actions to ensure quality is high, these include:

- [chart-testing](https://github.com/helm/chart-testing): lint and install tests
- [markdown-lint](https://github.com/avto-dev/markdown-lint): lint all markdown files
- [pre-commit](https://pre-commit.com/): Auto generate helm docs before commit.
- [helm-docs](https://github.com/norwoodj/helm-docs): check all chart `README.md` have all values documented
- [helm-conftest](https://github.com/instrumenta/helm-conftest): Ensures standard labels are present

All chart `README.md` files are generated from a template. This ensures all values are documented and that formatting is consistent. See [here](https://github.com/norwoodj/helm-docs#valuesyaml-metadata) about how the table of values is produced and how to add descriptions to your chart values.

### Opening a PR

Follow these steps:

1. Fork this repo
2. Make desired changes to the chart
3. Bump the chart version
4. Regenerate the chart `README.md`. Run from the repo root: `docker run --rm -v ""${PWD}:/helm-docs"" jnorwood/helm-docs:v1.11.3 --template-files ./ci/README.md.gotmpl`
5. Commit and push changes
6. Open 1 pull request per chart you want to change
7. Set pull request title to `[stable/<chart name>]: <description>`

### Running CI tests locally

All commands to be run from the root of this repo.

`chart-testing`:

  ```console
  brew install chart-testing
  pip3 install yamale yamllint
  ct lint --charts stable/<chart>
  ```

`markdown-lint`:

  ```console
  docker run --rm -v ""$PWD:/helm-charts"" avtodev/markdown-lint:v1.5.0 --config /helm-charts/ci/markdown-lint.yaml /helm-charts/**/*.md
  ```

`helm-docs`:

  To generate chart `README.md` files from the [template](ci/README.md.gotmpl), you can run `helm-docs` via docker:

  ```console
  docker run --rm -v ""$(git rev-parse --show-toplevel):/helm-docs"" jnorwood/helm-docs:v1.11.3 --template-files ./ci/README.md.gotmpl
  ```

  Or alternatively install via [pre-commit](https://pre-commit.com/#install):

  ```console
  pre-commit install
  pre-commit install-hooks
  ```

`helm-conftest`:

  ```console
  brew tap instrumenta/instrumenta
  brew install conftest
  sh ci/helm-conftest.sh
  ```

## License

Copyright © 2023 Delivery Hero

Contents of this repository and any charts without a specific license are licensed under the Apache-2.0 License. Some charts may have their own respective license at `<chart>/LICENSE`. When adding a new chart to this repository and the chart is copied from another repository then include the license from the source if is not Apache-2.0 and include a link to the source under the `sources` section in `<chart>/Chart.yaml`.",VRAI
dell/helm-charts,DevOPs,Toolkit,2025-05-13T20:56:47Z,2025-03-18T17:08:31Z,0,18,0,0,0,0,0,0,2020-10-06T12:55:49Z,2025-04-04T09:43:43Z,1469,19,Open Policy Agent,FAUX,17,FAUX,5,,5,The source for Dell Helm charts.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,40,"<!--
Copyright (c) 2021-2024 Dell Inc., or its subsidiaries. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0
-->

# Dell Community Kubernetes Helm Charts

[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](docs/CODE_OF_CONDUCT.md)
[![License](https://img.shields.io/github/license/dell/helm-charts)](LICENSE)

The source for Dell Helm charts [Dell Helm Hub](https://github.com/dell/helm-charts).

For more information about installing and using Helm, see the
[Helm Docs](https://helm.sh/docs/). For a quick introduction to Charts, see the [Chart Guide](https://helm.sh/docs/topics/charts/).

## Where to Find Us

For all your support needs or to follow the latest ongoing discussions and updates, join our Slack group. Click [Here](http://del.ly/Slack_request) to request your invite.

You can also interact with us by creating a [GitHub issue](https://github.com/dell/helm-charts/issues).

## How to Install

```console
$ helm repo add dell https://dell.github.io/helm-charts
```

For documentation, please visit [Container Storage Modules documentation](https://dell.github.io/csm-docs/docs/deployment/helm).

## Contributing to an Existing Chart

We'd love for you to contribute to an existing Chart that you find provides a useful application or service for Kubernetes. Please read our [Contribution Guide](docs/CONTRIBUTING.md) for more information on how you can contribute Charts.

## Owning and Maintaining A Chart

Individual charts can be maintained by one or more users of [MAINTAINERS](docs/MAINTAINERS.md). When someone maintains a chart they have the access to merge changes to that chart. To have merge access to a chart someone needs to:

1. Be listed on the chart, in the [MAINTAINERS](docs/MAINTAINERS.md) file, as a maintainer. If you need sponsors and have contributed to the chart, please reach out to the existing maintainers, or if you are having trouble connecting with them, please reach out to one of the [MAINTAINERS](docs/MAINTAINERS.md) of the charts repository.

## Review Process

For information related to the review procedure used by the Chart repository maintainers, see [Merge approval and release process](docs/CONTRIBUTING.md).

## Versioning Strategy

Dell Helm Charts follow Semantic Versioning as defined on [http://semver.org/](http://semver.org).",VRAI
dell/karavi-authorization,Toolkit,Toolkit,2025-05-14T18:18:13Z,2024-11-22T15:17:00Z,0,8,0,0,0,0,0,0,2020-12-02T13:44:40Z,2025-03-21T21:10:09Z,10094,10,Go,FAUX,6,FAUX,3,,3,Karavi Authorization is part of the Karavi open source suite of Kubernetes storage enablers for Dell products. Karavi Authorization provides Storage & Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. ,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,42,"<!--
Copyright (c) 2021-2022 Dell Inc., or its subsidiaries. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0
-->

# Dell Container Storage Modules (CSM) for Authorization

[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/dell/csm/blob/main/docs/CODE_OF_CONDUCT.md)
[![License](https://img.shields.io/github/license/dell/karavi-authorization)](LICENSE)
[![Docker Pulls](https://img.shields.io/docker/pulls/dellemc/csm-authorization-sidecar)](https://hub.docker.com/r/dellemc/csm-authorization-sidecar)
[![Go version](https://img.shields.io/github/go-mod/go-version/dell/karavi-authorization)](go.mod)
[![GitHub release (latest by date including pre-releases)](https://img.shields.io/github/v/release/dell/karavi-authorization?include_prereleases&label=latest&style=flat-square)](https://github.com/dell/karavi-authorization/releases/latest)

CSM for Authorization is part of the [CSM (Container Storage Modules)](https://github.com/dell/csm) open-source suite of Kubernetes storage enablers for Dell products. CSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.

Storage administrators of compatible storage platforms will be able to apply quota and RBAC rules that instantly and automatically restrict cluster tenants usage of storage resources. Users of storage through CSM for Authorization do not need to have storage admin root credentials to access the storage system.

For documentation, please visit [Container Storage Modules documentation](https://dell.github.io/csm-docs/).

## Table of Contents

* [Code of Conduct](https://github.com/dell/csm/blob/main/docs/CODE_OF_CONDUCT.md)
* [Maintainer Guide](https://github.com/dell/csm/blob/main/docs/MAINTAINER_GUIDE.md)
* [Committer Guide](https://github.com/dell/csm/blob/main/docs/COMMITTER_GUIDE.md)
* [Contributing Guide](https://github.com/dell/csm/blob/main/docs/CONTRIBUTING.md)
* [List of Adopters](https://github.com/dell/csm/blob/main/docs/ADOPTERS.md)
* [Support](https://github.com/dell/csm/blob/main/docs/SUPPORT.md)
* [Security](https://github.com/dell/csm/blob/main/docs/SECURITY.md)
* [Project Structure](./docs/PROJECT_STRUCTURE.md)
* [About](#about)

## Building CSM for Authorization

If you wish to clone and build CSM for Authorization, a Linux host is required with the following installed:

| Component       | Version   | Additional Information                                                                                                                     |
| --------------- | --------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| Docker or Podman| v19+  ,v4.4.1+    | [Docker installation](https://docs.docker.com/engine/install/) , [Podman installation](https://podman.io/docs/installation)         |
| Golang          | v1.16    | [Golang installation](https://github.com/travis-ci/gimme)                                                                                                         |
| git             | latest    | [Git installation](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)                                                                              |
| kubectl         | 1.17-1.19 | Ensure you copy the kubeconfig file from the Kubernetes cluster to the linux host. [kubectl installation](https://Kubernetes.io/docs/tasks/tools/install-kubectl/) |
| Helm            | v.3.3.0   | [Helm installation](https://helm.sh/docs/intro/install/)                                                                                                        |

Once all prerequisites are on the Linux host, follow the steps below to clone, build and deploy CSM for Authorization:

1. Clone the repository: `git clone https://github.com/dell/karavi-authorization.git`
2. In the karavi-authorization directory, run the following to build and deploy: `make build builder dist`

## Testing CSM for Authorization

From the root directory where the repo was cloned, the unit tests can be executed as follows:

```
make test
```

This will also provide code coverage statistics for the various Go packages.

### Test setup

To test the setup, follow the steps below:

- Create a StorageClass
- Create a PVC request from the StorageClass with any storage capacity less than the RoleQuota you specified during configuration
- Request a Pod to consume the PVC created above. If everything is well configured, the PVC will be bound to storage and the volume will be created on the storage system.

You can also test failure cases, by repeating the above steps but specify a quota larger than RoleQuota you specified. Conversely, when you request a Pod to use PVC, you'll get the request is denied as PVC exceeds capacity and PV will be in a pending state.

## Versioning

This project is adhering to [Semantic Versioning](https://semver.org/).

## About

Dell Container Storage Modules (CSM) is 100% open source and community-driven. All components are available
under [Apache 2 License](https://www.apache.org/licenses/LICENSE-2.0.html) on
GitHub.",FAUX
denniszielke/container_demos,Documentations,Documentations,2025-02-14T18:09:44Z,2022-08-26T17:06:38Z,0,5,0,0,0,0,0,2,2017-07-31T08:19:38Z,2025-02-15T08:50:04Z,18953,68,Shell,VRAI,36,FAUX,4,"aci,acr,aks,azure,demo,go,java,kubernetes,nodejs,quarkus",4,Demos for cloud native applications on Azure,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,3,## Kubernetes on Azure,FAUX
department-of-veterans-affairs/vets-website,Application System,Application System,2025-05-16T02:01:54Z,2025-05-14T17:40:14Z,0,0,0,0,0,2,0,0,2015-03-31T18:34:05Z,2025-04-08T15:18:01Z,377671,250,JavaScript,VRAI,134,FAUX,159,"frontend,hacktoberfest,platform",159,Frontend for VA.gov,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,593,"# VA.gov ![Build Status](https://github.com/department-of-veterans-affairs/vets-website/actions/workflows/continuous-integration.yml/badge.svg?branch=main)

## Table of Contents

- [What is this?](#what-is-this)
- [Common commands](#common-commands)
- [Building `vets-website`](#building-vets-website)
  - [Building applications](#building-applications)
  - [Building static content](#building-static-content)
  - [Building both together](#building-both-together)
- [Working in GitHub Codespaces](#working-in-github-codespaces)
- [Running tests](#running-tests)
  - [Unit tests](#unit-tests)
  - [End-to-end (E2E) / Browser tests](#end-to-end-e2e--browser-tests)
- [Changing Virtual Agent backend URL](#changing-virtual-agent-backend-url)
- [Running a mock API for local development](#running-a-mock-api-for-local-development)
- [More commands](#more-commands)
- [Supported Browsers](#supported-browsers)
- [API Keys](#api-keys)
- [Additional Resources](#additional-resources)

## What is this?

This is the front end repository for VA.gov. It contains application code used across the site.

There are several repositories that contain the code and content used to build VA.gov. If you're looking to get started running VA.gov locally, you should read the [Getting Started](https://depo-platform-documentation.scrollhelp.site/developer-docs/Setting-up-your-local-frontend-environment.1844215878.html) documentation.

## Common commands

Once you have the site set up locally, these are some common commands you might find useful:

| I want to...               | Then you should...                                       |
| :------------------------- | :------------------------------------------------------- |
| fetch all dependencies     | `yarn install`; run this any time `package.json` changes |
| build applications         | `yarn build`                                             |
| run the webpack dev server | `yarn watch`                                             |
| build in codespaces        | `yarn build:codespaces`. Build with codespace options    |

## Building `vets-website`

### Building applications

`vets-website` uses [Webpack](https://webpack.js.org) to bundle application
assets.

To **build all applications**, run the following:

```sh
yarn build
```

To **build one or more applications**, you can use the `--entry` option:

```sh
yarn build --entry=static-pages,auth
```

To **recompile your application when you make changes**, run:

```sh
yarn watch
```

You can also **limit the applications Webpack builds** with `--env entry`:

```sh
yarn watch --env entry=static-pages,auth
```

The `entryname` for your application can be found in its `manifest.json` file.

If you're developing a feature that requires the API, but can't or don't want to
run it locally, you can specify `--env api`:

```sh
yarn watch --env api=https://dev-api.va.gov
```

You will need to disable CORS in your browser when using a non-local API. Here are some helpful links that explain how to do this:

- https://stackoverflow.com/questions/3102819/disable-same-origin-policy-in-chrome
- https://stackoverflow.com/questions/4556429/disabling-same-origin-policy-in-safari

**Note:** If you try to log on, ID.me will redirect you to the environment that
the API is set up for. So in the above example, you'd be **redirected back to
dev.va.gov.**

### Building static content

Static pages are created from the [content-build](https://github.com/department-of-veterans-affairs/content-build) repository. See the [building static content](https://github.com/department-of-veterans-affairs/content-build#building-static-content) documentation.

### Building both together

After [building the applications](#building-applications), running `yarn build` in the `../content-build` directory will build content using the generated app bundles from `vets-website/build/localhost/generated`. The full build can be seen in `../content-build/build/localhost`.

## Working in GitHub Codespaces

[Read the Codespaces documentation for this repository](https://depo-platform-documentation.scrollhelp.site/developer-docs/Using-GitHub-Codespaces.1909063762.html#UsingGitHubCodespaces-Codespacesinvets-websiteandcontent-buildrepositories).

## Running tests

### Unit tests

To **run all unit tests**, use:

```sh
yarn test:unit
```

If you want to **run only one test file**, you can provide the path to it:

```sh
yarn test:unit src/applications/path/to/test-file.unit.spec.js
```

To **run all tests for a folder in src/applications**, you can use app-folder:

```sh
yarn test:unit --app-folder hca
```

To **run all tests in a directory**, you can use a glob pattern:

```sh
yarn test:unit src/applications/path/to/tests/**/*.unit.spec.js*
```

To **run tests with stack traces**, pass log-level `trace`:

```sh
yarn test:unit --log-level trace
```

To **run tests with coverage output**, you can pass the coverage option:

```sh
yarn test:unit --coverage
```

To **run tests with coverage and open the coverage report in your browser for a specific app** from `src/applications`:

```sh
yarn test:coverage-app {app-name}
```

For **help with test runner usage**, you can run:

```sh
yarn test:unit --help
```

### End-to-end (E2E) / Browser tests

- E2E or browser tests run in Cypress.

**Before running Cypress tests**, first make sure that:

1. `vets-website` is served locally on port 3001
   - You can do this with `yarn watch`
1. `vets-api` is **NOT** running
   - Any required APIs will be mocked by the Cypress test that needs them.

To **open the Cypress test runner UI and run any tests within it**:

```sh
yarn cy:open
```

To **open the Cypress test runner UI in Codespaces and run any tests within it**:

```sh
yarn cy:open-codespaces
```

Then visit http://localhost:6080/ and log in with the password `vscode`.

To **run Cypress tests from the command line**:

```sh
yarn cy:run
```

To **run specific Cypress tests from the command line**:

```sh
# Running one specific test.
yarn cy:run --spec ""path/to/test-file.cypress.spec.js""

# Running multiple specific tests.
yarn cy:run --spec ""path/to/test-a.cypress.spec.js,path/to/test-b.cypress.spec.js""

# Running tests that match a glob pattern.
yarn cy:run --spec ""src/applications/my-app/tests/*""
yarn cy:run --spec ""src/applications/my-app/tests/**/*""

# Running tests that match multiple glob patterns.
yarn cy:run --spec ""src/applications/a/tests/**/*,src/applications/b/tests/**/*""
```

To **run Cypress tests from the command line on a specific browser**:

```sh
yarn cy:run --browser chrome
yarn cy:run --browser firefox
```

To **run Cypress tests with reports**

```sh
yarn cy:run:localreports my-app-folder
```

Examples:

- `yarn cy:run:localreports appeals/995`
- `yarn cy:run:localreports ask-a-question`

Afterward, check `/mochawesome-report` contents.

**For other options with `yarn cy:run`,** [the same options for `cypress run` are applicable](https://docs.cypress.io/guides/guides/command-line.html#Commands).

## Changing Virtual Agent backend URL

If not already create a `.env` file in the root of the project and add
the following while updating the URL

```
VIRTUAL_AGENT_BACKEND_URL=https://some.url.com
```

## Running a mock API for local development

In separate terminal from your local dev server, run

```sh
yarn mock-api --responses path/to/responses.js
```

See the [mocker-api usage
documentation](https://github.com/jaywcjlove/mocker-api#usage) for how to use
the `responses.js`.

**If you need to log in**, go to your browser dev tools console and enter
`localStorage.setItem('hasSession', true)` and refresh the page. This will then
trigger a `/v0/user` call, which will then get the mocked response of a logged-in
user. (Assuming you've mocked that response, of course.)

Responses to common API requests, such as `/v0/user` and
`/v0/maintenance_windows`, you can use
[`src/platform/testing/local-dev-mock-api/common.js`](src/platform/testing/local-dev-mock-api/common.js)

```javascript
const commonResponses = require('src/platform/testing/local-dev-mock-api/common');

module.exports = {
  ...commonResponses,
  'GET path/to/endpoint': { foo: 'bar' },
};
```

## More commands

After a while, you may run into a less common task. We have a lot of commands
for doing very specific things.

| I want to...                                                                                                | Then you should...                                                                                                                                                                                                                        |
| :---------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| build the production site (dev features disabled).                                                          | `yarn build:production`                                                                                                                                                                                                                   |
| deploy the production site (dev features disabled).                                                         | `node src/platform/testing/e2e/test-server.js --buildtype=vagovprod`                                                                                                                                                                      |
| reset local environment (clean out node modules, Babel cache, and runs `npm install`)                       | `yarn reset:env`                                                                                                                                                                                                                          |
| run the app pages on the site for local development                                                         | `yarn watch --env scaffold`                                                                                                                                                                                                               |
| run the site for local development with automatic rebuilding of Javascript and sass **with** css sourcemaps | `yarn watch:css-sourcemaps` then visit `http://localhost:3001/`. You may also set `--env buildtype` and `NODE_ENV` though setting `NODE_ENV` to production will make incremental builds slow.                                             |
| run the site for local development with automatic rebuilding of code and styles for specific **apps**       | `yarn watch --env entry=disability-benefits,static-pages`. Valid application names are in each app's `manifest.json` under `entryName`                                                                                                    |
| run the site so that devices on your local network can access it                                            | `yarn watch --env host=0.0.0.0 --env public=192.168.x.x:3001` Note that we use CORS to limit what hosts can access different APIs, so accessing with a `192.168.x.x` address may run into problems                                        |
| run the site so that it can be accessed from a public codespaces URL                                        | `yarn watch --env api=https://${CODESPACE_NAME}-3000.app.github.dev public=https://${CODESPACE_NAME}-3001.app.github.dev` (only works from a codespaces terminal with mocks running and public port visibility)                           |
| watch file changes without starting the server                                                              | `yarn watch:no-server`                                                                                                                                                                                                                    |
| run all unit tests and watch                                                                                | `yarn test:watch`                                                                                                                                                                                                                         |
| run only E2E tests (headless)                                                                               | Make sure the site is running locally (`yarn watch`) and run the tests with `yarn cy:run`                                                                                                                                                 |
| run E2E tests in the browser                                                                                | `yarn cy:open`                                                                                                                                                                                                                            |
| count all Cypress E2E specs                                                                                 | `yarn cy:count`                                                                                                                                                                                                                           |
| run all linters                                                                                             | `yarn lint`                                                                                                                                                                                                                               |
| run only javascript linter                                                                                  | `yarn lint:js`                                                                                                                                                                                                                            |
| run only sass linter                                                                                        | `yarn lint:sass`                                                                                                                                                                                                                          |
| run lint on JS and fix anything that changed                                                                | `yarn lint:js:changed:fix`                                                                                                                                                                                                                |
| add new npm modules                                                                                         | `yarn add my-module`. Use the `--dev` flag for modules that are build or test related.                                                                                                                                                    |
| get the latest json schema                                                                                  | `yarn update:schema`. This updates our [`vets-json-schema`](https://github.com/department-of-veterans-affairs/vets-json-schema) to the most recent commit.                                                                                |
| check test coverage                                                                                         | `yarn test:coverage`                                                                                                                                                                                                                      |
| run bundle analyzer on our production JS bundles                                                            | `yarn build-analyze`                                                                                                                                                                                                                      |
| generate a stats file for analysis by bundle analyzer                                                       | `NODE_ENV=production yarn build:webpack --env buildtype=vagovprod --env analyzer`.                                                                                                                                                        |
| load the analyzer tool on a stats file                                                                      | `yarn analyze`                                                                                                                                                                                                                            |
| add a new React app                                                                                         | `yarn new:app` (make sure you have [`vagov-content`](https://github.com/department-of-veterans-affairs/vagov-content/) and [`content-build`](https://github.com/department-of-veterans-affairs/content-build/) sibling to `vets-website`) |

## Supported Browsers

| Browser                   | Minimum version | Note                                   |
| :------------------------ | :-------------: | :------------------------------------- |
| Internet Explorer         |       11        |                                        |
| Microsoft Edge            |       13        |                                        |
| Safari / iOS Safari       |        9        |                                        |
| Chrome / Android Web view |       44        | _Latest version with >0.5% of traffic_ |
| Firefox                   |       52        | _Latest version with >0.5% of traffic_ |

## API Keys

In order to work with the Facility Locator locally, you will need a Mapbox API key with dev access. see [this link](https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/platform/working-with-vsp/policies-work-norms/sensitive-guidance.md) for details on handling non public keys and tokens. You will need to access the paramater store within AWS Systems manager, and get the dev mapbox token from this location: /dsva-vagov/vets-website/dev/mapbox_token.

Create a .env file in the root of vets-website, and assign the above token to a variable called MAPBOX_TOKEN. The .env file should already be configured to work with dotenv for webpack. Ensure that the .env file is in .gitigore and take care not to expose this token in any public commits. See [this link](https://github.com/department-of-veterans-affairs/va.gov-team/issues/new?assignees=&labels=external-request%2Coperations%2Cops-access-request&template=aws-access-request.yml&title=AWS+access+for+%5Bindividual%5D) for instructions on requesting AWS access.

## Additional Resources

1. [Frontend developer documentation home](https://depo-platform-documentation.scrollhelp.site/developer-docs/frontend-developer-documentation)
1. [Manual](https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/platform/accessibility/testing/508-manual-testing.md) and [Automated](https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/platform/accessibility/testing/508-automated-testing.md) 508 Testing
1. [Using yarn Workspaces](https://depo-platform-documentation.scrollhelp.site/developer-docs/yarn-workspaces)

## Not a member of the repository and want to be added?

- If you're on a VA.gov Platform team, contact your Program Manager.
- If you're on a VFS team, you must complete [Platform Orientation](https://depo-platform-documentation.scrollhelp.site/getting-started/platform-orientation) to be added to this repository. This includes completing your Platform Orientation ticket(s) in GitHub.",FAUX
deployKF/deployKF,MLOps,MLOps,2024-05-27T23:47:30Z,2024-02-14T22:19:27Z,0,0,0,0,15,0,0,0,2023-03-14T21:07:43Z,2025-04-03T07:22:49Z,930,416,Shell,VRAI,57,FAUX,63,"argocd,artificial-intelligence,gitops,kubeflow,kubernetes,machine-learning,mlops",63,"deployKF builds machine learning platforms on Kubernetes. We combine the best of Kubeflow, Airflow†, and MLflow† into a complete platform.",FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,3,"<h1 align=""center"">
  <a href=""https://www.deploykf.org/"">deployKF</a>
</h1>

<div align=""center"">
  <h3>Your Open ML Platform</h3>
</div>

<!-- NOTE: we use this strange picture tag to prevent github making the image clickable -->
<div align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/deploykf-color.svg"">
    <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/deploykf-color.svg"">
    <img alt=""deployKF Logo"" src=""https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/deploykf-color.svg"" width=""140"">
  </picture>
</div>

<br>

<div align=""center"">
  <a href=""https://github.com/deployKF/deployKF/stargazers"">
    <img alt=""GitHub Stars"" src=""https://img.shields.io/github/stars/deployKF/deployKF?style=for-the-badge&color=ffcb2f&label=Support%20us%20with%20a%20%E2%AD%90%20on%20GitHub"" />
  </a>
</div>

<hr>
<br>

# About deployKF

<div>
  <a href=""https://github.com/deployKF/deployKF/releases"">
    <img alt=""Downloads"" src=""https://img.shields.io/github/downloads/deployKF/deployKF/total?style=flat-square&color=28a745"" />
  </a>
  <a href=""https://github.com/deployKF/deployKF/fork"">
    <img alt=""Contributors"" src=""https://img.shields.io/github/forks/deployKF/deployKF?style=flat-square&color=28a745"" />
  </a>
  <a href=""https://github.com/deployKF/deployKF/graphs/contributors"">
    <img alt=""Contributors"" src=""https://img.shields.io/github/contributors/deployKF/deployKF?style=flat-square&color=28a745"" />
  </a>
  <a href=""https://github.com/deployKF/deployKF/blob/master/LICENSE"">
    <img alt=""License"" src=""https://img.shields.io/github/license/deployKF/deployKF?style=flat-square&color=28a745"" />
  </a>
  <a href=""https://github.com/deployKF/deployKF/releases"">
    <img alt=""Latest Release"" src=""https://img.shields.io/github/v/release/deployKF/deployKF?style=flat-square&color=6f42c1&label=latest%20release"" />
  </a>
</div>

## What is deployKF?

> [<img src='https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/deploykf-color.svg' width='20'> __deployKF__](https://www.deploykf.org/) builds machine learning platforms on __Kubernetes__.
> <br>
> We combine the best of
>  [<img src='https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/kubeflow-color.svg' width='20'> __Kubeflow__](https://www.deploykf.org/reference/tools/#kubeflow-ecosystem),
>  [<img src='https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/airflow-color.svg' width='20'> __Airflow__](https://www.deploykf.org/reference/future-tools/#apache-airflow)<sup>†</sup>, and 
>  [<img src='https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/mlflow-color.svg' width='20'> __MLflow__](https://www.deploykf.org/reference/future-tools/#mlflow-model-registry)<sup>†</sup>
> into a complete platform that is easy to deploy and maintain.
>
> <sub><sup>†</sup><sup>Coming soon, see our [current](https://www.deploykf.org/reference/tools/) and [future](https://www.deploykf.org/reference/future-tools/) tools.</sup></sub>

## Why use deployKF?

> deployKF combines the _ease of a managed service_ with the flexibility of a self-hosted solution. 
>
> Our goal is that __any Kubernetes user__ can build a machine learning platform for their organization, 
> without needing specialized MLOps knowledge, or a team of experts to maintain it.
>
> The key features of deployKF are:
>
> - Run on [__any Kubernetes cluster__](https://www.deploykf.org/guides/getting-started/#kubernetes-cluster), including on-premises and in the cloud
> - Intuitive [__centralized configs__](https://www.deploykf.org/guides/values/#overview) for all aspects of the platform
> - Seamless [__in-place upgrades__](https://www.deploykf.org/guides/upgrade/#overview) and config updates
> - Connect your existing
>    [<img src='https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/istio-color.svg' width='20'> __Istio__](https://www.deploykf.org/guides/dependencies/istio/#can-i-use-my-existing-istio),
>    [<img src='https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/cert-manager-color.svg' width='20'> __cert-manager__](https://www.deploykf.org/guides/dependencies/cert-manager/#can-i-use-my-existing-cert-manager),
>    [<img src='https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/kyverno-color.svg' width='20'> __Kyverno__](https://www.deploykf.org/guides/dependencies/kyverno/#can-i-use-my-existing-kyverno),
>    [<img src='https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/s3-color.svg' width='20'> __S3__](https://www.deploykf.org/guides/tools/external-object-store/),
>    and [<img src='https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/mysql-color.svg' width='20'> __MySQL__](https://www.deploykf.org/guides/tools/external-mysql/)
> - Use any [__identity provider__](https://www.deploykf.org/guides/platform/deploykf-authentication/) via _OpenID Connect_ or _LDAP_
> - Native support for [__GitOps with ArgoCD__](https://www.deploykf.org/guides/dependencies/argocd/#how-does-deploykf-use-argo-cd)

## Video Introduction

> <div>
>   <a href=""https://www.youtube.com/watch?v=GDX4eLL_8E0"" target=""_blank"" rel=""noopener"">
>     <img src=""https://i.ytimg.com/vi/GDX4eLL_8E0/maxresdefault.jpg"" width=""720"" />
>   </a>
>   <div>
>     <b>Title</b>: deployKF: A better way to deploy Kubeflow (and more)
>     <br>
>     <b>Event</b>: Kubeflow Summit 2023
>   </div>
> </div>

## Featured Stories

> We are always excited to see __how and where__ deployKF is being used!
>
> Here are some stories of deployKF being used in the wild:
>
> Organization | Article / Video
> --- | ---
> <picture><source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/cloudflare-color.svg""><source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/cloudflare-color.svg""><img src=""https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/cloudflare-color.svg"" width=""20""></picture> Cloudflare | [_A look inside the Cloudflare ML Ops platform_](https://blog.cloudflare.com/mlops/)
>
> <sub><sup>
>   <em>Have a story to share? [Let us know](https://www.deploykf.org/about/community/#contact-us)!</em>
> </sup></sub>

---

<br>

# Using deployKF

## Getting Started

> To help you get started with deployKF, we have prepared a number of guides:
> 
> - [⭐ __Getting Started__](https://www.deploykf.org/guides/getting-started/) - learn how to run deployKF anywhere
> - [Local Quickstart](https://www.deploykf.org/guides/local-quickstart/) - try deployKF on your local machine
> - [Migrate from Kubeflow Distributions](https://www.deploykf.org/guides/kubeflow-distributions/) - how and why to migrate from other Kubeflow distributions

## Release Information

> For more information about our releases, please see:
> 
> - [Version Matrix](https://www.deploykf.org/releases/version-matrix/)
> - [Changelog](https://www.deploykf.org/releases/changelog-deploykf/)

## Support the Project

> deployKF is a new and growing project. 
> If you like what we are doing, please help others discover us by __sharing the project__ with your colleagues and/or the wider community.
> 
> We greatly appreciate GitHub Stars ⭐ on the `deployKF/deployKF` repository:
> 
> <picture>
>   <source media=""(prefers-color-scheme: dark)"" srcset=""https://api.star-history.com/svg?repos=deploykf/deploykf&type=Date&theme=dark"" />
>   <source media=""(prefers-color-scheme: light)"" srcset=""https://api.star-history.com/svg?repos=deploykf/deploykf&type=Date"" />
>   <img alt=""Star History Chart"" src=""https://api.star-history.com/svg?repos=deploykf/deploykf&type=Date"" width=""600"" />
> </picture>

---

<br>

# Other Resources

## Commercial Support

> To discuss commercial support options for deployKF, please connect with [<img src='https://raw.githubusercontent.com/deployKF/website/main/overrides/.icons/custom/aranui-solutions-color.svg' width='20'> __Aranui Solutions__](https://www.aranui.solutions/), the company started by the creators of deployKF.
> Learn more on the [Aranui Solutions Website](https://www.aranui.solutions/).

## Community

> The deployKF community uses the __Kubeflow Slack__ for informal discussions among users and contributors.
>
> Please see our [community page](https://www.deploykf.org/about/community/#slack) for more information.

## History of deployKF

> deployKF was originally created and is maintained by [Mathew Wicks](https://www.linkedin.com/in/mathewwicks/) (GitHub: [@thesuperzapper](https://github.com/thesuperzapper)), a Kubeflow lead and maintainer of the popular [Apache Airflow Helm Chart](https://github.com/airflow-helm/charts).
> deployKF is a community-led project that welcomes contributions from anyone who wants to help.",FAUX
devopscert202/k8sforbeginners,Documentations,Documentations,2025-03-30T16:40:01Z,2025-02-02T12:55:00Z,0,0,0,0,0,0,0,1,2024-11-20T05:18:11Z,2025-03-30T16:40:05Z,597,5,HTML,VRAI,62,FAUX,0,,0,,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,4,"# Kubernetes for beginners
<H3> 
This repo contains the kubernetes documenation and practice yaml files to learn Kubernetes and prepare for CKA.
</H3>",FAUX
devtron-labs/utilities,Documentations,Toolkit,2025-05-14T18:41:02Z,2025-04-21T12:03:51Z,0,0,0,0,4,0,0,0,2021-05-10T13:50:25Z,2025-04-07T14:09:56Z,254,5,HCL,VRAI,19,FAUX,15,,15,Utilities useful for devtron users,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,17,"# utilities
Utilities useful for devtron users",VRAI
devxp-tech/gitops,DevOPs,DevOPs,2024-09-09T11:06:14Z,2024-08-14T21:34:16Z,0,0,0,0,1,0,0,0,2021-11-03T15:47:35Z,2025-02-09T21:28:07Z,6562,43,Shell,VRAI,25,FAUX,12,"argo,argo-cd,argo-events,argo-rollouts,argo-workflows,charts,cicd,cncf,container,deploy,deployment,devops,docker,gitops,helm,infraascode,kubernetes",12,🏗️ GitOps Repository ,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,16,"[![main](https://github.com/devxp-tech/gitops/actions/workflows/main.yaml/badge.svg)](https://github.com/devxp-tech/gitops/actions/workflows/main.yaml)
[![Quality Gate Status](https://sonar.devxp-tech.io/api/project_badges/measure?project=gitops&metric=alert_status&token=sqb_f6faa6baaf2901c484b2fc037eb06ad36b704eaa)](https://sonar.devxp-tech.io/dashboard?id=gitops)
[![App Status](https://argocd.devxp-tech.io/api/badge?name=argo-cd&revision=true)](https://argocd.diegoluisi.eti.br/applications/argo-cd)
![GitHub last commit](https://img.shields.io/github/last-commit/devxp-tech/gitops)
![GitHub top language](https://img.shields.io/github/languages/top/devxp-tech/gitops)
![GitHub issues](https://img.shields.io/github/issues-raw/devxp-tech/gitops)
![GitHub](https://img.shields.io/github/license/devxp-tech/gitops)
![Twitter Follow](https://img.shields.io/twitter/follow/devxp_tech?style=social)
# ⚙️ GitOps

GitOps is a set of best practices where the entire code delivery process is controlled via Git, including infrastructure and application definition as code and automation to complete updates and rollbacks.

The core idea of GitOps is to have a git repository that contains declarative descriptions of the desired infrastructure in the production environment and has an automated process to make the production environment match the state described in that repository.

Now that we know the central idea, let's go back a little bit to the acronym GitOps and let's understand the role of git, the role of ops and how they meet.

The Key GitOps Principles:

- The entire system (infrastructure and applications) is described declaratively.
- The canonical desired system state is versioned in Git.
- Changes approved are automated and applied to the system.
- Software agents ensure correctness and alert on divergence.

### <img align=""left"" alt=""Git"" width=""18px"" src=""https://cncf-branding.netlify.app/img/projects/argo/icon/color/argo-icon-color.svg"" /> [Argo CD Autopilot](https://argocd-autopilot.readthedocs.io/en/stable/)

New users to GitOps and Argo CD are not often sure how they should structure their repos, add applications, promote apps across environments, and manage the Argo CD installation itself using GitOps.

### Example:

```bash
argocd-autopilot app create argo-events --app github.com/argoproj/argo-events/blob/master/manifests/namespace-install.yaml -p ops --wait-timeout 2m
```

### <img align=""left"" alt=""Git"" width=""18px"" src=""https://cncf-branding.netlify.app/img/projects/argo/icon/color/argo-icon-color.svg"" /> [Argo CD](https://argo-cd.readthedocs.io/en/stable/)

Argo CD automates the deployment of the desired application states in the specified target environments. Application deployments can track updates to branches, tags, or pinned to a specific version of manifests at a Git commit. See tracking strategies for additional details about the different tracking strategies available.

### <img align=""left"" alt=""Git"" width=""18px"" src=""https://cncf-branding.netlify.app/img/projects/argo/icon/color/argo-icon-color.svg"" /> [Argo Events](https://argoproj.github.io/argo-events/)

Argo Events is an event-driven workflow automation framework for Kubernetes which helps you trigger K8s objects, Argo Workflows, Serverless workloads, etc. on events from a variety of sources like webhooks, S3, schedules, messaging queues, gcp pubsub, sns, sqs, etc.

### <img align=""left"" alt=""Git"" width=""18px"" src=""https://cncf-branding.netlify.app/img/projects/argo/icon/color/argo-icon-color.svg"" /> [Argo Rollouts](https://argoproj.github.io/argo-rollouts/)

Argo Rollouts is a Kubernetes controller and set of CRDs which provide advanced deployment capabilities such as blue-green, canary, canary analysis, experimentation, and progressive delivery features to Kubernetes.

Argo Rollouts (optionally) integrates with ingress controllers and service meshes, leveraging their traffic shaping abilities to gradually shift traffic to the new version during an update. Additionally, Rollouts can query and interpret metrics from various providers to verify key KPIs and drive automated promotion or rollback during an update.

### <img align=""left"" alt=""Git"" width=""18px"" src=""https://cncf-branding.netlify.app/img/projects/argo/icon/color/argo-icon-color.svg"" /> [Argo Workflows](https://argoproj.github.io/argo-workflows/)

Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD (Custom Resource Definition).

### 🔩 Tooling

See [Versions](docs/tooling/versions.md).

### 🔨 To Do

See [To Do](docs/to-do.md).

### 🌳 Project Structure

See [Project Structure](docs/project-structure.md).

### 🖊️ Code of Conduct

See [Code of Conduct](docs/CODE-OF-CONDUCT.md).

### 🔒 Security

See [Security](docs/SECURITY.md).

## ✨ Contributions

We ❤️ contributions big or small. [See our guide](contributing.md) on how to get started.

### Thanks to all our contributors

<a href=""https://github.com/devxp-tech/gitops/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=devxp-tech/gitops"" />
</a>",FAUX
diranetafen/cursus-devops,Documentations,Documentations,2025-03-21T15:34:21Z,2024-09-20T09:01:59Z,0,0,0,0,1,0,0,0,2019-12-20T11:28:55Z,2025-04-07T11:58:44Z,8827,63,Shell,VRAI,212,FAUX,0,,0,repo used for devops cursus training,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,19,"# This repo is used for devops training by Dirane TAFEN (diranetafen@yahoo.com)

## Recommended Vagrant Version: [2.4.1](https://releases.hashicorp.com/vagrant/2.4.1/)

 - ​Windows hosts : [64 bits](https://releases.hashicorp.com/vagrant/2.4.1/vagrant_2.4.1_windows_amd64.msi) || [32 bits](https://releases.hashicorp.com/vagrant/2.4.1/vagrant_2.4.1_windows_i686.msi)
 - ​macOS : [AMD 64](https://releases.hashicorp.com/vagrant/2.4.1/vagrant_2.4.1_darwin_amd64.dmg) || [ARM 64](https://releases.hashicorp.com/vagrant/2.4.1/vagrant_2.4.1_darwin_arm64.dmg)
 - Debian / Ubuntu : [64 bits](https://releases.hashicorp.com/vagrant/2.4.1/vagrant_2.4.1-1_amd64.deb) || [32 bits](https://releases.hashicorp.com/vagrant/2.4.1/vagrant_2.4.1-1_i686.deb)

## Recommended VirtualBox Version: [7.0.20](https://www.virtualbox.org/wiki/Download_Old_Builds_7_0)

 - [​Windows hosts](https://download.virtualbox.org/virtualbox/7.0.20/VirtualBox-7.0.20-163906-Win.exe) 
 - [​macOS / Intel hosts](https://download.virtualbox.org/virtualbox/7.0.20/VirtualBox-7.0.20-163906-OSX.dmg) 
 - [​Ubuntu 24.04](https://download.virtualbox.org/virtualbox/7.0.20/virtualbox-7.0_7.0.20-163906~Ubuntu~noble_amd64.deb)  
 - [​Ubuntu 22.04](https://download.virtualbox.org/virtualbox/7.0.20/virtualbox-7.0_7.0.20-163906~Ubuntu~jammy_amd64.deb)",FAUX
dmwm/CMSKubernetes,Documentations,Documentations,2025-05-15T16:23:29Z,2025-04-15T18:46:31Z,0,0,0,0,0,0,0,38,2018-03-02T18:07:57Z,2025-04-02T20:58:29Z,17385,18,Shell,VRAI,44,FAUX,16,,16,Set of instructions and examples to deploy CMS data-services to Kubernetes cluster,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,65,"## CMSKubernetes
This repository contains all necessary tools and documentation to
build and deploy cms services to kubernetes (k8s). The repository
is organized in the following way:

- [docker](https://github.com/dmwm/CMSKubernetes/tree/master/docker)
area contains cmsweb service areas. Within individual area you'll find
Dockerfile and aux files required to build docker image for that service
- [kubernetes](https://github.com/dmwm/CMSKubernetes/tree/master/kubernetes)
area contains several projects for deploying cmsweb service to k8s.
Even though some of them are obsolete now we still keep them around for
the reference.
  - [kubernetes/cmsweb](https://github.com/dmwm/CMSKubernetes/tree/master/kubernetes/cmsweb)
  area contains all documentation about **current cmsweb k8s deployment**. 
  - [kubernetes/rucio](https://github.com/dmwm/CMSKubernetes/tree/master/kubernetes/rucio)
  area contains all files required for Rucio deployment.
  - [kuberentes/tfaas](https://github.com/dmwm/CMSKubernetes/tree/master/kubernetes/tfaas)
  provides all files for [TFaaS](https://github.com/vkuznet/TFaaS) k8s setup.
  - [kubernetes/cmsmon](https://github.com/dmwm/CMSKubernetes/tree/master/kubernetes/cmsmon)
  contains files for cmsmon service on k8s.
  - [kubernetes/monitoring](https://github.com/dmwm/CMSKubernetes/blob/master/kubernetes/monitoring)
  presents the CMS Monitoring cluster architecture and contains all the relevant files for the deployment of a monitoring cluster.
  - [kubernetes/legacy](https://github.com/dmwm/CMSKubernetes/tree/master/kubernetes/legacy) 
    contains the legacy code of cmsweb.
  - [kubernetes/whoami](https://github.com/dmwm/CMSKubernetes/tree/master/kubernetes/whoami)
  area contains all details of end-to-end deployment of cluster running two services httpgo and its httpsgo counterpart.
  <!---
  - [kubernetes/k8s-whoami-nginx](https://github.com/dmwm/CMSKubernetes/tree/master/kubernetes/k8s-whoami-nginx)
  area contains all files required for simple k8s whoami service based on nginx
  middleware
   - [kubernetes/traefik](https://github.com/dmwm/CMSKubernetes/tree/master/kubernetes/cmsweb-nginx)
  area contains previous cmsweb deployment using traefik middleware
  --->

- [helm](https://github.com/dmwm/CMSKubernetes/tree/master/helm) area contains helm files for all the cmsweb services.",FAUX
dnsimple/dnsimple-api-examples,Documentations,Documentations,2025-05-16T01:28:59Z,2023-08-02T02:45:07Z,1,0,0,0,0,0,0,0,2016-08-27T12:58:28Z,2025-03-11T16:02:04Z,762,28,Ruby,VRAI,16,FAUX,1,"dnsimple-policy-eng,dnsimple-policy-group-apiclient,dnsimple-policy-triage-application",1,Example scripts and code to demonstrate how to use the DNSimple API.,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,16,"# DNSimple Integration Examples

This repository contains examples showing how to use the DNSimple API and various other integration tools and libraries.

## API Clients

* [dnsimple-elixir](./elixir)
* [dnsimple-go](./golang)
* [dnsimple-node](./node)
* [dnsimple-php](./php)
* [dnsimple-python](./python)
* [dnsimple-ruby](./ruby)
* [dnsimple-rust](./rust)

**Other:**

* [coredns](./coredns)
* [curl](./curl)
* [terraform](./terraform)

## Terraform

* [Terraform Zone management](./terraform/terraform-zone/)

## Interested In Contributing?

Please review the [CONTRIBUTING.md](./CONTRIBUTING.md) document first.",FAUX
duneanalytics/spellbook,Toolkit,Toolkit,2025-05-15T16:30:16Z,2025-05-07T15:00:01Z,0,0,0,0,0,1,0,0,2020-01-31T08:49:46Z,2025-04-08T13:43:19Z,444335,1299,Python,VRAI,1249,FAUX,28,,28,SQL views for Dune,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,640,"![spellbook-logo@10x](https://user-images.githubusercontent.com/2520869/200791687-76f1bc4f-05d0-4384-a753-e3b5da0e7a4a.png#gh-light-mode-only)
![spellbook-logo-negative_10x](https://user-images.githubusercontent.com/2520869/200865128-426354af-8059-494d-83f7-46947aae271c.png#gh-dark-mode-only)

Welcome to [Spellbook](https://youtu.be/o7p0BNt7NHs). Cast a magical incantation to tame the blockchain.

## TL;DR

- Have a question on how something works in Spellbook, or why we design spells in a particular way?
  - Please visit the [docs](docs/) directory to find various topics & ideally answers to any question about Spellbook
- Spellbook has [introduced sub-projects](#sub-projects), with the intention to build a path forward for scaling Spellbook
- Are you building something new? **Please make sure to open a Draft PR**, so we minimize duplicated work, and other wizards can help you if you need
- Don't know where to start? The docs below will guide you, but as a summary:
  - Want to make an incremental improvement to one of our spells? (add a new project, fix a bug you found), simply open a PR with your changes.
    - Follow the guide for [Submitting a PR](#submitting-a-pr), [Setting up your dev environment](#setting-up-your-local-dev-environment) and [Using dbt to write spells](#how-to-use-dbt-to-create-spells) if you find yourself lost.
    - Not sure how to start? Follow the walkthrough [here](#introduction).
    - Make sure to open a draft PR if you will work on your spell for longer than a few days, to avoid duplicated work
  - Do you want to get started building spells and you don't know what to build? Check [Issues](https://github.com/duneanalytics/spellbook/issues) to see what the community needs.
  - Check the Discussions section to see what problems the community is trying to solve (i.e. non-incremental changes) or propose your own!
- Have questions? Head over to #spellbook on our [discord](https://discord.com/channels/757637422384283659/999683200563564655) and the community will be happy to help out!
- Like with most OSS projects, your contributions to Spellbook are your own IP, you can find more details in the [Contributor License Agreement](CLA.md)

## Table of Contents

- [Introduction](#introduction)
- [Docs](#docs)
- [Sub-projects](#sub-projects)
- [How to contribute](#ways-to-contribute-to-spellbook)
  - [Submitting a PR](#submitting-a-pr)
  - [Testing your Spell](#testing-your-spell)
  - [Connecting with other wizards](#connecting-with-other-wizards)
- [Setting up your dev environment](#setting-up-your-local-dev-environment)
  - [Prerequisites](#prerequisites)
  - [Initial Installation](#initial-installation)
  - [Coming back](#coming-back)
  - [What did I just do?](#what-did-i-just-do)
- [Using dbt to write spells](#how-to-use-dbt-to-create-spells)
  - [Generating and serving documentation:](#generating-and-serving-documentation)
  - [DBT Resources:](#dbt-resources)
        
## Introduction

Spellbook is Dune's interpretation layer, built for and by the community.

Spellbook is a [dbt](https://docs.getdbt.com/docs/introduction) project. Each model is a simple SQL query with minor syntactic sugar (meant to capture dependencies and help build the resulting tables), and does a small part of the task of turning raw and decoded records into interpretable blockchain data.

Spellbook is built for and by the community, you are welcome to close any gaps that you find by sending a PR, creating issues to propose small changes or track bugs, or participate in discussions to help steer the future of this project.

## Docs

Spellbook has a lot of moving parts & specific design principles for contributing to Dune's interpretation layer of data. In order to prepare contributors to participate most efficiently, the [docs](docs/) directory contains a wide ranging set of topics to answer common questions & provide info on why the repo is setup as it is. Please read & refer to this section when developing in Spellbook and questions arise. The Dune team will also link back to these docs to answer questions often, to help grow awareness and keep communications clean.

## Sub-projects

In order to scale Spellbook, the repo has introduced sub-projects to break out complex DBT lineages a bit & keep focus areas clean. This will also help downstream orchestration to keep spells fresh in production. DBT sub-projects in Spellbook are simply multiple DBT projects within one repo. The current structure for projects:
- `dbt_subprojects`
  - `daily_spellbook`
    - *note*: new spells will live here, unless otherwise directed by Dune team
    - all ""other"" spells which don't feed into larger sector-wide spells, refreshed on a daily basis
    - example: project specific, standalone spells
  - `hourly_spellbook`
    - ""other"" spells which have been promoted from daily to hourly, allowing for more frequent refreshes
    - feed into sector-level spells, with potential to be promoted into it's own project
    - required to fit latest spellbook best practices
    - requires approval from Dune team to be hourly
  - `dex`
    - all spells which live in the `dex` or `dex_aggregator` schemas, including upstream spells to help build the final sector-level spells
  - `nft`
    - all spells which live in the `nft` schema, including upstream spells to help build the final sector-level spells
  - `solana`
    - solana specific spells, don't fit into EVM code structure as easily
  - `tokens`
    - token metadata, transfers, balances

For further information on sub-projects, please visit [this discussion](https://github.com/duneanalytics/spellbook/discussions/6037) and ask any questions there.

## Ways to contribute to Spellbook

- **Build** spells - if you want to write code, simply clone the repo, write your code, and open a PR
  - If you already know what to build, there's no red tape to skip around, simply open a PR when you're ready. We advise opening draft PRs early, so we avoid duplication of efforts and you can get help from other wizards
  - If you don't know where to start, check out Issues for ideas. We're always looking for help fixing small bugs or implementing spells for small projects
- **Flag** gaps in spellbook - have you found a bug, or is there a project missing from one of the sectors that you'd like to add? You can create an [issue](https://github.com/duneanalytics/spellbook/issues) and bring other wizards to your aid.
  - **Bugs**: Found a record on a Spell that doesn't reflect chain data correctly? Please make sure you link to a block explorer showing the expected value, and a dune query showing the wrong value. If there's multiple records affected, any sense of scale (how many rows, affected USD volume) will also be very helpful.
- **Propose** changes to spellbook - [Discussions](https://github.com/duneanalytics/spellbook/discussions) are where we bring up, challenge and develop ideas to continue building spellbook. If you want to make a major change to a spell (e.g. major overhaul to a sector, launching a new sector, designing a new organic volume filter, etc.).

### Submitting a PR

Want to get right to work? Follow the guide [here](https://dune.com/docs/spellbook/?h=7+steps+adding+a+spell) to get started.

### Testing your spell

You don't need a complex local setup to test spells against Dune's engine. Once you send a PR, our CI pipeline will run and test it, and, if the job finishes successfully, you'll be able to query the data your PR created directly from dune.com.

Simply write a query like you would for any of our live tables, and use the test schema to fetch the tables your PR created.

`test_schema.git_dunesql_{{commit_hash}}_{{table_name}}`

You can find the exact names easily by looking at the logs from the `dbt slim ci` action, under `dbt run initial model(s)`.

Please note: the test tables built in the CI pipeline will exist for ~24 hours. If your table doesn't exist, trigger the pipeline to run again and recreate the test table.

### Connecting with other wizards

We use Discord to connect with our community. Head over to spellbook channel on [Dune's Discord](https://discord.com/invite/ErrzwBz) for questions or to ask for help with a particular PR. We encourage you to learn by doing, and leverage our vibrant community to help you get going.

## Setting up your Local Dev Environment

### Prerequisites

- Fork this repo and clone your fork locally. See Github's [guide](https://docs.github.com/en/get-started/quickstart/contributing-to-projects) on contributing to projects.
- We default to use unix (LF) line endings, windows users please set: `git config --global core.autocrlf true`. [more info](https://docs.github.com/en/get-started/getting-started-with-git/configuring-git-to-handle-line-endings)
- python 3.9 installed. Our recommendation is to follow the [Hitchhiker's Guide to Python](https://docs.python-guide.org/starting/installation/)
- [pip](https://pip.pypa.io/en/stable/installation/) installed
- [pipenv](https://pypi.org/project/pipenv/) installed
- paths for both pip and pipenv are set (this should happen automatically but sometimes does not). If you run into issues like ""pipenv: command not found"", try troubleshooting with the pip or pipenv documentation.

### Initial Installation

You can watch the video version of this if you scroll down a bit.

Navigate to the spellbook repo within your CLI (Command line interface).

```console
cd user\directory\github\spellbook
# Change this to wherever spellbook is stored locally on your machine.
```

Using the pipfile located in the spellbook repo, run the below install command to create a pipenv.

```console
pipenv install
```

If the install fails, one likely reason is our script looks for a static python version and the likelihood of an error for a wrong python version is pretty high. If that error occurs, check your python version with:

```console
python --version
```

Now use any text editor program to change the python version in the pipfile within the spellbook directory to your python version. You need to have at least python 3.9.
If you have changed the python version in the pipfile, run `pipenv install` again.

You are now ready to activate this project's virtual environment. Run the following command to enter the environment:

```console
pipenv shell
```

You have now created a virtual environment for this project. You can read more about virtual environments [here](https://realpython.com/pipenv-guide/).

Within the Spellbook repo, there are multiple dbt projects, located in the root directory. Navigate to the correct project, depending on your use case.

```console
cd ../spellbook/dbt_subprojects/<subproject_name>/
```

Each subproject has it's own dbt project file with varying configs. Once your CLI has navigated to the correct project directory, follow the below steps:
- To clean up the dbt project
  ```console
  dbt clean
  ```

- To pull the dbt project dependencies run:
  ```console
  dbt deps
  ```

- To compile models into raw SQL, to run on the dune app and validate:
  ```console
  dbt compile
  ```

Each Spellbook subproject includes a `profiles.yml` file, which helps tell dbt how to run commands. The profile is located in each subproject directory, such as [here](./dbt_subprojects/dex/profiles.yml). This should never need modified, unless done intentionally by the Dune team.  
Due to the `profiles.yml` file being stored in the root directory of each subproject, this is why users **must** be in the root directory per subproject on the command line to run `dbt compile` as expected.

dbt compile will compile the JINJA and SQL templated SQL into plain SQL which can be executed in the Dune UI. Your spellbook directory now has a folder named `target` containing plain SQL versions of all models in Dune. If you have made changes to the repo before completing all these actions, you can now be certain that at least the compile process works correctly, if there are big errors the compile process will not complete.
If you haven't made changes to the directory beforehand, you can now start adding, editing, or deleting files within the repository.
Afterwards, simply run `dbt compile` again once you are finished with your work in the directory and test the plain language sql queries on dune.com.

### Coming back

If you have done this installation on your machine once, to get back into dbt, simply navigate to the spellbook repo, run `pipenv shell`, and you can run `dbt compile` again.

### What did I just do?

You now have the ability to compile your dbt model statements and test statements into plain SQL. This allows you to test those queries on the usual dune.com environment and should therefore lead to a better experience while developing spells. Running the queries will immediately give you feedback on typos, logical errors, or mismatches.
This in turn will help us deploy these spells faster and avoid any potential mistakes.

## How to use dbt to create spells

There are a couple of new concepts to consider when making spells in dbt. The most common ones wizards will encounter are refs, sources, freshness, and tests.

In the body of each query, tables are referred to either as refs, ex `{{ ref('1inch_ethereum') }}` or sources, ex `{{ source('ethereum', 'traces') }}`. Refs refer to other dbt models and they should refer to the file name like `1inch_ethereum.sql`, even if the model itself is aliased. Sources refer to ""raw"" data or tables/views not generated by dbt. Using refs and sources allows us to automatically build dependency trees.

Sources and models are defined in schema.yml files where tests and other attributes are defined.

The best practice is to add tests unique and non_null tests to the primary key for every new model. Similarly, a freshness check should be added to every new source (although we will try not to re-test freshness if the source is used elsewhere).

Adding descriptions to tables and columns will help people find and use your tables.

```yaml
models:
  - name: 1inch_ethereum
    description: ""Trades on 1inch, a DEX aggregator""
    columns:
      - name: tx_hash
        description: ""Table primary key: a transaction hash (tx_hash) is a unique identifier for a transaction.""
        data_tests:
          - unique
          - not_null

  sources:
  - name: ethereum
    freshness:
      warn_after: { count: 12, period: hour }
      error_after: { count: 24, period: hour }
    tables:
      - name: traces
```

See links to more docs on dbt below.

### Generating and serving documentation:

To generate documentation and view it as a website, run the following commands:

- `dbt docs generate`
- `dbt docs serve`
  You must have set up dbt with `dbt init` but you don't need database credentials to run these commands.

See [dbt docs documentation](https://docs.getdbt.com/docs/building-a-dbt-project/documentation) for more information on
how to contribute to documentation.

As a preview, you can do [things](https://docs.getdbt.com/reference/resource-properties/description) like:

- Write simple one or many line descriptions of models or columns.
- Write longer descriptions as code blocks using markdown.
- Link to other models in your descriptions.
- Add images / project logos from the repo into descriptions.
- Use HTML in your description.

### DBT Resources:

- Learn more about dbt [in the docs](https://docs.getdbt.com/docs/introduction)
- Check out [Discourse](https://discourse.getdbt.com/) for commonly asked questions and answers
- Join the [chat](https://getdbt.com/community/join-the-community/) on Slack for live discussions and support
- Find [dbt events](https://getdbt.com/events/) near you
- Check out [the blog](https://getdbt.com/blog/) for the latest news on dbt's development and best practices",FAUX
EaW-Team/equestria_dev,Application System,Documentations,2025-04-30T20:04:29Z,2025-04-29T20:06:13Z,0,0,0,0,0,15,0,0,2017-09-20T09:05:14Z,2025-04-05T18:25:25Z,4706356,123,Python,VRAI,46,FAUX,0,,0,Developer repo for Equestria at War mod,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,254,"Equestria at War mod repository

This item is not authorized for posting on Steam, except under the Steam account named ""Equestria At War Team"" (https://steamcommunity.com/id/Equestria_At_War).

</a>
<a href=""https://github.com/EaW-Team/equestria_dev/issues"">
<img alt=""Issues"" src=""https://img.shields.io/github/issues/EaW-Team/equestria_dev?color=0088ff"" />
</a>
<a href=""https://github.com/EaW-Team/equestria_dev/pulls"">
<img alt=""GitHub pull requests"" src=""https://img.shields.io/github/issues-pr/EaW-Team/equestria_dev?color=0088ff"" />",VRAI
Eclipse-Station/NEV-Northern-Light,Application System,Documentations,2025-03-17T16:57:35Z,2024-07-13T21:28:12Z,0,0,0,0,0,4,0,0,2019-11-13T15:51:49Z,2025-03-17T16:57:53Z,581391,8,DM,VRAI,55,FAUX,129,"nev-northern-light,space-station-13,spacestation13,ss13",129,"Congratulations, [employee name here]! You have greatly advanced the expedition effort. You must be the pride of [employee hometown name here]! NanoTrasen commends your usefulness to the Corporation.",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,301,"![](http://www.eclipse-station.space/AEIOU_logo.png)
# Pride of NanoTrasen: NEV Northern Light [![Build Status](https://travis-ci.org/Eclipse-Station/NEV-Northern-Light.svg?branch=master)](https://travis-ci.org/Eclipse-Station/NEV-Northern-Light)


## CONTRIBUTING

Please see [CONTRIBUTING.md](CONTRIBUTING.md)",VRAI
eclipse-symphony/symphony,DevOPs,DevOPs,2025-05-16T01:33:24Z,2025-03-14T11:41:44Z,0,0,0,0,0,0,0,1,2024-01-04T08:15:45Z,2025-04-08T09:09:01Z,134891,46,Go,VRAI,32,FAUX,170,,170,Symphony project,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,41,"# Symphony

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
![build](https://github.com/eclipse-symphony/symphony/actions/workflows/go.yml/badge.svg)

_(last edit: 02/02/2024)_

Symphony is a powerful service orchestration engine that enables the organization of multiple intelligent edge services into a seamless, end-to-end experience. Its primary purpose is to address the inherent complexity of edge deployment by providing a set of technology-agnostic workflow APIs, which are designed to deliver a streamlined experience for users across all device profiles.

Symphony is uniquely capable of providing consistency across the entire software stack, from drivers to containers to configurations and policies. This comprehensive approach ensures that all aspects of your intelligent edge projects are effectively managed and optimized. Moreover, Symphony provides full support for the entire lifecycle of your edge computing initiatives, spanning from the initial deployment to ongoing updates and maintenance.

With Symphony, users can benefit from a powerful and versatile platform that streamlines edge service orchestration and management, while also ensuring seamless integration with existing technology stacks. Whether you are a small business owner or a large enterprise, Symphony is an ideal solution for enhancing the efficiency and effectiveness of your edge computing initiatives.

## Symphony Characteristics

* Standard-based

    Symphony is a versatile and standards-based solution that delivers exceptional flexibility and extensibility. It natively runs on Kubernetes, which means users can leverage all existing Kubernetes tooling to interact with Symphony. Moreover, Symphony supports a wide range of popular industrial standards, protocols, and frameworks, including [OpenTelemetry](https://opentelemetry.io/), [Distributed Application Runtime (Dapr)](https://dapr.io/), [Message Queuing Telemetry Transport (MQTT)](https://mqtt.org/), [Open Neural Network Exchange (ONNX)](https://onnx.ai/), [Akri](https://github.com/project-akri/akri), [kubectl](https://kubernetes.io/docs/reference/kubectl/kubectl/), [Helm](https://helm.sh/), and many others. This broad range of support makes Symphony an ideal solution for organizations seeking to build and deploy edge services that meet their specific needs.

    Symphony also supports running in a standalone mode independent from Kubernetes. All you need is a single Symphony binary and nothing else!

* Meet customers where they are

    Another key advantage of Symphony is its extensibility. It supports the integration of first-party and third-party services, and all Symphony capabilities, including device registry, device updates, and solution deployment, can be replaced with custom implementations. This means that Symphony can be tailored to meet the specific needs of any organization, regardless of their size or complexity.

* Zero-friction adoption

    Symphony's zero-friction adoption approach is another key feature that sets it apart from other solutions. Users can get started with Symphony using a single computer, and there is no need for special hardware, an Azure subscription, or Kubernetes to start experimenting with the solution. Additionally, the same Symphony artifacts used during testing and development can be carried over into production deployments, ensuring a smooth transition and reducing overall deployment time and costs.

- **Symphony is platform agnostic**

    Symphony was started by Microsoft as a platform-agnostic project, making it an ideal solution for organizations that already use Azure Edge and AI services like [Azure IoT Hub](https://docs.microsoft.com/azure/iot-hub/), [Azure IoT Edge](https://azure.microsoft.com/services/iot-edge/), [Azure Cognitive Services](https://azure.microsoft.com/services/cognitive-services/), [Azure Storage](https://azure.microsoft.com/products/category/storage/), [Azure ML](https://azure.microsoft.com/services/machine-learning/), [Azure Monitor](https://docs.microsoft.com/azure/azure-monitor/), and [Azure Arc](https://learn.microsoft.com/azure/azure-arc/overview). However, Symphony is also fully compatible with other non-Azure services or open-source software tools, allowing organizations to modify the solution to meet their specific needs. This flexibility ensures that Symphony meets customers where they are, making it an ideal solution for organizations of all sizes and complexities.

## Getting Started
There are several ways to get started with Symphony, including using the CLI tool, Helm, Docker, or the symphony-api binary.

### Using Symphony CLI

> **NOTE**: The following GitHub URL is a temporary parking location and is sugject to change.

The easiest way to get started with Symphony is by using Symphony's CLI tool, called maestro. The CLI tool can be installed on **Linux**, **WSL**, and **Mac** using the following command:

```Bash
wget -q https://raw.githubusercontent.com/eclipse-symphony/symphony/master/cli/install/install.sh -O - | /bin/bash
```
For **Windows**, the following PowerShell command can be used:
```PowerShell
powershell -Command ""iwr -useb https://raw.githubusercontent.com/eclipse-symphony/symphony/master/cli/install/install.ps1 | iex""
```
After Symphony is installed, you can use `maestro` to try out sample scenarios.

```bash
maestro up
```

### Using Helm
You can also install Symphony using Helm by running the following command:
```Bash
helm install symphony oci://ghcr.io/eclipse-symphony/helm/symphony --version '0.48.28'
```
After Symphony is installed, you can use maestro to try out sample scenarios.

### Using Docker
You can also install Symphony using Docker with the bundled `symphony-api.json` or volume mounting your own & injecting its reference via `CONFIG` env:
```Bash
docker run -d --name symphony-api -p 8080:8080 -e CONFIG=/symphony-api.json ghcr.io/eclipse-symphony/symphony-api:0.48.28
```
### Using symphony-api binary
You can also run Symphony in standalone mode as a single process by running the following command:
```Bash
./symphony-api -c ./symphony-api-dev.json -l Debug
```
## Provider Conformance Test Results
Symphony is an extensible system with the concept of providers. For each provider types, we define one or multiple conformance test suites that ensure provider implementations behaves consistently and predictably.

### Target Providers

| Provider | Basic<sup>1</sup> | 
|--------|--------|
| ```providers.target.adb``` |![](https://byob.yarr.is/Haishi2016/badges/target-adb-app)|
| ```providers.target.azure.adu``` |![](https://byob.yarr.is/Haishi2016/badges/target-adu-app)|
| ```providers.target.azure.iotedge``` |![](https://byob.yarr.is/Haishi2016/badges/target-iotedge-app)|
| ```providers.target.docker```|![](https://byob.yarr.is/Haishi2016/badges/target-docker-app)|
| ```providers.target.helm```|![](https://byob.yarr.is/Haishi2016/badges/target-helm-app)|
| ```providers.target.http```|![](https://byob.yarr.is/Haishi2016/badges/target-http-app)|
| ```providers.target.k8s``` |![](https://byob.yarr.is/Haishi2016/badges/target-k8s-app)|
| ```providers.target.kubectl```|![](https://byob.yarr.is/Haishi2016/badges/target-kubectl-app)|
| ```providers.target.mqtt```|![](https://byob.yarr.is/Haishi2016/badges/target-mqtt-app)|
| ```providers.target.proxy```|![](https://byob.yarr.is/Haishi2016/badges/target-proxy-app)|
| ```providers.target.script```|![](https://byob.yarr.is/Haishi2016/badges/target-script-app)|
| ```providers.target.staging```|![](https://byob.yarr.is/Haishi2016/badges/target-staging-app)|
| ```providers.target.win10```|![](https://byob.yarr.is/Haishi2016/badges/target-win10-app)|

1. **Basic** conformance level requires a provider to properly respond to missing properties

## What's Next

* [The Symphony Book](./docs/README.md)
* [Set up a local environment](./test/localenv/README.md)

## Community

### Communication and Discord

All your contributions and suggestions are greatly appreciated! One of the easiest ways to contribute is to participate in Discord discussions, report issues, or join the monthly community calls.

### Questions and issues

Reach out with any questions you may have and we'll make sure to answer them as soon as possible. Community members, please feel free to jump in to join discussions or answer questions!

| Platform  | Link        |
|:----------|:------------|
| Discord | Join the [Discord server](https://discord.gg/JvY8qBkWbw)

### Email announcements

Want to stay up to date with Symphony releases, community calls, and other announcements? Join the Google Group to stay up to date on the latest Symphony news.

| Group | Link |
|:------|:-----|
| symphonyoss | Join the [symphonyoss Group](https://groups.google.com/g/symphonyoss)

### Community meetings

Every month we host a community call to showcase new features, review upcoming milestones, and engage in a Q&A. For community calls, anyone from the Symphony community can participate or present a topic. All are welcome!


You can always catch up offline by watching the recordings below.

| Asset | Link        |
|:-----------|:------------|
| Meeting Link | [Teams Link](https://teams.microsoft.com/meet/267721771421?p=hyAHXrqsyVdDA0VAZQ)
| Meeting Recordings | [YouTube](https://www.youtube.com/@Eclipse-Symphony/videos)

### Upcoming calls

| Date & time |
|-------------|
| Wednesday March 26 <sup>th</sup>, 2025 8:00am Pacific Time (PST) |

### Previous calls

| Date & time | Link |
|-------------|:-------------|
| 12/11/2024 | [Recording Link](https://www.youtube.com/watch?v=0WEDia5JD-Y)|
| 01/15/2025 | [Recording Link](https://youtu.be/8b4wc21eOjM)|
| 02/26/2025 | [Recording Link](https://youtu.be/VAwGlObx0mQ)|


## Contributing

This project welcomes contributions and suggestions.  

### Eclipse Contributor Agreement

Before your contribution can be accepted by the project team contributors must electronically sign the Eclipse Contributor Agreement (ECA).

http://www.eclipse.org/legal/ECA.php
Commits that are provided by non-committers must have a Signed-off-by field in the footer indicating that the author is aware of the terms by which the contribution has been provided to the project. The non-committer must additionally have an Eclipse Foundation account and must have a signed Eclipse Contributor Agreement (ECA) on file.

For more information, please see the Eclipse Committer Handbook: https://www.eclipse.org/projects/handbook/#resources-commit

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.",FAUX
eclipse-tractusx/portal,Application System,Application System,2025-05-12T08:00:21Z,2025-05-07T11:40:00Z,0,0,0,0,0,9,0,0,2022-10-24T18:41:58Z,2025-04-08T08:04:35Z,2963,8,Smarty,VRAI,19,FAUX,16,,16,Portal - Helm charts,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,17,"![LeadingRepository](https://img.shields.io/badge/Leading_Repository-blue)

# Portal: Helm charts

This repository contains the following helm charts:

##  Portal

This helm chart installs the Portal application which consists of

* [portal-frontend](https://github.com/eclipse-tractusx/portal-frontend),
* [portal-frontend-registration](https://github.com/eclipse-tractusx/portal-frontend-registration),
* [portal-assets](https://github.com/eclipse-tractusx/portal-assets) and
* [portal-backend](https://github.com/eclipse-tractusx/portal-backend).

The Portal is designed to work with the [IAM](https://github.com/eclipse-tractusx/portal-iam).

For **installation** details and further information, please refer to the chart specific [README](./charts/portal/README.md).

Please refer to the `docs` directory of the [portal-assets](https://github.com/eclipse-tractusx/portal-assets) repository for the overarching user and developer documentation of the Portal application.

##  LocalDev Portal & IAM

This umbrella chart installs the Portal and the IAM Keycloak instances.

It's intended for the local setup of the those components in order to aid the local development.

For detailed information please refer to the chart specific [README](./charts/localdev/README.md).

## Notice for Docker images

The referenced container images in the helm charts are for demonstration purposes only.

## License

Distributed under the Apache 2.0 License.
See [LICENSE](./LICENSE) for more information.",FAUX
edgelesssys/contrast,DevOPs,DevOPs,2025-05-15T09:07:36Z,2025-05-06T11:05:23Z,0,4,0,0,0,0,0,0,2023-10-12T08:24:05Z,2025-04-07T15:13:34Z,137533,230,Go,VRAI,10,FAUX,25,,25,Deploy and manage confidential containers on Kubernetes,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,22,"![Contrast](docs/static/img/GitHub-Banner_Contrast_2024.gif)

# Contrast

Contrast runs confidential container deployments on Kubernetes at scale.

Contrast is based on the [Kata Containers](https://github.com/kata-containers/kata-containers) and
[Confidential Containers](https://github.com/confidential-containers) projects.
Confidential Containers are Kubernetes pods that are executed inside a confidential micro-VM and provide strong hardware-based isolation from the surrounding environment.
This works with unmodified containers in a lift-and-shift approach.
Contrast currently targets the [CoCo preview on AKS](https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-containers-on-aks-preview).

<img src=""docs/static/img/concept.svg"" alt=""Concept"" width=""80%""/>

## Goal

Contrast is designed to keep all data always encrypted and to prevent access from the infrastructure layer. It removes the infrastructure provider from the trusted computing base (TCB). This includes access from datacenter employees, privileged cloud admins, own cluster administrators, and attackers coming through the infrastructure, for example, malicious co-tenants escalating their privileges.

Contrast integrates fluently with the existing Kubernetes workflows. It's compatible with managed Kubernetes, can be installed as a day-2 operation and imposes only minimal changes to your deployment flow.

## Use cases

* Increasing the security of your containers
* Moving sensitive workloads from on-prem to the cloud with Confidential Computing
* Shielding the code and data even from your own cluster administrators
* Increasing the trustworthiness of your SaaS offerings
* Simplifying regulatory compliance
* Multi-party computation for data collaboration

## Features

### 🔒 Everything always encrypted

* Runtime encryption: All Pods run inside AMD SEV-based Confidential VMs (CVMs). Support for Intel TDX will be added in the future.
* PKI and mTLS: All pod-to-pod traffic can be encrypted and authenticated with Contrast's workload certificates.

### 🔍 Everything verifiable

* Workload attestation based on the identity of your container and the remote-attestation feature of [Confidential Containers](https://docs.edgeless.systems/contrast/basics/confidential-containers)
* ""Whole deployment"" attestation based on Contrast's [Coordinator attestation service](https://docs.edgeless.systems/contrast/components#the-coordinator)
* Runtime environment integrity verification based runtime policies
* Kata micro-VMs and single workload isolation provide a minimal Trusted Computing Base (TCB)

### 🏝️ Everything isolated

* Runtime policies enforce strict isolation of your containers from the Kubernetes layer and the infrastructure.
* Pod isolation: Pods are isolated from each other.
* Namespace isolation: Contrast can be deployed independently in multiple namespaces.

### 🧩 Lightweight and easy to use

* Install in Kubernetes cluster as a day-2 operation.
* Compatible with managed Kubernetes.
* Minimal DevOps involvement.
* Simple CLI tool to get started.

## Documentation

To learn more, see the [documentation](https://docs.edgeless.systems/contrast).
You may want to start with one of the following sections.

* [Getting started](https://docs.edgeless.systems/contrast/getting-started)
* [Security benefits](https://docs.edgeless.systems/contrast/basics/security-benefits)
* [Components](https://docs.edgeless.systems/contrast/components)

## Known limitations

See the current list of [known limitations](https://docs.edgeless.systems/contrast/features-limitations) in the documentation.

## Upcoming Contrast features

* Plugin for a key management service (KMS) for attestation/coordinator certificate-based key release
* High availability (distributed Contrast Coordinator)

## Contributing

See the [contributing guide](CONTRIBUTING.md).
Please follow the [Code of Conduct](/CODE_OF_CONDUCT.md).

## Support

* If something doesn't work, make sure to use the [latest release](https://github.com/edgelesssys/contrast/releases/latest) and check out the [known issues](https://github.com/edgelesssys/contrast/issues?q=is%3Aopen+is%3Aissue+label%3A%22known+issue%22).
* Please file an [issue](https://github.com/edgelesssys/contrast/issues) to get help or report a bug.
* Visit our [blog](https://www.edgeless.systems/blog/) for technical deep-dives and tutorials and follow us on [LinkedIn](https://www.linkedin.com/company/edgeless-systems) for news.
* Edgeless Systems also offers [Enterprise Support](https://www.edgeless.systems/products/contrast/).

## License

Contrast follows the open-core model.
The core project is licensed under AGPLv3.
The Enterprise Edition is licensed under BUSL-1.1.
See the [LICENSE](LICENSE) file for details.",FAUX
educates/educates-training-platform,DevOPs,Application System,2025-05-09T23:54:27Z,2025-04-22T10:11:29Z,0,0,0,0,11,0,0,0,2022-03-31T00:11:10Z,2025-04-06T05:39:22Z,26416,87,Python,VRAI,25,FAUX,100,"hands-on-lab,kubernetes,kubernetes-operator,learning-by-doing",100,"A platform for hosting interactive workshop environments in Kubernetes, or on top of a local container runtime.",FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,12,"Educates Training Platform
==========================

The Educates project provides a system for hosting interactive workshop
environments in Kubernetes, or on top of a local container runtime. It can be
used for self paced or supervised workshops. It can also be useful where you
need to package up demos of applications hosted in Kubernetes or a local
container runtime.

Educates documentation
----------------------

For detailed instructions on how to deploy and make use of Educates see the
[Educates user documentation](https://docs.educates.dev/).

Getting help with Educates
--------------------------

If you have questions about using Educates, use the `#educates` channel under
the [Kubernetes community Slack](https://kubernetes.slack.com/).

If you have found a bug in Educates or want to request a feature, you can use
our [GitHub issue
tracker](https://github.com/educates/educates-training-platform/issues).

Contributing to Educates
------------------------

If you would like to contribute to Educates, check out our [contribution
guidelines](CONTRIBUTING.md) and [developer
documenation](developer-docs/README.md).",VRAI
eksctl-io/eksctl,Toolkit,Toolkit,2025-05-12T20:05:40Z,2025-04-02T22:00:28Z,0,0,0,0,3,0,2,0,2018-05-23T08:41:03Z,2025-04-04T18:25:11Z,66560,5036,Go,VRAI,1447,FAUX,105,"amazon-web-services,aws-cloudformation,aws-ec2,aws-eks,eks,kubernetes,kubernetes-cluster,kubernetes-deployment,kubernetes-setup",105,The official CLI for Amazon EKS,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,364,"# `eksctl` - The official CLI for Amazon EKS ! <img src=""userdocs/src/img/logo.png"" alt=""eksctl"" width=""4%""/>

[![Go Report Card](https://goreportcard.com/badge/github.com/eksctl-io/eksctl)](https://goreportcard.com/report/github.com/eksctl-io/eksctl)

`eksctl` is a simple CLI tool for creating clusters on EKS - Amazon's new managed Kubernetes service for EC2. It is written in Go, and uses CloudFormation.

You can create a cluster in minutes with just one command – **`eksctl create cluster`**!

![Gophers: E, K, S, C, T, & L](logo/eksctl.png)

_Need help? Join [Eksctl Slack][slackjoin]._

## New: EKS Auto Mode Support

`eksctl` now supports EKS Auto Mode! EKS Auto Mode automates routine tasks for cluster compute, storage, and networking. 

* Learn how to [create an EKS Auto Mode Cluster with eksctl](https://docs.aws.amazon.com/eks/latest/userguide/automode-get-started-eksctl.html).
* Review the [eksctl docs](https://github.com/eksctl-io/eksctl/blob/main/userdocs/src/usage/auto-mode.md) for EKS Auto Mode.

Version 0.195.0 or greater is required.

## New: EKS Hybrid Nodes Support

`eksctl` now supports EKS Hybrid Nodes! Hybrid Nodes enables you to run on-premises and edge applications on customer-managed infrastructure with the same AWS EKS clusters, features, and tools you use in the AWS Cloud.

* For more information, see [Amazon EKS Hybrid Nodes overview](https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes-overview.html) in the EKS User Guide.
* Review the [eksctl docs](https://github.com/eksctl-io/eksctl/blob/main/userdocs/src/usage/hybrid-nodes.md) for EKS Hybrid Nodes.

Version 0.195.0 or greater is required.



## Installation

`eksctl` is available to install from official releases as described below. We recommend that you install `eksctl` from only the official GitHub releases. You may opt to use a third-party installer, but please be advised that AWS does not maintain nor support these methods of installation. Use them at your own discretion.

### Prerequisite

You will need to have AWS API credentials configured. What works for AWS CLI or any other tools (kops, Terraform, etc.) should be sufficient. You can use [`~/.aws/credentials` file][awsconfig]
or [environment variables][awsenv]. For more information read [AWS documentation](https://docs.aws.amazon.com/cli/latest/userguide/cli-environment.html).

[awsenv]: https://docs.aws.amazon.com/cli/latest/userguide/cli-environment.html
[awsconfig]: https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html

You will also need [AWS IAM Authenticator for Kubernetes](https://github.com/kubernetes-sigs/aws-iam-authenticator) command (either `aws-iam-authenticator` or `aws eks get-token` (available in version 1.16.156 or greater of AWS CLI) in your `PATH`.

The IAM account used for EKS cluster creation should have these minimal access levels.

| AWS Service      | Access Level                                           |
|------------------|--------------------------------------------------------|
| CloudFormation   | Full Access                                            |
| EC2              | **Full:** Tagging **Limited:** List, Read, Write       |
| EC2 Auto Scaling | **Limited:** List, Write                               |
| EKS              | Full Access                                            |
| IAM              | **Limited:** List, Read, Write, Permissions Management |
| Systems Manager  | **Limited:** List, Read                                |

The inline policy json is listed in [Minimal IAM Policies](https://eksctl.io/usage/minimum-iam-policies/).

### For Unix

To download the latest release, run:

```sh
# for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
ARCH=amd64
PLATFORM=$(uname -s)_$ARCH

curl -sLO ""https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz""

# (Optional) Verify checksum
curl -sL ""https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt"" | grep $PLATFORM | sha256sum --check

tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz

sudo mv /tmp/eksctl /usr/local/bin
```

### For Windows

#### Direct download (latest release): [AMD64/x86_64](https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_windows_amd64.zip) - [ARMv6](https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_windows_armv6.zip) - [ARMv7](https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_windows_armv7.zip) - [ARM64](https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_windows_arm64.zip)

Make sure to unzip the archive to a folder in the `PATH` variable.

Optionally, verify the checksum:

1. Download the checksum file: [latest](https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt)
2. Use Command Prompt to manually compare `CertUtil`'s output to the checksum file downloaded.

  ```cmd
  REM Replace amd64 with armv6, armv7 or arm64
  CertUtil -hashfile eksctl_Windows_amd64.zip SHA256
  ```

3. Using PowerShell to automate the verification using the `-eq` operator to get a `True` or `False` result:

```pwsh
# Replace amd64 with armv6, armv7 or arm64
 (Get-FileHash -Algorithm SHA256 .\eksctl_Windows_amd64.zip).Hash -eq ((Get-Content .\eksctl_checksums.txt) -match 'eksctl_Windows_amd64.zip' -split ' ')[0]
 ```

#### Using Git Bash

```sh
# for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
ARCH=amd64
PLATFORM=windows_$ARCH

curl -sLO ""https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.zip""

# (Optional) Verify checksum
curl -sL ""https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt"" | grep $PLATFORM | sha256sum --check

unzip eksctl_$PLATFORM.zip -d $HOME/bin

rm eksctl_$PLATFORM.zip
```

The `eksctl` executable is placed in `$HOME/bin`, which is in `$PATH` from Git Bash.

### Docker

For every release and RC a container image is pushed to ECR repository `public.ecr.aws/eksctl/eksctl`. Learn more about the usage on [ECR Public Gallery - eksctl](https://gallery.ecr.aws/eksctl/eksctl). For example,

```bash
docker run --rm -it public.ecr.aws/eksctl/eksctl version
```

### Third-Party Installers (Not Recommended)

#### For MacOS

[Homebrew](https://brew.sh)

```bash
brew tap weaveworks/tap
brew install weaveworks/tap/eksctl
```

[MacPorts](https://www.macports.org)

```bash
port install eksctl
```

#### For Windows

[chocolatey](https://chocolatey.org)

```bash
choco install eksctl
```

[scoop](https://scoop.sh)

```bash
scoop install eksctl
```

## Basic usage

To create a basic cluster, run:

```bash
eksctl create cluster
```

A cluster will be created with default parameters

- exciting auto-generated name, e.g. ""fabulous-mushroom-1527688624""
- 2x `m5.large` nodes (this instance type suits most common use-cases, and is good value for money)
- use official AWS EKS AMI
- `us-west-2` region
- dedicated VPC (check your quotas)

Once you have created a cluster, you will find that cluster credentials were added in `~/.kube/config`. If you have `kubectl` as well as `aws-iam-authenticator` commands in your `PATH`, you should be
able to use `kubectl`. You will need to make sure to use the same AWS API credentials for this also. Check [EKS docs][ekskubectl] for instructions.

[ekskubectl]: https://docs.aws.amazon.com/eks/latest/userguide/configure-kubectl.html

Example output:

```bash
 $ eksctl create cluster
[ℹ]  eksctl version 0.143.0
[ℹ]  using region eu-west-2
[ℹ]  setting availability zones to [eu-west-2a eu-west-2c eu-west-2b]
[ℹ]  subnets for eu-west-2a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for eu-west-2c - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for eu-west-2b - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  nodegroup ""ng-ac4c787c"" will use """" [AmazonLinux2/1.25]
[ℹ]  using Kubernetes version 1.25
[ℹ]  creating EKS cluster ""attractive-sculpture-1685534556"" in ""eu-west-2"" region with managed nodes
[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=eu-west-2 --cluster=attractive-sculpture-1685534556'
[ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster ""attractive-sculpture-1685534556"" in ""eu-west-2""
[ℹ]  CloudWatch logging will not be enabled for cluster ""attractive-sculpture-1685534556"" in ""eu-west-2""
[ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=eu-west-2 --cluster=attractive-sculpture-1685534556'
[ℹ]  
2 sequential tasks: { create cluster control plane ""attractive-sculpture-1685534556"", 
    2 sequential sub-tasks: { 
        wait for control plane to become ready,
        create managed nodegroup ""ng-ac4c787c"",
    } 
}
[ℹ]  building cluster stack ""eksctl-attractive-sculpture-1685534556-cluster""
[ℹ]  deploying stack ""eksctl-attractive-sculpture-1685534556-cluster""
[ℹ]  waiting for CloudFormation stack ""eksctl-attractive-sculpture-1685534556-cluster""
[ℹ]  building managed nodegroup stack ""eksctl-attractive-sculpture-1685534556-nodegroup-ng-ac4c787c""
[ℹ]  deploying stack ""eksctl-attractive-sculpture-1685534556-nodegroup-ng-ac4c787c""
[ℹ]  waiting for CloudFormation stack ""eksctl-attractive-sculpture-1685534556-nodegroup-ng-ac4c787c""
[ℹ]  waiting for the control plane to become ready
[✔]  all EKS cluster resources for ""attractive-sculpture-1685534556"" have been created
[ℹ]  nodegroup ""ng-ac4c787c"" has 2 node(s)
[ℹ]  node ""ip-192-168-20-235.eu-west-2.compute.internal"" is ready
[ℹ]  node ""ip-192-168-80-49.eu-west-2.compute.internal"" is ready
[ℹ]  waiting for at least 2 node(s) to become ready in ""ng-ac4c787c""
[ℹ]  nodegroup ""ng-ac4c787c"" has 2 node(s)
[ℹ]  node ""ip-192-168-20-235.eu-west-2.compute.internal"" is ready
[ℹ]  node ""ip-192-168-80-49.eu-west-2.compute.internal"" is ready
[ℹ]  kubectl command should work with "".../.kube/config"", try 'kubectl get nodes'
[✔]  EKS cluster ""attractive-sculpture-1685534556"" in ""eu-west-2"" region is ready
$
```

Install `eksctl` by following the [installation instructions](https://eksctl.io/installation).

To discover the full range of capabilities that eksctl offers, visit [eksctl.io](https://eksctl.io).

A great starting point is the [Getting Started](https://eksctl.io/getting-started/) guide.

You can find a comprehensive list of features [here](https://eksctl.io/installation/#features).


## Contributions

Code contributions are very welcome. If you are interested in helping make `eksctl` great then see our [contributing guide](CONTRIBUTING.md).

We follow the [CNCF Code of Conduct](CODE_OF_CONDUCT.md).

## Releases

Minor releases of `eksctl` should be expected every two weeks and patch releases will be made available as needed.

One or more release candidate(s) (RC) builds will be made available prior to each minor release. RC builds are intended only for testing purposes.

## [Security Policy](SECURITY.md)

If you discover a potential security issue in `eksctl` project, please
follow [AWS Vulnerability Reporting process.](https://aws.amazon.com/security/vulnerability-reporting/)

Do not open security related issues in the open source project.

## Get in touch

[Create an issue](https://github.com/eksctl-io/eksctl/issues/new), or login to [Eksctl Community Slack (#eksctl)][slackchan] ([signup][slackjoin]).

[slackjoin]: https://slack.k8s.io/
[slackchan]: https://slack.k8s.io/messages/eksctl/

> **_Logo Credits_**
>
> _Original Gophers drawn by [Ashley McNamara](https://twitter.com/ashleymcnamara), unique E, K, S, C, T & L Gopher identities had been produced with [Gopherize.me](https://github.com/matryer/gopherize.me/)._",FAUX
elastic/cloud-on-k8s,DevOPs,DevOPs,2025-05-05T18:30:45Z,2025-03-27T07:46:13Z,0,0,0,0,3,0,0,0,2018-10-30T10:43:58Z,2025-04-08T03:00:39Z,52536,2696,Go,VRAI,737,FAUX,429,"elasticsearch,go,kibana,kubernetes,kubernetes-operators",429,Elastic Cloud on Kubernetes,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,156,"[![Build status](https://badge.buildkite.com/8fe262ce6fc1da017fc91c35465c1fe0addbc94c38afc9f04b.svg?branch=main)](https://buildkite.com/elastic/cloud-on-k8s-operator)
[![GitHub release](https://img.shields.io/github/v/release/elastic/cloud-on-k8s.svg)](https://github.com/elastic/cloud-on-k8s/releases/latest)

# Elastic Cloud on Kubernetes (ECK)

Elastic Cloud on Kubernetes automates the deployment, provisioning, management, and orchestration of Elasticsearch, Kibana, APM Server, Enterprise Search, Beats, Elastic Agent, Elastic Maps Server, and Logstash on Kubernetes based on the operator pattern.

Current features:

*  Elasticsearch, Kibana, APM Server, Enterprise Search, and Beats deployments
*  TLS Certificates management
*  Safe Elasticsearch cluster configuration & topology changes
*  Persistent volumes usage
*  Custom node configuration and attributes
*  Secure settings keystore updates

Supported versions:

*  Kubernetes 1.28-1.32
*  OpenShift 4.14-4.18
*  Elasticsearch, Kibana, APM Server: 7.17+, 8+, 9+
*  Enterprise Search: 7.7+, 8+
*  Beats: 7.17+, 8+, 9+
*  Elastic Agent: 7.17+ (standalone), 7.17+, 8+, 9+ (Fleet)
*  Elastic Maps Server: 7.17+, 8+, 9+
*  Logstash 8.12+, 9+

Check the [Quickstart](https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html) to deploy your first cluster with ECK.

If you want to contribute to the project, check our [contributing guide](CONTRIBUTING.md) and see [how to setup a local development environment](dev-setup.md).

For general questions, please see the Elastic [forums](https://discuss.elastic.co/c/eck).",FAUX
elastic/cloudbeat,Toolkit,Application System,2025-05-09T09:19:47Z,2025-04-09T14:27:51Z,0,762,0,0,0,0,0,0,2022-02-14T10:43:26Z,2025-04-02T14:59:45Z,14061,48,Go,VRAI,42,FAUX,115,"cloud,cspm,golang,vulnerability-detection,vulnerability-scanners",115,Analyzing Cloud Security Posture,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,36,"[![Coverage Status](https://coveralls.io/repos/github/elastic/cloudbeat/badge.svg?branch=main)](https://coveralls.io/github/elastic/cloudbeat?branch=main)
[![Go Report Card](https://goreportcard.com/badge/github.com/elastic/cloudbeat)](https://goreportcard.com/report/github.com/elastic/cloudbeat)
[![Build status](https://badge.buildkite.com/82f39bb3a95eeb7f46e28891fb48a623cf184fbfca2eff545a.svg)](https://buildkite.com/elastic/cloudbeat)

# Cloudbeat

Cloudbeat is a tool that analyzes cloud assets for security compliance and sends findings to Elasticsearch.
It is designed to be used as part of the [Cloud Security](https://www.elastic.co/blog/secure-your-cloud-with-elastic-security) plugin in Kibana.


### CSP Security Policies

Cloudbeat uses [security policies](./security-policies) to evaluate cloud resources.

## Getting Started

To get started with Cloud Security on your cluster, please refer to our documentation:

- [Get started with Kubernetes Security Posture Management (KSPM)](https://www.elastic.co/guide/en/security/current/get-started-with-kspm.html)

- [Get started with Cloud Security Posture Management (CSPM)](https://www.elastic.co/guide/en/security/current/cspm.html)

- [Get started with Cloud Native Vulnerability Management (CNVM)](https://www.elastic.co/guide/en/security/current/vuln-management-get-started.html)

---

## Deployment

To run Cloudbeat, you need to have Elastic Stack (Elasticsearch, Kibana, etc) running (locally/cloud). See our [Elastic Stack Deployment options](dev-docs/ELK-Deployment.md) documentation.

Once your Elastic Stack is deployed, you can proceed with the deployment of Cloudbeat. For deployment instructions, see [Cloudbeat Deployment options](dev-docs/Cloudbeat-Deployment.md).

## Development

### Prerequisites

We use [Hermit](https://cashapp.github.io/hermit/usage/get-started/) to manage our development tooling. Please refer to our [README](/bin/README.hermit.md) for detailed instructions on setting it up.

___

> **Note** If you are a developer or contributor, or if you are looking for additional information, please visit our [development documentation](dev-docs/Development.md)",VRAI
elastic/harp,Toolkit,Toolkit,2025-04-30T22:12:09Z,2025-04-22T02:48:13Z,0,4,0,0,0,0,0,0,2020-11-25T10:11:02Z,2025-02-20T22:23:51Z,5483,149,Go,VRAI,19,FAUX,18,"cloud,cloud-security,cloud-storage,consul,encryption,etcdv3,gitops,golang,golang-library,kubernetes,kv-store,paseto,pipeline,rego,secret-management,secret-storage,unix-command,vault,yaml,zookeeper",18,Secret management by contract toolchain ,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,5,"[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Go Report Card](https://goreportcard.com/badge/github.com/elastic/harp)](https://goreportcard.com/report/github.com/elastic/harp)
[![made-with-Go](https://img.shields.io/badge/Made%20with-Go-1f425f.svg)](http://golang.org)
[![GitHub release](https://img.shields.io/github/release/elastic/harp.svg)](https://github.com/elastic/harp/releases/)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://github.com/elastic/harp/graphs/commit-activity)

- [Harp](#harp)
  - [TL;DR.](#tldr)
  - [Visual overview](#visual-overview)
  - [Why harp?](#why-harp)
  - [Use cases](#use-cases)
  - [How does it work?](#how-does-it-work)
    - [Like a Data pipeline but for secret](#like-a-data-pipeline-but-for-secret)
    - [Immutable transformation](#immutable-transformation)
  - [What can I do?](#what-can-i-do)
  - [FAQ](#faq)
  - [License](#license)
- [Homebrew install](#homebrew-install)
- [Build instructions](#build-instructions)
  - [Clone repository](#clone-repository)
  - [Manual dev environment](#manual-dev-environment)
    - [Check your go version](#check-your-go-version)
      - [Go 1.17/1.16](#go-117116)
      - [Go 1.18 (beta)](#go-118-beta)
    - [Install mage](#install-mage)
      - [From source](#from-source)
    - [Daily](#daily)
  - [With nix-shell](#with-nix-shell)
  - [Bootstrap tools](#bootstrap-tools)
  - [Docker](#docker)
    - [For Tools](#for-tools)
    - [For CLI](#for-cli)
- [Plugins](#plugins)
- [Community](#community)

# Harp

Harp is for Harpocrates (Ancient Greek: Ἁρποκράτης) the god of silence, secrets
and confidentiality in the Hellenistic religion. - [Wikipedia](https://en.wikipedia.org/wiki/Harpocrates)

## TL;DR.

Harp is a tool set to operate secret management by contract. The objective is
to reduce the value centric management by handling secret data in a
`reproducible` way by providing a technical stack to describe how your value is
managed by contracts and pipelines. Furthermore, we know that `naming thing is hard`,
as a consequence a secret could be `consistently` associated to a `predictable`
secret identifier used as a key to refer to the secret value. Finally, the secret
can hold additional metadata (ownership, rotation period, leak severity, etc.)
which can be consumed during the pipeline executions.

These key/value associations (path ⇒ value) form a `Bundle` stored in an
immutable file named a `Container`. This `Container` acts as a pivot format to
allow Harp commands to communicate and create data management pipelines.

In addition to that, it provides a `template engine` used to generate various
confidence values (password, passphrase, crypto keys, etc.) and allow more
sophisticated rendering operations (configuration files, etc.).

Finally, it provides a `SDK` to allow developers to integrate `Harp` features
in their products, and/or extend the Harp pipeline features by creating new
[plugins](#plugins).

## Visual overview

![Visual overview](docs/harp/img/HARP_FLOW.png)

## Why harp?

* Secret management is in essence a collection of processes that must be
  auditable, executable and reproducible for infosec and operation requirements;
* Secret provisioning must be designed with secret rotation as a day one task,
  due to the fact that secret data must be rotated periodically to keep its
  secret property;
* `Developers` should negotiate secret value for the secret consumer they are
  currently developing, by the contract based on a path (reference to the secret)
  and a value specification (for code contract) without the knowledge of the
  final deployed value;
* `Secret Operators` use different set of tools to achieve secret
  management operation which increases the error/secret exposure probability due to
  tool count involved in the process (incompatibility, changes, etc.);
* Without a defined secret naming convention, the secret storage becomes difficult to
  handle in time (naming is hard) and secret naming could not be helped to
  get a consistent, reliable and flexible secret tree;
* Secret storage backend can use various implementations in different environments
  and should be provisioned consistently.

## Use cases

* You want to have a `single secret value` and you are asking yourself
  `how to generate a strong password` - Harp has a template engine with secret
  value generation functions to allow you to generate such values.
* You have `thousands secrets` to handle to deploy your platform/customers
  `on multiple cloud providers` with `different secret storages` - Harp will help you
  to define consistent secret provisioning bundles and pipelines.
* You need a `ephemeral secret storage` to `bootstrap` your long term cloud
  secret storage - Harp will help you to create
  secret containers that can be consumed on deployment.
* You want to `migrate massively` your secrets from one secret storage to
  another - Harp provides you a secret container to store these secrets while
  they are going to be distributed in other secret storage implementations.
* You have to `alter/modifiy` a secret (rotation/deprecation/renewal) - Harp
  provides you a `GitOps-able` secret `storage agnostic operation set`, so that you
  can define a specification to describe how your secret operation is going to
  be applied offline on the secret container.

## How does it work?

![Secret management Pipeline](docs/harp/img/SM-HARP-PIPELINE.png)

### Like a Data pipeline but for secret

`harp` allows you to handle secrets using deterministic pipelines expressed
using an atomic series of CLI operations applied to a commonly shared container
immutable and standalone file system used to store secret collection (Bundle)
generated from a template engine via user specification, or external secret
value coming from files or external secret storage.

![Pipelines](docs/harp/img/SM-HARP.png)

These pipelines use the immutable container file system as a data exchange
protocol and could be extended for new input, intermediary operation or output
via plugins created with the `harp` SDK.

### Immutable transformation

Each applied transformation creates a container with transformed data inside.
This will enforce container reproducibility by eliminating cumulative
side effects applied to the same container.

The container handles for you the confidentiality and integrity protection applied
to the secret collection stored inside and manipulated by copy during the
pipeline execution.

## What can I do?

> New to harp, let's start with [onboarding tutorial](docs/onboarding/README.md) !
> TL;DR - [Features overview](FEATURES.md)

Harp provides :

* A methodology to design your secret management;
  * Secret naming convention (CSO);
  * A defined common language and complete processes to achieve secret management
    operations;
* A SDK to create your own tools to orchestrate your secret management pipelines;
  * A container manipulation library exposed as `github.com/elastic/harp/pkg/container`;
  * A secret bundle specification to store and manipulate secrets exposed as `github.com/elastic/harp/pkg/bundle`;
  * An `on-steroid` template engine exposed as `github.com/elastic/harp/pkg/template`
  * A path name validation library exposed as `github.com/elastic/harp/pkg/cso`
* A CLI for secret management implementation
  * CI/CD integration;
  * Based on human-readable definitions (YAML);
  * In order to create auditable and reproducible pipelines.
  * An extensible tool which can be enhanced via [plugins](https://github.com/elastic/harp-plugins).

And allows :

* Bundle level operations
  * Create a bundle from scratch / template / JSON (more via plugins);
  * Generate a complete bundle using a YAML Descriptor (`BundleTemplate`) to describe secret and their usages;
  * Read value stored in the K/V virtual file system;
  * Update the K/V virtual file system;
  * Reproducible patch applied on immutable container (copy-on-write);
  * Import / Export to Vault.
* Immutable container level operations
  * Seal / Unseal a container for integrity and confidentiality property conservation
    to enforce at-rest encryption (aes256-gcm96 or chacha20-poly1305);
  * Multiple identities sealing algorithm;

## FAQ

* Is it used internally at Elastic? - Yes. It is used to generate bootstrap
  secrets used to bootstrap the new region infrastructure components.
  #ChickenEggProblem

* Harp is only supporting `Vault`? - No, it has been published with only vault
  support built-in, but it supports many other secret storage implementations via
  plugins.

* What's the difference with `Vault`? - HashiCorp Vault is an encrypted highly
  available K/V store with advanced authorization engine, it doesn't handle
  secret provisioning for you. You can't ask Vault to generate secrets for your
  application and store them using a defined logic. Harp is filling this
  requirement.

## License

`harp` artifacts and source code is released under [Apache 2.0 Software License](LICENSE).

# Homebrew install

Download a [release](https://github.com/elastic/harp/releases) or build from source.

For stable version

```sh
brew tap elastic/harp
brew install elastic/harp/harp
```

# Build instructions

Download a [release](https://github.com/elastic/harp/releases) or build from source.

## Clone repository

```sh
$ git clone git@github.com:elastic/harp.git
$ export HARP_REPOSITORY=$(pwd)/harp
```

## Manual dev environment

### Check your go version

> Only last 2 minor versions of a major are supported.

#### Go 1.17/1.16

`Harp` is compiled with :

```sh
$ go version
go version go1.17.8 linux/amd64
```

> Simple go version manager - <https://github.com/stefanmaric/g>

#### Go 1.18 (beta)

Go 1.18 compilation is enabled for testing purpose and `golangci-lint` looks to
hang, so it has been disabled for the moment.

### Install mage

[Mage](https://magefile.org/) is an alternative to Make where language used is Go.
You can install it using 2 different methods.

#### From source

```sh
# Install mage
git clone https://github.com/magefile/mage
cd mage
go run bootstrap.go
```

### Daily

```sh
export PATH=$HARP_REPOSITORY/tools/bin:$PATH
# Build harp in bin folder
mage
```

## With nix-shell

Install `nix` on your system, if not already installed.

```sh
$ sudo install -d -m755 -o $(id -u) -g $(id -g) /nix
$ curl -L https://nixos.org/nix/install | sh
```

> More information? - <https://nixos.wiki/wiki/Nix_Installation_Guide>

```sh
$ cd $HARP_REPOSITORY
$ nix-shell
```

## Bootstrap tools

```sh
# Go to tools submodule
cd $HARP_REPOSITORY/tools
# Resolve dependencies
go mod tidy
go mod vendor
# Pull tools sources, compile them and install executable in tools/bin
mage
```

## Docker

### For Tools

You have to build this image once before executing artifact pipelines.

```sh
mage docker:tools
```

Or you can download `harp-tools` from GitHub registry

```sh
# Standard usecase
$ docker pull ghcr.io/elastic/harp/harp-tools:latest
# FIPS compliant go toolchain
$ docker pull ghcr.io/elastic/harp/harp-tools-fips:latest
```

Check image integrity with `cosign` and the public key `build/artifact/cosign.pub`

```sh
cosign verify --key build/artifact/cosign.pub ghcr.io/elastic/harp/harp-tools:latest

Verification for ghcr.io/elastic/harp/harp-tools:latest --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - The signatures were verified against the specified public key
  - Any certificates were verified against the Fulcio roots.

[{""critical"":{""identity"":{""docker-reference"":""ghcr.io/elastic/harp/harp-tools""},""image"":{""docker-manifest-digest"":""sha256:1be31528e7b00c9e836479aadfdf49319f3b4d7916e705c43ffd0b14965763a8""},""type"":""cosign container image signature""},""optional"":{""ref"":""40714fef947d018e6053991f5ddb54283f466b04"",""repo"":""elastic/harp"",""workflow"":""Build and push docker tools""}}]
```

### For CLI

```sh
# or docker image [distroless:static, rootless, noshell]
mage docker:harp
# To execute in the container
docker run --rm -ti --read-only elastic/harp:<version>
```

# Plugins

You can find more Harp feature extensions - <https://github.com/elastic/harp-plugins>

# Community

Here is the list of external projects used as inspiration :

* [Kubernetes](https://github.com/kubernetes/)
* [Helm](https://github.com/helm/)
* [Open Policy Agent ConfTest](https://github.com/open-policy-agent/conftest)
* [SaltPack](https://github.com/keybase/saltpack)
* [Hashicorp Vault](https://github.com/hashicorp/vault)
* [AWS SDK Go](https://github.com/aws/aws-sdk-go)",FAUX
elastisys/compliantkubernetes-apps,Toolkit,Application System,2025-05-15T11:32:11Z,2025-03-27T13:13:22Z,0,22,0,0,5,0,0,30,2020-10-30T12:42:20Z,2025-04-07T16:52:37Z,11524,50,Shell,VRAI,10,FAUX,223,,223,"Elastisys Compliant Kubernetes is an open source, Certified Kubernetes distribution designed according to the ISO27001 controls: providing you with security tooling and observability from day one.",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,43,"# Elastisys Welkin® Apps

[![tests](https://github.com/elastisys/compliantkubernetes-apps/actions/workflows/tests.yml/badge.svg)](https://github.com/elastisys/compliantkubernetes-apps/actions/workflows/tests.yml)

## Overview

This repository is part of the [Elastisys Welkin®][welkin] application platform.
The platform consists of the following repositories:

- [compliantkubernetes-kubespray][compliantkubernetes-kubespray] - Code for managing Kubernetes clusters and the infrastructure around them.
- [compliantkubernetes-apps][compliantkubernetes-apps] - Code, configuration and tools for running various services and applications on top of Kubernetes clusters.

The Elastisys Welkin® application platform runs two Kubernetes clusters.
One called ""service"" and one called ""workload"".

The _service cluster_ provides observability, log aggregation, private container registry with vulnerability scanning and authentication using the following services:

- Prometheus and Grafana
- OpenSearch and OpenSearch Dashboards
- Harbor
- Dex

The _workload cluster_ manages the user applications as well as providing intrusion detection, security policies, log forwarding and monitoring using the following services:

- Falco
- Open Policy Agent
- Fluentd
- Prometheus

[welkin]: https://elastisys.io/welkin/
[compliantkubernetes-kubespray]: https://github.com/elastisys/compliantkubernetes-kubespray
[compliantkubernetes-apps]: https://github.com/elastisys/compliantkubernetes-apps

This repository installs all the applications of ck8s on top of already created clusters.
To setup the clusters see [compliantkubernetes-kubespray][compliantkubernetes-kubespray].
A service-cluster (sc) or workload-cluster (wc) can be created separately but all of the applications will not work correctly unless both are running.

We follow the ""configuration as code"" principle which means that all configuration necessary to configure and operate the platform resides in the `CK8S_CONFIG_PATH` directory.
There will be four config files: `common-config.yaml`, `wc-config.yaml`, `sc-config.yaml` and `secrets.yaml`.

We strongly suggest to make your config directory part of a git repository so that it is stored safely and to allow you to rollback previously applied changes.
We additionally suggest that you make Apps a submodule of your config repository in order to properly track which version of Apps you have applied and make your config repository the single source of truth of your environment.

All operations are done through the `./bin/ck8s` command line tool. Run `./bin/ck8s help` for a complete set of possible commands.

For more information please read our public documentation:

- [Understand the basics](https://elastisys.io/welkin/operator-manual/understand-the-basics/)
- [Understand Welkin](https://elastisys.io/welkin/operator-manual/understand-welkin/)

See [Quickstart](#quickstart) for instructions on how to initialize the repo

### :cloud: Cloud providers :cloud:

Currently we support the following cloud providers:

- AWS
- Azure
- Citycloud/Cleura
- Elastx
- Exoscale
- Openstack
- Safespring
- UpCloud
- In addition to this we support running Welkin on bare metal (beta).

## Setup

The apps are installed using a combination of helm charts and manifests with the help of helmfile and some bash scripts.

### :wrench: Requirements :wrench:

To operate compliantkubernetes-apps some tools need to be installed.
They are declared in the file [REQUIREMENTS](./REQUIREMENTS) as [PURLs](https://github.com/package-url/purl-spec).

Install the requirements to use compliantkubernetes-apps:

```bash
./bin/ck8s install-requirements
```

Note that you will need a service and workload cluster.

#### Developer requirements and guidelines

See [DEVELOPMENT.md](DEVELOPMENT.md).

### :closed_lock_with_key: PGP :closed_lock_with_key:

Configuration secrets in ck8s are encrypted using [SOPS](https://github.com/getsops/sops).
We currently only support using PGP when encrypting secrets.
Because of this, before you can start using ck8s, you need to generate your own PGP key:

```bash
gpg --full-generate-key
```

Note that it's generally preferable that you generate and store your primary key and revocation certificate offline.
That way you can make sure you're able to revoke keys in the case of them getting lost, or worse yet, accessed by someone that's not you.

Instead create subkeys for specific devices such as your laptop that you use for encryption and/or signing.

If this is all new to you, here's a [link](https://riseup.net/en/security/message-security/openpgp/best-practices) worth reading!

## Usage

### Quickstart

> [!NOTE]
> **You probably want to check the [compliantkubernetes-kubespray][compliantkubernetes-kubespray] repository first, since compliantkubernetes-apps depends on having two clusters already set up.**

> [!NOTE]
> Depending on your configuration of the clusters and OIDC, you might not have access to workload cluster before installing Dex in the service cluster. You would then have to install Apps in each cluster separately, starting with the service cluster.

1. Decide on a name for this environment, the cloud provider to use as well as the flavor and set them as environment variables:
    Note that these will be later kept as global values in the common defaults config to prevent them from being inadvertently changed, as they will affect the default options of the configuration when generated or updated.
    To change them remove the common defaults config, set the new environment variables, and then generate a new configuration.

    ```bash
    export CK8S_ENVIRONMENT_NAME=my-ck8s-cluster
    export CK8S_FLAVOR=[dev|prod|air-gapped] # defaults to dev

    #
    # If 'none', no infra provider tailored configuration will be performed!
    #
    export CK8S_CLOUD_PROVIDER=[exoscale|safespring|citycloud|elastx|upcloud|azure|aws|baremetal|openstack|none]
    export CK8S_K8S_INSTALLER=[kubespray|capi] # set this to whichever installer was used for the kubernetes layer
    ```

> [!NOTE]
> The `air-gapped` flavor has a lot of the same settings as the `prod` flavor but with some additional variables that you need to configure yourself (these are set to `set-me`).

<!-- markdownlint-disable MD029 -->

2. Then set the path to where the ck8s configuration should be stored and the PGP fingerprint of the key(s) to use for encryption:

    ```bash
    export CK8S_CONFIG_PATH=${HOME}/.ck8s/my-ck8s-cluster
    export CK8S_PGP_FP=<PGP-fingerprint1,PGP-fingerprint2,...>
    ```

1. Initialize your environment and configuration:
    Note that the configuration is split between read-only default configs found in the `defaults/` directory, and the override configs `common-config.yaml`, `sc-config.yaml` and `wc-config.yaml` which are editable and will override any default value.
    The `common-config.yaml` will be applied to both the service and workload cluster, although it will be overridden by the any value set in the `sc-config.yaml` or `wc-config.yaml` respectively.
    When new configs are created this will generate new random passwords for all services.
    When configs are updated this will _not_ overwrite existing values in the override configs.
    It will create a backup of the old override configs placed in `backups/`, generate new default configs in `defaults/`, merge common values into `common-config.yaml`, and clear out redundant values set in the override configs that matches the default values.
    See [elastisys.io/welkin](https://elastisys.io/welkin) if you are uncertain about what order you should do things in.

    ```bash
    ./bin/ck8s init both
    ```

> [!NOTE]
> It is possible to initialize `wc` and `sc` clusters separately by replacing `both` when running the `init` command:
>
> ```bash
> ./bin/ck8s init wc
> ./bin/ck8s init sc
> ```

<!-- markdownlint-disable MD029 -->

4. Edit the configuration files that have been initialized in the configuration path.
    Make sure that the `objectStorage` values are set in `common-config.yaml` or `sc-config.yaml` and `wc-config.yaml`, as well as required credentials in `secrets.yaml` according to your `objectStorage.type`.
    The type may already be set in the default configuration found in the `defaults/` directory depending on your selected cloud provider.
    Set `objectStorage.s3.*` if you are using S3 or `objectStorage.gcs.*` if you are using GCS.
    Enable ExternalDNS `externalDns.enabled` and set the required variables, if you want ExternalDNS to manage your records from inside your cluster.
    It requires credentials to route53, `txtOwnerId`, `endpoints` if `externalDns.sources.crd` is enabled.

> [!NOTE]
> One important configuration is whether or not you need to use proxy protocol for the ingress controller which depends on what infrastructure you use. You enable it and need to set an annotation depending on your infrastructure. Example for openstack
> `ingressNginx.controller.config.useProxyProtocol: true`
> `ingressNginx.controller.service.annotations: { loadbalancer.openstack.org/proxy-protocol: ""true"" }`

<!-- markdownlint-disable MD029 -->

5. Create S3 buckets - optional
    If you have set `objectStorage.type: s3`, then you need to create the buckets specified under `objectStorage.buckets` in your configuration files.
    You can run the script `scripts/S3/entry.sh create` to create the buckets required.
    The script uses `s3cmd` in the background and it uses the `${HOME}/.s3cfg` file for configuration and authentication for your S3 provider.
    There's also a helper script `scripts/S3/generate-s3cfg.sh` that will allow you to generate an appropriate `s3cfg` config file for a few providers.

    ```bash
    # Use your s3cmd config file.
    scripts/S3/entry.sh create

    # Use custom config file for s3cmd.
    scripts/S3/generate-s3cfg.sh aws ${AWS_ACCESS_KEY} ${AWS_ACCESS_SECRET_KEY} s3.eu-north-1.amazonaws.com eu-north-1 > s3cfg-aws
    scripts/S3/entry.sh --s3cfg s3cfg-aws create
    ```

1. Test S3 configuration - optional
    If you enable object storage you also need to make sure that the buckets specified in `objecStorage.buckets` exist.
    You can run the following snippet to ensure that you've configured S3 correctly:

    ```bash
    (
      access_key=$(sops exec-file ${CK8S_CONFIG_PATH}/secrets.yaml 'yq r {} ""objectStorage.s3.accessKey""')
      secret_key=$(sops exec-file ${CK8S_CONFIG_PATH}/secrets.yaml 'yq r {} ""objectStorage.s3.secretKey""')
      sc_config=$(yq m ${CK8S_CONFIG_PATH}/defaults/common-config.yaml ${CK8S_CONFIG_PATH}/defaults/sc-config.yaml ${CK8S_CONFIG_PATH}/common-config.yaml ${CK8S_CONFIG_PATH}/sc-config.yaml -a overwrite -x)
      region=$(echo ${sc_config} | yq r - 'objectStorage.s3.region')
      host=$(echo ${sc_config} | yq r -  'objectStorage.s3.regionEndpoint')

      for bucket in $(echo ${sc_config} | yq r -  'objectStorage.buckets.*'); do
          s3cmd --access_key=${access_key} --secret_key=${secret_key} \
              --region=${region} --host=${host} \
              ls s3://${bucket} > /dev/null
          [ ${?} = 0 ] && echo ""Bucket ${bucket} exists!""
      done
    )
    ```

1. Update Network Policies

    ```bash
    ./bin/ck8s update-ips both dry-run
    ./bin/ck8s update-ips both apply
    ```

1. Validate config and fill in missing values
    This should indicate any missing configuration that still needs to be set.

    ```bash
    ./bin/ck8s validate sc
    ./bin/ck8s validate wc
    ```

1. If you decide to not use ExternalDNS for DNS records, you will need to manually set up the following DNS entries (replace `example.com` with your domain).

    - Manually point these domains to the workload cluster ingress controller:

      - `*.example.com`

    - Manually point these domains to the service cluster ingress controller:

        - `*.ops.example.com`
        - `dex.example.com`
        - `grafana.example.com`
        - `harbor.example.com`
        - `opensearch.example.com`

    Depending on your infrastructure, you might utilize a Service of type LoadBalancer for the ingress controller. This means you will not have an IP for the domains before installing the ingress controller. After configuring and validating the config, you can install just the ingress controller before the rest of apps with the following command

    ```bash
    ./bin/ck8s ops helmfile sc apply -lapp=ingress-nginx --include-transitive-needs
    ./bin/ck8s ops helmfile wc apply -lapp=ingress-nginx --include-transitive-needs
    ```

    The IP is then available on the ingress controller Service

    ```bash
    ./bin/ck8s ops kubectl sc -n ingress-nginx get svc ingress-nginx-controller
    ./bin/ck8s ops kubectl wc -n ingress-nginx get svc ingress-nginx-controller
    ```

    After configuring the DNS, update the Network Policies again.

    ```bash
    ./bin/ck8s update-ips both dry-run
    ./bin/ck8s update-ips both apply
    ```

1. **Note**, for this step each cluster need to be up and running already.
    Deploy the apps:

    ```bash
    ./bin/ck8s apply sc
    ./bin/ck8s apply wc
    ```

1. Test that the cluster is running correctly with:

    ```bash
    ./bin/ck8s test sc
    ./bin/ck8s test wc
    ```

1. You should now have a fully working environment.
    Check the next section for some additional steps to finalize it and set up user access.

### On-boarding and final touches

If you followed the steps in the quickstart above, you should now have deployed the applications and have a fully functioning environment.
However, there are a few steps remaining to make all applications ready for the user.

#### User access

After the cluster setup has completed RBAC resources and namespaces will have been created for the user.
You can configure what namespaces should be created and which users that should get access using the following configuration options in `wc-config.yaml`:

```yaml
user:
  namespaces:
    - demo1
    - demo2
  adminUsers:
    - admin1@example.com
    - admin2@example.com""
```

A **kubeconfig file for the user** (`${CK8S_CONFIG_PATH}/user/kubeconfig.yaml`) can be created by running the script `bin/ck8s kubeconfig user`.
The user kubeconfig will be configured to use the first namespace by default.

**OpenSearch Dashboards** access for the user can be provided either by setting up OIDC or using the internal user database in OpenSearch:

- OIDC:
    - Set `opensearch.sso.enabled=true` in `sc-config.yaml`.
    - Configure extra role mappings under `opensearch.extraRoleMappings` to give the users the necessary roles.

    ```yaml
    extraRoleMappings:
      - mapping_name: kibana_user
        definition:
          users:
            - ""configurer""
            - ""User Name""
      - mapping_name: kubernetes_log_reader
        definition:
          users:
            - ""User Name""
    ```

- Internal user database:
    - Log in to OpenSearch Dashboards using the admin account.
    - Create an account for the user.
    - Give the `kibana_user` and `kubernetes_log_reader` roles to the user.

Users will be able to log in to **Grafana** using dex, but they will have read only access by default.
To give them more privileges, you need to first ask them to log in (so that they show up in the users list) and then change their roles.

**Harbor** works in a multi-tenant way so that each logged in user will be able to create their own projects and manage them as admins (including adding more users as members).
However, users will not be able to see each others (private) projects (unless explicitly invited) and won't have global admin access in Harbor.
This also naturally means that container images uploaded to these private registries cannot automatically be pulled in to the Kubernetes cluster.
The user will first need to add pull secrets that gives some ServiceAccount access to them before they can be used.

For more details and a list of available services see the [user guide](https://elastisys.io/welkin/user-guide/).

### Harbor HA - work in progress

It is possible to run harbor in HA mode.
This section describes the necessary configuration needed to setup harbor in HA mode.
More information about harbor ha can be found [here](https://goharbor.io/docs/2.2.0/install-config/harbor-ha-helm/).

Both Postgres and Redis needs to be external, as harbor does not handle HA deployment of postgres and redis.
It is up to the operator to set these up in a HA mode.

#### Postgres requirements

The following list is requirements on the external postgres

- Password encryption: none or md5
    - [scram-sha-256 is not supported](https://github.com/goharbor/harbor/issues/15731#issuecomment-1100666831).
- Initial empty databases must be created before harbor starts
    - registry

**Config**:

Harbor backup is not designed to work with a external database.
You will have to provide your own backup solution.

In `$CK8S_CONFIG_PATH/sc-config.yaml` set the following configs

```yaml
harbor:
  ...
  backup:
    enabled: false
  database:
    type: external
    external:
      host: ""set-me""
      port: ""5432""
      username: ""set-me""
      # ""disable"" - No SSL
      # ""require"" - Always SSL (skip verification)
      # ""verify-ca"" - Always SSL (verify that the certificate presented by the
      # server was signed by a trusted CA)
      # ""verify-full"" - Always SSL (verify that the certification presented by the
      # server was signed by a trusted CA and the server host name matches the one
      # in the certificate)
      sslmode: ""disable""
```

In `$CK8S_CONFIG_PATH/secrets.yaml` add the postgres user password

```yaml
harbor:
  external:
    databasePassword: set-me
```

Also configure network policies to access database

```yaml
networkPolicies:
    database:
      internal:
        ingress:
          peers: []
      externalEgress:
        peers:
          - namespaceSelectorLabels:
              kubernetes.io/metadata.name: postgres-system
            podSelectorLabels:
              cluster-name: harbor-cluster
        ports:
          - 5432
```

#### Redis

**Config**:

In `$CK8S_CONFIG_PATH/sc-config.yaml` set the following configs

```yaml
harbor:
  redis:
    type: external
    external:
      addr: ""rfs-redis-harbor.redis-system:26379""
      sentinelMasterSet: ""mymaster""
```

Also configure network policies to access redis

```yaml
networkPolicies:
    redis:
      internalIngress:
        peers:
          - namespaceSelectorLabels:
              kubernetes.io/metadata.name: redis-system
            podSelectorLabels:
              app.kubernetes.io/name: redis-harbor
        ports:
          - 26379
          - 6379
```

### Capacity Management

For capacity management, `compliantkubernetes-apps` comes with some [Prometheus alerts](https://github.com/elastisys/compliantkubernetes-apps/blob/main/helmfile.d/charts/prometheus-alerts/templates/alerts/cluster-capacity-management-alerts.yaml) and a [Grafana dashboard](https://github.com/elastisys/compliantkubernetes-apps/blob/main/helmfile.d/charts/grafana-dashboards/dashboards/capacity-management-dashboard.json), which facilitate monitoring on a per Node as well as Node Group basis. The Node Group is meant to represent a logical grouping of Nodes, e.g., `worker` and `control-plane`. As such, in order to make use of these you first have to label your nodes with `elastisys.io/node-group=<node-group>`, for example:

```bash
kubectl label node <node-name> elastisys.io/node-group=<node-group>
```

### Management of the clusters

The [`bin/ck8s`](bin/ck8s) script provides an entry point to the clusters.
It should be used instead of using for example `kubectl`or `helmfile` directly as an operator.
To use the script, set the `CK8S_CONFIG_PATH` to the environment you want to access:

```bash
export CK8S_CONFIG_PATH=${HOME}/.ck8s/my-ck8s-cluster
```

Run the script to see what options are available.

#### Examples

- Deploy apps to the workload cluster:

  ```bash
  ./bin/ck8s apply wc
  ```

- Run tests on the service cluster:

  ```bash
  ./bin/ck8s test sc
  ```

- Port-forward to a Service in the workload cluster:

  ```bash
  ./bin/ck8s ops kubectl wc port-forward svc/<service> --namespace <namespace> <port>
  ```

- Run `helmfile diff` on a helm release:

  ```bash
  ./bin/ck8s ops helmfile sc -l <label=selector> diff
  ```

#### Autocompletion for ck8s in bash

Add this to `~/.bashrc`:

```bash
CK8S_APPS_PATH= # fill this in
source <($CK8S_APPS_PATH/bin/ck8s completion bash)
```

### Upgrading compliantkubernetes-apps

The [`bin/ck8s`](bin/ck8s) script also provides commands to upgrade an environment in two steps `prepare` and `apply`.
The former runs scripted configuration steps that do not change the state of the environment, while the latter runs scripted upgrade steps that modifies the state of the environment.
On unexpected failures the command will try to perform a rollback when possible to ensure that the environment continues to function.

```bash
./bin/ck8s upgrade both vX.Y prepare
./bin/ck8s upgrade both vX.Y apply
```

> [!NOTE]
> It is possible to upgrade `wc` and `sc` clusters separately by replacing `both` when running the `upgrade` command, e.g. the following will only upgrade the workload cluster:
>
> ```bash
> ./bin/ck8s upgrade wc vX.Y prepare
> ./bin/ck8s upgrade wc vX.Y apply
> ```

It is possible to upgrade from one minor version to the next regardless of patch versions (`vX.Y -> vX.Y+1`), and from one patch version to any later patch versions (`vX.Y.Z -> vX.Y.Z+N`).
Version validation will require that you are on a release tag matching version specified in the command, and that your environment is at most one minor version behind.
When on a specific commit add the commit hash under `global.ck8sVersion` to pass validation, and for development set `any` to circumvent version validation completely.

### Removing compliantkubernetes-apps from your cluster

There are two simple scripts that can be used to clean up you clusters.

To clean up the service cluster run:

```bash
./scripts/clean-sc.sh
```

To clean up the workload cluster run:

```bash
./scripts/clean-wc.sh
```

### Operator manual

See <https://elastisys.io/welkin/operator-manual/>.

### Setting up Google as identity provider for dex

1. Go to the [Google console](https://console.cloud.google.com/) and create a project.

1. Go to the [Oauth consent screen](https://console.cloud.google.com/apis/credentials/consent) and name the application with the same name as the project of your google cloud project add the top level domain e.g. `elastisys.se` to Authorized domains.

1. Go to [Credentials](https://console.cloud.google.com/apis/credentials) and press `Create credentials` and select `OAuth client ID`.
    Select `web application` and give it a name and add the URL to dex in the `Authorized Javascript origins` field, e.g. `dex.demo.elastisys.se`.
    Add `<dex url>/callback` to Authorized redirect URIs field, e.g. `dex.demo.elastisys.se/callback`.

1. Configure the following options in `CK8S_CONFIG_PATH/secrets.yaml`

    ```yaml
      dex:
        googleClientID:
        googleClientSecret:
    ```

## Known issues

- OpenSearch Dashboards Single Sign On (SSO) via OpenID/Dex requires LetsEncrypt Production.

For more, please check the public GitHub issues: <https://github.com/elastisys/compliantkubernetes-apps/issues>.

## 📜 Licensing Information

All source files in this repository are licensed under the Apache License, Version 2.0 unless otherwise stated.
See the [LICENSE](./LICENSE) file for full details.",VRAI
element-hq/matrix-authentication-service,Application System,Application System,2025-05-09T07:57:18Z,2025-05-07T17:25:17Z,0,11,0,0,0,0,0,0,2024-09-06T15:30:30Z,2025-04-07T08:17:06Z,20641,93,Rust,VRAI,29,FAUX,143,,143,,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,41,"# OAuth2.0 + OpenID Connect Provider for Matrix Homeservers

MAS (Matrix Authentication Service) is an OAuth 2.0 and OpenID Provider server for Matrix.

It has been created to support the migration of Matrix to an OpenID Connect (OIDC) based authentication layer as per [MSC3861](https://github.com/matrix-org/matrix-doc/pull/3861).

See the [Documentation](https://element-hq.github.io/matrix-authentication-service/index.html) for information on installation and use.

You can learn more about Matrix and OIDC at [areweoidcyet.com](https://areweoidcyet.com/).

![Delegated OIDC architecture with MAS overview](overview.png)

## Features

- Supported homeservers
  - ✅ Synapse
- Authentication methods:
  - ✅ Upstream OIDC
  - 🚧 Local password
  - ‼️ [Application Services login](https://element-hq.github.io/matrix-authentication-service/as-login.html) (**Encrypted bridges**)
- Migration support
  - ✅ Compatibility layer for legacy Matrix authentication
  - ✅ Advisor on migration readiness
  - ✅ Import users from Synapse
  - ✅ Import password hashes from Synapse
  - ✅ Import of external subject IDs for upstream identity providers from Synapse

## Upstream Identity Providers

MAS is known to work with the following upstream IdPs via OIDC:

- [Keycloak](https://www.keycloak.org/)
- [Dex](https://dexidp.io/)
- [Google](https://developers.google.com/identity/openid-connect/openid-connect)",FAUX
enterprise-contract/ec-cli,Toolkit,Application System,2025-05-15T13:33:38Z,2025-05-01T20:58:43Z,0,35,0,0,0,0,0,0,2022-05-31T09:01:15Z,2025-04-07T14:34:19Z,12721,30,Go,VRAI,37,FAUX,52,hacktoberfest,52,Supply chain artifact verifier and policy checker,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,31,"# `ec` a command line client for verifying artifacts and evaluating policies

The `ec` tool is used to evaluate Conforma policies for Software
Supply Chain. Various sub-commands can be used to assert facts about an artifact
such as:
  * Validating container image signature
  * Validating container image provenance
  * Evaluating [policies][pol] over the container image provenance
  * Fetching artifact authorization

Consult the [documentation][docs] for available sub-commands, descriptions and
examples of use.

## Building

Run `make build` from the root directory and use the `dist/ec` executable, or
run `make dist` to build for all supported architectures.

## Testing

Run `make test` to run the unit tests, and `make acceptance` to run the
acceptance tests.

## Linting

Run `make lint` to check for linting issues, and `make lint-fix` to fix linting
issues (formatting, import order, ...).

## Demo

Run `hack/demo.sh` to evaluate the policy against images that have been
built ahead of time.

To regenerate those images, say in case of change in the attestation data, run
`hack/rebuild.sh`.

## Troubleshooting

The `--debug` parameter enables debug logging. Setting `EC_DEBUG` environment
variable can be set to prevent deletion of temporary `ec-work-*` directories so
that the attestations, policy and data files can be examined.

#### **1. Go Module Checksum Mismatch Error**

When downloading dependencies, you might encounter a checksum mismatch error like this:
```
go: downloading github.com/googleapis/enterprise-certificate-proxy v0.3.3
verifying github.com/googleapis/enterprise-certificate-proxy@v0.3.3: checksum mismatch
        downloaded: h1:G6q7VHBoU74wQHXFsZSLMPl0rFw0ZDrlZ3rt6/aTBII=
        go.sum:     h1:QRje2j5GZimBzlbhGA2V2QlGNgL8G6e+wGo/+/2bWI0=
```

This issue may be resolved by running the following command to set the Go proxy, which helps resolve checksum mismatches:
``` bash
$ go env -w GOPROXY='https://proxy.golang.org,direct'
```

#### **2. Docker Container Start Failures in Acceptance Tests**

When running acceptance tests you may experience issues with starting enough Docker containers to successfully complete testing. These issues may appear as repeated failures, such as seen below, and a failed acceptance test run:
```
time=""2024-03-08T09:10:50-05:00"" level=warning msg=""Failed, retrying in 1s ... (3/3). Error: trying to reuse blob sha256:b5976a979c30628edfeee0a1f1797362b0c84cf6cb4760776aa64ec8e3e4c2b3 at destination: pinging container registry localhost:37837: Get \""http://localhost:37837/v2/\"": read tcp 127.0.0.1:34090->127.0.0.1:37837: read: connection reset by peer""
```

This issue may be resolved by increasing the total number of `fs.inotify.max_user_watches` by executing the following on: Red Hat / Fedora systems (other systems may need modifications to this)
``` bash
$ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p
```

#### **3. Apiserver and Rekor Host Resolution Failure**

Apiserver and Rekor Host Resolution Failure: While running the acceptance tests, if you encounter issues related to apiserver and rekor hosts, like:
```
+ Error: unable to fetch EnterpriseContractPolicy: Get ""http://apiserver.localhost:32971/apis/appstudio.redhat.com/v1alpha1/namespaces/acceptance/enterprisecontractpolicies/mismatched-image-digest"": dial tcp: lookup apiserver.localhost on 127.0.0.1:53: no such host

Post \""${REKOR}/api/v1/log/entries/retrieve\"": POST ${REKOR}/api/v1/log/entries/retrieve giving up after 4 attempt(s): Post \""${REKOR}/api/v1/log/entries/retrieve\"": dial tcp: lookup rekor.localhost on 127.0.0.1:53: no such host
```

This issue may be resolved by adding the below entries in the `/etc/hosts` file:
```
127.0.0.1 apiserver.localhost
127.0.0.1 rekor.localhost
```

[pol]: https://github.com/enterprise-contract/ec-policies/
[docs]: https://conforma.dev/docs/ec-cli/ec.html",VRAI
enterprise-contract/ec-policies,Toolkit,Documentations,2025-05-15T22:28:40Z,2025-04-28T16:33:19Z,0,142,0,0,0,0,0,0,2022-03-31T17:22:28Z,2025-04-04T17:06:39Z,5979,13,Open Policy Agent,VRAI,35,FAUX,21,,21,Rego policies for use with Enterprise Contract,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,26,"# ec-policies

[Rego][rego] policies related to the Conforma.

## Getting started for developers

### Makefile

The [`Makefile`](Makefile) contains a lot of useful scripts and commands. Run
`make` by itself to see the help.

### Dependencies

Go is required for development. Tools like [`conftest`][conftest] and [`opa`][opa] are executed with
the Go binary - they do not need to be installed in your system. See the top of the [go.mod](./go.mod)
file for the minimum version of Go required.

Most of the maintainers use [asdf][asdf] to seamlessly use the right version of Go.

Some, optional, make targets may require additional tooling. For example, `make live-test` requires
[entr][entr] to be installed.

### Formatting

The rego files should be formatted using the standard format. To apply the
standard format run this before committing:

    make fmt

### Documentation

The documentation is built using [Antora][antora].

Those docs are published [here][docs].

When making changes to policy rules, the docs will likely need to be re-generated. To do so run:

    make generate-docs

Commit all of the modified files.

### Running tests

From the top level directory you can run all tests and formatting checks, as
well as check that the docs are up to date, like this:

    make ci

You can run a single test like this:

    ec opa test ./policy -r <test_name_matcher>

or

    go run github.com/enterprise-contract/ec-cli opa test ./policy -r <test_name_matcher>

The `<test_name_matcher>` is a regex, so you can use it to run more than one
test.

See [`Makefile`](Makefile) for other ways to run the tests.

### Writing tests

Policies must have unit tests validating them.
All test files must adhere to the naming convention:

    file.rego
    file_test.rego

Refer to the [policy testing docs][testing] for more information.

The CI also enforces that there is 100% test coverage. If you're not at 100%
you can use this to show what lines of code are not covered:

    make coverage

### Running policies against real pipline run image build attestations

Fetch an image attestation from a registry:

    make fetch-att
    more input/input.json # to look at it

or:

    make fetch-att IMAGE=<some-image-with-an-attestation-ref>

Create a dummy policy config file:

    make dummy-config
    cat data/config.json # to look at it

Then to verify the build using the defined policies:

    make check-release

### Running policies against real pipeline definitions

For example to fetch a pipeline definition from your local cluster:

    make fetch-pipeline
    make fetch-pipeline PIPELINE=<some-pipeline-name>
    more input/input.json # to look at it

For a realistic Konflux pipeline definition that
doesn't require cluster access, if you have the [build-definitions][builddefs]
repo checked out nearby you can do something like this:

    ( cd ../build-definitions && kustomize build pipelines/hacbs | yq 'select(document_index == 2)' -o json ) > input/input.json

Then to verify the pipeline definition using the defined policies:

    make check-pipeline

### Running policies against local [ec-cli] build

Build a local version of `ec-cli` in your local ec-cli repo:

    make build

Create a `policy.yaml` file in your local `ec-cli` repo with something like:

    ---
    sources:
      - policy:
          - <path-to>/ec-policies/policy/lib
          - <path-to>/ec-policies/policy/release
        data:
          - oci::quay.io/konflux-ci/tekton-catalog/data-acceptable-bundles:latest
          - github.com/release-engineering/rhtap-ec-policy//data

Run the locally built `ec-cli` command

    dist/ec_<arch> validate image --verbose --images '{""components"": [{""containerImage"": ""<container-image>"", ""name"":""my-image"", ""source"":{""git"":{""url"":""<repository-url>"",""revision"":""<commit-id>""}}}]}' --policy 'policy.yaml' --public-key <public-key-to-verify-the-image> --strict false  --ignore-rekor --verbose --output=text

## Policy bundles

The policies defined here are bundled and pushed as OCI artifacts using
`conftest`. There are three bundles, one for each of the release and pipeline
policies, and one for the data which is used by both.

The [push-bundles](.github/workflows/push-bundles.yml) automates creating and
pushing these bundles to [quay.io][quay], and generating a related PR in the
[infra-deployments repo][infradeployments] so the
latest bundles are used.

See also the [policy bundle documentation](./antora/docs/modules/ROOT/pages/policy_bundles.adoc).

## Getting started for policy authors

See the [Policy Authoring][authoring] documentation for guidance on
contributing to the definition of policy rules.

## See also

* [Policy rule documentation][policydocs]
* [""Verify Enterprise Contract"" task definition][taskdef]
* [github.com/enterprise-contract][contract]
* [github.com/konflux-ci][konflux-ci]

[asdf]: https://asdf-vm.com/
[rego]: https://www.openpolicyagent.org/docs/latest/policy-language/
[conftest]: https://www.conftest.dev/
[opa]: https://www.openpolicyagent.org/docs/latest/
[entr]: https://github.com/eradman/entr
[testing]: https://www.openpolicyagent.org/docs/latest/policy-testing/
[docs]: https://conforma.dev/
[policydocs]: https://conforma.dev/docs/ec-policies/release_policy.html
[taskdef]: https://github.com/enterprise-contract/ec-cli/blob/main/tasks/verify-enterprise-contract/0.1/verify-enterprise-contract.yaml
[contract]: https://github.com/enterprise-contract
[ec-cli]: https://github.com/enterprise-contract/ec-cli
[konflux-ci]: https://github.com/konflux-ci
[builddefs]: https://github.com/konflux-ci/build-definitions
[authoring]: https://conforma.dev/docs/ec-policies/authoring.html
[antora]: https://docs.antora.org/antora/latest/install-and-run-quickstart/
[quay]: https://quay.io/
[infradeployments]: https://github.com/redhat-appstudio/infra-deployments",VRAI
env0/acme-demo,Documentations,Application System,2025-05-06T14:03:16Z,2025-01-23T21:58:38Z,0,1674,2,0,0,0,5,0,2021-05-17T19:31:42Z,2025-03-23T17:00:32Z,33794,7,Open Policy Agent,VRAI,14,FAUX,8,,8,"ACME, Inc. Demo environment for Sales",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,13,"# acme-demo

ACME, Inc. Demo environment for Sales

This is a test on the merge to master!",VRAI
env0/acme-fitness,Application System,Documentations,2024-09-11T21:09:28Z,2024-08-23T18:11:21Z,0,4,0,0,0,0,0,0,2022-01-05T22:17:26Z,2024-09-11T21:09:42Z,204,5,HCL,VRAI,10,FAUX,0,,0,A public repo to help customers quickly try env0,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,2,"# [<img src=""Env0-Color.svg"" width=""300"" alt=""env0 Logo"">](https://env0.com)

## What?

### acme-fitness
acme-fitness is a public repo to help users quickly try out env0 with a collection of env0 templates and examples. It is a pretend company with its own set of services and infrastructure as code (IaC) files to manage and be deployed.

This repo will help showcase a collection of [features](https://docs.env0.com/docs/features) from env0.

Get an overview of how to use this repo from this [video](https://www.youtube.com/c/envZero/videos)(TBA)

## How?

1. Fork this repo, and clone it locally. See [GitHub Docs on Forking & Cloning](https://docs.github.com/en/get-started/quickstart/fork-a-repo#forking-a-repository)
2. Sign-up for an env0 Account https://app.env0.com
3. Admin access to an AWS Account or [Create a Free AWS Account](https://aws.amazon.com/free/)
4. Deploy the `env0/bootstrap` folder in env0 using create ""Environment from VCS""

## History
2024/08 - Refactor with bootstrap
2022    - Initial commit

Disclaimer: 
Any resemblance to real persons or other real-life entities is purely coincidental. Or All characters and other entities appearing in this work are fictitious. Any resemblance to real persons, dead or alive, or other real-life entities, past or present, is purely coincidental.

Copyright (C) 2024 envZero, Inc.",VRAI
env0/templates,Application System,Documentations,2025-05-06T10:47:45Z,2024-05-07T12:00:05Z,0,3,3,0,0,0,0,0,2020-02-23T14:43:32Z,2025-03-19T08:02:13Z,2787,28,HCL,VRAI,65,FAUX,2,,2,Sample Terraform templates ready for env0 integration,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,33,,VRAI
erkules/k8sworkshop,Documentations,Documentations,2025-05-06T01:51:52Z,2024-09-17T00:01:15Z,0,0,0,0,18,0,0,10,2018-05-26T17:54:18Z,2025-03-27T01:28:43Z,1517,53,Smarty,VRAI,24,FAUX,0,,0,,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,6,,FAUX
fabianlee/blogcode,Application System,Documentations,2024-12-02T00:50:12Z,2023-11-27T17:45:02Z,0,0,0,0,2,0,0,0,2017-04-03T23:51:49Z,2025-04-04T09:17:12Z,20892,64,Shell,VRAI,46,FAUX,1,,1,,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,7,"# blogcode

Supporting code for blog
https://fabianlee.org/",VRAI
factset/enterprise-sdk,Toolkit,Toolkit,2025-05-14T13:21:39Z,2024-10-30T14:15:37Z,0,0,0,0,0,3,0,0,2021-11-25T15:44:51Z,2025-04-07T17:59:07Z,92608,33,Python,VRAI,13,FAUX,2,"api,factset,sdk",2,The FactSet Enterprise SDK facilitates access to APIs,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,3,"<img alt=""FactSet"" src=""https://www.factset.com/hubfs/Assets/images/factset-logo.svg"" height=""56"" width=""290"">

# FactSet Enterprise SDK

[![Apache-2 license](https://img.shields.io/badge/license-Apache2-brightgreen.svg)](https://www.apache.org/licenses/LICENSE-2.0)

The FactSet Enterprise SDK is a collection of client libraries designed to make it easier to use [FactSet's APIs](https://developer.factset.com/api-catalog) in .NET, Java, Python and TypeScript/JavaScript (Node.js). For more information on what APIs are available and how to subscribe, please see the [Developer Portal](https://developer.factset.com/).

## Overview

The FactSet Enterprise SDK is made up of two main components, API client libraries and utility libraries. The API client libraries are auto-generated for .NET, Java, Python and TypeScript/JavaScript using the [OpenAPI Generator](https://github.com/OpenAPITools/openapi-generator) based on the OpenAPI specifications published on the [Developer Portal](https://developer.factset.com/). Whereas the utility libraries are handwritten and contain shared functionality that spans all APIs, such as authentication logic.

The auto-generated source code for the API client libraries can be found in the `code/` directory of this repository, which contains language-specific sub-directories and API-specific sub-directories under each language (e.g. `code/python/PAEngine`).

| **Language**          | **Client libraries**                                                                                  | **Utility library**                                                                |
| --------------------- | ----------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| .NET                  | [.NET source](https://github.com/factset/enterprise-sdk/tree/main/code/dotnet)                      | [.NET utilities](https://github.com/factset/enterprise-sdk-utils-dotnet)           |
| Java                  | [Java source](https://github.com/factset/enterprise-sdk/tree/main/code/java)                        | [Java utilities](https://github.com/factset/enterprise-sdk-utils-java)             |
| Python                | [Python source](https://github.com/factset/enterprise-sdk/tree/main/code/python)                    | [Python utilities](https://github.com/factset/enterprise-sdk-utils-python)         |
| TypeScript/JavaScript | [TypeScript/JavaScript source](https://github.com/factset/enterprise-sdk/tree/main/code/typescript) | [TypeScript utilities](https://github.com/factset/enterprise-sdk-utils-typescript) |

### Definitions

* **Software Development Kit (SDK)** - A programming-language-specific group of client libraries that a FactSet customer can use to programmatically access FactSet content and analytics.
* **Client library** - a programming-language-specific module that contains a grouping of related logic.  Each SDK is made up of one or more client libraries.  When implementing an application or service that depends on FactSet APIs, a customer would leverage one or more client libraries.
* **Package** - a client library that has been built for distribution to customers, often through a package manager like PyPi, Maven Central, NuGet or NPM.
* **Semantic Versioning** - a common [specification](https://semver.org/) for versioning software, where a version number looks like `MAJOR.MINOR.PATCH` (e.g. `1.2.3`).
    * **MAJOR** - incremented when there is a breaking change, like changing a response type adding a new required parameter or changing status codes.
    * **MINOR** - incremented when functionality is added in a backward compatible way, like adding a new endpoint, adding new optional query string arguments or adding a new optional property to the request body.
    * **PATCH** - incremented when a backwards-compatible bug fix is added, like fixing a typo in documentation.

## Usage

Each API and utility library is published as its own package to the following package managers:

* .NET - [NuGet](https://www.nuget.org/packages?q=FactSet.SDK) (`FactSet.SDK` prefix)
* Java - [Maven](https://search.maven.org/search?q=com.factset.sdk) (`com.factset.sdk` group ID)
* Python - [PyPI](https://pypi.org/search/?q=%22fds.sdk%22) (`fds.sdk` prefix)
* TypeScript/JavaScript - [npm](https://www.npmjs.com/~enterprisesdk) (`@factset/sdk` prefix)

Each library linked above in the [overview](#overview) section also contains embedded sample code to help with getting started.

## Authentication

The FactSet Enterprise SDK supports two forms of authentication, both must be setup through [FactSet's Developer Portal](https://developer.factset.com).

1. (preferred) [OAuth 2.0](https://developer.factset.com/learn/authentication-oauth2) - client credentials flow
2. [API Key](https://developer.factset.com/learn/authentication-api-key)

## Contributing

Please refer to the [contributing guide](CONTRIBUTING.md).

## Copyright

Copyright 2022 FactSet Research Systems Inc

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.",FAUX
fits/try_samples,Documentations,Documentations,2025-05-04T10:55:57Z,2025-01-24T07:45:10Z,0,0,4,0,0,0,0,0,2010-05-02T02:23:55Z,2025-03-30T03:01:16Z,21106,37,Rust,VRAI,23,FAUX,51,,51,samples,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,4,技術調査のためのサンプル集,FAUX
FIWARE/keycloak-vc-issuer,Toolkit,Toolkit,2024-04-24T06:43:01Z,2023-03-28T07:42:56Z,0,0,0,0,0,1,0,0,2023-01-26T15:01:09Z,2025-03-03T13:27:44Z,14465,19,Java,VRAI,7,FAUX,1,,1,Keycloak provider implementation to support SIOP-2 clients and the issuance of VerifiableCredentials through the Account-Console.,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,5,"# Keycloak VC-Issuer

The Keycloak-VC-Issuer is plugin for [Keycloak](https://www.keycloak.org/) to
support [SIOP-2](https://openid.net/specs/openid-connect-self-issued-v2-1_0.html)/ [OIDC4VP](https://openid.net/specs/openid-4-verifiable-presentations-1_0.html)
clients and issue [VerifiableCredentials](https://www.w3.org/TR/vc-data-model/) through
the [OIDC4VCI-Protocol](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html)
to compliant wallets.

[![FIWARE Security](https://nexus.lab.fiware.org/repository/raw/public/badges/chapters/security.svg)](https://www.fiware.org/developers/catalogue/)
[![License badge](https://img.shields.io/github/license/fiware/keycloak-vc-issuer.svg)](https://opensource.org/licenses/Apache-2.0)<br>
[![Container Repository on Quay](https://img.shields.io/badge/quay.io-fiware%2Fkeycloak--vc--issuer-grey?logo=red%20hat&labelColor=EE0000 ""Container Repository on Quay"")](https://quay.io/repository/fiware/keycloak-vc-issuer)
[![Integration-Test](https://github.com/FIWARE/keycloak-vc-issuer/actions/workflows/integration-test.yml/badge.svg)](https://github.com/FIWARE/keycloak-vc-issuer/actions/workflows/integration-test.yml)
[![Unit-Test](https://github.com/FIWARE/keycloak-vc-issuer/actions/workflows/test.yml/badge.svg)](https://github.com/FIWARE/keycloak-vc-issuer/actions/workflows/test.yml)

## Contents

- [Background](#background)
    - [Compatibility](#compatibility)
    - [OpenID for Verifiable Credential Issuance(OIDC4VCI)](#openid-for-verifiable-credential-issuance)
- [Install](#install)
    - [Jar-File](#jar-file)
    - [OCI-Container](#oci-container)
    - [UI-Integration](#ui-integration)
    - [WaltId-SSIKit](#waltid-ssikit)
    - [Configuration](#configuration)
- [Usage](#usage)
    - [API](#api)
    - [Protocol](#protocol)
    - [Configure Claims for Credential-Types](#configure-claims-for-credential-types)
      - [Static values](#static-values)
      - [Dynamic values](#dynamic-values)
- [Demo](#demo)
- [Testing](#testing)
  - [Unit-Testing](#unit-testing)
  - [Integration-Testing](#integration-testing)
  - [Compatibility-Testing](#compatibility-testing)
- [License](#license)

## Background

[Keycloak](https://www.keycloak.org/) is a well established OpenSource Identity Management System. It's relied on in
numerous environments and serves credentials and secrets to be used in different protocols, for different types of
clients. While a VerifiableCredentials-based decentralized Identity Management does not necessarily require an IDM, a
component to issue credentials to users is required. Since Keycloak already provides capabilities for managing users,
clients and their roles, it's well suited to also serve VerifiableCredentials. The Keycloak-VC-Issuer therefor extends
the Keycloak-APIs with such functionality and provides a [theme](./theme) that extends the Account Console with a UI for
issuance of credentials to users.

### Compatibility
 
The plugin is developed with the [20.0.3 libraries](https://github.com/keycloak/keycloak/tree/20.0.3) and tested against
all Keycloak Minor-Releases >=18.0.0. Please check the [Compatibility-Matrix](./doc/compatibility/compatibility.md) for
more information. The matrix gets updated every night.

### OpenID for Verifiable Credential Issuance

The plugin targets compliance with
the [OpenID for Verifiable Credential Issuance(OIDC4VCI)](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html)
standard, in order to be compatible with Wallets complying with
the [European Digital Identity Wallet Architecture and Reference Framework](https://digital-strategy.ec.europa.eu/en/library/european-digital-identity-wallet-architecture-and-reference-framework)
and any other standard-conformant Wallet-implementation. As of now, it supports the following parts of the spec:

- [3.5 Pre-Authorized Code Flow](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html#name-pre-authorized-code-flow):
    - in order to securely issue credentials, the plugin can offer pre-authorized authorization codes to authenticated
      users
    - the code is connected to the user-session that requested
      the [Credential Offer](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html#name-credential-offer)
    - the code can be exchanged for an access-token through a token-endpoint as described
      in [RFC 6749](https://www.rfc-editor.org/info/rfc6749)
- [4. Credential Offer Endpoint](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html#name-credential-offer-endpoint):
    - to initiate standard conformant issuance, an endpoint to
      retrieve [Credential Offer](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html#name-credential-offer)
      is provided
    - the endpoint is available at ```/realms/{realm-id}/verifiable-credential/{issuer-did}/credential-offer/{nonce}```. Nonce should be
      retrieved from the endpoint ```/realms/{realm-id}/verifiable-credential/{issuer-did}/credential-offer-uri```, which accepts the type
      and format of the credential
    - see [api-spec](./api/api.yaml) for more
- [6. Token Endpoint](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html#name-token-endpoint)
    - supports token exchange through the pre-authorized flow
    - available per issuer at ```/realms/{realm-id}/verifiable-credential/{issuer-did}/token```
    - see [api-spec](./api/api.yaml) for more
    - pin-check is currently not supported
- [7. Credential Endpoint](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html#name-credential-endpoint)
    - provides a valid credential, according to the requested type and format
    - currently supports jwt_vc_json, jwt_vc_json-ld, ldp_vc and for backward-compatibility jwt_vc(which defaults to
      jwt_vc_json)
    - proof-checking for the request is only supported for proof-type jwt(yet)
- [10.2. Credential Issuer Metadata](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html#name-credential-issuer-metadata)
    - provides the metadata for the issuer

## Install

> :warning: Since this is a plugin for Keycloak, having an instance of Keycloak running is a logical precondition.
> See [the official Keycloak-Documentation](https://www.keycloak.org/guides#server) on how to set it up.

### Jar-File

The VC Issuer is a fully-self-contained provider, thus its jar-file only has to be added to the ```providers```-folder
of Keycloak(typically under ```/opt/keycloak/providers```). Keycloak will automatically pick up the provider at
start-time. The plugin is available as jar-file
through [the github-releases](https://github.com/fiware/keycloak-vc-issuer/releases).

### OCI-Container

In order to ease the deployment in containerized environments, a container including the jar-file is available
at [quay.io](https://quay.io/repository/fiware/keycloak-vc-issuer). The container can be used in containerized
environments, to copy the jar file into a Keycloak instance, without having to manipulate the Keycloak-Image itself. An
example usage of the container as an init-container in Kubernetes setups can be found in
the [integration test-setup(based on k3s)](./src/test/k3s).

### UI-Integration

To have the functionality integrated into the account-console, the ```SIOP-2```-theme has to be enabled for the realm:

![setup-theme](doc/siop-theme.png)

### WaltId-SSIKit

In addition to Keycloak, an installation of the [WaltID-SSIKit](https://github.com/walt-id/waltid-ssikit) needs to be
provided. WaltId manages the requiered [decentralized identifiers](https://www.w3.org/TR/did-core/) and creates the
actual credentials based on templates. It can f.e. be deployed
via [Helm-Chart](https://github.com/i4Trust/helm-charts/tree/main/charts/vcwaltid) or as a
plain [docker-container](https://hub.docker.com/r/waltid/ssikit).

All CredentialTypes to be supported by the SIOP-2 clients need to have a corresponding template in WaltId.
See [WaltId Templates](https://docs.walt.id/v/ssikit/concepts/credential-templates)
for details about the templating and
the [helm-chart](https://github.com/i4Trust/helm-charts/blob/main/charts/vcwaltid/values.yaml#L354) as an example to
provide them.

### Configuration

In order to provide the capabilities of issuing VerifiableCredentials and handling DIDs, Keycloak relies
on [Walt-ID](https://github.com/walt-id/waltid-ssikit) as a downstream component.
The [integration-test setup](src/test/k3s) provides an example on how to run and integrate it. The configuration is
provided via environment variables:

|Name| Description                                                                          | Default    |
|----|--------------------------------------------------------------------------------------|------------|
|VCISSUER_WALTID_ADDRESS| Base address of walt-id. Has to include the protocol.                                ||
|VCISSUER_WALTID_CORE_PORT| Port to be used for connecting the walt-id's core-api.                               | ```7000``` |
|VCISSUER_WALTID_SIGNATORY_PORT| Port to be used for connecting the walt-id's signatory-api.                          | ```7001``` |
|VCISSUER_ISSUER_DID| DID to be used for issuing credentials. If none is provided, Keycloak will create one. | |
|VCISSUER_ISSUER_KEY_FILE| Path to the file containing the issuer key.                                          | |

## Usage

The VC-Issuer plugin provides an integration for [VerifiableCredentials](https://www.w3.org/TR/vc-data-model/)
into [Keycloak](https://www.keycloak.org/). It allows managing potential receivers of VerifiableCredentials
as [SIOP-2 Clients](https://openid.net/specs/openid-connect-self-issued-v2-1_0.html), allowing to manage users and roles
in the well-known Keycloak way. In addition to that, it provides the endpoints for (authenticated) users to receive
VerifiableCredentials for their account. To integrate with the Account-Console frontend, a theme(```siop-2```) is
included.

### API

The plugin provides multiple endpoints through its API ([see OpenApi-Doc](./doc/api.yaml)) as a realm resource. They
seperate into two categories. See the [see OpenApi-Doc](./doc/api.yaml) for detailed information and examples:

- [OpenID for Verifiable Credential Issuance](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html)
  compatible endpoints - tag ```OIDC4VCI```
- convenience endpoints to allow a more comfortable frontend integration - tag ```IssuerPlugin```

Most endpoints are only available to authenticated users, the following three informatory endpoints are publicly
available:

- /{issuerDid}/.well-known/openid-credential-issuer - provides the issuer metadata in an OIDC4VCI compliant way
- /{issuerDid}/.well-known/openid-configuration - provides the openid-configuration in an OIDC4VCI compliant(and
  therefor als [RFC8414](https://www.rfc-editor.org/info/rfc8414) compliant) way
- /issuer - provides just the did of the configured issuer, that can be used to construct the other paths. Provided to
  ease frontend integrations

### Protocol

The provider does support the protocol-type ```SIOP-2```, therefore such clients can be created and managed. Since
integration into the Admin-Console is still open, the clients need to be created through the api. A registration will
look like:

```json
{
  ""clientId"": ""did:key:z6Mkv4Lh9zBTPLoFhLHHMFJA7YAeVw5HFYZV8rkdfY9fNtm3"",
  ""enabled"": true,
  ""description"": ""Client to receive Verifiable Credentials."",
  ""protocol"": ""SIOP-2"",
  ""supportedVCTypes"": [
    {
      ""type"": ""PacketDeliveryService"",
      ""format"": ""ldp_vc""
    },
    {
      ""type"": ""PacketDeliveryService"",
      ""format"": ""jwt_vc_json""
    }
  ]
}
```

Alternatively, the client can also be directly create through the clients api(for example when using declarative
configuration via [keycloak-config-cli](https://github.com/adorsys/keycloak-config-cli), find an
example [here](https://github.com/FIWARE-Ops/fiware-gitops/tree/master/aws/dsba/animal-goods/keycloak)):

```json
 {
  ""clientId"": ""did:key:z6MkigCEnopwujz8Ten2dzq91nvMjqbKQYcifuZhqBsEkH7g"",
  ""enabled"": true,
  ""description"": ""Client to receive Verifiable Credentials"",
  ""surrogateAuthRequired"": false,
  ""alwaysDisplayInConsole"": false,
  ""clientAuthenticatorType"": ""client-secret"",
  ""defaultRoles"": [],
  ""redirectUris"": [],
  ""webOrigins"": [],
  ""notBefore"": 0,
  ""bearerOnly"": false,
  ""consentRequired"": false,
  ""standardFlowEnabled"": true,
  ""implicitFlowEnabled"": false,
  ""directAccessGrantsEnabled"": false,
  ""serviceAccountsEnabled"": false,
  ""publicClient"": false,
  ""frontchannelLogout"": false,
  ""protocol"": ""SIOP-2"",
  ""attributes"": {
    ""expiryInMin"": ""3600"",
    // attributes are of type ""string"": ""string"", thus we provide the prefixed-type, together with a comma-seperated list of formats 
    ""vctypes_BatteryPassAuthCredential"": ""ldp_vc,jwt_vc_json"",
    // in order to provide static values to the credentials template, fields of structur vc_<claim-name> will be provided to the credential
    ""vc_subjectDid"": ""did:web:my.did.de"",
    // defines the claims to be included in the given credential type
    ""BatteryPassAuthCredential_claims"": ""email,firstName,roles,subjectDid""
  },
  ""authenticationFlowBindingOverrides"": {},
  ""fullScopeAllowed"": true,
  ""nodeReRegistrationTimeout"": -1,
  ""defaultClientScopes"": [],
  ""optionalClientScopes"": []
}
```

Once the client is created, roles and role-assignemnts can be managed the same way as for every other type, through the
API or the Admin-Console.

### Configure claims for Credential-Types

Depending on the credentials type and the used template(
see [WaltId documentation](https://docs.walt.id/v/ssikit/concepts/credential-templates)), Keycloak can provide either
static or dynamic values to the credential.

#### Static values

Static values can be configured on a ""per-client""-base. Every client attribute, that is prefixed with ""vc_"" will be
provided to the resource provider and can be used for the credential. F.e.:

```json
{
  ...
  attibutes: {
    ...
    ""vc_myFirstClaim"": ""first"",
    ""vc_mySecondClaim"": ""second"",
    ""CredentialType1_claims"": ""myFirstClaim"",
    ""CredentialType2_claims"": ""myFirstClaim,mySecondClaim""
  }
}
```

The configuration would provide the 2 claims ```myFirstClaim``` and ```mySecondClaim``` with the static
values ```first``` and ```second``` to the credentials provider. They are used in credentials that are configured to be
supported. For each credentials type, a comma-separated list of claim-names can be provided. They have to either
correspond to one of the statically configured one or need to be available as dynamic claims.

#### Dynamic Values

The Credential Provider supports some claims, filled by the user-attributes available:

- ```email``` - if an email address is configured for the user, it can be via the claim ```email```
- ```firstName``` - if the user has a firstName set, it can be via the claim ```firstName```
- ```familyName``` - if the user has a firstName set, it can be via the claim ```familyName```
- ```roles``` - to use the credentials in authorization frameworks, roles can be provided to the credential. ```roles```
  will include all roles configured for the user in the given client

## Demo

In order to issue credentials, first a SIOP-2 client has to be created. Integration in the admin-console is still open,
thus has to be done through the api:

```shell
url --location --request POST '<KEYCLOAK_HOST>/realms/master/clients-registrations/SIOP-2' \
--header 'Authorization: Bearer <TOKEN>' \
--header 'Content-Type: application/json' \
--data-raw '{
    // did of the client
    ""clientDid"": ""did:key:z6MkmxVQztpb1JpAEgfJaqFN5g7CcJFPSMsJ1S6PiBjxR7Vxp"",
    // type of the supported credentials
    ""supportedVCTypes"": [
        {
          ""type"": ""PacketDeliveryService"",
          ""format"": ""ldp_vc""
        },
        {
          ""type"": ""PacketDeliveryService"",
          ""format"": ""jwt_vc_json""
        }
    ],
    // 'traditional' description
    ""description"": ""Client to receive Verifiable Credentials."",
    // max lifetime of the VC
    ""expiryInMin"": 3600,
    // additionalClaims to be added to the VC
    ""additionalClaims"": {
        ""a"":""b"",
        ""c"":""d""
    }
}'
```

Client in the console:

![admin-console](doc/admin-console.png)

Once the client is created, it is available in the admin-console. Through the standard interfaces, client-roles can be
created and assigned to users. Once that is done, a logged in user can use the account-interface to get Verifiable
Credentials:

Account-Console overview:

![account-console](doc/account.png)

Get a VC:

![get-vc](doc/vc.png)

The displayed QR provides a URI-formatted reference to the OIDC4VCI compatible credential-offer, that can be scanned and used by
compliant wallets. For demonstrational purposes, the [demo-wallet.fiware.dev](https://demo-wallet.fiware.dev) can be
used. It's [browser based wallet](https://github.com/FIWARE/VCWallet), intended to be used in demo-scenarios, not
suitable for real-world use-cases.

### The Cross-Device Flow

The example shows a ""Cross-Device Flow"", as described by  [OIDC4VCI](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html#name-credential-offer-cross-devi). 
See the following diagramm for the detailed flow: 

![cross-device-flow](doc/cross-device-flow.svg)

## Testing

### Unit-Testing

The unit-tests are located at [src/test/java](./src/test/java) and are postfixed with ```Test```. The tests
use [Junit5](https://junit.org/junit5/docs/current/user-guide/)
and [Mockito](https://site.mockito.org/) and will cover the essential logic inside the plugin.

### Integration-Testing

Since the plugin has to work as part of [Keycloak]((https://www.keycloak.org/)) and does
use [WaltId](https://docs.walt.id/v/ssikit/ssi-kit/readme)
as a downstream dependency, integration-test are essential. The tests use a [k3s](https://k3s.io/) setup, integrated
through the [k3s-maven-plugin](https://github.com/kokuwaio/k3s-maven-plugin), which provides a preconfigured Keycloak
and WaltId environment. The manifests can be found at [src/test/k3s](./src/test/k3s). The test implementations are
postfixed with ```IntegrationTest```. To run the full integration test suite, use the maven
profile ```integration-test```:

```shell
mvn clean install -Pintegration-test
```

This will automatically build and deploy the current development, bind selected services to localhost and run all
integration-tests. To support local development, the same test-setup can be run locally via:

```shell
mvn clean install -Pdev
```

This will not tear-down the environement after the excution, thus can be used for testing/debugging from the IDE.
See [pom.xml](pom.xml)-Profile ```dev```-```k3s-maven-plugin``` for the available ports.

### Compatibility-Testing

To ensure compatibility with released Keycloak-Versions, the integration-tests support exectuion with different
Keycloak-Versions. In order to execute the tests with a specific version, provide either the
property ```keycloak.version```(which has to be a valid tag from the [official quay-image](quay.io/keycloak/keycloak))
or ```keycloak.image```. Be aware that the configuration might differ for different builds of Keycloak, thus alternative
images might require some additional changes in the k3s-setup. The compatibility tests are executed as part of the
pipeline and additionally once every night. The results can be found
at [the compatibility-matrix](./doc/compatibility/compatibility.md)

## License

Keycloak VC-Issuer is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.

© 2023 FIWARE Foundation e.V.",FAUX
fjudith/saltstack-kubernetes,DevOPs,DevOPs,2024-02-01T23:08:18Z,2023-05-09T00:37:37Z,0,0,0,0,0,0,0,2,2019-05-18T12:51:19Z,2024-12-20T13:49:47Z,16890,71,Jinja,VRAI,20,FAUX,0,"cluster,cncf,hyperconverged,kubernetes,saltstack,terraform",0,Deploy the lowest-cost production ready Kubernetes cluster using terraform and saltstack.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,9,"[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2188/badge)](https://bestpractices.coreinfrastructure.org/projects/2188)
[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Ffjudith%2Fsaltstack-kubernetes.svg?type=shield)](https://app.fossa.io/projects/git%2Bgithub.com%2Ffjudith%2Fsaltstack-kubernetes?ref=badge_shield)

<img src=""https://i.imgur.com/SJAtDZk.png"" width=""460"" height=""125"" >

---

Saltstack-Kubernetes is an open source Kubernetes cluster deployment platform which aims to evaluate and run Cloud Native Applications like those registered in the [CNCF landscape](https://landscape.cncf.io).
Server provisionning is managed using [Terraform](https://www.terraform.io) with a primarly target on low-cost Cloud providers like [Scaleway](https://scaleway.com) and [Hetzner](https://hetzner.com/cloud).
Kubernetes cluster deployment is managed using [Saltstack](https://saltstack.com) to deploy the various software binaries, configuration files and cloud native applications required to operate.

---

## Solution design

<img src=""./docs/diagram/kubernetes-cluster.svg"" >

The solution design carries the following requirements:

1. **Cloud provider agnostic**: Works similarly on any clouds
2. **Networking privacy**: All intra-cluster communications are TLS encrypted, pod network is encrypted, Firewall is enabled by default.
2. **Cluster security**: Node security and RBAC are enabled by default
4. **Public endpoint**: Leverage two servers stanting as edge gateway and allow the use of a single redudant Public IP address
5. **Secure admin network**: Establish a private Mesh VPN between all servers
6. **Composable CRI**: Support various Container Runtime Interface plugins (see: [Features](./docs/features.md))
7. **Composable CNI**: Support various Container Network Interface plugins (see: [Features](./docs/features.md))
8. **Converged Storage**: Persistent storage provided by cluster nodes
9. **API driven DNS**: DNS records are managed just-in-time during the deployment
10. **Stable**: Only leverage stable versions of software components

---

## Major components versions

<table>
  <tr>
    <th>Cloud provider</th>
    <th>DNS provider</th>
    <th>Kubernetes version</th>
    <th>Container runtime</th>
    <th>Container network</th>
  </tr>
  <tr>
    <td><ul><li><b>hetzner</b></li><li>scaleway</li></ul></td>
    <td><ul><li><b>cloudflare</b></li></ul></td>
    <td><ul><li><b>1.26.4</b></li></ul></td>
    <td><ul><li>docker 19.03.13</li><li><b>containerd 1.6.20</b></li><li>cri-o 1.27.0</li></ul></td>
    <td><ul><li><b>cni 1.2.0</b></li><li>calico 3.25.1</li><li>flannel 0.1.0</li><li>weave 2.6.5</li><li><b>cillium 1.13.2</b></li></ul></td>
  </tr>
</table>

* Default: **bold**

---

## Quick start

### Pre-requisits

Before starting check that following requirements are met:

* [ ] Register a public domain name
* [ ] Associate the domain name with [Cloudflare (Free)](https://www.cloudflare.com)
* [ ] Register with the cloud provider of your choice. Expect 100$ for a full month (i.e [Scaleway](https://scaleway.com), [Hetzner](https://hetzner.com/cloud))
* [ ] Setup the `terraform/terraform.tfvars` with your appropriate credentials and configuration using this [Example](./terraform/terraform.tfvars.example)
* [ ] Setup the `srv/pillar/cluster_config.sls` with your appropriate credentials and configuration using this [Example](./srv/pillar/cluster_config.sls.example)
  * Use this [guide](./docs/password.md) to customize the various credentials.
* [ ] Install the [required tools](./docs/prerequisits.md) (i.e. terraform, jq, wireguard-tools, etc.)
* [ ] Create the SSH key required to send commands to the servers.


> **Notice**: The configuration files are recorded in the `.gitignore` file to avoid the accidental uploads on the Web.

### Server creation 

Once the requirements are met, use the following command lines instanciate the server and the appropriate dns records.

```bash
cd terrafrom/
terraform init
terraform plan
terraform apply
```

14 servers are instanciated by default. Terraform task parallelism is constrained in order to contraint the load on the cloud provider API. 

At the end of the process a similar output should be displayed, listing all the generated servers and associated IP adresses.

```text
Outputs:

hostnames = [
    edge01,
    edge02,
    etcd01,
    etcd02,
    etcd03,
    master01,
    master02,
    master03,
    node01,
    node02,
    node03,
    node04,
    node05,
    node06
]

...

vpn_ips = [
    172.17.4.251,
    172.17.4.252,
    172.17.4.51,
    172.17.4.52,
    172.17.4.53,
    172.17.4.101,
    172.17.4.102,
    172.17.4.103,
    172.17.4.201,
    172.17.4.202,
    172.17.4.203,
    172.17.4.204,
    172.17.4.205,
    172.17.4.206
]
```

### Kubernetes cluster deployment

The Kubernetes cluster deployment is acheived by connecting to the **salt-master** server (i.e edge01) to execute the salt states.

This can be acheived using the following one-liner...

```bash
ssh root@edge01.example.com -C ""salt-run state.orchestrate _orchestrate""
```

... Or by opening first a SSH session to get benefit of the salt state output coloring.

```bash
ssh root@edge01.example.com

root@edge01 ~ # salt-run state.orchestrate _orchestrate
```

---

## Accessing

> **Replace** example.com"" with the ""public-domain"" value from the salt pillar.

Retrieve the admin user token stored in the salt pillar (i.e /srv/pillar/cluster_config.sls).

Install [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl).

Download the Kubernetes cluster CA certificate.

```bash
export CLUSTER_DOMAIN=""example.com""

mkdir -p ~/.kube/ssl/${CLUSTER_DOMAIN}
scp root@edge01.${CLUSTER_DOMAIN}:/etc/kubernetes/ssl/ca.pem ~/.kube/ssl/${CLUSTER_DOMAIN}/
```

Create the kubectl configuration file.

```bash
export CLUSTER_TOKEN=mykubernetestoken
export CLUSTER_NAME=""example""
export KUBECONFIG=""~/.kube/config""

kubectl config set-cluster ${CLUSTER_NAME} \
--server=https://kubernetes.${CLUSTER_DOMAIN}:6443 \
--certificate-authority=~/.kube/ssl/${CLUSTER_DOMAIN}/ca.pem

kubectl config set-credentials admin-${CLUSTER_NAME} \
--token=${CLUSTER_TOKEN}

kubectl config set-context ${CLUSTER_NAME} \
--cluster=${CLUSTER_NAME} \
--user=admin-${CLUSTER_NAME}

kubectl config use-context ${CLUSTER_NAME}
```

### Kubernetes cluster access

Check the Kubernetes cluster component health.

```bash
kubectl get componentstatus

NAME                 STATUS    MESSAGE              ERROR
etcd-2               Healthy   {""health"": ""true""}
etcd-1               Healthy   {""health"": ""true""}
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {""health"": ""true""}
```

Check the Kubernetes cluster nodes status.

```bash
kubectl get nodes

NAME       STATUS   ROLES          AGE   VERSION
edge01     Ready    ingress,node   32d   v1.26.4
edge02     Ready    ingress,node   32d   v1.26.4
master01   Ready    master         32d   v1.26.4
master02   Ready    master         32d   v1.26.4
master03   Ready    master         32d   v1.26.4
node01     Ready    node           32d   v1.26.4
node02     Ready    node           32d   v1.26.4
node03     Ready    node           32d   v1.26.4
node04     Ready    node           32d   v1.26.4
node05     Ready    node           32d   v1.26.4
node06     Ready    node           32d   v1.26.4
```

Retreive the URLs protected by the Kube-APIserver.

```bash
kubectl cluster-info

Kubernetes control plane is running at https://kubernetes.example.com:6443
Elasticsearch is running at https://kubernetes.example.com:6443/api/v1/namespaces/kube-system/services/elasticsearch-logging:db/proxy
Kibana is running at https://kubernetes.example.com:6443/api/v1/namespaces/kube-system/services/kibana-logging/proxy
CoreDNS is running at https://kubernetes.example.com:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

## Kubectl Proxy

The URLs returned by `kubectl cluster-info` are protected by a mutual TLS authentification. Meaning that direct access from your Web Browser is denied until you register the appropriate certificate and private key in it.

Prefer the `kubectl proxy` command which enables the access to URL protected by the Kube-APIServer.
Once launched. URLs are available from the **localhost** on the HTTP port 8001.

> e.g. http://localhost:8001/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

![Kubernetes Dashboard](./docs/media/kubernetes_dashboard.png)

---

## Credits

This project is vastly inspired by the following projects:

* [Kubernetes-Saltstack](https://github.com/valentin2105/Kubernetes-Saltstack) from [@valentin2105](https://github.com/valentin2105)
* [hobby-kube](https://github.com/hobby-kube/provisionning)  from [@pstadler](https://github.com/pstadler)
* [Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way) from [@kelseyhightower](https://github.com/kelseyhightower)
* [Saltformula-Kubernetes](https://github.com/salt-formulas/salt-formula-kubernetes)
* [Kubernetes Icons](https://github.com/octo-technology/kubernetes-icons)

## License
[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Ffjudith%2Fsaltstack-kubernetes.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2Ffjudith%2Fsaltstack-kubernetes?ref=badge_large)",FAUX
fleetdm/fleet,Toolkit,DevOPs,2025-05-16T00:15:37Z,2025-05-14T01:22:27Z,0,2,0,0,0,0,0,0,2020-11-03T22:17:18Z,2025-04-07T19:47:32Z,1003944,4665,Go,VRAI,540,FAUX,2260,"device-management,employee-experience,endpoint-ops,endpoint-security,gitops,mdm-api,open-source,osquery,security-analytics,vulnerability-management",2260,"Open-source platform for IT, security, and infrastructure teams. (Linux, macOS, Chrome, Windows, cloud, data center)",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,274,"<h1><a href=""https://fleetdm.com""><img width=""200"" alt=""Fleet logo, landscape, dark text, transparent background"" src=""https://github.com/user-attachments/assets/5b52c536-f33e-4159-b2a3-d48f31868cd2""></a></h1>


#### [News](https://fleetdm.com/announcements) &nbsp; · &nbsp; [Report a bug](https://github.com/fleetdm/fleet/issues/new) &nbsp; · &nbsp; [Handbook](https://fleetdm.com/handbook/company) &nbsp; · &nbsp; [Why open source?](https://fleetdm.com/handbook/company/why-this-way#why-open-source) &nbsp; · &nbsp; [Art](https://fleetdm.com/logos)


Open-source platform for IT and security teams with thousands of computers.  Designed for APIs, GitOps, webhooks, YAML, and humans.

<a href=""https://fleetdm.com/logos""><img src=""https://github.com/fleetdm/fleet/assets/618009/f835ec29-1cb9-49ba-a0f3-395ffd9d5c9f"" alt=""A glass city in the clouds""/></a>


## What's it for?
Organizations like Fastly and Gusto use Fleet for vulnerability reporting, detection engineering, device management (MDM), device health monitoring, posture-based access control, managing unused software licenses, and more.

#### Explore data
To see what kind of data you can use Fleet to gather, check out the [table reference documentation](https://fleetdm.com/tables).

#### Out-of-the-box policies
Fleet includes out-of-the box support for all [CIS benchmarks for macOS and Windows](https://fleetdm.com/docs/using-fleet/cis-benchmarks), as well as many [simpler queries](https://fleetdm.com/queries).

Take as much or as little as you need for your organization.

#### Supported platforms
Here are the platforms Fleet currently supports:

- Linux (all distros)
- macOS
- Windows
- Chromebooks
- Amazon Web Services (AWS)
- Google Cloud (GCP)
- Azure (Microsoft cloud)
- Data centers
- Containers (kube, etc)
- Linux-based IoT devices

## Lighter than air
Fleet is lightweight and modular.  You can use it for security without using it for MDM, and vice versa.  You can turn off features you are not using.

#### Openness
Fleet is dedicated to flexibility, accessibility, and clarity.  We think [everyone can contribute](https://fleetdm.com/handbook/company#openness) and that tools should be as easy as possible for everyone to understand.

#### Good neighbors
Fleet has no ambition to replace all of your other tools.  (Though it might replace some, if you want it to.)  Ready-to-use, enterprise-friendly integrations exist for Snowflake, Splunk, GitHub Actions, Vanta, Elastic Jira, Zendesk, and more.

Fleet plays well with Munki, Chef, Puppet, and Ansible, as well as with security tools like Crowdstrike and SentinelOne.  For example, you can use the free version of Fleet to quickly report on what hosts are _actually_ running your EDR agent.

#### Free as in free
The free version of Fleet will [always be free](https://fleetdm.com/pricing).  Fleet is [independently backed](https://linkedin.com/company/fleetdm) and actively maintained with the help of many amazing [contributors](https://github.com/fleetdm/fleet/graphs/contributors).

#### Longevity
The [company behind Fleet](https://fleetdm.com/handbook/company) is founded (and majority-owned) by [true believers in open source](https://fleetdm.com/handbook/company/why-this-way#why-open-source).  The company's business model is influenced by GitLab (NYSE: GTLB), with great investors, happy customers, and the capacity to become profitable at any time.

In keeping with Fleet's value of openness, [Fleet Device Management's company handbook](https://fleetdm.com/handbook/company) is public and open source.  You can read about the [history of Fleet and osquery](https://fleetdm.com/handbook/company#history) and our commitment to improving the product.

<!-- > To upgrade from Fleet ≤3.2.0, just follow the upgrading steps for the earliest subsequent major release from this repository (it'll work out of the box until the release of Fleet 5.0). -->


## Is it any good?
Fleet is used in production by IT and security teams with thousands of laptops and servers.  Many deployments support tens of thousands of hosts, and a few large organizations manage deployments as large as 400,000+ hosts.



## Chat
Please join us in [MacAdmins Slack](https://www.macadmins.org/) or in [osquery Slack](https://fleetdm.com/slack).

The Fleet community is full of [kind and helpful people](https://fleetdm.com/handbook/company#empathy).  Whether or not you are a paying customer, if you need help, just ask.


## Contributing &nbsp; [![Run Tests](https://github.com/fleetdm/fleet/actions/workflows/test.yml/badge.svg)](https://github.com/fleetdm/fleet/actions/workflows/test.yml) &nbsp; [![Go Report Card](https://goreportcard.com/badge/github.com/fleetdm/fleet)](https://goreportcard.com/report/github.com/fleetdm/fleet) &nbsp; [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5537/badge)](https://bestpractices.coreinfrastructure.org/projects/5537) &nbsp; [![Twitter Follow](https://img.shields.io/twitter/follow/fleetctl.svg?style=social&maxAge=3600)](https://twitter.com/fleetctl) &nbsp; 

The landscape of cybersecurity and IT is too complex.  Let's open it up.

Contributions are welcome, whether you answer questions on [Slack](https://fleetdm.com/slack) / [GitHub](https://github.com/fleetdm/fleet/issues) / [StackOverflow](https://stackoverflow.com/search?q=osquery) / [LinkedIn](https://linkedin.com/company/fleetdm) / [Twitter](https://twitter.com/fleetctl), improve the documentation or [website](./website), write a tutorial, give a talk at a conference or local meetup, give an [interview on a podcast](https://fleetdm.com/podcasts), troubleshoot reported issues, or [submit a patch](https://fleetdm.com/docs/contributing/contributing).  The Fleet code of conduct is [on GitHub](https://github.com/fleetdm/fleet/blob/main/CODE_OF_CONDUCT.md).

<!-- - Great contributions are motivated by real-world use cases or learning.
- Some of the most valuable contributions might not touch any code at all.
- Small, iterative, simple (boring) changes are the easiest to merge. -->

## What's next?
To see what Fleet can do, head over to [fleetdm.com](https://fleetdm.com) and try it out for yourself, grab time with one of the maintainers to discuss, or visit the docs and roll it out to your organization.

#### Production deployment
Fleet is simple enough to [spin up for yourself](https://fleetdm.com/docs/get-started/tutorials-and-guides).  Or you can have us [host it for you](https://fleetdm.com/pricing).  Premium features are [available](https://fleetdm.com/pricing) either way.

#### Documentation
Complete documentation for Fleet can be found at [https://fleetdm.com/docs](https://fleetdm.com/docs).


## License
The free version of Fleet is available under the MIT license.  The commercial license is also designed to allow contributions to paid features for users whose employment agreements allow them to contribute to open source projects.  (See LICENSE.md for details.)

> Fleet is built on [osquery](https://github.com/osquery/osquery), [nanoMDM](https://github.com/micromdm/nanomdm), [Nudge](https://github.com/macadmins/nudge), and [swiftDialog](https://github.com/swiftDialog/swiftDialog).",VRAI
fluxcd/flux2-multi-tenancy,DevOPs,Documentations,2025-05-01T05:51:04Z,2023-12-29T07:11:25Z,0,0,0,0,2,0,0,0,2020-11-27T08:24:15Z,2025-03-21T21:03:32Z,196,520,Shell,VRAI,262,FAUX,7,"example,flux2,fluxcd,gitops,kubernetes,multitenancy",7,Manage multi-tenant clusters with Flux,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,23,"# flux2-multi-tenancy

[![test](https://github.com/fluxcd/flux2-multi-tenancy/workflows/test/badge.svg)](https://github.com/fluxcd/flux2-multi-tenancy/actions)
[![e2e](https://github.com/fluxcd/flux2-multi-tenancy/workflows/e2e/badge.svg)](https://github.com/fluxcd/flux2-multi-tenancy/actions)
[![license](https://img.shields.io/github/license/fluxcd/flux2-multi-tenancy.svg)](https://github.com/fluxcd/flux2-multi-tenancy/blob/main/LICENSE)

This repository serves as a starting point for managing multi-tenant clusters with Git and Flux v2.

![](docs/img/flux2-multi-tenancy.png)

## Roles

**Platform Admin**

- Has cluster admin access to the fleet of clusters
- Has maintainer access to the fleet Git repository
- Manages cluster wide resources (CRDs, controllers, cluster roles, etc)
- Onboards the tenant’s main `GitRepository` and `Kustomization` 
- Manages tenants by assigning namespaces, service accounts and role binding to the tenant's apps

**Tenant** 

- Has admin access to the namespaces assigned to them by the platform admin
- Has maintainer access to the tenant Git repository and apps repositories 
- Manages app deployments with `GitRepositories` and `Kustomizations`
- Manages app releases with `HelmRepositories` and `HelmReleases`

## Repository structure

The [platform admin repository](https://github.com/fluxcd/flux2-multi-tenancy/tree/main) contains the following top directories:

- **clusters** dir contains the Flux configuration per cluster
- **infrastructure** dir contains common infra tools such as admission controllers, CRDs and cluster-wide polices
- **tenants** dir contains namespaces, service accounts, role bindings and Flux custom resources for registering tenant repositories

```
├── clusters
│   ├── production
│   └── staging
├── infrastructure
│   ├── kyverno
│   └── kyverno-policies
└── tenants
    ├── base
    ├── production
    └── staging
```

A [tenant repository](https://github.com/fluxcd/flux2-multi-tenancy/tree/dev-team) contains the following top directories:

- **base** dir contains `HelmRepository` and `HelmRelease` manifests
- **staging** dir contains `HelmRelease` Kustomize patches for deploying pre-releases on the staging cluster
- **production** dir contains `HelmRelease` Kustomize patches for deploying stable releases on the production cluster

```
├── base
│   ├── kustomization.yaml
│   ├── podinfo-release.yaml
│   └── podinfo-repository.yaml
├── production
│   ├── kustomization.yaml
│   └── podinfo-values.yaml
└── staging
    ├── kustomization.yaml
    └── podinfo-values.yaml
```

## Bootstrap the staging cluster

Install the Flux CLI and fork this repository on your personal GitHub account
and export your GitHub username and repo name:

```sh
export GITHUB_USER=<your-username>
export GITHUB_REPO=<repository-name>
```

Verify that your staging cluster satisfies the prerequisites with:

```sh
flux check --pre
```

Set the `--context` argument to the kubectl context to your staging cluster and bootstrap Flux:

```sh
flux bootstrap github \
    --context=your-staging-context \
    --owner=${GITHUB_USER} \
    --repository=${GITHUB_REPO} \
    --branch=main \
    --personal \
    --path=clusters/staging
```

At this point flux cli will ask you for your `GITHUB_TOKEN` (a.k.a [Personal Access Token]).

> **NOTE:** The `GITHUB_TOKEN` is used exclusively by the flux CLI during the bootstrapping process,
> and does not leave your machine. The credential is used for
> configuring the GitHub repository and registering the deploy key.

The bootstrap command commits the manifests for the Flux components in `clusters/staging/flux-system` dir
and creates a deploy key with read-only access on GitHub, so it can pull changes inside the cluster.

Wait for the staging cluster reconciliation to finish:

```console
$ flux get kustomizations --watch
NAME            	READY  	MESSAGE                                                        	
flux-system     	True   	Applied revision: main/616001c38e7bc81b00ef2c65ac8cfd58140155b8	
kyverno         	Unknown	Reconciliation in progress
kyverno-policies	False  	Dependency 'flux-system/kyverno' is not ready
tenants         	False  	Dependency 'flux-system/kyverno-policies' is not ready
```

Verify that the tenant Git repository has been cloned:

```console
$ flux -n apps get sources git
NAME    	READY	MESSAGE 
dev-team	True 	Fetched revision: dev-team/ca8ec25405cc03f2f374d2f35f9299d84ced01e4
```

Verify that the tenant Helm repository index has been downloaded:

```console
$ flux -n apps get sources helm
NAME   	READY	MESSAGE
podinfo	True 	Fetched revision: 2022-05-23T10:09:58.648748663Z
```

Wait for the demo app to be installed:

```console
$ watch flux -n apps get helmreleases
NAME   	READY	MESSAGE                         	REVISION	SUSPENDED 
podinfo	True 	Release reconciliation succeeded	5.0.3   	False 
```

To expand on this example, check the [enforce tenant isolation](#enforce-tenant-isolation) for security related considerations. 

[Personal Access Token]: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token

### Onboard new tenants

The Flux CLI offers commands to generate the Kubernetes manifests needed to define tenants.

Assuming a platform admin wants to create a tenant named `dev-team` with access to the `apps` namespace.

Create the tenant base directory:

```sh
mkdir -p ./tenants/base/dev-team
```

Generate the namespace, service account and role binding for the dev-team:

```sh
flux create tenant dev-team --with-namespace=apps \
    --export > ./tenants/base/dev-team/rbac.yaml
```

Create the sync manifests for the tenant Git repository:

```sh
flux create source git dev-team \
    --namespace=apps \
    --url=https://github.com/<org>/<dev-team> \
    --branch=main \
    --export > ./tenants/base/dev-team/sync.yaml

flux create kustomization dev-team \
    --namespace=apps \
    --service-account=dev-team \
    --source=GitRepository/dev-team \
    --path=""./"" \
    --export >> ./tenants/base/dev-team/sync.yaml
```

Create the base `kustomization.yaml` file:

```sh
cd ./tenants/base/dev-team/ && kustomize create --autodetect --namespace apps 
```

Create the staging overlay and set the path to the staging dir inside the tenant repository:

```sh
cat << EOF | tee ./tenants/staging/dev-team-patch.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: dev-team
  namespace: apps
spec:
  path: ./staging
EOF

cat << EOF | tee ./tenants/staging/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: apps
resources:
  - ../base/dev-team
patches:
  - path: dev-team-patch.yaml
EOF
```

With the above configuration, the Flux instance running on the staging cluster will clone the
dev-team's repository, and it will reconcile the `./staging` directory from the tenant's repo
using the `dev-team` service account. Since that service account is restricted to the `apps` namespace,
the dev-team repository must contain Kubernetes objects scoped to the `apps` namespace only.

#### Tenant onboarding via Kyverno

Alternatively to the `flux create tenant` approach, Kyverno's [resource generation] feature can
be leveraged to the same effect.

[resource generation]: https://kyverno.io/docs/writing-policies/generate/

## Enforce tenant isolation

To enforce tenant isolation, cluster admins must configure Flux to reconcile 
the `Kustomization` and `HelmRelease` kinds by impersonating a service account
from the namespace where these objects are created. 

Flux has built-in [multi-tenancy lockdown] features which enables tenant isolation 
at Control Plane level without the need of external admission controllers (e.g. Kyverno). The
recommended patch:

- Enforce controllers to block cross namespace references.
  Meaning that a tenant can’t use another tenant’s sources or subscribe to their events.
- Deny accesses to Kustomize remote bases, thus ensuring all resources refer to local files. 
  Meaning that only approved Flux Sources can affect the cluster-state.
- Sets a default service account via `--default-service-account` to `kustomize-controller` and `helm-controller`.
  Meaning that, if a tenant does not specify a service account in a Flux `Kustomization` or 
  `HelmRelease`, it would automatically default to said account. 

> **NOTE:** It is recommended that the default service account has no privileges.
> And each named service account used observes the least privilege model.

This repository applies this patch automatically via
[kustomization.yaml](clusters/production/flux-system/kustomization.yaml) in both clusters.

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - gotk-components.yaml
  - gotk-sync.yaml
patches:
  - patch: |
      - op: add
        path: /spec/template/spec/containers/0/args/-
        value: --no-cross-namespace-refs=true
    target:
      kind: Deployment
      name: ""(kustomize-controller|helm-controller|notification-controller|image-reflector-controller|image-automation-controller)""
  - patch: |
      - op: add
        path: /spec/template/spec/containers/0/args/-
        value: --no-remote-bases=true
    target:
      kind: Deployment
      name: ""kustomize-controller""
  - patch: |
      - op: add
        path: /spec/template/spec/containers/0/args/-
        value: --default-service-account=default
    target:
      kind: Deployment
      name: ""(kustomize-controller|helm-controller)""
  - patch: |
      - op: add
        path: /spec/serviceAccountName
        value: kustomize-controller
    target:
      kind: Kustomization
      name: ""flux-system""
```

### Side Effects

When Flux is bootstrapped with the patch both `kustomize-controller` and `helm-controller` will impersonate the `default`
service account in the tenant namespace when applying changes to the cluster. The `default` service account 
exist in all namespaces and should always be kept without any privileges.

To enable a tenant to operate, a service account must be created with the required permissions and its name set 
to the `spec.serviceAccountName` of all `Kustomization` and `HelmRelease` resources the tenant has.

### Tenancy policies

Depending on the aimed security posture, the Platform Admin may impose additional policies to enforce specific 
behaviours. Below are a few consideration points, some of which are already implemented in this repository.

#### Image provenance

Assuring the provenance of container images across a cluster can be achieved on several ways.

The [verify-flux-images policy](infrastructure/kyverno-policies/verify-flux-images.yaml)
ensures that all Flux images used are the ones built and signed by the Flux team:

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: verify-flux-images
spec:
  validationFailureAction: enforce
  background: false
  webhookTimeoutSeconds: 30
  failurePolicy: Fail
  rules:
    - name: verify-cosign-signature
      match:
        resources:
          kinds:
            - Pod
      verifyImages:
        - imageReferences:
            - ""ghcr.io/fluxcd/source-controller:*""
            - ""ghcr.io/fluxcd/kustomize-controller:*""
            - ""ghcr.io/fluxcd/helm-controller:*""
            - ""ghcr.io/fluxcd/notification-controller:*""
          attestors:
            - entries:
                - keyless:
                    subject: ""https://github.com/fluxcd/*""
                    issuer: ""https://token.actions.githubusercontent.com""
                    rekor:
                      url: https://rekor.sigstore.dev
```

Other policies to explore:
- Restrict what repositories can be accessed in each cluster. Some deployments may need this to be environment-specific.
- Align image policies with pods that require `securityContext` that are highly privileged.

#### Flux Sources

Flux uses sources to define the origin of flux manifests. Some deployments may require that 
all of them come from a specific GitHub Organisation, as the
[verify-git-repositories policy](infrastructure/kyverno-policies/verify-git-repositories.yaml) shows:

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: verify-git-repositories
spec:
  validationFailureAction: audit # Change to 'enforce' once the specific org url is set.
  rules:
    - name: github-repositories-only
      exclude:
        resources:
          namespaces:
            - flux-system
      match:
        resources:
          kinds:
            - GitRepository
      validate:
        message: "".spec.url must be from a repository within the organisation X""
        anyPattern:
        - spec:
            url: ""https://github.com/fluxcd/?*"" # repositories in fluxcd via https
        - spec:
            url: ""ssh://git@github.com:fluxcd/?*"" # repositories in fluxcd via ssh
```

Other policies to explore:
- Expand the policies to `HelmRepository` and `Bucket`.
- For `HelmRepository` and `GitRepository` consider which protocols should be allowed.
- For `Bucket`, consider restrictions on providers and regions.

#### Make serviceAccountName mandatory

The lockdown patch sets a default service account that is applied to any `Kustomization` and `HelmRelease` 
instances that have no `spec.ServiceAccountName` set.

If the recommended best practices above are followed, such instances won't be able to apply changes to
a cluster as the default service account has no permissions to do so. 

An additional extra could be taken to make the `spec.ServiceAccountName` field  mandatory via a validation 
webhook, for example [Kyverno](https://github.com/kyverno/kyverno) or
[OPA Gatekeeper](https://github.com/open-policy-agent/gatekeeper).
Resulting on `Kustomization` and `HelmRelease` instances not being admitted when `spec.ServiceAccountName` is not set.

#### Reconciliation hierarchy

On cluster bootstrap, you need to configure Flux to deploy the validation webhook and its policies before 
reconciling the tenants repositories.

Inside the `clusters` dir we define in which order the infrastructure items,
and the tenant workloads are going to be reconciled on the staging and production clusters:

```
./clusters/
├── production
│   ├── infrastructure.yaml
│   └── tenants.yaml
└── staging
    ├── infrastructure.yaml
    └── tenants.yaml
```

First we setup the reconciliation of custom resource definitions and their controllers. For this 
example we'll use [Kyverno](https://github.com/kyverno/kyverno):

```yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: kyverno
  namespace: flux-system
spec:
  interval: 10m
  sourceRef:
    kind: GitRepository
    name: flux-system
  path: ./infrastructure/kyverno
  prune: true
  wait: true
  timeout: 5m
```

Then we setup [cluster policies](./infrastructure/kyverno-policies/verify-git-repositories.yaml) 
(Kyverno custom resources) to enforce a specific GitHub Organisation:

```yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: kyverno-policies
  namespace: flux-system
spec:
  dependsOn:
    - name: kyverno
  interval: 5m
  sourceRef:
    kind: GitRepository
    name: flux-system
  path: ./infrastructure/kyverno-policies
  prune: true
```

With `dependsOn` we tell Flux to install Kyverno before deploying the cluster policies.

And finally we setup the reconciliation for the tenants workloads with:

```yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: tenants
  namespace: flux-system
spec:
  dependsOn:
    - name: kyverno-policies
  interval: 5m
  sourceRef:
    kind: GitRepository
    name: flux-system
  path: ./tenants/staging
  prune: true
```

With the above configuration, we ensure that the Kyverno validation webhook will reject `GitRepository`
that don't originate from a specific GitHub Organisation, in our case `fluxcd`.

## Onboard tenants with private repositories

You can configure Flux to connect to a tenant repository
using SSH or token-based authentication. The tenant credentials will be stored 
in the platform admin repository as a Kubernetes secret. 

### Encrypt Kubernetes secrets in Git

In order to store credentials safely in a Git repository, you can use Mozilla's
SOPS CLI to encrypt Kubernetes secrets with OpenPGP, Age or KMS.

Install [gnupg](https://www.gnupg.org/) and [sops](https://github.com/mozilla/sops):

```sh
brew install gnupg sops
```

Generate a GPG key for Flux without specifying a passphrase and retrieve the GPG key ID:

```console
$ gpg --full-generate-key
Email address: fluxcdbot@users.noreply.github.com

$ gpg --list-secret-keys fluxcdbot@users.noreply.github.com
sec   rsa3072 2020-09-06 [SC]
      1F3D1CED2F865F5E59CA564553241F147E7C5FA4
```

Create a Kubernetes secret in the `flux-system` namespace with the GPG private key:

```sh
gpg --export-secret-keys \
--armor 1F3D1CED2F865F5E59CA564553241F147E7C5FA4 |
kubectl create secret generic sops-gpg \
--namespace=flux-system \
--from-file=sops.asc=/dev/stdin
```

You should store the GPG private key in a safe place for disaster recovery,
in case you need to rebuild the cluster from scratch.
The GPG public key can be shared with the platform team, so anyone with 
write access to the platform repository can encrypt secrets.

### Git over SSH

Generate a Kubernetes secret with the SSH and known host keys:

```sh
flux -n apps create secret git dev-team-auth \
    --url=ssh://git@github.com/<org>/<dev-team> \
    --export > ./tenants/base/dev-team/auth.yaml
```

Print the SSH public key and add it as a read-only deploy key to the dev-team repository:

```sh
yq eval '.stringData.""identity.pub""' ./tenants/base/dev-team/auth.yaml
```

### Git over HTTP/S

Generate a Kubernetes secret with basic auth credentials:

```sh
flux -n apps create secret git dev-team-auth \
    --url=https://github.com/<org>/<dev-team> \
    --username=$GITHUB_USERNAME \
    --password=$GITHUB_TOKEN \
    --export > ./tenants/base/dev-team/auth.yaml
```

The GitHub token must have read-only access to the dev-team repository.

### Configure Git authentication

Encrypt the `dev-team-auth` secret's data field with sops:

```sh
sops --encrypt \
    --pgp=1F3D1CED2F865F5E59CA564553241F147E7C5FA4 \
    --encrypted-regex '^(data|stringData)$' \
    --in-place ./tenants/base/dev-team/auth.yaml
```

Create the sync manifests for the tenant Git repository referencing the `git-auth` secret:

```sh
flux create source git dev-team \
    --namespace=apps \
    --url=https://github.com/<org>/<dev-team> \
    --branch=main \
    --secret-ref=dev-team-auth \
    --export > ./tenants/base/dev-team/sync.yaml

flux create kustomization dev-team \
    --namespace=apps \
    --service-account=dev-team \
    --source=GitRepository/dev-team \
    --path=""./"" \
    --export >> ./tenants/base/dev-team/sync.yaml
```

Create the base kustomization.yaml file:

```sh
cd ./tenants/base/dev-team/ && kustomize create --autodetect
```

Configure Flux to decrypt secrets using the `sops-gpg` key:

```yaml
flux create kustomization tenants \
  --depends-on=kyverno-policies \
  --source=flux-system \
  --path=""./tenants/staging"" \
  --prune=true \
  --interval=5m \
  --validation=client \
  --decryption-provider=sops \
  --decryption-secret=sops-gpg \
  --export > ./clusters/staging/tenants.yaml
```

With the above configuration, the Flux instance running on the staging cluster will:

* create the tenant namespace, service account and role binding
* decrypt the tenant Git credentials using the GPG private key
* create the tenant Git credentials Kubernetes secret in the tenant namespace
* clone the tenant repository using the supplied credentials
* apply the `./staging` directory from the tenant's repo using the tenant's service account

## Testing

Any change to the Kubernetes manifests or to the repository structure should be validated in CI before
a pull request is merged into the main branch and synced on the cluster.

This repository contains the following GitHub CI workflows:

* the [test](./.github/workflows/test.yaml) workflow validates the Kubernetes manifests
  and Kustomize overlays with [kubeconform](https://github.com/yannh/kubeconform)
* the [e2e](./.github/workflows/e2e.yaml) workflow starts a Kubernetes cluster in CI
  and tests the staging setup by running Flux in Kubernetes Kind


[multi-tenancy lockdown]: https://fluxcd.io/flux/installation/configuration/multitenancy/",VRAI
gardener/gardener,Toolkit,DevOPs,2025-05-15T21:56:07Z,2025-05-12T07:23:39Z,0,0,0,0,4,4,0,0,2018-01-13T20:57:47Z,2025-04-08T04:12:00Z,211505,3103,Go,VRAI,505,FAUX,126,"cluster,controller,extensibility,gardener,golang,hcp,hosted-control-planes,hosted-controlplanes,k8s,k8s-in-k8s,kubernetes,kubernetes-cluster,kubernetes-in-kubernetes",126,Homogeneous Kubernetes clusters at scale on any infrastructure using hosted control planes.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,234,"# [Gardener](https://gardener.cloud)

![Gardener Logo](logo/gardener-large.png)

[![REUSE status](https://api.reuse.software/badge/github.com/gardener/gardener)](https://api.reuse.software/info/github.com/gardener/gardener)
[![CI Build status](https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-master/jobs/master-head-update-job/badge)](https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-master/jobs/master-head-update-job)
[![Slack workspace](https://img.shields.io/badge/Slack-Gardener%20Project-brightgreen.svg?logo=slack)](https://gardener-cloud.slack.com/)
[![Go Report Card](https://goreportcard.com/badge/github.com/gardener/gardener)](https://goreportcard.com/report/github.com/gardener/gardener)
[![GoDoc](https://godoc.org/github.com/gardener/gardener?status.svg)](https://godoc.org/github.com/gardener/gardener)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1822/badge)](https://bestpractices.coreinfrastructure.org/projects/1822)

Gardener implements the automated management and operation of [Kubernetes](https://kubernetes.io/) clusters as a service and provides a fully validated extensibility framework that can be adjusted to any programmatic cloud or infrastructure provider.

Gardener is 100% Kubernetes-native and exposes its own Cluster API to create homogeneous clusters on all supported infrastructures. This API differs from [SIG Cluster Lifecycle](https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle)'s [Cluster API](https://github.com/kubernetes-sigs/cluster-api#cluster-api) that only harmonizes how to get to clusters, while [Gardener's Cluster API](./docs/api-reference/core.md#shoot) goes one step further and also harmonizes the make-up of the clusters themselves. That means, Gardener gives you homogeneous clusters with exactly the same bill of material, configuration and behavior on all supported infrastructures, which you can see further down below in the section on our K8s Conformance Test Coverage.

In 2020, SIG Cluster Lifecycle's Cluster API made a huge step forward with [`v1alpha3`](https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/) and the newly added support for declarative control plane management. This made it possible to integrate managed services like GKE or Gardener. We would be more than happy, if the community would be interested, to contribute a Gardener control plane provider. For more information on the relation between Gardener API and SIG Cluster Lifecycle's Cluster API, please see [here](docs/concepts/cluster-api.md).

Gardener's main principle is to **leverage Kubernetes concepts for all of its tasks**.

In essence, Gardener is an [extension API server](https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/) that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called **garden** cluster) in order to use them for the management of end-user Kubernetes clusters (which are called **shoot** clusters). These shoot clusters are described via [declarative cluster specifications](https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml) which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.

To accomplish these tasks reliably and to offer a high quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called *control plane* components are hosted in Kubernetes clusters themselves (which are called **seed** clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds (the architecture is commonly referred to as kubeception or inception design). This does not only effectively reduce the total cost of ownership but also allows easier implementations for ""day-2 operations"" (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.

Gardener reuses the identical Kubernetes design to span a scalable multi-cloud and multi-cluster landscape. Such familiarity with known concepts has proven to quickly ease the initial learning curve and accelerate developer productivity:

* Kubernetes API Server = Gardener API Server
* Kubernetes Controller Manager = Gardener Controller Manager
* Kubernetes Scheduler = Gardener Scheduler
* Kubelet = Gardenlet
* Node = Seed cluster
* Pod = Shoot cluster

Please find more information regarding the concepts and a detailed description of the architecture in our [Gardener Wiki](https://github.com/gardener/gardener/blob/master/docs/concepts/architecture.md) and our blog posts on kubernetes.io: [Gardener - the Kubernetes Botanist (17.5.2018)](https://kubernetes.io/blog/2018/05/17/gardener) and [Gardener Project Update (2.12.2019)](https://kubernetes.io/blog/2019/12/02/gardener-project-update).

----

## K8s Conformance Test Coverage <img src=""https://raw.githubusercontent.com/cncf/artwork/main/projects/kubernetes/certified-kubernetes/versionless/color/certified-kubernetes-color.svg"" alt=""certified kubernetes logo"" width=""50"" align=""right""/>

Gardener takes part in the [Certified Kubernetes Conformance Program](https://www.cncf.io/certification/software-conformance/) to attest its compatibility with the K8s conformance testsuite.
Currently, Gardener is certified for K8s versions up to v1.31, see [the conformance spreadsheet](https://docs.google.com/spreadsheets/d/1uF9BoDzzisHSQemXHIKegMhuythuq_GL3N1mlUUK2h0/edit#gid=0&range=107:108).

Continuous conformance test results of the latest stable Gardener release are uploaded regularly to the CNCF test grid:

| Provider/K8s | v1.32 | v1.31 | v1.30 | v1.29 | v1.28 | v1.27 |
| ------------ |-----| ------------ | ------------ | ------------ | ------------ | ------------ |
| **AWS** | [![Gardener v1.32 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.32%20AWS/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.32%20AWS) | [![Gardener v1.31 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.31%20AWS/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.31%20AWS) | [![Gardener v1.30 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.30%20AWS/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.30%20AWS) | [![Gardener v1.29 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.29%20AWS/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.29%20AWS) | [![Gardener v1.28 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.28%20AWS/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.28%20AWS) | [![Gardener v1.27 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.27%20AWS/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.27%20AWS) |
| **Azure** | [![Gardener v1.32 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.32%20Azure/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.32%20Azure) | [![Gardener v1.31 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.31%20Azure/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.31%20Azure) | [![Gardener v1.30 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.30%20Azure/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.30%20Azure) | [![Gardener v1.29 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.29%20Azure/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.29%20Azure) | [![Gardener v1.28 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.28%20Azure/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.28%20Azure) | [![Gardener v1.27 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.27%20Azure/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.27%20Azure) |
| **GCP** | [![Gardener v1.32 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.32%20GCE/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.32%20GCE) | [![Gardener v1.31 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.31%20GCE/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.31%20GCE) | [![Gardener v1.30 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.30%20GCE/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.30%20GCE) | [![Gardener v1.29 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.29%20GCE/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.29%20GCE) | [![Gardener v1.28 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.28%20GCE/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.28%20GCE) | [![Gardener v1.27 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.27%20GCE/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.27%20GCE) |
| **OpenStack** | [![Gardener v1.32 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.32%20OpenStack/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.32%20OpenStack) | [![Gardener v1.31 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.31%20OpenStack/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.31%20OpenStack) | [![Gardener v1.30 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.30%20OpenStack/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.30%20OpenStack) | [![Gardener v1.29 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.29%20OpenStack/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.29%20OpenStack) | [![Gardener v1.28 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.28%20OpenStack/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.28%20OpenStack) | [![Gardener v1.27 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.27%20OpenStack/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.27%20OpenStack) |
| **Alicloud** | [![Gardener v1.32 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.32%20Alibaba%20Cloud/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.32%20Alibaba%20Cloud) | [![Gardener v1.31 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.31%20Alibaba%20Cloud/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.31%20Alibaba%20Cloud) | [![Gardener v1.30 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.30%20Alibaba%20Cloud/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.30%20Alibaba%20Cloud) | [![Gardener v1.29 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.29%20Alibaba%20Cloud/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.29%20Alibaba%20Cloud) | [![Gardener v1.28 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.28%20Alibaba%20Cloud/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.28%20Alibaba%20Cloud) | [![Gardener v1.27 Conformance Tests](https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.27%20Alibaba%20Cloud/tests_status?style=svg)](https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.27%20Alibaba%20Cloud) |

Get an overview of the test results at [testgrid](https://testgrid.k8s.io/conformance-gardener).

## Quickstart with the demo environment

Check out our [Gardener demo environment](https://demo.gardener.cloud)!

It is a preconfigured playground which includes lots of tutorials and examples to get you started with Gardener.

## Start using or developing the Gardener locally

See our documentation in the `/docs` repository, please [find the index here](docs/README.md).

## Setting up your own Gardener landscape

Bootstrapping and maintaining a Gardener landscape has never been easier. Check out our [Gardener landscape setup guide](docs/deployment/setup_gardener.md) to learn about the operator and other key concepts.

## Feedback and Support

Feedback and contributions are always welcome!

All channels for getting in touch or learning about our project are listed under the [community](https://gardener.cloud/docs/contribute/#community) section. We are cordially inviting interested parties to join our [bi-weekly meetings](https://gardener.cloud/community/).

Please report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as [GitHub issues](https://github.com/gardener/gardener/issues) or reach out on [Slack](https://gardener-cloud.slack.com/) (join the workspace [here](https://gardener.cloud/community/community-bio/)).

## Learn More!

Please find further resources about our project here:

* [Our landing page gardener.cloud](https://gardener.cloud/)
* [""Gardener Project Update"" blog on kubernetes.io](https://kubernetes.io/blog/2019/12/02/gardener-project-update/).
* [""Gardener, the Kubernetes Botanist"" blog on kubernetes.io](https://kubernetes.io/blog/2018/05/17/gardener/)
* [""Thinking Cloud Native"" talk at EclipseCon 2018](https://www.youtube.com/watch?v=bfw22WPg99A)
* [Blog - ""Showcase of Gardener at OSCON 2018""](https://blogs.sap.com/2018/07/26/showcase-of-gardener-at-oscon/)",VRAI
GeoscienceAustralia/dea-knowledge-hub,Application System,Application System,2025-05-15T05:51:22Z,2025-05-13T02:20:08Z,0,0,0,0,0,77,0,0,2019-08-30T01:36:18Z,2025-04-08T06:03:39Z,266156,10,Python,VRAI,8,FAUX,10,"digitalearthaustralia,geoscienceaustralia",10,"DEA Knowledge Hub — The Knowledge Hub brings together information about Digital Earth Australia’s products and services, allowing you to utilise our free and open-source satellite imagery archive.",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,36,"# DEA Knowledge Hub

This is the open-source codebase of the [DEA Knowledge Hub][KnowledgeHub] website.

If you like the design and functionality of this website, feel free to **fork this repository and replace the content with your own**. This is an open-source repository and we encourage other science agencies, organisations, and geoscience projects to use it as the basis for their own documentation website.

This documentation site contains the following features.

* Sphinx static site generator with PyData theme
* Markdown content format
* Jupyter Notebooks rendering
* Docstrings rendering
* Data-driven templates
* URL redirects
* SASS styling (CSS preprocessing)
* Image lightbox and link behaviour scripts

**Visit the website:** [DEA Knowledge Hub][KnowledgeHub]

**Other relevant repositories:**

* [dea-notebooks](https://github.com/GeoscienceAustralia/dea-notebooks)

**Keywords:** `DEA Docs, dea-docs, Content Management Interface, CMI`

**Internal staff:** [View the internal documentation](https://docs.dev.dea.ga.gov.au/public_services/dea_knowledge_hub/index.html)

[KnowledgeHub]: https://knowledge.dea.ga.gov.au/",FAUX
getgrit/gritql,Application System,Toolkit,2025-04-14T19:49:22Z,2024-10-23T14:27:22Z,0,0,0,0,0,3,0,0,2024-03-08T23:18:51Z,2025-04-08T07:44:37Z,56765,3636,Rust,VRAI,94,FAUX,120,"ast,codemod,javascript,linter,refactoring,rust,search,tree-sitter",120,"GritQL is a query language for searching, linting, and modifying code.",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,25,"<div align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/getgrit/gritql/main/assets/grit-logo-darkmode.png"">
    <img alt=""Grit logo"" src=""https://raw.githubusercontent.com/getgrit/gritql/main/assets/grit-logo.png"" width=""40%"">
  </picture>
</div>

<br>

<div align=""center"">

[![CI Status](https://img.shields.io/github/actions/workflow/status/getgrit/gritql/main.yaml)](https://github.com/getgrit/gritql/actions/workflows/main.yaml)
[![MIT License](https://img.shields.io/github/license/getgrit/gritql)](https://github.com/getgrit/gritql/blob/main/LICENSE)
[![Discord](https://img.shields.io/discord/1063097320771698699?logo=discord&label=discord)](https://docs.grit.io/discord)

[Playground](https://app.grit.io/studio) |
[Tutorial](https://docs.grit.io/tutorials/gritql) |
[Docs](https://docs.grit.io/language)

</div>

<hr>

GritQL is a declarative query language for searching and modifying source code.

- 📖 Start simply without learning AST details: any code snippet is a valid GritQL query
- ⚡️ Use Rust and query optimization to scale up to 10M+ line repositories
- 📦 Use Grit's built-in module system to reuse 200+ [standard patterns](https://github.com/getgrit/stdlib) or [share your own](https://docs.grit.io/guides/sharing#anchor-publishing-patterns)
- ♻️ Once you learn GritQL, you can use it to rewrite any [target language](https://docs.grit.io/language/target-languages): JavaScript/TypeScript, Python, JSON, Java, Terraform, Solidity, CSS, Markdown, YAML, Rust, Go, or SQL
- 🔧 GritQL makes it easy to include auto-fix rules for faster remediation

## Getting started

Read the [documentation](https://docs.grit.io/language/overview), [interactive tutorial](https://docs.grit.io/tutorials/gritql), or run `grit --help`.

### Installation

Install the Grit CLI:

```
curl -fsSL https://docs.grit.io/install | bash
```

### Usage

Search for all your `console.log` calls by putting the desired pattern in backticks:

```
grit apply '`console.log($_)`'
```

Replace `console.log` with `winston.log`, using `=>` to create rewrites:

```
grit apply '`console.log($msg)` => `winston.log($msg)`'
```

Save the pattern to a [`grit.yaml`](https://docs.grit.io/guides/config) file and exclude test cases in a where clause:

```
cat << 'EOF' > .grit/grit.yaml
patterns:
  - name: use_winston
    level: error
    body: |
      `console.log($msg)` => `winston.log($msg)` where {
        $msg <: not within or { `it($_, $_)`, `test($_, $_)`, `describe($_, $_)` }
      }
EOF
grit apply use_winston
```

Run `grit check` to enforce your patterns as [custom lints](https://docs.grit.io/guides/ci).

```
grit check
```

## Examples

### Remove all `console.log` calls, unless they are inside a try-catch block

```grit
`console.log($log)` => . where {
  $log <: not within `try { $_ } catch { $_ }`
}
```

### Replace a method call with a new method call

```grit
`$instance.oldMethod($args)` => `$instance.newMethod($args)` where {
  $program <: contains `$instance = new TargetClass($_)`
}
```

### More examples

Many more examples can be found in the [GritQL standard library](https://github.com/getgrit/stdlib/blob/main/.grit/patterns/).

Patterns can be combined to create complex queries, including [large refactors](https://github.com/getgrit/stdlib/blob/main/.grit/patterns/python/openai.md).

## Why GritQL?

GritQL comes from our experiences with conducting large scale refactors and migrations.

Usually, migrations start with exploratory work to figure out the scope of the problem—often using simple grep searches. These are easy to start with, but most migrations end up accumulating additional requirements like ensuring the right packages are imported and excluding cases which don’t have a viable migration path.

Eventually, any complex migration ends up being a full codemod program written with a tool like [jscodeshift](https://github.com/facebook/jscodeshift). This comes with its own problems:
- Most of the exploratory work has to be abandoned as you figure out how to represent your original regex search as an AST.
- Reading/writing a codemod requires mentally translating from AST names back to what source code actually looks like.
- Most frameworks are not composable, so you’re stuck copying patterns back and forth.
- Performance is often an afterthought, so iterating on a large codemod can be painfully slow.
- Codemod frameworks are language-specific, so if you’re hopping between multiple languages—or trying to migrate a shared API—you have to learn different frameworks.

GritQL is our attempt to develop a powerful middle ground:
- Exploratory analysis is easy: just put a code snippet in backticks and use `$metavariables` for holes you want to represent.
- Incrementally add complexity by introducing side conditions with where clauses.
- Reuse named patterns to avoid rebuilding queries, and use shared patterns from our [standard library](https://github.com/getgrit/stdlib) for common tasks like ensuring modules are imported.
- Written in Rust for maximum performance: rewrite millions of lines of code in seconds.

## Acknowledgements

GritQL uses [tree-sitter](https://github.com/tree-sitter/tree-sitter) for all language parsers and benefits greatly from the Rust ecosystem.

GritQL is released under the MIT license.

## Contributing

Contributions are welcome. To get started, check out the [**contributing guidelines**](./CONTRIBUTING.md).

You can also join us on [**Discord**](https://docs.grit.io/discord).",VRAI
ghostunnel/ghostunnel,Application System,Application System,2025-05-10T22:57:56Z,2025-04-16T04:42:05Z,0,3,0,0,0,0,0,0,2015-10-07T18:53:38Z,2025-04-04T17:48:59Z,25435,2022,Go,VRAI,277,FAUX,14,"crypto,go,hsm,keychain,pkcs11,proxy,security,ssl,stunnel,tls,tunnel",14,A simple SSL/TLS proxy with mutual authentication for securing non-TLS services.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,47,"Ghostunnel
==========

[![license](http://img.shields.io/badge/license-apache_2.0-blue.svg?style=flat)](https://raw.githubusercontent.com/ghostunnel/ghostunnel/master/LICENSE) [![release](https://img.shields.io/github/release/ghostunnel/ghostunnel.svg?style=flat)](https://github.com/ghostunnel/ghostunnel/releases) [![docker](https://img.shields.io/badge/docker-hub-blue.svg?style=flat)](https://hub.docker.com/r/ghostunnel/ghostunnel) [![test](https://img.shields.io/github/checks-status/ghostunnel/ghostunnel/master)](https://github.com/ghostunnel/ghostunnel/actions) [![coverage](https://img.shields.io/codecov/c/github/ghostunnel/ghostunnel/master)](https://app.codecov.io/gh/ghostunnel/ghostunnel/) [![report](https://goreportcard.com/badge/github.com/ghostunnel/ghostunnel)](https://goreportcard.com/report/github.com/ghostunnel/ghostunnel)

👻

Ghostunnel is a simple TLS proxy with mutual authentication support for
securing non-TLS backend applications.

Ghostunnel supports two modes, client mode and server mode. Ghostunnel in
server mode runs in front of a backend server and accepts TLS-secured
connections, which are then proxied to the (insecure) backend. A backend can be
a TCP domain/port or a UNIX domain socket. Ghostunnel in client mode accepts
(insecure) connections through a TCP or UNIX domain socket and proxies them to
a TLS-secured service. In other words, ghostunnel is a replacement for stunnel.

**Supported platforms**: Ghostunnel is developed primarily for Linux and Darwin
(macOS), although it should run on any UNIX system that exposes `SO_REUSEPORT`,
including FreeBSD, OpenBSD and NetBSD. Ghostunnel also supports running on
Windows, though with a reduced feature set.

Features
========

**[Access control](#access-control-flags)**: Ghostunnel enforces mutual
authentication by requiring a valid client certificate for all connections.
Policies can enforce checks on the peer certificate in a connection, either
via simple flags or declarative policies using [Open
Policy Agent](https://www.openpolicyagent.org). This is useful
for restricting access to services that don't have native access control.

**[Certificate hotswapping](#certificate-hotswapping)**: Ghostunnel can reload
certificates at runtime without dropping existing connections. Certificates can
be loaded from disk, the [SPIFFE Workload API](https://spiffe.io), or a PKCS#11 module.
This allows short-lived certificates to be used with Ghostunnel as you can pick
up new certificates transparently.

**[ACME Support](#acme-support)**: In server mode, Ghostunnel can optionally
obtain and automatically renew a public TLS certificate via the ACME protocol,
such as through Let's Encrypt. Note that this requires a valid FQDN accessible
on the public internet for verification.

**[Monitoring and metrics](#metrics--profiling)**: Ghostunnel has a built-in
status feature that can be used to collect metrics and monitor a running
instance. Metrics can be fed into Graphite or Prometheus to see number of
open connections, rate of new connections, connection lifetimes, timeouts, and
other info.

**Emphasis on security**: We have put some thought into making Ghostunnel
secure by default and prevent accidental misconfiguration. For example, we
always negotiate TLS v1.2 (or greater) and only use safe cipher suites.
Ghostunnel also supports loading certificates from the Windows/macOS keychains
or via PKCS#11 which makes it possible to use Hardware Security Modules (HSMs)
to protect private keys.

Getting Started
===============

To get started and play around with the Ghostunnel you will need X.509 client
and server certificates. If you don't already maintain a PKI, a good way to get
started is to use a package like [cloudflare/cfssl](https://github.com/cloudflare/cfssl).
If you only need some test certificates for playing around with the tunnel you
can find some pre-generated ones in the `test-keys` directory (alongside instructions
on how to generate new ones with OpenSSL).

### Install

Ghostunnel is available through [GitHub releases][rel] and through [Docker Hub][hub].

Please note that the official release binaries are best effort, and are usually
built directly via Github Actions on the latest available images. If you need
compatibility for specific OS versions, we recommend building yourself.

To build Ghostunnel, simply run:

    # Compile binary
    make ghostunnel

    # Generate man page
    make ghostunnel.man

Note that Ghostunnel requires Go 1.22 or later to build, and CGO is required.

[rel]: https://github.com/ghostunnel/ghostunnel/releases
[hub]: https://hub.docker.com/r/ghostunnel/ghostunnel

### Develop

Ghostunnel has an extensive suite of integration tests. Our integration test
suite requires Python 3.5 (or later) and [gocovmerge][gcvm] to run. We use [Go
modules][gomod] for managing vendored dependencies.

To run tests:

    # Option 1: run unit & integration tests locally
    make test

    # Option 2: run unit & integration tests in a Docker container
    # This also runs PKCS#11 integration tests using SoftHSM in the container
    GO_VERSION=1.24 make docker-test

    # Open coverage information in browser
    go tool cover -html coverage/all.profile

For more information on how to contribute, please see the [CONTRIBUTING](CONTRIBUTING.md) file.

[gcvm]: https://github.com/wadey/gocovmerge
[gomod]: https://github.com/golang/go/wiki/Modules

Usage
=====

To see available commands and flags, run `ghostunnel --help`. You can get more
information about a command by adding `--help` to the command, like `ghostunnel
server --help` or `ghostunnel client --help`. There's also a [MANPAGE](docs/MANPAGE.md).

By default, ghostunnel runs in the foreground and logs to stdout. You can set
`--syslog` to log to syslog instead of stdout. If you want to run ghostunnel
in the background, we recommend using a service manager.

[runit]: http://smarden.org/runit
[systemd]: https://www.freedesktop.org/wiki/Software/systemd
[daemonize]: http://software.clapper.org/daemonize
[dumb-init]: https://github.com/Yelp/dumb-init

### Certificates

Ghostunnel accepts certificates in multiple different file formats.

The `--keystore` flag can take a PKCS#12 keystore or a combined PEM file with the
certificate chain and private key as input (format is auto-detected). The `--cert` /
`--key` flags can be used to load a certificate chain and key from separate PEM files
(instead of a combined one).

Ghostunnel also supports loading identities from the macOS keychain or the
SPIFFE Workload API and having private keys backed by PKCS#11 modules, see the
""Advanced Features"" section below for more information.

### Server mode

This is an example for how to launch ghostunnel in server mode, listening for
incoming TLS connections on `localhost:8443` and forwarding them to
`localhost:8080`. Note that while we use TCP sockets on `localhost` in this
example, both the listen and target flags can also accept paths to UNIX domain
sockets as their argument.

To set allowed clients, you must specify at least one of `--allow-all`,
`--allow-cn`, `--allow-ou`, `--allow-dns` or `--allow-uri`. All checks are made
against the certificate of the client. Multiple flags are treated as a logical
disjunction (OR), meaning clients can connect as long as any of the flags
matches (see [ACCESS-FLAGS](docs/ACCESS-FLAGS.md) for more information). In
this example, we assume that the CN of the client cert we want to accept
connections from is `client`.

Start a backend server:

    nc -l localhost 8080

Start a Ghostunnel in server mode to proxy connections:

    ghostunnel server \
        --listen localhost:8443 \
        --target localhost:8080 \
        --keystore test-keys/server-keystore.p12 \
        --cacert test-keys/cacert.pem \
        --allow-cn client

Verify that clients can connect with their client certificate:

    openssl s_client \
        -connect localhost:8443 \
        -cert test-keys/client-combined.pem \
        -key test-keys/client-combined.pem \
        -CAfile test-keys/cacert.pem

Now we have a TLS proxy running for our backend service. We terminate TLS in
Ghostunnel and forward the connections to the insecure backend.

### Client mode

This is an example for how to launch Ghostunnel in client mode, listening on
`localhost:8080` and proxying requests to a TLS server on `localhost:8443`.

Start a backend TLS server:

    openssl s_server \
        -accept 8443 \
        -cert test-keys/server-combined.pem \
        -key test-keys/server-combined.pem \
        -CAfile test-keys/cacert.pem

Start a Ghostunnel with a client certificate to forward connections:

    ghostunnel client \
        --listen localhost:8080 \
        --target localhost:8443 \
        --keystore test-keys/client-combined.pem \
        --cacert test-keys/cacert.pem

Verify that we can connect to `8080`:

    nc -v localhost 8080

Now we have a TLS proxy running for our client. We take the insecure local
connection, wrap them in TLS, and forward them to the secure backend.

### Full tunnel (client plus server)

We can combine the above two examples to get a full tunnel. Note that you can
start the tunnels in either order.

Start netcat on port `8001`:

    nc -l localhost 8001

Start the ghostunnel server:

    ghostunnel server \
        --listen localhost:8002 \
        --target localhost:8001 \
        --keystore test-keys/server-combined.pem \
        --cacert test-keys/cacert.pem \
        --allow-cn client

Start the ghostunnel client:

    ghostunnel client \
        --listen localhost:8003 \
        --target localhost:8002 \
        --keystore test-keys/client-keystore.p12 \
        --cacert test-keys/cacert.pem

Verify that we can connect to `8003`:

    nc -v localhost 8003

Now we have a full tunnel running. We take insecure client connections,
forward them to the server side of the tunnel via TLS, and finally terminate
and proxy the connection to the insecure backend.

Advanced Features
=================

### Access Control Flags

Ghostunnel supports different types of access control flags in both client and
server modes to enforce authorization checks. Ghostunnel can check various
attributes of peer certificates directly, including a SPIFFE ID from a peer
using a [SPIFFE][spiffe] [X.509 SVIDs][svid]. In addition to this, Ghostunnel
also supports implementing authorization checks via [Open Policy Agent](https://www.openpolicyagent.org/)
(OPA) policies for maximum flexibility. Policies can be reloaded at runtime
much like certificates.

See [ACCESS-FLAGS](docs/ACCESS-FLAGS.md) for details.

[spiffe]: https://spiffe.io/
[svid]: https://github.com/spiffe/spiffe/blob/master/standards/X509-SVID.md

### Logging Options

You can silence specific types of log messages using the `--quiet=...` flag,
such as `--quiet=conns` or `--quiet=handshake-errs`. You can pass this flag
repeatedly if you want to silence multiple different kinds of log messages.

Supported values are:
* `all`: silences **all** log messages
* `conns`: silences log messages about new and closed connections.
* `conn-errs`: silences log messages about connection errors encountered (post handshake).
* `handshake-errs`: silences log messages about failed handshakes.

In particular we recommend setting `--quiet=handshake-errs` if you are
running TCP health checks in Kubernetes on the listening port, and you
want to avoid seeing error messages from aborted connections on each health
check.

### Certificate Hotswapping

To trigger a reload, simply send `SIGHUP` to the process or set a time-based
reloading interval with the `--timed-reload` flag. This will cause Ghostunnel
to reload the certificate and private key from the files on disk. Once
successful, the reloaded certificate will be used for new connections going
forward.

Additionally, Ghostunnel uses `SO_REUSEPORT` to bind the listening socket on
platforms where it is supported (Linux, Apple macOS, FreeBSD, NetBSD, OpenBSD
and DragonflyBSD). This means a new Ghostunnel can be started on the same
host/port before the old one is terminated, to minimize dropped connections (or
avoid them entirely depending on how the OS implements the `SO_REUSEPORT`
feature).

Note that if you are using an HSM/PKCS#11 module, only the certificate will
be reloaded. It is assumed that the private key in the HSM remains the same.
This means the updated/reissued certificate much match the private key that
was loaded from the HSM previously, everything else works the same.

### ACME Support

Ghostunnel in server mode supports the ACME protocol for automatically
obtaining and renewing a public certificate, assuming it's exposed publically
on tcp/443 and there are valid public DNS FQDN records that resolve to the
listening interface IP.

See [ACME](docs/ACME.md) for details.

### Metrics & Profiling

Ghostunnel has a notion of ""status port"", a TCP port (or UNIX socket) that can
be used to expose status and metrics information over HTTPS. The status port
feature can be controlled via the `--status` flag. Profiling endpoints on the
status port can be enabled with `--enable-pprof`.

See [METRICS](docs/METRICS.md) for details.

### HSM/PKCS#11 support

Ghostunnel has support for loading private keys from PKCS#11 modules, which
should work with any hardware security module that exposes a PKCS#11 interface.

See [HSM-PKCS11](docs/HSM-PKCS11.md) for details.

### Windows/MacOS Keychain Support

Ghostunnel supports loading certificates from the Windows and macOS keychains.
This is useful if you have identities stored in your local keychain that you
want to use with Ghostunnel, e.g. if you want your private key(s) to be backed
by the SEP on newer Touch ID MacBooks.

See [KEYCHAIN](docs/KEYCHAIN.md) for details.

### SPIFFE Workload API

Ghostunnel has support for maintaining up-to-date, frequently rotated
identities and trusted CA certificates from the SPIFFE Workload API.

See [SPIFFE-WORKLOAD-API](docs/SPIFFE-WORKLOAD-API.md) for details.

### Socket Activation

Ghostunnel supports socket activation via both systemd (on Linux) and launchd
(on macOS). Socket activation is support for the `--listen` and `--status`
flags, and can be used by passing an address of the form `systemd:<name>` or
`launchd:<name>`, where `<name>` should be the name of the socket as defined in
your systemd/launchd configuration.

See [SOCKET-ACTIVATION](docs/SOCKET-ACTIVATION.md) for examples.

### PROXY Protocol Support

Ghostunnel in server mode supports signalling of transport connection information
to the backend using the [PROXY protocol](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)
(v2), just pass the `--proxy-protocol` flag on startup. Note that the backend must
also support the PROXY protocol and must be configured to use it when setting
this option.",FAUX
github/codeql,Toolkit,Toolkit,2025-05-15T16:17:01Z,2025-05-13T14:57:32Z,0,0,0,0,0,1,0,0,2018-07-31T16:35:51Z,2025-04-08T13:58:35Z,417302,8210,CodeQL,VRAI,1647,FAUX,1193,"codeql,github-advanced-security,github-security-lab,semmle-ql,works-with-codespaces",1193,"CodeQL: the libraries and queries that power security researchers around the world, as well as code scanning in GitHub Advanced Security",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,384,"# CodeQL

This open source repository contains the standard CodeQL libraries and queries that power [GitHub Advanced Security](https://github.com/features/security/code) and the other application security products that [GitHub](https://github.com/features/security/) makes available to its customers worldwide.

## How do I learn CodeQL and run queries?

There is extensive documentation about the [CodeQL language](https://codeql.github.com/docs/), writing CodeQL using the [CodeQL extension for Visual Studio Code](https://docs.github.com/en/code-security/codeql-for-vs-code/) and using the [CodeQL CLI](https://docs.github.com/en/code-security/codeql-cli).

## Contributing

We welcome contributions to our standard library and standard checks. Do you have an idea for a new check, or how to improve an existing query? Then please go ahead and open a pull request! Before you do, though, please take the time to read our [contributing guidelines](CONTRIBUTING.md). You can also consult our [style guides](https://github.com/github/codeql/tree/main/docs) to learn how to format your code for consistency and clarity, how to write query metadata, and how to write query help documentation for your query.

For information on contributing to CodeQL documentation, see the ""[contributing guide](docs/codeql/CONTRIBUTING.md)"" for docs.

## License

The code in this repository is licensed under the [MIT License](LICENSE) by [GitHub](https://github.com).

The CodeQL CLI (including the CodeQL engine) is hosted in a [different repository](https://github.com/github/codeql-cli-binaries) and is [licensed separately](https://github.com/github/codeql-cli-binaries/blob/main/LICENSE.md). If you'd like to use the CodeQL CLI to analyze closed-source code, you will need a separate commercial license; please [contact us](https://github.com/enterprise/contact) for further help.

## Visual Studio Code integration

If you use Visual Studio Code to work in this repository, there are a few integration features to make development easier.

### CodeQL for Visual Studio Code

You can install the [CodeQL for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=GitHub.vscode-codeql) extension to get syntax highlighting, IntelliSense, and code navigation for the QL language, as well as unit test support for testing CodeQL libraries and queries.

### Tasks

The `.vscode/tasks.json` file defines custom tasks specific to working in this repository. To invoke one of these tasks, select the `Terminal | Run Task...` menu option, and then select the desired task from the dropdown. You can also invoke the `Tasks: Run Task` command from the command palette.",FAUX
GoogleCloudPlatform/anthos-config-management-samples,Documentations,Documentations,2025-05-06T16:44:34Z,2024-04-25T15:44:48Z,0,0,0,0,0,0,0,6,2021-04-21T14:43:19Z,2025-04-03T22:27:02Z,3308,88,Shell,VRAI,241,FAUX,6,"acm,anthos,config-management,config-sync,gke,policy-controller,samples",6,Code samples for Anthos Config Management,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,28,"# Anthos Config Management Samples 

This repository contains sample applications used in
[Anthos Config Management (ACM)](https://cloud.google.com/anthos-config-management/) tutorials.

See the following resources to learn more:

- [Anthos Config Management - Overview](https://cloud.google.com/anthos-config-management/docs/overview)
- [Anthos Config Management - Quickstart](https://cloud.google.com/anthos-config-management/docs/tutorials/create-configure-cluster)

## List of samples 

### [Quickstart](quickstart/)

A single-cluster example showing how to sync configurations from git using
Config Sync. This includes examples for both [multi-repo mode](https://cloud.google.com/kubernetes-engine/docs/add-on/config-sync/how-to/multi-repo)
and the legacy mode.

### [Foo-Corp](foo-corp/)

A single cluster example showing several features of Anthos Config Management
working together.

### [Hello, Namespace!](hello-namespace/)

A simple example to generalize how to define and enforce configuration.

### [Using Hierarchical Repos with Config Sync](hierarchical-format/)

Demonstrates how to set up a hierarchical repository for Config Sync.

### [Locality-Specific Policy](locality-specific-policy/)

Configure policy to apply only to resources in specific regions.

### [Namespace Inheritance](namespace-inheritance/) 

Shows how to use namespace inheritance with a Config Sync hierarchical repo. 

### [Rendering Configs with Kustomize](kustomize-pipeline/)

Demonstrates how to use Kustomize and Cloud Build to prepare configs for deployment with Config Sync.

### [CI Pipeline](ci-pipeline/)

Create a CloudBuild CI pipeline on a structured config directory.

### [Unstructured CI Pipeline](ci-pipeline-unstructured/)

Create a CloudBuild CI pipeline on an unstructured directory.

### [Application Pipeline](ci-app/)

Validate your application against company policies.

### [Deploying a Helm Chart with ConfigSync](helm-component/)

Demonstrates how to use Config Sync to sync a rendered Helm Chart. 

### [Multi-Cluster Anthos Config Management Setup](multi-cluster-acm-setup/)

Deploy multiple GKE clusters and install Anthos Config Management on them.

### [Multi-Cluster Fan-out](multi-cluster-fan-out/)

Manage identical Namespaces, RoleBindings, and ResourceQuotas across multiple GKE clusters using Anthos Config Management and GitOps.

### [Multi-Cluster Access and Quota](multi-cluster-access-and-quota/)

Manage cluster-specific and namespace-specific Namespaces, RoleBindings, and ResourceQuotas across multiple clusters using Anthos Config Management, GitOps, and Kustomize.

### [Multi-Cluster Ingress](multi-cluster-ingress/)

Manage an application with Multi-Cluster Ingress using Anthos Config Management, GitOps, and Kustomize.

### [Multi-cluster + Multiple Environments with Kustomize](multi-environments-kustomize/) 

Manage an application spanning multiple GCP projects, across dev and prod environments, with Config Sync, Kustomize, and Cloud Build. 

### [Namespace-specific policy](namespace-specific-policy/)

Configure namespace specific policies such as Role, RoleBinding and
NetworkPolicy.

### [Manage team resources with Config Sync](fleet-tenancy)

Use Config Sync and Terraform to dynamically create team-scoped resources across
a fleet of clusters.

## CRDs

### [ConfigManagement](crds/)

The ConfigManagement CRD is used to install Anthos Config Management.

## Contributing

* See [CONTRIBUTING.md](/.github/CONTRIBUTING.md)

## Licensing

* See [LICENSE](/LICENSE)",FAUX
GoogleCloudPlatform/click-to-deploy,DevOPs,Documentations,2025-05-15T09:03:52Z,2025-01-17T14:38:51Z,0,0,0,0,0,0,0,1,2018-04-05T21:20:03Z,2025-04-07T04:25:25Z,322207,743,Python,VRAI,458,FAUX,186,"cloud,gcp,gke,google-cloud-marketplace,k8s",186,Source for Google Click to Deploy solutions listed on Google Cloud Marketplace.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,76,"# About

Source for Google Click to Deploy solutions listed on Google Cloud Marketplace.

# Disclaimer

This is not an officially supported Google product.

# :warning: About log4j Vulnerabilities

We are constantly working on updating the scripts used in this repository to use non-impacted versions of the OSS that are used here. So updates are constanly being made in this repository.

Below is the list of solutions, in this repo, currently affected by the log4j CVEs, or using a log4j version < 2.16

## Virtual Machines

| Solution | Notes |
| --- | --- |
| elasticsearch | Vulnerable |
| logstash | Vulnerable |
| magento | Vulnerable (uses elasticsearch) |
| sonarqube | Vulnerable (uses elasticsearch) |
| liferay | Vulnerable |

## Kubernetes Applications

| Solution | Notes |
| --- | --- |
| activemq | Uses log4j 1.x |
| elastic-gke-logging | Vulnerable |
| elasticsearch | Vulnerable |
| kafka | Uses log4j 1.x |
| magento |  Vulnerable (uses elasticsearch) |
| sonarqube | Vulnerable (uses elasticsearch) |
| zookeeper | Vulnerable (uses elasticsearch) |

# Cloud Build CI

This repository uses Cloud Build for continuous integration. Each type of application has its own configuration file.

For detailed information on each configuration, see the following documentations:

*   [Docker images](docker/README.md#cloud-build-ci)
*   [K8s applications](k8s/README.md#cloud-build-ci)
*   [VM applications](vm/README.md#cloud-build-ci)

## GCB custom worker pools

The Cloud Build configurations use Google Cloud Build (GCB) custom worker pools.

If you want to create a new worker pool, run the following command:

```shell
gcloud beta builds worker-pools create gcb-workers-pool-e2 \
  --project=[PROJECT_ID] \
  --peered-network=projects/[NETWORK_PROJECT_NUMBER]/global/networks/default \
  --region=us-central1 \
  --worker-machine-type=e2-standard-2
```

Where:

*   `[PROJECT_ID]` is the GCP project ID where you want to create your custom worker pool.
*   `[NETWORK_PROJECT_NUMBER]` is the project number of the Cloud project that holds your VPC network.

For more information, see the
[gcloud beta builds worker-pools commands](https://cloud.google.com/sdk/gcloud/reference/beta/builds/worker-pools/).",VRAI
GoogleCloudPlatform/pbmm-on-gcp-onboarding,DevOPs,Documentations,2025-01-13T23:25:24Z,2024-08-01T14:38:01Z,0,3,0,0,0,0,0,81,2021-05-06T15:22:53Z,2025-03-30T17:53:29Z,43224,50,HCL,VRAI,59,FAUX,21,,21,GCP Canadian Public Sector Landing Zone overlay on top of the TEF via CFT modules - a secure cloud foundation,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,93,"# pbmm-on-gcp-onboarding

This repository is used to create a Protected B Medium-Medium (PBMM) compliant Landing Zone on Google Cloud. 

Please refer to [Technical Design Documentation](./docs/technical-design-document.md) for the full detailed design. 

Pour les documents fourni en français, veuillez vous référer à la [Documentation de conception technique](./docs/document-de-conception-technique.md).

## ADO Automation 

A end-to-end ADO automation has been provided as part of this repo.  The [ADO Pipeline Documentation](./docs/ado-pipeline-documentation.md) comprehensively outlines the Azure DevOps pipeline based deployment option, from its architectural foundation to execution and troubleshooting. 

Pour les documents fourni en français, veuillez vous référer à la [Documentation du pipeline ADO](./docs/documentation-du-pipeline-ado.md).


# Terraform Example Foundation (TEF)

This repository is a specific deployment of the [Terraform Example Foundation (TEF)](https://github.com/GoogleCloudPlatform/terraform-example-foundation).  It is intended to compliment the TEF to provide additional resources necessary for PBMM compliance.

# About terraform-example-foundation

This example repository shows how the CFT Terraform modules can build a secure Google Cloud foundation, following the [Google Cloud Enterprise Foundations Blueprint](https://cloud.google.com/architecture/security-foundations) (previously called the _Security Foundations Guide_).
The supplied structure and code is intended to form a starting point for building your own foundation with pragmatic defaults that you can customize to meet your own requirements.

The intended audience of this blueprint is large enterprise organizations with a dedicated platform team responsible for deploying and maintaining their GCP environment, who is commited to separation of duties across multiple teams and managing their environment solely through version-controlled Infrastructure as Code. Smaller organizations looking for a turnkey solution might prefer other options such as [Google Cloud Setup](https://console.cloud.google.com/cloud-setup/overview)

## Intended usage and support

This repository is intended as an example to be forked, tweaked, and maintained in the user's own version-control system; the modules within this repository are not intended for use as remote references.

Though this blueprint can help accelerate your foundation design and build, we assume that you have the engineering skills and teams to deploy and customize your own foundation based on your own requirements.

We will support:
 - Code is semantically valid, pinned to known good versions, and passes terraform validate and lint checks
 - All PR to this repo must pass integration tests to deploy all resources into a test environment before being merged
 - Feature requests about ease of use of the code, or feature requests that generally apply to all users, are welcome

We will not support:
 - In-place upgrades from a foundation deployed with an earlier version to a more recent version, even for minor version changes, might not be feasible. Repository maintainers do not have visibility to what resources a user deploys on top of their foundation or how the foundation was customized in deployment, so we make no guarantee about avoiding breaking changes.
 - Feature requests that are specific to a single user's requirement and not representative of general best practices

## Overview

This repo contains several distinct Terraform projects, each within their own directory that must be applied separately, but in sequence.
Stage `0-bootstrap` is manually executed, and subsequent stages are executed using your preferred CI/CD tool.

Each of these Terraform projects are to be layered on top of each other, and run in the following order.

### [0. bootstrap](./0-bootstrap/)

This stage executes the [CFT Bootstrap module](https://github.com/terraform-google-modules/terraform-google-bootstrap) which bootstraps an existing Google Cloud organization, creating all the required Google Cloud resources and permissions to start using the Cloud Foundation Toolkit (CFT).
For [CI/CD Pipelines](/docs/GLOSSARY.md#foundation-cicd-pipeline), you can use either Cloud Build (by default) or Jenkins. If you want to use Jenkins instead of Cloud Build, see [README-Jenkins](./0-bootstrap/README-Jenkins.md) on how to use the Jenkins sub-module.

The bootstrap step includes:

- The `prj-b-seed` project that contains the following:
  - Terraform state bucket
  - Custom service accounts used by Terraform to create new resources in Google Cloud
- The `prj-b-cicd` project that contains the following:
  - A [CI/CD Pipeline](/docs/GLOSSARY.md#foundation-cicd-pipeline) implemented with either Cloud Build or Jenkins
  - If using Cloud Build, the following items:
    - Cloud Source Repository
    - Artifact Registry
  - If using Jenkins, the following items:
    - A Compute Engine instance configured as a Jenkins Agent
    - Custom service account to run Compute Engine instances for Jenkins Agents
    - VPN connection with on-prem (or wherever your Jenkins Controller is located)

It is a best practice to separate concerns by having two projects here: one for the Terraform state and one for the CI/CD tool.
  - The `prj-b-seed` project stores Terraform state and has the service accounts that can create or modify infrastructure.
  - The `prj-b-cicd` project holds the CI/CD tool (either Cloud Build or Jenkins) that coordinates the infrastructure deployment.

To further separate the concerns at the IAM level as well, a distinct service account is created for each stage. The Terraform custom service accounts are granted the IAM permissions required to build the foundation.
If using Cloud Build as the CI/CD tool, these service accounts are used directly in the pipeline to execute the pipeline steps (`plan` or `apply`).
In this configuration, the baseline permissions of the CI/CD tool are unchanged.

If using Jenkins as the CI/CD tool, the service account of the Jenkins Agent (`sa-jenkins-agent-gce@prj-b-cicd-xxxx.iam.gserviceaccount.com`) is granted [impersonation](https://cloud.google.com/iam/docs/create-short-lived-credentials-direct) access so it can generate tokens over the Terraform custom Service Accounts.
In this configuration, the baseline permissions of the CI/CD tool are limited.

After executing this step, you will have the following structure:

```
example-organization/
└── fldr-bootstrap
    ├── prj-b-cicd
    └── prj-b-seed
```

When this step uses the Cloud Build submodule, it sets up the cicd project (`prj-b-cicd`) with Cloud Build and Cloud Source Repositories for each of the stages below.
Triggers are configured to run a `terraform plan` for any non-environment branch and `terraform apply` when changes are merged to an environment branch (`development`, `nonproduction` or `production`).
Usage instructions are available in the 0-bootstrap [README](./0-bootstrap/README.md).

### [1. org](./1-org/)

The purpose of this stage is to set up the common folder used to house projects that contain shared resources such as Security Command Center notification, Cloud Key Management Service (KMS), org level secrets, and org level logging.
This stage also sets up the network folder used to house network related projects such as DNS Hub, Interconnect, network hub, and base and restricted projects for each environment  (`development`, `nonproduction` or `production`).
This will create the following folder and project structure:

```
example-organization
└── fldr-common
    ├── prj-c-logging
    ├── prj-c-billing-export
    ├── prj-c-scc
    ├── prj-c-kms
    └── prj-c-secrets
└── fldr-network
    ├── prj-net-hub-base
    ├── prj-net-hub-restricted
    ├── prj-net-dns
    ├── prj-net-interconnect
    ├── prj-d-shared-base
    ├── prj-d-shared-restricted
    ├── prj-n-shared-base
    ├── prj-n-shared-restricted
    ├── prj-p-shared-base
    └── prj-p-shared-restricted
```

#### Logs

Under the common folder, a project `prj-c-logging` is used as the destination for organization wide sinks. This includes admin activity audit logs from all projects in your organization and the billing account.

Logs are collected into a logging bucket with a linked BigQuery dataset, which can be used for ad-hoc log investigations, querying, or reporting. Log sinks can also be configured to export to Pub/Sub for exporting to external systems or Cloud Storage for long-term storage.

**Notes**:

- Log export to Cloud Storage bucket has optional object versioning support via `log_export_storage_versioning`.
- The various audit log types being captured in BigQuery are retained for 30 days.
- For billing data, a BigQuery dataset is created with permissions attached, however you will need to configure a billing export [manually](https://cloud.google.com/billing/docs/how-to/export-data-bigquery), as there is no easy way to automate this at the moment.

#### Security Command Center notification

Another project created under the common folder. This project will host the Security Command Center notification resources at the organization level.
This project will contain a Pub/Sub topic, a Pub/Sub subscription, and a [Security Command Center notification](https://cloud.google.com/security-command-center/docs/how-to-notifications) configured to send all new findings to the created topic.
You can adjust the filter when deploying this step.

#### KMS

Another project created under the common folder. This project is allocated for [Cloud Key Management](https://cloud.google.com/security-key-management) for KMS resources shared by the organization.

Usage instructions are available for the org step in the [README](./1-org/README.md).

#### Secrets

Another project created under the common folder. This project is allocated for [Secret Manager](https://cloud.google.com/secret-manager) for secrets shared by the organization.

Usage instructions are available for the org step in the [README](./1-org/README.md).

#### DNS hub

This project is created under the network folder. This project will host the DNS hub for the organization.

#### Interconnect

Another project created under the network folder. This project will host the Dedicated Interconnect [Interconnect connection](https://cloud.google.com/network-connectivity/docs/interconnect/concepts/terminology#elements) for the organization. In case of Partner Interconnect, this project is unused and the [VLAN attachments](https://cloud.google.com/network-connectivity/docs/interconnect/concepts/terminology#for-partner-interconnect) will be placed directly into the corresponding hub projects.

#### Networking

Under the network folder, two projects, one for base and another for restricted network, are created per environment (`development`, `nonproduction`, and `production`) which is intended to be used as a [Shared VPC host project](https://cloud.google.com/vpc/docs/shared-vpc) for all projects in that environment.
This stage only creates the projects and enables the correct APIs, the following networks stages, [3-networks-dual-svpc](./3-networks-dual-svpc/) and [3-networks-hub-and-spoke](./3-networks-hub-and-spoke/), create the actual Shared VPC networks.

### [2. environments](./2-environments/)

The purpose of this stage is to set up the environments folders that contain shared projects for each environemnt.
This will create the following folder and project structure:

```
example-organization
└── fldr-development
    ├── prj-d-kms
    └── prj-d-secrets
└── fldr-nonproduction
    ├── prj-n-kms
    └── prj-n-secrets
└── fldr-production
    ├── prj-p-kms
    └── prj-p-secrets
```

#### KMS

Under the environment folder, a project is created per environment (`development`, `nonproduction`, and `production`), which is intended to be used by [Cloud Key Management](https://cloud.google.com/security-key-management) for KMS resources shared by the environment.

Usage instructions are available for the environments step in the [README](./2-environments/README.md).

#### Secrets

Under the environment folder, a project is created per environment (`development`, `nonproduction`, and `production`), which is intended to be used by [Secret Manager](https://cloud.google.com/secret-manager) for secrets shared by the environment.

Usage instructions are available for the environments step in the [README](./2-environments/README.md).

### [3. networks-dual-svpc](./3-networks-dual-svpc/)

This step focuses on creating a [Shared VPC](https://cloud.google.com/architecture/security-foundations/networking#vpcsharedvpc-id7-1-shared-vpc-) per environment (`development`, `nonproduction`, and `production`) in a standard configuration with a reasonable security baseline. Currently, this includes:

- (Optional) Example subnets for `development`, `nonproduction`, and `production` inclusive of secondary ranges for those that want to use Google Kubernetes Engine.
- Hierarchical firewall policy created to allow remote access to [VMs through IAP](https://cloud.google.com/iap/docs/using-tcp-forwarding), without needing public IPs.
- Hierarchical firewall policy created to allow for [load balancing health checks](https://cloud.google.com/load-balancing/docs/health-checks#firewall_rules).
- Hierarchical firewall policy created to allow [Windows KMS activation](https://cloud.google.com/compute/docs/instances/windows/creating-managing-windows-instances#kms-server).
- [Private service networking](https://cloud.google.com/vpc/docs/configure-private-services-access) configured to enable workload dependant resources like Cloud SQL.
- Base Shared VPC with [private.googleapis.com](https://cloud.google.com/vpc/docs/configure-private-google-access#private-domains) configured for base access to googleapis.com and gcr.io. Route added for VIP so no internet access is required to access APIs.
- Restricted Shared VPC with [restricted.googleapis.com](https://cloud.google.com/vpc-service-controls/docs/supported-products) configured for restricted access to googleapis.com and gcr.io. Route added for VIP so no internet access is required to access APIs.
- Default routes to internet removed, with tag based route `egress-internet` required on VMs in order to reach the internet.
- (Optional) Cloud NAT configured for all subnets with logging and static outbound IPs.
- Default Cloud DNS policy applied, with DNS logging and [inbound query forwarding](https://cloud.google.com/dns/docs/overview#dns-server-policy-in) turned on.

Usage instructions are available for the networks step in the [README](./3-networks-dual-svpc/README.md).

### [3. networks-hub-and-spoke](./3-networks-hub-and-spoke/)

This step configures the same network resources that the step 3-networks-dual-svpc does, but this time it makes use of the architecture based on the [hub-and-spoke](https://cloud.google.com/architecture/security-foundations/networking#hub-and-spoke) reference network model.

Usage instructions are available for the networks step in the [README](./3-networks-hub-and-spoke/README.md).

### [4. projects](./4-projects/)

This step is focused on creating service projects with a standard configuration that are attached to the Shared VPC created in the previous step and application infrastructure pipelines.
Running this code as-is should generate a structure as shown below:

```
example-organization/
└── fldr-development
    └── fldr-development-bu1
        ├── prj-d-bu1-sample-floating
        ├── prj-d-bu1-sample-base
        ├── prj-d-bu1-sample-restrict
        ├── prj-d-bu1-sample-peering
    └── fldr-development-bu2
        ├── prj-d-bu2-sample-floating
        ├── prj-d-bu2-sample-base
        ├── prj-d-bu2-sample-restrict
        └── prj-d-bu2-sample-peering
└── fldr-nonproduction
    └── fldr-nonproduction-bu1
        ├── prj-n-bu1-sample-floating
        ├── prj-n-bu1-sample-base
        ├── prj-n-bu1-sample-restrict
        ├── prj-n-bu1-sample-peering
    └── fldr-nonproduction-bu2
        ├── prj-n-bu2-sample-floating
        ├── prj-n-bu2-sample-base
        ├── prj-n-bu2-sample-restrict
        └── prj-n-bu2-sample-peering
└── fldr-production
    └── fldr-production-bu1
        ├── prj-p-bu1-sample-floating
        ├── prj-p-bu1-sample-base
        ├── prj-p-bu1-sample-restrict
        ├── prj-p-bu1-sample-peering
    └── fldr-production-bu2
        ├── prj-p-bu2-sample-floating
        ├── prj-p-bu2-sample-base
        ├── prj-p-bu2-sample-restrict
        └── prj-p-bu2-sample-peering
└── fldr-common
    ├── prj-c-bu1-infra-pipeline
    └── prj-c-bu2-infra-pipeline
```

The code in this step includes two options for creating projects.
The first is the standard projects module which creates a project per environment, and the second creates a standalone project for one environment.
If relevant for your use case, there are also two optional submodules which can be used to create a subnet per project, and a dedicated private DNS zone per project.

Usage instructions are available for the projects step in the [README](./4-projects/README.md).

### [5. app-infra](./5-app-infra/)

The purpose of this step is to deploy a simple [Compute Engine](https://cloud.google.com/compute/) instance in one of the business unit projects using the infra pipeline set up in 4-projects.

Usage instructions are available for the app-infra step in the [README](./5-app-infra/README.md).

### Final view

After all steps above have been executed, your Google Cloud organization should represent the structure shown below, with projects being the lowest nodes in the tree.

```
example-organization
└── fldr-common
    ├── prj-c-logging
    ├── prj-c-billing-export
    ├── prj-c-scc
    ├── prj-c-kms
    ├── prj-c-secrets
    ├── prj-c-bu1-infra-pipeline
    └── prj-c-bu2-infra-pipeline
└── fldr-network
    ├── prj-net-hub-base
    ├── prj-net-hub-restricted
    ├── prj-net-dns
    ├── prj-net-interconnect
    ├── prj-d-shared-base
    ├── prj-d-shared-restricted
    ├── prj-n-shared-base
    ├── prj-n-shared-restricted
    ├── prj-p-shared-base
    └── prj-p-shared-restricted
└── fldr-development
    ├── prj-d-kms
    └── prj-d-secrets
    └── fldr-development-bu1

        ├── prj-d-bu1-sample-floating
        ├── prj-d-bu1-sample-base
        ├── prj-d-bu1-sample-restrict
        ├── prj-d-bu1-sample-peering
    └── fldr-development-bu2

        ├── prj-d-bu2-sample-floating
        ├── prj-d-bu2-sample-base
        ├── prj-d-bu2-sample-restrict
        └── prj-d-bu2-sample-peering
└── fldr-nonproduction
    ├── prj-n-kms
    └── prj-n-secrets
    └── fldr-nonproduction-bu1

        ├── prj-n-bu1-sample-floating
        ├── prj-n-bu1-sample-base
        ├── prj-n-bu1-sample-restrict
        ├── prj-n-bu1-sample-peering
    └── fldr-nonproduction-bu2

        ├── prj-n-bu2-sample-floating
        ├── prj-n-bu2-sample-base
        ├── prj-n-bu2-sample-restrict
        └── prj-n-bu2-sample-peering
└── fldr-production
    ├── prj-p-kms
    └── prj-p-secrets
    └── fldr-production-bu1

        ├── prj-p-bu1-sample-floating
        ├── prj-p-bu1-sample-base
        ├── prj-p-bu1-sample-restrict
        ├── prj-p-bu1-sample-peering
    └── fldr-production-bu2

        ├── prj-p-bu2-sample-floating
        ├── prj-p-bu2-sample-base
        ├── prj-p-bu2-sample-restrict
        └── prj-p-bu2-sample-peering
└── fldr-bootstrap
    ├── prj-b-cicd
    └── prj-b-seed
```

### Branching strategy

There are three main named branches: `development`, `nonproduction`, and `production` that reflect the corresponding environments. These branches should be [protected](https://docs.github.com/en/github/administering-a-repository/about-protected-branches). When the [CI/CD Pipeline](/docs/GLOSSARY.md#foundation-cicd-pipeline) (Jenkins or Cloud Build) runs on a particular named branch (say for instance `development`), only the corresponding environment (`development`) is applied. An exception is the `shared` environment, which is only applied when triggered on the `production` branch. This is because any changes in the `shared` environment may affect resources in other environments and can have adverse effects if not validated correctly.

Development happens on feature and bug fix branches (which can be named `feature/new-foo`, `bugfix/fix-bar`, etc.) and when complete, a [pull request (PR)](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) or [merge request (MR)](https://docs.gitlab.com/ee/user/project/merge_requests/) can be opened targeting the `development` branch. This will trigger the [CI/CD Pipeline](/docs/GLOSSARY.md#foundation-cicd-pipeline) to perform a plan and validate against all environments (`development`, `nonproduction`, `shared`, and `production`). After the code review is complete and changes are validated, this branch can be merged into `development`. This will trigger a [CI/CD Pipeline](/docs/GLOSSARY.md#foundation-cicd-pipeline) that applies the latest changes in the `development` branch on the `development` environment.

After validated in `development`, changes can be promoted to `nonproduction` by opening a PR or MR targeting the `nonproduction` branch and merging them. Similarly, changes can be promoted from `nonproduction` to `production`.

### Policy validation

This repo uses the [terraform-tools](https://cloud.google.com/docs/terraform/policy-validation/validate-policies) component of the `gcloud` CLI to validate the Terraform plans against a [library of Google Cloud policies](https://github.com/GoogleCloudPlatform/policy-library).

The [Scorecard bundle](https://github.com/GoogleCloudPlatform/policy-library/blob/master/docs/bundles/scorecard-v1.md) was used to create the [policy-library folder](./policy-library) with [one extra constraint](https://github.com/GoogleCloudPlatform/policy-library/blob/master/samples/serviceusage_allow_basic_apis.yaml) added.

See the [policy-library documentation](https://github.com/GoogleCloudPlatform/policy-library/blob/master/docs/index.md) if you need to add more constraints from the [samples folder](https://github.com/GoogleCloudPlatform/policy-library/tree/master/samples) in your configuration based in your type of workload.

Step 1-org has [instructions](./1-org/README.md#deploying-with-cloud-build) on the creation of the shared repository to host these policies.

### Optional Variables

Some variables used to deploy the steps have default values, check those **before deployment** to ensure they match your requirements. For more information, there are tables of inputs and outputs for the Terraform modules, each with a detailed description of their variables. Look for variables marked as **not required** in the section **Inputs** of these READMEs:

- Step 0-bootstrap: If you are using Cloud Build in the [CI/CD Pipeline](/docs/GLOSSARY.md#foundation-cicd-pipeline), check the main [README](./0-bootstrap/README.md#Inputs) of the step. If you are using Jenkins, check the [README](./0-bootstrap/modules/jenkins-agent/README.md#Inputs) of the module `jenkins-agent`.
- Step 1-org: The [README](./1-org/envs/shared/README.md#Inputs) of the environment `shared`.
- Step 2-environments: The READMEs of the environments [development](./2-environments/envs/development/README.md#Inputs), [nonproduction](./2-environments/envs/nonproduction/README.md#Inputs), and [production](./2-environments/envs/production/README.md#Inputs)
- Step 3-networks-dual-svpc: The READMEs of the environments [shared](./3-networks-dual-svpc/envs/shared/README.md#inputs), [development](./3-networks-dual-svpc/envs/development/README.md#Inputs), [nonproduction](./3-networks/envs/nonproduction/README.md#Inputs), and [production](./3-networks/envs/production/README.md#Inputs)
- Step 3-networks-hub-and-spoke: The READMEs of the environments [shared](./3-networks-hub-and-spoke/envs/shared/README.md#inputs), [development](./3-networks-hub-and-spoke/envs/development/README.md#Inputs), [nonproduction](./3-networks/envs/nonproduction/README.md#Inputs), and [production](./3-networks/envs/production/README.md#Inputs)
- Step 4-projects: The READMEs of the environments [shared](./4-projects/business_unit_1/shared/README.md#inputs), [development](./4-projects/business_unit_1/development/README.md#Inputs), [nonproduction](./4-projects/business_unit_1/nonproduction/README.md#Inputs), and [production](./4-projects/business_unit_1/production/README.md#Inputs)

## Errata summary

Refer to the [errata summary](./ERRATA.md) for an overview of the delta between the example foundation repository and the [Google Cloud security foundations guide](https://cloud.google.com/architecture/security-foundations).

## Contributing

Refer to the [contribution guidelines](./CONTRIBUTING.md) for information on contributing to this module.",VRAI
GoogleCloudPlatform/professional-services,DevOPs,Toolkit,2025-05-12T14:37:09Z,2025-03-31T21:46:42Z,0,8,0,0,0,0,0,28,2017-05-18T19:29:27Z,2025-04-07T18:50:44Z,408597,2882,Python,VRAI,1354,FAUX,41,"bigquery,examples,gke,google-cloud-compute,google-cloud-dataflow,google-cloud-ml,google-cloud-platform,solutions,tools",41,Common solutions and tools developed by Google Cloud's Professional Services team. This repository and its contents are not an officially supported Google product.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,305,"# Professional Services

Common solutions and tools developed by Google Cloud's Professional Services
team.

## Disclaimer

This repository and its contents are not an officially supported Google product.

## License

All solutions within this repository are provided under the
[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) license. Please see
the [LICENSE](/LICENSE) file for more detailed terms and conditions.

[![Open in Cloud Shell](http://gstatic.com/cloudssh/images/open-btn.svg)](https://console.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fprofessional-services.git)

## Examples

The examples folder contains example solutions across a variety of Google Cloud
Platform products. Use these solutions as a reference for your own or extend
them to fit your particular use case.

*   [Anthos Service Mesh Multi-Cluster](examples/anthos-service-mesh-multicluster) -
    Solution to federate two private GKE clusters using Anthos Service Mesh.
*   [Anthos CICD with Gitlab](examples/anthos-cicd-with-gitlab) - A step-by-step
    guide to create an example CI/CD solution using Anthos and Gitlab.
*   [Audio Content Profiling](examples/ml-audio-content-profiling) - A tool that
    builds a pipeline to scale the process of moderating audio files for
    inappropriate content using machine learning APIs.
*   [Bigdata generator](tools/bigdata-generator/) - 
    Solution that generates large amounts of data for stress-testing bigdata solutions (e.g BigQuery). 
    For each of the fields you want to generate, you can specify rules for generating their values. The generated data can stored in BigQuery or GCS (Avro, CSV).
*   [BigQuery Analyze Realtime Reddit Data](examples/bigquery-analyze-realtime-reddit-data/) - 
    Solution to deploy a ([reddit](https://www.reddit.com)) social media data collection
    architecture on Google Cloud Platform.  Analyzes reddit comments in realtime and
    provides free natural-language processing / sentiment.
*   [BigQuery Audit Log Dashboard](examples/bigquery-audit-log) - Solution to
    help audit BigQuery usage using Data Studio for visualization and a sample
    SQL script to query the back-end data source consisting of audit logs.
*   [BigQuery Audit Log Anomaly Detection](examples/bigquery-auditlog-anomaly-detection) -
    Sample of using BigQuery audit logs for automated anomaly detection and
    outlier analysis. Generates user friendly graphs for quick bq environment
    analysis.
*   [BigQuery Automated Email Exports](examples/bq-email-exports) - Serverless
    solution to automate the sending of BigQuery export results via email on a
    scheduled interval. The email will contain a link to a signed or unsigned
    URL, allowing the recipient to view query results as a JSON, CSV, or Avro
    file.
*   [BigQuery Automated Schema Management](tools/bqman) - Command-line utility
    for automated provisioning and management of BigQuery datasets and tables.
*   [BigQuery Billing Dashboard](examples/bigquery-billing-dashboard) - Solution
    to help displaying billing info using Data Studio for visualization and a
    sample SQL script to query the back-end billing export table in BigQuery.
*   [BigQuery Cross Project Slot Monitoring](examples/bigquery-cross-project-slot-monitoring) -
    Solution to help monitoring slot utilization across multiple projects, while
    breaking down allocation per project.
*   [BigQuery Data Consolidator](tools/bigquery-data-consolidator) - Solution to
    consolidate data within an organization from multiple projects into one target
    Dataset/Table where all Source tables are of same schema (like Billing Exports!); specifically
    useful for data consolidation and further reporting in Cloud FinOps engagements.
*   [BigQuery DDL Validator](examples/bigquery-ddl-validator) -
    A utility that will read the Legacy DDL and compare it against the previously extracted DDL and 
    produce an output with the name of the objects where the DDL is no longer matching.
*   [BigQuery Group Sync For Row Level Access](examples/bigquery-row-access-groups) -
    Sample code to synchronize group membership from G Suite/Cloud Identity into
    BigQuery and join that with your data to control access at row level.
*   [BigQuery Long Running Optimization Utility](examples/bigquery-long-running-optimizer) -
    A utility that reads the entire SQL and provides a list of suggestions that would help to optimize the query and avoid the long running issues.
*   [BigQuery Oracle DDL Migration Utility](examples/bigquery-oracle-ddl-migration-utility) - 
    Oracle DDL Migration Utility to migrate the tables schema (DDL) from Oracle DB to BigQuery.
    The utility leverages BigQuery Translation API and offers additional features
    such as adding partitioning, clustering, metadata columns and prefixes to table names.
*   [BigQuery Pipeline Utility](tools/bqpipeline) - Python utility class for
    defining data pipelines in BigQuery.
*   [BigQuery Remote Function](examples/bq-remote-function) - It allows user to implement custom
    services or libraries in languages other than SQL or Javascript which are not part of UDFs.
    The utility contains sample string format Java code to deploy cloud run gen2 instance and invoke 
    the service from BigQuery using remote function.
*   [BigQuery Amazon S3 Migration Tool](tools/bigquery-s3tobq) - Bigquery Migration Tool to transfer data
    from files in Amazon S3 to BigQuery Tables based on configuration provided.
*   [BigQuery Snowflake TabRle Migration Tool](examples/bigquery-snowflake-tables-migration-utility) -
    BigQuery Snowflake Table Migration Tool helps to migrate the table DDL's from Snowflake to BigQuery.
    The utility leverages BigQuery Translation API and offers additional features
    such as adding partitioning, clustering, metadata columns and prefixes to table names.
*   [BigQuery Table Access Pattern Analysis](examples/bigquery-table-access-pattern-analysis) -
    Sample code to analyse data pipeline optimisation points, by pinpointing
    suboptimal pipeline scheduling between tables in a data warehouse ELT job.
*   [BigQuery Tink Toolkit](tools/bigquery-tink-toolkit) - Python utility class
    for working with Tink-based cryptography in on-prem or GCP systems in a way
    that is interoperable with BigQuery's field-level encryption. Includes a
    sample PySpark job and a script for generating and uploading KMS-encrypted
    Tink keysets to BigQuery.
*   [BigQuery to XML Export](tools/bigquery-to-xml) - Python tool that takes a
    BigQuery query and returns the output as an XML string.
*   [BigQuery Translation Validator](examples/bigquery-translation-validator-utility) - A python utility to compare 2 SQL Files and  point basic differences like column names,
    table names, joins, function names, is-Null and query syntax.
*   [BigQuery Generic DDL Migration Utility](examples/bigquery-generic-ddl-migration-utility) - 
    Generic DDL Migration Utility to migrate the tables schema (DDL) from Database(Oracle, Snowflake, MSSQL, Vertica, Neteeza) DB to BigQuery.
    The utility leverages BigQuery Translation API and offers additional features
    such as adding partitioning, clustering, metadata columns and prefixes to table names.
*   [Bigtable Dataflow Cryptocurrencies Exchange RealTime Example](examples/cryptorealtime) -
    Apache Beam example that reads from the Crypto Exchanges WebSocket API as
    Google Cloud Dataflow pipeline and saves the feed in Google Cloud Bigtable.
    Real time visualization and query examples from GCP Bigtable running on
    Flask server are included.
*   [Bigtable Dataflow Update Table Key Pipeline](examples/bigtable-change-key) -
    Dataflow pipeline with an example of how to update the key of an existing
    table. It works with any table, regardless the schema. It shows how to
    update your key for a table with existing data, to try out different
    alternatives to improve performance.
*   [Carbon Footprint Reporting](examples/carbon-foortprint-dashboard) - Example of 
    using the prebuilt Data studio & Looker template for analysing GCP Carbon Footprint Estimates.
*   [Cloud Audit Log Samples](examples/audit-log-examples/) - A sample
    collection of Audit Logs for Users and Customers to better the structure,
    contents, and values contained in various log events.
*   [Cloud Build Application CICD Examples](examples/cloudbuild-application-cicd) - 
    Cloud Build CI/CD Examples for Applications like containerization &
    deployment to Cloud Run.
*   [Cloud Build with Proxy Running in Background](examples/cloudbuild-with-tcp-proxy) -
    Examples of cloudbuild with docker-compose running tcp proxy in the
    background for all build steps.
*   [Cloud Composer CI/CD](examples/cloud-composer-cicd) - Examples of using
    Cloud Build to deploy airflow DAGs to Cloud Composer.
*   [Cloud Composer Deployment in Shared VPC](examples/composer-shared-vpc) -
    Terraform code to deploy cloud composer in shared VPC environment.
*   [Cloud Composer Dependency Management](examples/cloud-composer-dependency-management-example) - Example of
    Cloud Composer Dependency Management designed to orchestrate complex task dependencies within Apache Airflow which addresses the challenge of managing parent-child DAG relationships across varying temporal frequencies (yearly, monthly, weekly etc)
*   [Cloud Composer Examples](examples/cloud-composer-examples) - Examples of
    using Cloud Composer, GCP's managed Apache Airflow service.
*   [Cloud Data Fusion Functions and Plugins](examples/cloud-datafusion-functions-plugins) -
    Examples of Cloud Data Fusion Functions and Plugins.
*   [Cloud DNS load balancing](examples/cloud-dns-load-balancing) - Multi-region HA setup for GCE VMs and Cloud Run based applications utilizing Cloud DNS load balancing and multiple Google Cloud load balancer types.
*   [Cloud DNS public zone monitoring](examples/cloud-dns-public-zone-dashboard) - Visualizing Cloud DNS public zone query data using log-based metrics and Cloud Monitoring.
*   [Cloud Function Act As](examples/cloud-function-act-as) - Example of
    executing a Cloud Function on behalf and with IAM permissions of the GitHub
    Workload Identity caller.
*   [Cloud Function VM Delete Event Handler Example](examples/gcf-pubsub-vm-delete-event-handler) -
    Solution to automatically delete A records in Cloud DNS when a VM is
    deleted. This solution implements a [Google Cloud Function][gcf]
    [Background Function][gcf-bg] triggered on `compute.instances.delete` events
    published through [Stackdriver Logs Export][logs-export].
*   [Certificate Authority Service Hierarchy](examples/certificate-authority-service-hierarchy) - Root and Subordinate Certificate Authority Service CA Pools and CAs with examples for domain ownership validation and sample load test script.
*   [Cloud Run to BQ](examples/cloudrun-to-bq) - Solution to accept events/data
    on HTTP REST Endpoint and insert into BQ.
*   [Cloud SQL Custom Metric](examples/cloud-sql-custom-metric) - An example of
    creating a Stackdriver custom metric monitoring Cloud SQL Private Services
    IP consumption.
*   [Cloud Support API](examples/cloud-support) - Sample code using Cloud
    Support API
*   [CloudML Bank Marketing](examples/cloudml-bank-marketing) - Notebook for
    creating a classification model for marketing using CloudML.
*   [CloudML Bee Health Detection](examples/cloudml-bee-health-detection) -
    Detect if a bee is unhealthy based on an image of it and its subspecies.
*   [CloudML Churn Prediction](examples/cloudml-churn-prediction) - Predict
    users' propensity to churn using Survival Analysis.
*   [CloudML Customer Support and Complaint Handling](examples/cloudml-support-routing) -
    BigQuery + AutoML pipeline classifying customer complaints based on expected
    resolution; adaptable to other support communications use cases.
*   [CloudML Deep Collaborative Filtering](examples/cloudml-collaborative-filtering) -
    Recommend songs given either a user or song.
*   [CloudML Energy Price Forecasting](examples/cloudml-energy-price-forecasting) -
    Predicting the future energy price based on historical price and weather.
*   [CloudML Fraud Detection](examples/cloudml-fraud-detection) - Fraud
    detection model for credit-cards transactions.
*   [CloudML Scikit-learn Pipeline](examples/cloudml-sklearn-pipeline) - This is
    a example for building a scikit-learn-based machine learning pipeline
    trainer that can be run on AI Platform. The pipeline can be trained locally
    or remotely on AI platform. The trained model can be further deployed on AI
    platform to serve online traffic.
*   [CloudML Sentiment Analysis](examples/cloudml-sentiment-analysis) -
    Sentiment analysis for movie reviews using TensorFlow `RNNEstimator`.
*   [CloudML TensorFlow Profiling](examples/tensorflow-profiling-examples) -
    TensorFlow profiling examples for training models with CloudML
*   [Data Generator](examples/dataflow-data-generator) - Generate random data
    with a custom schema at scale for integration tests or demos.
*   [Dataflow BigQuery Transpose Example](examples/dataflow-bigquery-transpose) -
    An example pipeline to transpose/pivot/rotate a BigQuery table.
*   [Dataflow Custom Templates Example](examples/dataflow-custom-templates) - An
    example that demonstrates how to build custom Dataflow templates.
*   [Dataflow Elasticsearch Indexer](examples/dataflow-elasticsearch-indexer) -
    An example pipeline that demonstrates the process of reading JSON documents
    from Cloud Pub/Sub, enhancing the document using metadata stored in Cloud
    Bigtable and indexing those documents into
    [Elasticsearch](https://www.elastic.co/).
*   [Dataflow BigQuery to AlloyDB](examples/dataflow-bigquery-to-alloydb/) -
    Example that shows how to move data from BigQuery to an AlloyDB table using Dataflow.
*   [Dataflow Flex Template in Restricted Networking Env](examples/dataflow-flex-python/) -
    Example implements a python flex template which can be run in an environment
    where workers can not download python packages due to egress traffic restrictions.
*   [Dataflow Python Examples](examples/dataflow-python-examples) - Various ETL
    examples using the Dataflow Python SDK.
*   [Dataflow Streaming Benchmark](examples/dataflow-streaming-benchmark) -
    Utility to publish randomized fake JSON messages to a Cloud Pub/Sub topic at
    a configured QPS.
*   [Dataflow Streaming Schema Changes Handler](examples/dataflow-streaming-schema-handler) -
    Dataflow example to handle schema changes using schema enforcement and DLT
    approach
*   [Dataflow Streaming XML to GCS](examples/dataflow-xml-pubsub-to-gcs) -
    Dataflow example to handle streaming of xml encoded messages and write them to Google Cloud Storage
*   [Dataflow DLP Hashpipeline](examples/dataflow-dlp-hash-pipeline) - Match DLP
    Social Security Number findings against a hashed dictionary in Firestore.
    Use Secret Manager for the hash key.
*   [Dataflow Template Pipelines](https://github.com/GoogleCloudPlatform/DataflowTemplates) -
    Pre-implemented Dataflow template pipelines for solving common data tasks on
    Google Cloud Platform.
*   [Dataflow Production Ready](examples/dataflow-production-ready) - Reference
    implementation for best practices around Beam, pipeline structuring, testing
    and continuous deployment.
*   [Dataflow XML to BigQuery](examples/dataflow-xmlio-to-bq) - Example of
    loading XML data into BigQuery with DataFlow via XMLIO.
*   [Data Loss Prevention hybrid inspection for MongoDB](examples/mongodb-hybrid-dlp) - 
    A Cloud Function using MongoDB Change Streams that uses Sensitive Data Protection's
    hybrid Data Loss Prevention inspection API in near real-time.
*   [Dataproc Spanner](examples/dataproc-spanner) - Dataproc cluster write to Spanner using Apache Spark in Scala.
*   [Dataproc GCS Connector](examples/dataproc-gcs-connector) - Install and test
    unreleased features on the GCS Connector for Dataproc.
*   [Dataproc Job Optimization Guide](examples/dataproc-job-optimization-guide) - Step-by-step
    guide for optimizing a sample Dataproc Job.
*   [Dataproc Persistent History Server for Ephemeral Clusters](examples/dataproc-persistent-history-server) -
    Example of writing logs from an ephemeral cluster to GCS and using a
    separate single node cluster to look at Spark and YARN History UIs.
*   [Dataproc Lifecycle Management via Composer](examples/dataproc-lifecycle-via-composer) - Ephemeral Dataproc lifecycle management and resources optimization via Composer, Terraform template to deploy Composer and additional reqs, Dynamically generated DAGs from jobs config files.
*   [Dataproc Running Notebooks](examples/dataproc-running-notebooks) - Orchestrating the workflow of running Jupyter Notebooks on a Dataproc cluser via PySpark job
*   [dbt-on-cloud-composer](examples/dbt-on-cloud-composer) - Example of using
    dbt to manage BigQuery data pipelines, utilizing Cloud Composer to run and
    schedule the dbt runs.
*   [Data Format Description Language (DFDL) Processesor with Firestore and
    Pubsub](examples/dfdl-firestore-pubsub-example) - Example to process a
    binary using DFDL definition and Daffodil libraries. The DFDL definition is
    stored in firestore, the request to process is done through a pubsub
    subcription and the output is published is a JSON format in a Pubsub topic.
*   [Data Format Description Language (DFDL) Processesor with Bigtable and
    Pubsub](examples/dfdl-bigtable-pubsub-example) - Example to process a binary
    using DFDL definition and Daffodil libraries. The DFDL definition is stored
    in bigtable, the request to process is done through a pubsub subcription and
    the output is published is a JSON format in a Pubsub topic.
*   [Dialogflow Webhook Example](examples/dialogflow-webhook-example) - Webhook
    example for dialogflow in Python.
*   [Dialogflow CX Private Webhook Example](examples/dialogflowcx-private-webhook-example) -
    Webhook example for Dialogflow CX in Python.
*   [Dialogflow Middleware Example](examples/ccai-dialogflow-middleware) -
    Dialogflow middleware example in Java.
*   [Dialogflow Entities Creation and Update](examples/dialogflow-entities-example) -
    Creation and update of entities for Dialogflow in Python.
*   [DLP API Examples](examples/dlp) - Examples of the DLP API usage.
*   [Ephemeral Projects](examples/ephemeral-projects) - Creating short lived gcp projects for sandbox purposes.
*   [GCE Access to Google AdminSDK](examples/gce-to-adminsdk) - Example to help
    manage access to Google's AdminSDK using GCE's service account identity
*   [GCS Client Side Encryption via Sidecar](examples/gcs-client-encrypt/) - Example to show how to implement GCS client side encyrption via a sidecar
*   [GCS Hive External Table File Optimization](examples/gcs-hive-external-table-file-optimization) - 
    Example solution to showcase impact of file count, file size, and file
    type on Hive external tables and query speeds.
*   [GCS to BQ using serverless services](examples/gcs-to-bq-serverless-services) -
    Example to ingest GCS to BigQuery using serverless services such as Cloud
    Function, Pub/Sub and Serverless Spark.
*   [GDCE Terraform Example](examples/gdce-terraform-example) - Example for provisioning GDCE
    resources using terraform.
*   [GKE HA setup using spot VMs](examples/gke-ha-setup-using-spot-vms/) -
    Example for running an application with high availability requirements on
    GKE spot nodes using on-demand nodes as fallback
*   [Grpc Server connected to Spanner Database](examples/grpc_spanner_example) -
    Basic example of a Grpc server that is connected to a Spanner database.
*   [Grpc Server connected to Redis](examples/grpc_redis_example) - Basic
    example of a Grpc server that is connected to Redis.
*   [Gitlab KAS agent for GKE](examples/gitlab-kas-gke) - Terraform solution for          deploying a Gitlab KAS agent for synchronizing container deployments from Gitlab repos into a GKE cluster 
*   [Home Appliance Status Monitoring from Smart Power Readings](examples/e2e-home-appliance-status-monitoring) -
    An end-to-end demo system featuring a suite of Google Cloud Platform
    products such as IoT Core, ML Engine, BigQuery, etc.
*   [IAP User Profile](examples/iap-user-profile) - An example to retrieve user
    profile from an IAP-enabled GAE application.
*   [IoT Nirvana](examples/iot-nirvana) - An end-to-end Internet of Things
    architecture running on Google Cloud Platform.
*   [Kubeflow Pipelines Sentiment Analysis](examples/kubeflow-pipelines-sentiment-analysis) -
    Create a Kubeflow Pipelines component and pipelines to analyze sentiment for
    New York Times front page headlines using Cloud Dataflow (Apache Beam Java)
    and Cloud Natural Language API.
*   [Kubeflow Fairing Example](examples/kubeflow-fairing-example) - Provided
    three notebooks to demonstrate the usage of Kubeflow Faring to train machine
    learning jobs (Scikit-Learn, XGBoost, Tensorflow) locally or in the Cloud
    (AI platform training or Kubeflow cluster).
*   [Left-Shift Validation Pre-Commit Hook](examples/left-shift-validation-pre-commit-hook/) -
    An example that uses a set of Bash scripts to set up a pre-commit hook that
    validates Kubernetes resources with Gatekeeper constraints and constraint
    templates from your choice of sources.
*   [LookerStudio Cost Optimization Dashboard](examples/cost-optimization-dashboard) -
    SQL scripts to help build Cost Optimization LookerStudio Dashboard.
*   [Migrate Kafka to GMK using MM2](examples/mm2-gmk-migration) -  Terraform code to deploy resources to migrate data between two Google Managed Kafka clustes using MirrorMaker2
*   [Personal Workbench Notebooks Deployer](examples/personal-workbench-notebooks-deployer) - Terraform sample modules to provision Dataproc Hub using personal auth clusters, and workbench managed notebooks for individual analytical users.
*   [Project factory with Terragrunt](examples/terragrunt-project-factory-gcp/) -
    This implements a `State-Scalable` project factory pattern for creating Google Cloud Platform projects using Terragrunt and public Terraform modules
*   [Python CI/CD with Cloud Builder and CSR](examples/python-cicd-with-cloudbuilder) -
    Example that uses Cloud Builder and Cloud Source Repositories to automate
    testing and linting.
*   [Pub/Sub Client Batching Example](examples/pubsub-publish-avro-example) -
    Batching in Pub/Sub's Java client API.
*   [QAOA](examples/qaoa) - Examples of parsing a max-SAT problem in a
    proprietary format, for Quantum Approximate Optimization Algorithm (QAOA)
*   [React single-page app on Cloud Run + Cloud Storage](examples/react-spa-app) - End-to-end example of deploying 
    a React SPA on serverless Google Cloud services.
*   [Redis Cluster on GKE Example](examples/redis-cluster-gke) - Deploying Redis
    cluster on GKE.
*   [Risk Analysis Asset](examples/risk-analysis-asset) - Deploying Reliability Risk analysis tool on Cloud Run. 
*   [Spanner Interleave Subquery](examples/spanner-interleave-subquery) -
    Example code to benchmark Cloud Spanner's subqueries for interleaved tables.
*   [Spanner Change Stream to BigQuery using Dataflow](examples/spanner-changestreams-bigquery) - 
    Terraform code to deploy Spanner change stream and publish changes to BigQuery using Dataflow Streaming Job.
*   [Spinnaker](examples/spinnaker) - Example pipelines for a Canary /
    Production deployment process.
*   [STS Metrics from STS Notification](examples/sts-metrics) - Example code to 
    generate custom metrics from STS notification.
*   [TensorFlow Serving on GKE and Load Testing](examples/tf-load-testing) -
    Examples how to implement Tensorflow model inference on GKE and to perform a
    load testing of such solution.
*   [TensorFlow Unit Testing](examples/tensorflow-unit-testing) - Examples how
    to write unit tests for TensorFlow ML models.
*   [Terraform Internal HTTP Load Balancer](examples/terraform-ilb) - Terraform
    example showing how to deploy an internal HTTP load balancer.
*   [Terraform NetApp CVS](examples/tf-netapp-cvs) - This example shows how to deploy NetApp CVS volumes using
    terraform.
*   [Terraform Resource Change Policy Library](examples/terraform-resource-change-policy-library) -
    Contains a library of policies written in the
    [OPA Constraint Framework](https://github.com/open-policy-agent/frameworks/blob/master/constraint/README.md)
    format to be used by `gcloud beta terraform vet` to validate Terraform resource
    changes in a CI/CD pipeline.
*   [Uploading files directly to Google Cloud Storage by using Signed URL](examples/direct-upload-to-gcs) -
    Example architecture to enable uploading files directly to GCS by using
    [Signed URL](https://cloud.google.com/storage/docs/access-control/signed-urls).
*   [TSOP object transfer Log prosessor](examples/tsop-log-processor/) - This example shows
    how to log object transfer logs by TSOP to Cloud Logging.
*   [GCS CSV files to BigQuery](https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/v1/README_GCS_CSV_to_BigQuery.md) - This example shows how to load files in CSV format stored in GCS to load to BigQuery tables. The files can be uncompressed or be compressed in formats such as Bzip2, GZIP and etc. See https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/Compression.html for the list of support compression method.

## Tools

The tools folder contains ready-made utilities which can simplify Google Cloud
Platform usage.

*   [Agile Machine Learning API](tools/agile-machine-learning-api) - A web
    application which provides the ability to train and deploy ML models on
    Google Cloud Machine Learning Engine, and visualize the predicted results
    using LIME through simple post request.
*   [Airflow DAG Metadata Generator](tools/airflow-dag-metadata-generator) - Use Google's
    generative models to analyze Airflow DAGs and supplement them with generated `description`,
    `tags`, and `doc_md` values.      
*   [Airflow States Collector](tools/airflow-states-collector) -
      A tool that creates and uploads an airflow dag to the dags GCS folder. The dag incrementally collect airflow task states and stores to BQ. It also autogenerates a LookerStudio dashboard querying the BQ view. 
*   [Airpiler](tools/airpiler) - A python script to convert Autosys JIL files to
    dag-factory format to be executed in Cloud Composer (managed airflow
    environment).
*   [Ansible Module for Anthos on Bare Metal](tools/anthosbm-ansible-module) -
    Ansible module for installation of Anthos on Bare Metal
*   [Anthos Bare Metal Installer](tools/anthosbm-ansible-module) - An
    [ansible](https://www.ansible.com/resources/get-started) playbook that can
    be used to install
    [Anthos Bare Metal](https://cloud.google.com/anthos/clusters/docs/bare-metal).
*   [Apache Beam Client Throttling](tools/apachebeam-throttling) - A library
    that can be used to limit the number of requests from an Apache Beam
    pipeline to an external service. It buffers requests to not overload the
    external service and activates client-side throttling when the service
    starts rejecting requests due to out of quota errors.
*   [API Key Rotation Checker](tools/api-key-rotation) - A tool that checks your
    GCP organization for API keys and compares them to a customizable rotation
    period. Regularly rotating API keys is a Google and industry standard
    recommended best practice.
*   [AssetInventory](tools/asset-inventory) - Import Cloud Asset Inventory
    resourcs into BigQuery.
*   [BigQuery Discount Per-Project Attribution](tools/kunskap) - A tool that
    automates the generation of a BigQuery table that uses existing exported
    billing data, by attributing both CUD and SUD charges on a per-project
    basis.
*   [BigQuery Policy Tag Utility](tools/bqtag) - Utility class for tagging BQ
    Table Schemas with Data Catalog Taxonomy Policy Tags. Create BQ Authorized
    Views using Policy Tags. Helper utility to provision BigQuery Dataset, Data
    Catalog Taxonomy and Policy Tags.
*   [BigQuery Query Plan Exporter](tools/bigquery-query-plan-exporter) - Command
    line utility for exporting BigQuery query plans in a given date range.
*   [BigQuery Query Plan Visualizer](tools/bq-visualizer) - A web application
    which provides the ability to visualise the execution stages of BigQuery
    query plans to aid in the optimization of queries.
*   [BigQuery z/OS Mainframe Connector](tools/bigquery-zos-mainframe-connector) -
    A utility used to load COBOL MVS data sets into BigQuery and execute query
    and load jobs from the IBM z/OS Mainframe.
*   [Boolean Organization Policy Enforcer](tools/boolean-org-policy-enforcer) -
    A tool to find the projects that do not set a boolean organization policy to
    its expected state, subsequently, set the organization policy to its
    expected set.
*   [Capacity Planner CLI](tools/capacity-planner-cli) - A stand-alone tool to
    extract peak resource usage values and corresponding timestamps for a given
    GCP project, time range and timezone.
*   [Capacity Planner Sheets Extension](tools/capacity-planner-sheets-extension) - 
    A Google Sheets extension to extract peak resource usage values and corresponding
    timestamps for a given GCP project, time range and timezone.
*   [CloudConnect](tools/cloudconnect) - A package that automates the setup of
    dual VPN tunnels between AWS and GCP.
*   [Cloudera Parcel GCS Connector](tools/cloudera-parcel-gcsconnector) - This
    script helps you create a Cloudera parcel that includes Google Cloud Storage
    connector. The parcel can be deployed on a Cloudera managed cluster. This
    script helps you create a Cloudera parcel that includes Google Cloud Storage
    connector. The parcel can be deployed on a Cloudera managed cluster.
*   [Cloud AI Vision Utilities](tools/cloud-vision-utils) - This is an
    installable Python package that provides support tools for Cloud AI Vision.
    Currently there are a few scripts for generating an AutoML Vision dataset
    CSV file from either raw images or image annotation files in PASCAL VOC
    format.
*   [Cloud Composer Backup and Recovery](tools/cloud-composer-backup-restore) - A
    command line tool for applying backup and recovery operations on Cloud
    Composer Airflow environments.
*   [Cloud Composer DAG Validation](tools/cloud-composer-dag-validation) - An automated process for running validation and testing against DAGs in Composer.
*   [Cloud Composer Migration Complexity Assessment](tools/cloud-composer-migration-complexity-assessment) - An Airflow DAG that uses a variety
    of tools to analyze a Cloud Composer 1 environment, determine a work estimate, and
    accelerate the conversion of airflow 1 dags to airflow 2 dags.
*   [Cloud Composer Migration Terraform Generator](tools/cloud-composer-migration-terraform-generator) - Analyzes an existing Cloud Composer 1
    / Airflow 1 environment and generates terraform. Configures new Cloud Composer 2
    environment to meet your workload demands.
*   [CUD Prioritized Attribution](tools/cuds-prioritized-attribution) - A tool
    that allows GCP customers who purchased Committed Use Discounts (CUDs) to
    prioritize a specific scope (e.g. project or folder) to attribute CUDs first
    before letting any unconsumed discount float to other parts of an
    organization.
*   [Custom Module for Security Health Analytics Library](tools/custom-module-security-health-analytics-library) - 
    A library of custom modules for SCC Security Health Analytics. It includes 
    tools to easily generate custom modules and provisioning them on your organization.
    This library helps organization to detect configuration and compliance drifts.
*   [Custom Organization Policy Library](tools/custom-organization-policy-library) - A library 
    of custom organization policy constraints and samples. It includes tools to easily generate policies for provisioning across your organization using either Google Cloud (gcloud) or Terraform.
*   [Custom Role Analyzer](tools/custom-roles-analyzer) - This tool will provide
    useful insights with respect to custom roles at organization level as well
    as project level to find predefined roles from which the custom role is
    built.
*   [Custom Role Manager](tools/custom-role-manager) - Manages organization- or
    project-level custom rol",VRAI
GoogleCloudPlatform/pubsec-declarative-toolkit,Documentations,Toolkit,2025-01-29T20:37:53Z,2024-03-07T15:19:17Z,0,0,0,0,0,0,0,11,2021-05-07T18:14:53Z,2025-03-17T06:58:37Z,11863,34,Shell,VRAI,28,FAUX,269,,269,The GCP PubSec Declarative Toolkit is a collection of declarative solutions to help you on your Journey to Google Cloud. Solutions are designed using Config Connector and deployed using Config Controller.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,20,"# GCP PubSec Declarative Toolkit

The GCP PubSec Declarative Toolkit is a collection of declarative solutions to help you on your Journey to Google Cloud. Solutions are designed using [Config Connector](https://cloud.google.com/config-connector/docs/overview) and deployed using [Config Controller](https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview).

## Current Solutions

| Name | Description | Documentation |
| --- | --- | --- |
| Guardrails | Base Infrastructure for 30 Day Guardrail Deployment | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/guardrails) |
| Organization Policy Bundle | Package of Baseline Organization Policies | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/org-policies) |
| Guardrails Policy Bundle | Policy Bundle to help analyze compliance for Guardrails |  [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/guardrails-policies) |
| KCC Namespaces | This solution is a simple fork of the KCC Project Namespaces blueprint found [here](https://cloud.google.com/anthos-config-management/docs/tutorials/project-namespace-blueprint) | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/kcc-namespaces) |
| Landing Zone v2 (LZv2) | **(In development)** PBMM Landing Zone built in collaboration with Shared Services Canada |  [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/docs/landing-zone-v2/README.md)
| Gatekeeper Policy (LZv2) | Policy Bundle | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/gatekeeper-policies) |
| Core Landing Zone (LZv2) | Foundational resources building the landing zone | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/core-landing-zone) |
| Client Setup (LZv2) | Package to create the initial client folder and namespaces | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/client-setup) |
| Client Landing Zone (LZv2)  | Package to create the client folder sub-structure and a standard Shared VPC | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/client-landing-zone) |
| Client Project Setup (LZv2) | Package to create a service project and host workloads | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/client-project-setup) |
| GKE Setup (LZv2) | Package to prepare a service project for GKE clusters | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/gke/configconnector/gke-setup) |
| GKE Defaults (LZv2) | A package to deploy common GKE resources | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/gke/configconnector/gke-defaults) |
| GKE Cluster Autopilot (LZv2) | A GKE Autopilot Cluster running in a service project | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/gke/configconnector/gke-cluster-autopilot) |
| Cluster Defaults (LZv2) | This package deploys default resources that have to exist on all GKE clusters | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/gke/kubernetes/cluster-defaults) |
| Namespace Defaults (LZv2) | This package deploys a workload namespace and it's associated configuration | [link](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/tree/main/solutions/gke/kubernetes/namespace-defaults) |

When getting a package you can use the `@` to indicate what tag or branch you will be getting with the `kpt pkg get` command for example `kpt pkg get https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit.git/solutions/core-landing-zone@main`.

You can find the latest release versions in the `releases` [page](https://github.com/GoogleCloudPlatform/pubsec-declarative-toolkit/releases).

## Quickstart

Deploying an example landing zone requires two steps:
- A [Config Connector](https://cloud.google.com/config-connector/docs/overview) enabled Kubernetes cluster
- One or more [solutions](#current-solutions) packages like the [core-landing-zone](solutions/core-landing-zone) and [experimentation core-landing-zone](solutions/experimentation/core-landing-zone) documented in section 2 of [landing-zone-v2](docs/landing-zone-v2#2-create-your-landing-zone)

In order to deploy the [solutions](#current-solutions) you will need a Kubernetes cluster with [Config Connector](https://cloud.google.com/config-connector/docs/overview) installed.

We recommend using the Managed Config Controller service which comes bundled with [Config Connector](https://cloud.google.com/config-connector/docs/overview) and [Anthos Config Management](https://cloud.google.com/anthos/config-management), alternatively you can [install](https://cloud.google.com/config-connector/docs/how-to/advanced-install#manual) Config Connector on any CNCF compliant Kubernetes cluster.

See the Google Cloud [quickstart](https://cloud.google.com/anthos-config-management/docs/tutorials/manage-resources-config-controller) guide for getting up and running with Config Controller

We have put together the following [guide](docs/advanced-install.md) to deploy a standalone Config Controller instance or see the examples [directory](examples/) for example installation methods.

After the Kubernetes cluster is fully provisioned - proceed to [Deploy a landing zone v2 package](docs/landing-zone-v2/README.md).

## Additional Documentation

For further documentation on the project, including the setup pre-requirements and supporting service such as Config Connector and Config Management.

- [Multi-Tenancy](https://cloud.google.com/anthos-config-management/docs/tutorials/project-namespace-blueprint)
- [Scalability Guidelines](https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-scalability)
- [View Config Controller Status](https://cloud.google.com/anthos-config-management/docs/how-to/config-controller-repo-status)
- [Monitor Resources](https://cloud.google.com/config-connector/docs/how-to/monitoring-your-resources)
- [Config Connector Resources](https://cloud.google.com/config-connector/docs/reference/overview)
- [Config Connector OSS on GitHub](https://github.com/GoogleCloudPlatform/k8s-config-connector)
- [Known Issues](docs/issues.md)
- [Fleet Management at Spotify (Part 2): The Path to Declarative Infrastructure](https://engineering.atspotify.com/2023/05/fleet-management-at-spotify-part-2-the-path-to-declarative-infrastructure/)

## Additional Resources

- [Awesome KRM](https://github.com/askmeegs/learn-krm)
- [I do declare! Infrastructure automation with Configuration as Data](https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes)
- [Rationale Behind kpt](https://kpt.dev/guides/rationale)
- [KRM Blueprints](https://github.com/GoogleCloudPlatform/blueprints)
- [How Goldman Sachs manages Google Cloud resources with Anthos Config Management at Google Next](https://www.youtube.com/watch?v=5ENId064XLo)

## Disclaimer

This is not an officially supported Google product.",VRAI
GoogleCloudPlatform/solution-acceleration-toolkit,Toolkit,DevOPs,2025-05-13T19:54:29Z,2022-05-16T16:45:47Z,0,3,0,0,0,0,0,98,2020-01-09T16:20:55Z,2025-04-02T16:01:38Z,5649,162,Go,VRAI,69,FAUX,35,,35,"Deploy, monitor & audit on GCP simplified",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,22,"# Google Cloud Healthcare Data Protection Suite

Stable releases:

![version](https://img.shields.io/github/v/release/GoogleCloudPlatform/healthcare-data-protection-suite?color=green&label=Binaries&sort=semver)

This repository contains a suite of tools that can be used to manage key areas
of your Google Cloud organization.

- Deploy
- Monitor
- Audit

## Tools

- [Terraform Engine](./docs/tfengine): Generate end-to-end infra-as-code for
    Google Cloud with security, compliance, and best practices built in.

- [Policy Generator](./docs/policygen): Generate best practices policies for
    Forseti and other monitoring solutions, customized for your infra.

- [Terraform Importer](./docs/tfimport): Automatically detect and import
    existing resources defined by your Terraform configs.

## Tutorial Video

[Deploying the Data Protection Toolkit](https://www.youtube.com/watch?v=-wIutctaqr0)

Note that YAML-formatted configs were used at the time when the Tutorial video
was made. The config format has been changed to
[HCL](https://github.com/hashicorp/hcl).

## Releases

Please see [RELEASING.md](./RELEASING.md) for our release strategy.",VRAI
GoogleCloudPlatform/terraform-google-enterprise-genai,Documentations,Documentations,2024-10-08T16:13:43Z,2024-05-13T17:07:14Z,0,0,0,0,0,0,0,0,2024-03-25T20:24:25Z,2025-03-13T01:13:08Z,3049,27,HCL,VRAI,15,FAUX,24,,24,,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,12,,FAUX
GoogleCloudPlatform/terraform-google-secured-data-warehouse,DevOPs,Documentations,2025-01-09T22:35:10Z,2023-08-03T18:25:30Z,0,3,0,0,0,0,0,81,2021-04-30T20:29:00Z,2025-03-17T13:47:58Z,1409,82,HCL,VRAI,38,FAUX,24,"cft-terraform,data-analytics,end-to-end",24,Deploys a secured BigQuery data warehouse,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,20,"# Secured Data Warehouse Blueprint

[FAQ](./docs/FAQ.md) | [Troubleshooting Guide](./docs/TROUBLESHOOTING.md).

This repository contains Terraform configuration modules that allow Google Cloud customers to
quickly deploy a secured [BigQuery](https://cloud.google.com/bigquery) data warehouse,
following the [Secure a BigQuery data warehouse that stores confidential data](https://cloud.google.com/architecture/confidential-data-warehouse-blueprint) guide.
The blueprint allows customers
to use Google Cloud's core strengths in data analytics, and to overcome typical
challenges that include:

- Limited knowledge/experience with best practices for creating, deploying, and operating in Google
Cloud.
- Security/risk concerns and restrictions from their internal security, risk, and compliance teams.
- Regulatory and compliance approval from external auditors.

The Terraform configurations in this repository provide customers with an opinionated architecture
that incorporates and documents best practices for a performant and scalable design, combined with
security by default for control, logging and evidence generation. It can be  simply deployed by
customers through a Terraform workflow.

## Resources created by this module

- Data Ingestion
  - Data Ingestion bucket
  - Data Flow Bucket
  - Data Ingestion Pub/Sub topic
  - DataFlow Controller Service Account
- Data Governance
  - Cloud KMS Keyring
  - Cloud KMS Keys
    - Data Ingestion Key
    - BigQuery Key
    - Re-Identification Key
    - De-Identification Key
  - Encrypters and Decrypters roles
- Non-confidential Data
  - Big Query Dataset
- Confidential Data
  - DataFlow Bucket
  - BigQuery Dataset
  - DataFlow Controller Service Account
- VPC Service Control
  - Data Ingestion Perimeter
  - Data Governance Perimeter
  - Confidential Data Perimeter
  - Access Level policy
  - VPC SC Bridges between:
    - Confidential Data and Data Governance
    - Confidential Data and Data Ingestion
    - Data Ingestion and Data Governance
- IAM
  - Remove Owner roles
  - Grant roles to groups listed at [Security Groups](#security-groups) section
- Organization Policies
  - Restrict Protocol Forwarding Creation Policy
  - Disable Serial Port Logging Policy
  - Require OS Login
  - Trusted VPC Subnetwork Policy
  - VM External IP Access Policy
  - Location Restriction Policy
  - Service Account Disable Key Policy
  - Service Account Disable Creation Policy

## Disclaimer

When using this blueprint, it is important to understand how you manage [separation of duties](https://cloud.google.com/kms/docs/separation-of-duties). We recommend you remove all primitive `owner` roles in the projects used as inputs for the *Data Warehouse module*. The secured data warehouse itself does not need any primitive owner roles for correct operations.

When using this blueprint in the example mode or when using this blueprint to create the new projects with default configurations for the deployment, we automatically remove the owner role as it has too broad access.

However, if you choose to use this blueprint with pre-existing projects in your organization, we will not proactively remove any pre-existing owner role assignments, as we won’t know your intent for or dependency on these role assignments in your pre-existing workloads. The pre-existing presence of these roles does expand the attack and risk surface of the resulting deployment. Therefore, we highly recommend you review your use of owner roles in these pre-existing cases and see if you can eliminate them to improve your resulting security posture. Only you can determine the appropriate trade-off to meet your business requirements.

You can check the current situation of your project with either of the following methods:

- Using [Security Health Analytics](https://cloud.google.com/security-command-center/docs/concepts-vulnerabilities-findings#security-health-analytics-detectors) (SHA), checking the [KMS vulnerability findings](https://cloud.google.com/security-command-center/docs/concepts-vulnerabilities-findings#kms-findings), for the Detector `KMS_PROJECT_HAS_OWNER`.
  - You can search for the SHA findings with category `KMS_PROJECT_HAS_OWNER` in the Security Command Center in the  Google Cloud Console.
- You can also use Cloud Asset Inventory [search-all-iam-policies](https://cloud.google.com/asset-inventory/docs/searching-iam-policies#search_policies) gcloud command doing a [Query by role](https://cloud.google.com/asset-inventory/docs/searching-iam-policies#examples_query_by_role) to search for owner of the project.

See the [terraform-example-foundation](https://github.com/terraform-google-modules/terraform-example-foundation) for additional good practices.

## Usage

Basic usage of this module is as follows:

```hcl
module ""secured_data_warehouse"" {
  source  = ""terraform-google-modules/secured-data-warehouse/google""
  version = ""~> 0.1""

  org_id                           = ORG_ID
  data_governance_project_id       = DATA_GOVERNANCE_PROJECT_ID
  confidential_data_project_id     = CONFIDENTIAL_DATA_PROJECT_ID
  non_confidential_data_project_id = NON_CONFIDENTIAL_DATA_PROJECT_ID
  data_ingestion_project_id        = DATA_INGESTION_PROJECT_ID
  sdx_project_number               = EXTERNAL_TEMPLATE_PROJECT_NUMBER
  terraform_service_account        = TERRAFORM_SERVICE_ACCOUNT
  access_context_manager_policy_id = ACCESS_CONTEXT_MANAGER_POLICY_ID
  bucket_name                      = DATA_INGESTION_BUCKET_NAME
  pubsub_resource_location         = PUBSUB_RESOURCE_LOCATION
  location                         = LOCATION
  trusted_locations                = TRUSTED_LOCATIONS
  dataset_id                       = DATASET_ID
  confidential_dataset_id          = CONFIDENTIAL_DATASET_ID
  cmek_keyring_name                = CMEK_KEYRING_NAME
  perimeter_additional_members     = PERIMETER_ADDITIONAL_MEMBERS
  data_engineer_group              = DATA_ENGINEER_GROUP
  data_analyst_group               = DATA_ANALYST_GROUP
  security_analyst_group           = SECURITY_ANALYST_GROUP
  network_administrator_group      = NETWORK_ADMINISTRATOR_GROUP
  security_administrator_group     = SECURITY_ADMINISTRATOR_GROUP
  delete_contents_on_destroy       = false
}
```

**Note:** There are three inputs related to GCP Locations in the module:

- `pubsub_resource_location`: is used to define which GCP location will be used to [Restrict Pub/Sub resource locations](https://cloud.google.com/pubsub/docs/resource-location-restriction). This policy offers a way to ensure that messages published to a topic are never persisted outside of a Google Cloud regions you specify, regardless of where the publish requests originate. **Zones or multi-region locations are not supported**.
- `location`: is used to define which GCP region will be used for all other resources created: [Cloud Storage buckets](https://cloud.google.com/storage/docs/locations), [BigQuery datasets](https://cloud.google.com/bigquery/docs/locations), and [Cloud KMS key rings](https://cloud.google.com/kms/docs/locations). **Multi-region locations are supported**.
- `trusted_locations`: is a list of locations that are used to set an [Organization Policy](https://cloud.google.com/resource-manager/docs/organization-policy/defining-locations#location_types) that restricts the GCP locations that can be used in the projects of the Secured Data Warehouse. Both `pubsub_resource_location` and `location` must respect this restriction.

<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->
## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| access\_context\_manager\_policy\_id | The id of the default Access Context Manager policy. Can be obtained by running `gcloud access-context-manager policies list --organization YOUR-ORGANIZATION_ID --format=""value(name)""`. | `string` | `""""` | no |
| bucket\_class | The storage class for the bucket being provisioned. | `string` | `""STANDARD""` | no |
| bucket\_lifecycle\_rules | List of lifecycle rules to configure. Format is the same as described in provider documentation https://www.terraform.io/docs/providers/google/r/storage_bucket.html#lifecycle_rule except condition.matches\_storage\_class should be a comma delimited string. | <pre>list(object({<br>    # Object with keys:<br>    # - type - The type of the action of this Lifecycle Rule. Supported values: Delete and SetStorageClass.<br>    # - storage_class - (Required if action type is SetStorageClass) The target Storage Class of objects affected by this Lifecycle Rule.<br>    action = any<br><br>    # Object with keys:<br>    # - age - (Optional) Minimum age of an object in days to satisfy this condition.<br>    # - created_before - (Optional) Creation date of an object in RFC 3339 (e.g. 2017-06-13) to satisfy this condition.<br>    # - with_state - (Optional) Match to live and/or archived objects. Supported values include: ""LIVE"", ""ARCHIVED"", ""ANY"".<br>    # - matches_storage_class - (Optional) Storage Class of objects to satisfy this condition. Supported values include: MULTI_REGIONAL, REGIONAL, NEARLINE, COLDLINE, STANDARD, DURABLE_REDUCED_AVAILABILITY.<br>    # - matches_prefix - (Optional) One or more matching name prefixes to satisfy this condition.<br>    # - matches_suffix - (Optional) One or more matching name suffixes to satisfy this condition<br>    # - num_newer_versions - (Optional) Relevant only for versioned objects. The number of newer versions of an object to satisfy this condition.<br>    condition = any<br>  }))</pre> | <pre>[<br>  {<br>    ""action"": {<br>      ""type"": ""Delete""<br>    },<br>    ""condition"": {<br>      ""age"": 30,<br>      ""matches_storage_class"": ""STANDARD"",<br>      ""with_state"": ""ANY""<br>    }<br>  }<br>]</pre> | no |
| bucket\_name | The name of the bucket being provisioned. | `string` | n/a | yes |
| cmek\_keyring\_name | The Keyring prefix name for the KMS Customer Managed Encryption Keys being provisioned. | `string` | n/a | yes |
| confidential\_data\_access\_level\_allowed\_device\_management\_levels | Condition - A list of allowed device management levels. An empty list allows all management levels. | `list(string)` | `[]` | no |
| confidential\_data\_access\_level\_allowed\_encryption\_statuses | Condition - A list of allowed encryptions statuses. An empty list allows all statuses. | `list(string)` | `[]` | no |
| confidential\_data\_access\_level\_combining\_function | How the conditions list should be combined to determine if a request is granted this AccessLevel. If AND is used, each Condition must be satisfied for the AccessLevel to be applied. If OR is used, at least one Condition must be satisfied for the AccessLevel to be applied. | `string` | `""AND""` | no |
| confidential\_data\_access\_level\_ip\_subnetworks | Condition - A list of CIDR block IP subnetwork specification. May be IPv4 or IPv6. Note that for a CIDR IP address block, the specified IP address portion must be properly truncated (that is, all the host bits must be zero) or the input is considered malformed. For example, ""192.0.2.0/24"" is accepted but ""192.0.2.1/24"" is not. Similarly, for IPv6, ""2001:db8::/32"" is accepted whereas ""2001:db8::1/32"" is not. The originating IP of a request must be in one of the listed subnets in order for this Condition to be true. If empty, all IP addresses are allowed. | `list(string)` | `[]` | no |
| confidential\_data\_access\_level\_minimum\_version | The minimum allowed OS version. If not set, any version of this OS satisfies the constraint. Format: ""major.minor.patch"" such as ""10.5.301"", ""9.2.1"". | `string` | `""""` | no |
| confidential\_data\_access\_level\_negate | Whether to negate the Condition. If true, the Condition becomes a NAND over its non-empty fields, each field must be false for the Condition overall to be satisfied. | `bool` | `false` | no |
| confidential\_data\_access\_level\_os\_type | The operating system type of the device. | `string` | `""OS_UNSPECIFIED""` | no |
| confidential\_data\_access\_level\_regions | Condition - The request must originate from one of the provided countries or regions. Format: A valid ISO 3166-1 alpha-2 code. | `list(string)` | `[]` | no |
| confidential\_data\_access\_level\_require\_corp\_owned | Condition - Whether the device needs to be corp owned. | `bool` | `false` | no |
| confidential\_data\_access\_level\_require\_screen\_lock | Condition - Whether or not screenlock is required for the DevicePolicy to be true. | `bool` | `false` | no |
| confidential\_data\_dataflow\_deployer\_identities | List of members in the standard GCP form: user:{email}, serviceAccount:{email} that will deploy Dataflow jobs in the Confidential Data project. These identities will be added to the VPC-SC secure data exchange egress rules. | `list(string)` | `[]` | no |
| confidential\_data\_egress\_policies | A list of all [egress policies](https://cloud.google.com/vpc-service-controls/docs/ingress-egress-rules#egress-rules-reference) for the Confidential Data perimeter, each list object has a `from` and `to` value that describes egress\_from and egress\_to. See also [secure data exchange](https://cloud.google.com/vpc-service-controls/docs/secure-data-exchange#allow_access_to_a_google_cloud_resource_outside_the_perimeter) and the [VPC-SC](https://github.com/terraform-google-modules/terraform-google-vpc-service-controls/blob/v3.1.0/modules/regular_service_perimeter/README.md) module.  You can use the placeholders `DATA_INGESTION_DATAFLOW_CONTROLLER_SA` and `CONFIDENTIAL_DATA_DATAFLOW_CONTROLLER_SA` to refer to the services accounts being created by the main module. | <pre>list(object({<br>    from = any<br>    to   = any<br>  }))</pre> | `[]` | no |
| confidential\_data\_ingress\_policies | A list of all [ingress policies](https://cloud.google.com/vpc-service-controls/docs/ingress-egress-rules#ingress-rules-reference), each list object has a `from` and `to` value that describes ingress\_from and ingress\_to.<br><br>Example: `[{ from={ sources={ resources=[], access_levels=[] }, identities=[], identity_type=""ID_TYPE"" }, to={ resources=[], operations={ ""SRV_NAME""={ OP_TYPE=[] }}}}]`<br><br>Valid Values:<br>`ID_TYPE` = `null` or `IDENTITY_TYPE_UNSPECIFIED` (only allow indentities from list); `ANY_IDENTITY`; `ANY_USER_ACCOUNT`; `ANY_SERVICE_ACCOUNT`<br>`SRV_NAME` = ""`*`"" (allow all services) or [Specific Services](https://cloud.google.com/vpc-service-controls/docs/supported-products#supported_products)<br>`OP_TYPE` = [methods](https://cloud.google.com/vpc-service-controls/docs/supported-method-restrictions) or [permissions](https://cloud.google.com/vpc-service-controls/docs/supported-method-restrictions). You can use the placeholders `DATA_INGESTION_DATAFLOW_CONTROLLER_SA` and `CONFIDENTIAL_DATA_DATAFLOW_CONTROLLER_SA` to refer to the services accounts being created by the main module. | <pre>list(object({<br>    from = any<br>    to   = any<br>  }))</pre> | `[]` | no |
| confidential\_data\_perimeter | Existing confidential data perimeter to be used instead of the auto-created perimeter. The service account provided in the variable `terraform_service_account` must be in an access level member list for this perimeter **before** this perimeter can be used in this module. | `string` | `""""` | no |
| confidential\_data\_project\_id | Project where the confidential datasets and tables are created. | `string` | n/a | yes |
| confidential\_data\_required\_access\_levels | Condition - A list of other access levels defined in the same Policy, referenced by resource name. Referencing an AccessLevel which does not exist is an error. All access levels listed must be granted for the Condition to be true. | `list(string)` | `[]` | no |
| confidential\_dataset\_id | Unique ID for the confidential dataset being provisioned. | `string` | `""secured_dataset""` | no |
| data\_analyst\_group | Google Cloud IAM group that analyzes the data in the warehouse. | `string` | n/a | yes |
| data\_engineer\_group | Google Cloud IAM group that sets up and maintains the data pipeline and warehouse. | `string` | n/a | yes |
| data\_governance\_access\_level\_allowed\_device\_management\_levels | Condition - A list of allowed device management levels. An empty list allows all management levels. | `list(string)` | `[]` | no |
| data\_governance\_access\_level\_allowed\_encryption\_statuses | Condition - A list of allowed encryptions statuses. An empty list allows all statuses. | `list(string)` | `[]` | no |
| data\_governance\_access\_level\_combining\_function | How the conditions list should be combined to determine if a request is granted this AccessLevel. If AND is used, each Condition must be satisfied for the AccessLevel to be applied. If OR is used, at least one Condition must be satisfied for the AccessLevel to be applied. | `string` | `""AND""` | no |
| data\_governance\_access\_level\_ip\_subnetworks | Condition - A list of CIDR block IP subnetwork specification. May be IPv4 or IPv6. Note that for a CIDR IP address block, the specified IP address portion must be properly truncated (that is, all the host bits must be zero) or the input is considered malformed. For example, ""192.0.2.0/24"" is accepted but ""192.0.2.1/24"" is not. Similarly, for IPv6, ""2001:db8::/32"" is accepted whereas ""2001:db8::1/32"" is not. The originating IP of a request must be in one of the listed subnets in order for this Condition to be true. If empty, all IP addresses are allowed. | `list(string)` | `[]` | no |
| data\_governance\_access\_level\_minimum\_version | The minimum allowed OS version. If not set, any version of this OS satisfies the constraint. Format: ""major.minor.patch"" such as ""10.5.301"", ""9.2.1"". | `string` | `""""` | no |
| data\_governance\_access\_level\_negate | Whether to negate the Condition. If true, the Condition becomes a NAND over its non-empty fields, each field must be false for the Condition overall to be satisfied. | `bool` | `false` | no |
| data\_governance\_access\_level\_os\_type | The operating system type of the device. | `string` | `""OS_UNSPECIFIED""` | no |
| data\_governance\_access\_level\_regions | Condition - The request must originate from one of the provided countries or regions. Format: A valid ISO 3166-1 alpha-2 code. | `list(string)` | `[]` | no |
| data\_governance\_access\_level\_require\_corp\_owned | Condition - Whether the device needs to be corp owned. | `bool` | `false` | no |
| data\_governance\_access\_level\_require\_screen\_lock | Condition - Whether or not screenlock is required for the DevicePolicy to be true. | `bool` | `false` | no |
| data\_governance\_egress\_policies | A list of all [egress policies](https://cloud.google.com/vpc-service-controls/docs/ingress-egress-rules#egress-rules-reference) for the Data Governance perimeter, each list object has a `from` and `to` value that describes egress\_from and egress\_to. See also [secure data exchange](https://cloud.google.com/vpc-service-controls/docs/secure-data-exchange#allow_access_to_a_google_cloud_resource_outside_the_perimeter) and the [VPC-SC](https://github.com/terraform-google-modules/terraform-google-vpc-service-controls/blob/v3.1.0/modules/regular_service_perimeter/README.md) module.  You can use the placeholders `DATA_INGESTION_DATAFLOW_CONTROLLER_SA` and `CONFIDENTIAL_DATA_DATAFLOW_CONTROLLER_SA` to refer to the services accounts being created by the main module. | <pre>list(object({<br>    from = any<br>    to   = any<br>  }))</pre> | `[]` | no |
| data\_governance\_ingress\_policies | A list of all [ingress policies](https://cloud.google.com/vpc-service-controls/docs/ingress-egress-rules#ingress-rules-reference), each list object has a `from` and `to` value that describes ingress\_from and ingress\_to.<br><br>Example: `[{ from={ sources={ resources=[], access_levels=[] }, identities=[], identity_type=""ID_TYPE"" }, to={ resources=[], operations={ ""SRV_NAME""={ OP_TYPE=[] }}}}]`<br><br>Valid Values:<br>`ID_TYPE` = `null` or `IDENTITY_TYPE_UNSPECIFIED` (only allow indentities from list); `ANY_IDENTITY`; `ANY_USER_ACCOUNT`; `ANY_SERVICE_ACCOUNT`<br>`SRV_NAME` = ""`*`"" (allow all services) or [Specific Services](https://cloud.google.com/vpc-service-controls/docs/supported-products#supported_products)<br>`OP_TYPE` = [methods](https://cloud.google.com/vpc-service-controls/docs/supported-method-restrictions) or [permissions](https://cloud.google.com/vpc-service-controls/docs/supported-method-restrictions).  You can use the placeholders `DATA_INGESTION_DATAFLOW_CONTROLLER_SA` and `CONFIDENTIAL_DATA_DATAFLOW_CONTROLLER_SA` to refer to the services accounts being created by the main module. | <pre>list(object({<br>    from = any<br>    to   = any<br>  }))</pre> | `[]` | no |
| data\_governance\_perimeter | Existing data governance perimeter to be used instead of the auto-created perimeter. The service account provided in the variable `terraform_service_account` must be in an access level member list for this perimeter **before** this perimeter can be used in this module. | `string` | `""""` | no |
| data\_governance\_project\_id | The ID of the project in which the data governance resources will be created. | `string` | n/a | yes |
| data\_governance\_required\_access\_levels | Condition - A list of other access levels defined in the same Policy, referenced by resource name. Referencing an AccessLevel which does not exist is an error. All access levels listed must be granted for the Condition to be true. | `list(string)` | `[]` | no |
| data\_ingestion\_access\_level\_allowed\_device\_management\_levels | Condition - A list of allowed device management levels. An empty list allows all management levels. | `list(string)` | `[]` | no |
| data\_ingestion\_access\_level\_allowed\_encryption\_statuses | Condition - A list of allowed encryptions statuses. An empty list allows all statuses. | `list(string)` | `[]` | no |
| data\_ingestion\_access\_level\_combining\_function | How the conditions list should be combined to determine if a request is granted this AccessLevel. If AND is used, each Condition must be satisfied for the AccessLevel to be applied. If OR is used, at least one Condition must be satisfied for the AccessLevel to be applied. | `string` | `""AND""` | no |
| data\_ingestion\_access\_level\_ip\_subnetworks | Condition - A list of CIDR block IP subnetwork specification. May be IPv4 or IPv6. Note that for a CIDR IP address block, the specified IP address portion must be properly truncated (that is, all the host bits must be zero) or the input is considered malformed. For example, ""192.0.2.0/24"" is accepted but ""192.0.2.1/24"" is not. Similarly, for IPv6, ""2001:db8::/32"" is accepted whereas ""2001:db8::1/32"" is not. The originating IP of a request must be in one of the listed subnets in order for this Condition to be true. If empty, all IP addresses are allowed. | `list(string)` | `[]` | no |
| data\_ingestion\_access\_level\_minimum\_version | The minimum allowed OS version. If not set, any version of this OS satisfies the constraint. Format: ""major.minor.patch"" such as ""10.5.301"", ""9.2.1"". | `string` | `""""` | no |
| data\_ingestion\_access\_level\_negate | Whether to negate the Condition. If true, the Condition becomes a NAND over its non-empty fields, each field must be false for the Condition overall to be satisfied. | `bool` | `false` | no |
| data\_ingestion\_access\_level\_os\_type | The operating system type of the device. | `string` | `""OS_UNSPECIFIED""` | no |
| data\_ingestion\_access\_level\_regions | Condition - The request must originate from one of the provided countries or regions. Format: A valid ISO 3166-1 alpha-2 code. | `list(string)` | `[]` | no |
| data\_ingestion\_access\_level\_require\_corp\_owned | Condition - Whether the device needs to be corp owned. | `bool` | `false` | no |
| data\_ingestion\_access\_level\_require\_screen\_lock | Condition - Whether or not screenlock is required for the DevicePolicy to be true. | `bool` | `false` | no |
| data\_ingestion\_dataflow\_deployer\_identities | List of members in the standard GCP form: user:{email}, serviceAccount:{email} that will deploy Dataflow jobs in the Data Ingestion project. These identities will be added to the VPC-SC secure data exchange egress rules. | `list(string)` | `[]` | no |
| data\_ingestion\_egress\_policies | A list of all [egress policies](https://cloud.google.com/vpc-service-controls/docs/ingress-egress-rules#egress-rules-reference) for the Data Ingestion perimeter, each list object has a `from` and `to` value that describes egress\_from and egress\_to. See also [secure data exchange](https://cloud.google.com/vpc-service-controls/docs/secure-data-exchange#allow_access_to_a_google_cloud_resource_outside_the_perimeter) and the [VPC-SC](https://github.com/terraform-google-modules/terraform-google-vpc-service-controls/blob/v3.1.0/modules/regular_service_perimeter/README.md) module.  You can use the placeholders `DATA_INGESTION_DATAFLOW_CONTROLLER_SA` and `CONFIDENTIAL_DATA_DATAFLOW_CONTROLLER_SA` to refer to the services accounts being created by the main module. | <pre>list(object({<br>    from = any<br>    to   = any<br>  }))</pre> | `[]` | no |
| data\_ingestion\_ingress\_policies | A list of all [ingress policies](https://cloud.google.com/vpc-service-controls/docs/ingress-egress-rules#ingress-rules-reference), each list object has a `from` and `to` value that describes ingress\_from and ingress\_to.<br><br>Example: `[{ from={ sources={ resources=[], access_levels=[] }, identities=[], identity_type=""ID_TYPE"" }, to={ resources=[], operations={ ""SRV_NAME""={ OP_TYPE=[] }}}}]`<br><br>Valid Values:<br>`ID_TYPE` = `null` or `IDENTITY_TYPE_UNSPECIFIED` (only allow indentities from list); `ANY_IDENTITY`; `ANY_USER_ACCOUNT`; `ANY_SERVICE_ACCOUNT`<br>`SRV_NAME` = ""`*`"" (allow all services) or [Specific Services](https://cloud.google.com/vpc-service-controls/docs/supported-products#supported_products)<br>`OP_TYPE` = [methods](https://cloud.google.com/vpc-service-controls/docs/supported-method-restrictions) or [permissions](https://cloud.google.com/vpc-service-controls/docs/supported-method-restrictions).  You can use the placeholders `DATA_INGESTION_DATAFLOW_CONTROLLER_SA` and `CONFIDENTIAL_DATA_DATAFLOW_CONTROLLER_SA` to refer to the services accounts being created by the main module. | <pre>list(object({<br>    from = any<br>    to   = any<br>  }))</pre> | `[]` | no |
| data\_ingestion\_perimeter | Existing data ingestion perimeter to be used instead of the auto-created perimeter. The service account provided in the variable `terraform_service_account` must be in an access level member list for this perimeter **before** this perimeter can be used in this module. | `string` | `""""` | no |
| data\_ingestion\_project\_id | The ID of the project in which the data ingestion resources will be created | `string` | n/a | yes |
| data\_ingestion\_required\_access\_levels | Condition - A list of other access levels defined in the same Policy, referenced by resource name. Referencing an AccessLevel which does not exist is an error. All access levels listed must be granted for the Condition to be true. | `list(string)` | `[]` | no |
| dataset\_default\_table\_expiration\_ms | TTL of tables using the dataset in MS. The default value is null. | `number` | `null` | no |
| dataset\_description | Dataset description. | `string` | `""Data-ingestion dataset""` | no |
| dataset\_id | Unique ID for the dataset being provisioned. | `string` | n/a | yes |
| dataset\_name | Friendly name for the dataset being provisioned. | `string` | `""Data-ingestion dataset""` | no |
| delete\_contents\_on\_destroy | (Optional) If set to true, delete all the tables in the dataset when destroying the resource; otherwise, destroying the resource will fail if tables are present. | `bool` | `false` | no |
| enable\_bigquery\_read\_roles\_in\_data\_ingestion | (Optional) If set to true, it will grant to the dataflow controller service account created in the data ingestion project the necessary roles to read from a bigquery table. | `bool` | `false` | no |
| key\_rotation\_period\_seconds | Rotation period for keys. The default value is 30 days. | `string` | `""2592000s""` | no |
| kms\_key\_protection\_level | The protection level to use when creating a key. Possible values: [""SOFTWARE"", ""HSM""] | `string` | `""HSM""` | no |
| labels | (Optional) Labels attached to Data Warehouse resources. | `map(string)` | `{}` | no |
| location | The location for the KMS Customer Managed Encryption Keys, Cloud Storage Buckets, and Bigquery datasets. This location can be a multi-region. | `string` | `""us-east4""` | no |
| network\_administrator\_group | Google Cloud IAM group that reviews network configuration. Typically, this includes members of the networking team. | `string` | n/a | yes |
| non\_confidential\_data\_project\_id | The ID of the project in which the Bigquery will be created. | `string` | n/a | yes |
| org\_id | GCP Organization ID. | `string` | n/a | yes |
| perimeter\_additional\_members | The list additional members to be added on perimeter access. Prefix user: (user:email@email.com) or serviceAccount: (serviceAccount:my-service-account@email.com) is required. | `list(string)` | `[]` | no |
| pubsub\_resource\_location | The location in which the messages published to Pub/Sub will be persisted. This location cannot be a multi-region. | `string` | `""us-east4""` | no |
| remove\_owner\_role | (Optional) If set to true, remove all owner roles in all projects in case it has been found in some project. | `bool` | `false` | no |
| sdx\_project\_number | The Project Number to configure Secure data exchange with egress rule for dataflow templates. Required if using a dataflow job template from a private storage bucket outside of the perimeter. | `string` | `""""` | no |
| security\_administrator\_group | Google Cloud IAM group that administers security configurations in the organization(org policies, KMS, VPC service perimeter). | `string` | n/a | yes |
| security\_analyst\_group | Google Cloud IAM group that monitors and responds to security incidents. | `string` | n/a | yes |
| terraform\_service\_account | The email address of the service account that will run the Terraform code. | `string` | n/a | yes |
| trusted\_locations | This is a list of trusted regions where location-based GCP resources can be created. | `list(string)` | <pre>[<br>  ""us-locations""<br>]</pre> | no |
| trusted\_subnetworks | The URI of the subnetworks where resources are going to be deployed. | `list(string)` | `[]` | no |

## Outputs

| Name | Description |
|------|-------------|
| blueprint\_type | Type of blueprint this module represents. |
| cmek\_bigquery\_crypto\_key | The Customer Managed Crypto Key for the BigQuery service. |
| cmek\_bigquery\_crypto\_key\_name | The Customer Managed Crypto Key name for the BigQuery service. |
| cmek\_confidential\_bigquery\_crypto\_key | The Customer Managed Crypto Key for the confidential BigQuery service. |
| cmek\_confidential\_bigquery\_crypto\_key\_name | The Customer Managed Crypto Key name for the confidential BigQuery service. |
| cmek\_data\_ingestion\_crypto\_key | The Customer Managed Crypto Key for the data ingestion crypto boundary. |
| cmek\_data\_ingestion\_crypto\_key\_name | The Customer Managed Crypto Key name for the data ingestion crypto boundary. |
| cmek\_keyring\_name | The Keyring name for the KMS Customer Managed Encryption Keys. |
| cmek\_reidentification\_crypto\_key | The Customer Managed Crypto Key for the Confidential crypto boundary. |
| cmek\_reidentification\_crypto\_key\_name | The Customer Managed Crypto Key name for the reidentification crypto boundary. |
| confidential\_access\_level\_name | Access context manager access level name. |
| confidential\_data\_dataflow\_bucket\_name | The name of the bucket created for dataflow in the confidential data pipeline. |
| confidential\_dataflow\_controller\_service\_account\_email | The confidential Dataflow controller service account email. See https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#specifying_a_user-managed_controller_service_account. |
| confidential\_service\_perimeter\_name | Access context manager service perimeter name. |
| data\_governance\_access\_level\_name | Access context manager access level name. |
| data\_governance\_service\_perimeter\_name | Access context manager service perimeter name. |
| data\_ingestion\_access\_level\_name | Access context manager access level name. |
| data\_ingestion\_bigquery\_dataset | The bigquery dataset created for data ingestion pipeline. |
| data\_ingestion\_bucket\_name | The name of the bucket created for the data ingestion pipeline. |
| data\_ingestion\_dataflow\_bucket\_name | ",VRAI
GoogleContainerTools/kpt-config-sync,DevOPs,Application System,2025-05-15T22:08:23Z,2025-04-10T17:06:05Z,0,0,0,0,0,0,0,11,2022-03-02T07:20:59Z,2025-04-07T18:00:13Z,49947,250,Go,VRAI,45,FAUX,23,"configuration-management,gitops,kubernetes",23,"Config Sync - used to sync Git, OCI and Helm charts to your clusters.",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,18,"# Config Sync

Config Sync lets cluster operators and platform administrators deploy consistent configurations
and policies across multiple clusters. This simplifies and automates configuration and policy
management at scale. See [config-sync-overview] for a more detailed introduction.

## Start using Config Sync

Follow the [installation guide] to install OSS Config Sync. If you are
using GKE or Anthos, you can also install Config Sync through the Google Cloud GUI or Google
Cloud CLI.

Once Config Sync is installed, follow the [usage guide] to configure OSS Config Sync.

## Start contributing to Config Sync

We welcome contributions to Config Sync from the community. Take a look at our
[contribution guide] and [development guide] to get started.

## Community, discussion and support
You can reach the maintainers of this project at:

* [Config Sync discussions]
* [Config Sync issues]

[installation guide]: docs/installation.md
[usage guide]: docs/usage.md
[contribution guide]: docs/contributing.md
[development guide]: docs/development.md
[k8s slack]: https://slack.k8s.io
[Config Sync issues]: https://github.com/GoogleContainerTools/kpt-config-sync/issues
[Config Sync discussions]: https://github.com/GoogleContainerTools/kpt-config-sync/discussions
[config-sync-overview]: https://cloud.google.com/anthos-config-management/docs/config-sync-overview",VRAI
google/gke-policy-automation,Toolkit,Toolkit,2025-03-21T09:28:45Z,2023-12-29T09:47:02Z,0,187,0,0,0,0,0,0,2022-03-15T23:39:19Z,2025-04-03T18:22:58Z,2937,521,Go,VRAI,27,FAUX,5,"gcp,gke,opa,policy,rego",5,Tool and policy library for reviewing Google Kubernetes Engine clusters against best practices,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,11,"<!-- markdownlint-disable MD041 -->
<img src=""assets/gke-policy-automation-logo.png"" alt=""GKE Policy Automation logo""
title=""GKE Policy Automation"" align=""left"" height=""70"" />
<!-- markdownlint-enable MD041 -->

# GKE Policy Automation

This repository contains the tool and the [policy library](./gke-policies-v2) for validating [GKE](https://cloud.google.com/kubernetes-engine)
clusters against configuration [best practices](#checking-best-practices)
and [scalability limits](#checking-scalability-limits).

[![Build](https://github.com/google/gke-policy-automation/actions/workflows/build.yml/badge.svg)](https://github.com/google/gke-policy-automation/actions/workflows/build.yml)
[![Policy tests](https://github.com/google/gke-policy-automation/actions/workflows/policy-test.yml/badge.svg)](https://github.com/google/gke-policy-automation/actions/workflows/policy-test.yml)
[![Version](https://img.shields.io/github/v/release/google/gke-policy-automation?label=version)](https://img.shields.io/github/v/release/google/gke-policy-automation?label=version)
[![Go Report Card](https://goreportcard.com/badge/github.com/google/gke-policy-automation)](https://goreportcard.com/report/github.com/google/gke-policy-automation)
[![GoDoc](https://godoc.org/github.com/google/gke-policy-automation?status.svg)](https://godoc.org/github.com/google/gke-policy-automation)
![GitHub](https://img.shields.io/github/license/google/gke-policy-automation)

![GKE Policy Automation Demo](./assets/gke-policy-automation-demo.gif)

Note: this is not an officially supported Google product.

---

## Table of Contents

* [Installation](#installation)
* [Usage](#usage)
  * [Checking best practices](#checking-best-practices)
  * [Checking scalability limits](#checking-scalability-limits)
  * [Common check options](#common-check-options)
  * [Defining inputs](#defining-inputs)
  * [Defining outputs](#defining-outputs)
  * [Authentication](#authentication)
  * [Serverless execution](#serverless-execution)
* [Contributing](#contributing)
* [License](#license)

## Installation

### Container image

The container images with GKE Policy Automation tool are hosted on `ghcr.io`. Check the [packages page](https://github.com/google/gke-policy-automation/pkgs/container/gke-policy-automation)
for a list of all tags and versions.

```sh
docker pull ghcr.io/google/gke-policy-automation:latest
docker run --rm ghcr.io/google/gke-policy-automation check \
-project my-project -location europe-west2 -name my-cluster
```

### Krew

The GKE Policy Automation is available as a [Krew](https://krew.sigs.k8s.io) plugin.

```sh
kubectl krew install gke-policy
kubectl gke-policy check --discovery -p my-project
```

### Binary

Binaries for Linux, Windows and Mac are available as tarballs in the
[release page](https://github.com/google/gke-policy-automation/releases).

### Source code

Go [v1.23](https://go.dev/doc/install) or newer is required. Check the [development guide](./DEVELOPMENT.md)
for more details.

```sh
git clone https://github.com/google/gke-policy-automation.git
cd gke-policy-automation
make build
./gke-policy check \
--project my-project --location europe-west2 --name my-cluster
```

## Usage

**Full user guide**: [GKE Policy Automation User Guide](./docs/user-guide.md).

### Checking best practices

The configuration best practices check validates GKE clusters against the set of
GKE configuration policies.

```sh
./gke-policy check \
--project my-project --location europe-west2 --name my-cluster
```

### Checking scalability limits

The scalability limits check validates GKE clusters against the GKE quotas and limits.
The tool will report violations when the current values will cross the certain thresholds.

```sh
./gke-policy check scalability \
--project my-project --location europe-west2 --name my-cluster
```

**NOTE**: you need to run `kube-state-metrics` to export cluster metrics to use cluster scalability
limits check. Refer to the [kube-state-metrics installation & configuration guide](./docs/kube-state-metrics.md)
for more details.

The tool assumes that metrics are available in Cloud Monitoring, i.e. in a result of
[Google Cloud Managed Service for Prometheus](https://cloud.google.com/stackdriver/docs/managed-prometheus)
based metrics collection. If self managed Prometheus collection is used, be sure to:

* Configure Prometheus scraping for `kube-state-metrics` using `PodMonitor` / `ServiceMonitor` and
 corresponding annotations, i.e. `prometheus.io/scrape`
* Configure custom Prometheus API server address in a tool

  * Prepare `config.yaml`:

     ```yaml
     inputs:
       metricsAPI:
         enabled: true
         address: http://my-prometheus-svc:8080 # Prometheus server API endpoint
         username: user   # username for basic authentication (optional)
         password: secret # password for basic authentication (optional)
     ```

  * Run `./gke-policy check scalability -c config.yaml`

### Common check options

The common options apply to all types of check commands.

#### Selecting multiple clusters

Check multiple GKE clusters using the config file.

```sh
./gke-policy check -c config.yaml
```

The `config.yaml` file:

```yaml
clusters:
  - name: prod-central
    project: my-project-one
    location: europe-central2
  - id: projects/my-project-two/locations/europe-west2/clusters/prod-west
```

#### Using cluster discovery

Check multiple clusters by discovering them in a selected GCP projects, folders or in the entire organization
using [Cloud Asset Inventory](https://cloud.google.com/asset-inventory) and configuration file.

```sh
./gke-policy check -c config.yaml
```

The `config.yaml` file:

```yaml
clusterDiscovery:
  enabled: true
  organization: ""123456789012""
```

It is possible to use cluster discovery on a given project using command line flags only:

```sh
./gke-policy check --discovery -p my-project-id
```

### Defining inputs

Data for cluster validation can be retrieved from multiple data sources,
eg. GKE API, Cloud Monitoring API or local JSON file exported from GKE API.
For best practices checks GKE API is enabled by default,
and for scalability checks, metrics API is enabled as well.
Check [Inputs user guide](./docs/user-guide.md#inputs) for more details.

Example:

* Metrics API input from Cloud Monitoring configured in dedicated project
and other values set with defaults for scalability check

```yaml
inputs:
  gkeAPI:
    enabled: true
  gkeLocal:
    enabled: false
    file:
  metricsAPI:
    enabled: true
    project: sample-project
    metrics:
```

### Defining outputs

The cluster validation results can be published to multiple outputs, including JSON file, Pub/Sub topic,
Cloud Storage bucket or Security Command Center. Check [Outputs user guide](./docs/user-guide.md#outputs)
for more details.

Examples:

* JSON file output with command line flags

  ```sh
  ./gke-policy check \
  --project my-project --location europe-west2 --name my-cluster \
  --out-file output.json
  ```

* All outputs enabled in a configuration file

  ```yaml
  clusters:
    - name: my-cluster
      project: my-project
      location: europe-west2
  outputs:
    - file: output.json
    - pubsub:
        topic: Test
        project: my-pubsub-project
    - cloudStorage:
        bucket: bucket-name
        path: path/to/write
    - securityCommandCenter:
        organization: ""153963171798""
  ```

#### Custom Policy repository

Specify custom repository with the GKE cluster best practices and check the cluster against them.

* Custom policies source with command line flags

  ```sh
  ./gke-policy check \
  --project my-project --location europe-west2 --name my-cluster \
  --git-policy-repo ""https://github.com/google/gke-policy-automation"" \
  --git-policy-branch ""main"" \
  --git-policy-dir ""gke-policies-v2""
  ```

* Custom policies source with configuration file

  ```sh
  ./gke-policy check -c config.yaml
  ```

  The `config.yaml` file:

  ```yaml
  clusters:
    - name: my-cluster
      project: my-project
      location: europe-west2
  policies:
    - repository: https://domain.com/your/custom/repository
      branch: main
      directory: gke-policies-v2
  ```

### Authentication

The tool is fetching GKE cluster details using GCP APIs. The [application default credentials](https://cloud.google.com/docs/authentication/production)
are used by default.

* When running the tool in GCP environment, the tool will use the [attached service account](https://cloud.google.com/iam/docs/impersonating-service-accounts#attaching-to-resources)
by default
* When running locally, use `gcloud auth application-default login` command to get application
default credentials
* To use credentials from service account key file pass `--creds` parameter with a path to the file.

The minimum required IAM role is `roles/container.clusterViewer`
on a cluster projects. Additional roles may be needed, depending on configured outputs
\- check [authentication section](./docs/user-guide.md#authentication) in the user guide.

### Serverless execution

The GKE Policy Automation tool can be executed in a serverless way to perform automatic evaluations
of a clusters running in your organization. Please check our [reference Terraform Solution](./terraform/README.md)
that leverages GCP serverless solutions including Cloud Scheduler and Cloud Run.

## Contributing

Please check out [Contributing](./CONTRIBUTING.md) and [Code of Conduct](./docs/code-of-conduct.md)
docs before contributing.

### Development

Please check [GKE Policy Automation development](./DEVELOPMENT.md) for guides on building and developing
the application.

### Policy authoring

Please check [GKE Policy authoring guide](./gke-policies-v2/README.md) for guides on authoring REGO rules
for GKE Policy Automation.

## License

[Apache License 2.0](LICENSE)",FAUX
google/oss-fuzz,Toolkit,Application System,2025-05-16T02:25:31Z,2025-05-05T14:59:58Z,0,0,0,0,0,1,0,0,2016-07-20T19:39:50Z,2025-04-08T04:39:54Z,46747,10959,Shell,VRAI,2335,FAUX,509,"fuzz-testing,fuzzing,oss-fuzz,security,stability,vulnerabilities",509,OSS-Fuzz - continuous fuzzing for open source software.,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,1149,"# OSS-Fuzz: Continuous Fuzzing for Open Source Software

[Fuzz testing] is a well-known technique for uncovering programming errors in
software. Many of these detectable errors, like [buffer overflow], can have
serious security implications. Google has found [thousands] of security
vulnerabilities and stability bugs by deploying [guided in-process fuzzing of
Chrome components], and we now want to share that service with the open source
community.

[Fuzz testing]: https://en.wikipedia.org/wiki/Fuzz_testing
[buffer overflow]: https://en.wikipedia.org/wiki/Buffer_overflow
[thousands]: https://issues.chromium.org/issues?q=label:Stability-LibFuzzer%20-status:Duplicate,WontFix
[guided in-process fuzzing of Chrome components]: https://security.googleblog.com/2016/08/guided-in-process-fuzzing-of-chrome.html

In cooperation with the [Core Infrastructure Initiative] and the [OpenSSF],
OSS-Fuzz aims to make common open source software more secure and stable by
combining modern fuzzing techniques with scalable, distributed execution.
Projects that do not qualify for OSS-Fuzz (e.g. closed source) can run their own
instances of [ClusterFuzz] or [ClusterFuzzLite].

[Core Infrastructure Initiative]: https://www.coreinfrastructure.org/
[OpenSSF]: https://www.openssf.org/

We support the [libFuzzer], [AFL++], and [Honggfuzz] fuzzing engines in
combination with [Sanitizers], as well as [ClusterFuzz], a distributed fuzzer
execution environment and reporting tool.

[libFuzzer]: https://llvm.org/docs/LibFuzzer.html
[AFL++]: https://github.com/AFLplusplus/AFLplusplus
[Honggfuzz]: https://github.com/google/honggfuzz
[Sanitizers]: https://github.com/google/sanitizers
[ClusterFuzz]: https://github.com/google/clusterfuzz
[ClusterFuzzLite]: https://google.github.io/clusterfuzzlite/

Currently, OSS-Fuzz supports C/C++, Rust, Go, Python, Java/JVM, and JavaScript code. Other languages
supported by [LLVM] may work too. OSS-Fuzz supports fuzzing x86_64 and i386
builds.

[LLVM]: https://llvm.org

## Overview
![OSS-Fuzz process diagram](docs/images/process.png)

## Documentation
Read our [detailed documentation] to learn how to use OSS-Fuzz.

[detailed documentation]: https://google.github.io/oss-fuzz

## Trophies
As of August 2023, OSS-Fuzz has helped identify and fix over [10,000] vulnerabilities and [36,000] bugs across [1,000] projects.

[10,000]: https://bugs.chromium.org/p/oss-fuzz/issues/list?q=Type%3DBug-Security%20label%3Aclusterfuzz%20-status%3ADuplicate%2CWontFix&can=1
[36,000]: https://bugs.chromium.org/p/oss-fuzz/issues/list?q=Type%3DBug%20label%3Aclusterfuzz%20-status%3ADuplicate%2CWontFix&can=1
[1,000]: https://github.com/google/oss-fuzz/tree/master/projects

## Blog posts
* 2023-08-16 - [AI-Powered Fuzzing: Breaking the Bug Hunting Barrier]
* 2023-02-01 - [Taking the next step: OSS-Fuzz in 2023]
* 2022-09-08 - [Fuzzing beyond memory corruption: Finding broader classes of vulnerabilities automatically]
* 2021-12-16 - [Improving OSS-Fuzz and Jazzer to catch Log4Shell]
* 2021-03-10 - [Fuzzing Java in OSS-Fuzz]
* 2020-12-07 - [Improving open source security during the Google summer internship program]
* 2020-10-09 - [Fuzzing internships for Open Source Software]
* 2018-11-06 - [A New Chapter for OSS-Fuzz]
* 2017-05-08 - [OSS-Fuzz: Five months later, and rewarding projects]
* 2016-12-01 - [Announcing OSS-Fuzz: Continuous fuzzing for open source software]

[AI-Powered Fuzzing: Breaking the Bug Hunting Barrier]: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html
[Announcing OSS-Fuzz: Continuous fuzzing for open source software]: https://opensource.googleblog.com/2016/12/announcing-oss-fuzz-continuous-fuzzing.html
[OSS-Fuzz: Five months later, and rewarding projects]: https://opensource.googleblog.com/2017/05/oss-fuzz-five-months-later-and.html
[A New Chapter for OSS-Fuzz]: https://security.googleblog.com/2018/11/a-new-chapter-for-oss-fuzz.html
[Fuzzing internships for Open Source Software]: https://security.googleblog.com/2020/10/fuzzing-internships-for-open-source.html
[Improving open source security during the Google summer internship program]: https://security.googleblog.com/2020/12/improving-open-source-security-during.html
[Fuzzing Java in OSS-Fuzz]: https://security.googleblog.com/2021/03/fuzzing-java-in-oss-fuzz.html
[Improving OSS-Fuzz and Jazzer to catch Log4Shell]: https://security.googleblog.com/2021/12/improving-oss-fuzz-and-jazzer-to-catch.html
[Fuzzing beyond memory corruption: Finding broader classes of vulnerabilities automatically]: https://security.googleblog.com/2022/09/fuzzing-beyond-memory-corruption.html
[Taking the next step: OSS-Fuzz in 2023]: https://security.googleblog.com/2023/02/taking-next-step-oss-fuzz-in-2023.html",VRAI
grafana/mimir,Toolkit,Application System,2025-05-15T17:15:00Z,2025-05-06T22:09:17Z,0,9,0,0,0,0,0,0,2021-07-13T14:34:08Z,2025-04-07T12:32:17Z,357625,4413,Go,VRAI,577,FAUX,663,"metrics,observability,opentelemetry,otlp,prometheus,tsdb",663,"Grafana Mimir provides horizontally scalable, highly available, multi-tenant, long-term storage for Prometheus.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,512,"# Grafana Mimir

<p align=""center""><img src=""images/logo.png"" alt=""Grafana Mimir logo"" width=""400""></p>

Grafana Mimir is an open source software project that provides a scalable long-term storage for [Prometheus](https://prometheus.io). Some of the core strengths of Grafana Mimir include:

- **Easy to install and maintain:** Grafana Mimir’s extensive documentation, tutorials, and deployment tooling make it quick to get started. Using its monolithic mode, you can get Grafana Mimir up and running with just one binary and no additional dependencies. Once deployed, the best-practice dashboards, alerts, and runbooks packaged with Grafana Mimir make it easy to monitor the health of the system.
- **Massive scalability:** You can run Grafana Mimir's horizontally-scalable architecture across multiple machines, resulting in the ability to process orders of magnitude more time series than a single Prometheus instance. Internal testing shows that Grafana Mimir handles up to 1 billion active time series.
- **Global view of metrics:** Grafana Mimir enables you to run queries that aggregate series from multiple Prometheus instances, giving you a global view of your systems. Its query engine extensively parallelizes query execution, so that even the highest-cardinality queries complete with blazing speed.
- **Cheap, durable metric storage:** Grafana Mimir uses object storage for long-term data storage, allowing it to take advantage of this ubiquitous, cost-effective, high-durability technology. It is compatible with multiple object store implementations, including AWS S3, Google Cloud Storage, Azure Blob Storage, OpenStack Swift, as well as any S3-compatible object storage.
- **High availability:** Grafana Mimir replicates incoming metrics, ensuring that no data is lost in the event of machine failure. Its horizontally scalable architecture also means that it can be restarted, upgraded, or downgraded with zero downtime, which means no interruptions to metrics ingestion or querying.
- **Natively multi-tenant:** Grafana Mimir’s multi-tenant architecture enables you to isolate data and queries from independent teams or business units, making it possible for these groups to share the same cluster. Advanced limits and quality-of-service controls ensure that capacity is shared fairly among tenants.

## Migrating to Grafana Mimir

If you're migrating to Grafana Mimir, refer to the following documents:

- [Migrating from Thanos or Prometheus to Grafana Mimir](https://grafana.com/docs/mimir/latest/set-up/migrate/migrate-from-thanos-or-prometheus/).
- [Migrating from Cortex to Grafana Mimir](https://grafana.com/docs/mimir/latest/set-up/migrate/migrate-from-cortex/)

## Deploying Grafana Mimir

For information about how to deploy Grafana Mimir, refer to [Deploy Grafana Mimir](https://grafana.com/docs/mimir/latest/operators-guide/deploy-grafana-mimir/).

## Getting started

If you’re new to Grafana Mimir, read the [Get started guide](https://grafana.com/docs/mimir/latest/get-started/).

Before deploying Grafana Mimir in a production environment, read:

1. [An overview of Grafana Mimir’s architecture](https://grafana.com/docs/mimir/latest/operators-guide/architecture/)
1. [Configure Grafana Mimir](https://grafana.com/docs/mimir/latest/operators-guide/configure/)
1. [Run Grafana Mimir in production](https://grafana.com/docs/mimir/latest/operators-guide/run-production-environment/)

## Documentation

Refer to the following links to access Grafana Mimir documentation:

- [Latest release](https://grafana.com/docs/mimir/latest/)
- [Upcoming release](https://grafana.com/docs/mimir/next/), at the tip of the `main` branch

## Contributing

To contribute to Grafana Mimir, refer to [Contributing to Grafana Mimir](https://github.com/grafana/mimir/tree/main/docs/internal/contributing).

## Join the Grafana Mimir discussion

If you have any questions or feedback regarding Grafana Mimir, join the [Grafana Mimir Discussion](https://github.com/grafana/mimir/discussions). Alternatively, consider joining the monthly [Grafana Mimir Community Call](https://docs.google.com/document/d/1E4jJcGicvLTyMEY6cUFFZUg_I8ytrBuW8r5yt1LyMv4).

Your feedback is always welcome, and you can also share it via the [`#mimir` Slack channel](https://slack.grafana.com/).

## License

Grafana Mimir is distributed under [AGPL-3.0-only](LICENSE).",VRAI
h3mmy/bloopySphere,DevOPs,Documentations,2025-05-14T22:26:19Z,2025-05-13T13:28:00Z,0,0,0,0,18,0,0,0,2021-10-27T19:05:13Z,2025-04-07T12:34:41Z,23066,62,HCL,VRAI,6,FAUX,76,"gitops,k8s-at-home,kubernetes",76,My home cluster. ,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,7,"#
<!-- markdownlint-disable-next-line -->
##
<!-- markdownlint-disable-next-line -->
<div align=""center"">

### A home Kubernetes cluster :sailboat

_... managed with Flux and Renovate_ :robot:

</div>
<!-- markdownlint-disable-next-line -->
<br/>
<!-- markdownlint-disable-next-line -->
<div align=""center"">

[![k3s](https://img.shields.io/badge/k3s-v1.30.1-brightgreen?style=for-the-badge&logo=kubernetes&logoColor=white)](https://k3s.io/)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white&style=for-the-badge)](https://github.com/pre-commit/pre-commit)
[![renovate](https://img.shields.io/badge/renovate-enabled-brightgreen?style=for-the-badge&logo=renovatebot&logoColor=white)](https://github.com/renovatebot/renovate)

</div>
<!-- markdownlint-disable-next-line -->
<div align=""center"">

[![Mozilla HTTP Observatory Grade](https://img.shields.io/mozilla-observatory/grade-score/bloopnet.xyz?publish&style=for-the-badge)](https://observatory.mozilla.org/)
[![Uptime](https://img.shields.io/uptimerobot/ratio/m790142441-faed6f7043db9c588f5e949f?style=for-the-badge)](https://uptimerobot.com)
[![GitHub last commit](https://img.shields.io/github/last-commit/h3mmy/bloopySphere?style=for-the-badge)](https://github.com/h3mmy/bloopySphere/commits/main)

[![GitHub branch checks state](https://img.shields.io/github/checks-status/h3mmy/bloopySphere/main?style=for-the-badge)](https://github.com/h3mmy/bloopySphere/actions?query=branch%3Amain)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/h3mmy/bloopySphere/deploy-keycloak-theme.yaml?branch=main&label=Keycloak%20Theme&style=for-the-badge)](https://github.com/h3mmy/bloopySphere/actions/workflows/deploy-keycloak-theme.yaml)
[![GitHub Workflow Status](https://img.shields.io/github/workflow/status/h3mmy/bloopysphere/Lint?label=Lint&style=for-the-badge)](https://github.com/h3mmy/bloopySphere/actions/workflows/lint.yaml)

![Snyk Vulnerabilities for GitHub Repo](https://img.shields.io/snyk/vulnerabilities/github/h3mmy/bloopysphere?style=for-the-badge)
</div>

---

## :book:&nbsp; Overview

This is my home Kubernetes cluster. [Flux](https://github.com/fluxcd/flux2) watches this Git repository and makes the changes to my cluster based on the manifests in the [cluster](./cluster/) directory.
 [Renovate](https://github.com/renovatebot/renovate) also watches this Git repository and creates pull requests when it finds updates to Docker images, Helm charts, and other dependencies.

~~For more information, head on over to my [docs](https://h3mmy.github.io/bloopySphere/).~~
I have nested README files that should be visible as you browse the repo.

My [Gitlab](https://gitlab.com/h3mmy) has more of my projects

## Useful Snippets

List of container images in use cluster-wide

`kubectl get pods --all-namespaces -o go-template --template=""{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}"" | sed 's/ /\n/g' | uniq > ./container_images_in_use.txt`

List of container images in use that have arm64 images available (grep -q --> grep -vq for inversion)

`kubectl get po -A -o yaml | grep 'image:' | cut -f2- -d':' | sed 's/^[[:space:]]*//g' | grep '/' | sort -u | xargs -I{} bash -c ""docker manifest inspect {} | grep -q arm64 && echo {}"" > ./container_images_with_arm64.txt`

Snippet for nodeAffinity for non-ARM pods

`affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: ""kubernetes.io/arch""
                operator: In
                values:
                  - amd64
                  - i386
                  - i686
                  - x86`

If using a node-taint for arm nodes[1], this will allow toleration

`tolerations:

- key: ""arch""
  operator: ""Equal""
  value: ""arm64""
  effect: ""NoSchedule""`

[1]While Bootstrapping: `--kubelet-extra-args` `--register-with-taints=""kubernetes.io/arch=arm64:NoSchedule""`
Else: `kubectl taint no k8s-0 kubernetes.io/arch=arm64:NoSchedule`

Other useful snippets:
`kubectl label node k8s-0 node-role.kubernetes.io/worker=true`

`kubectl apply --kustomize=./cluster/base/flux-system`

`cat ~/.config/sops/age/keys.txt |
kubectl -n flux-system create secret generic sops-age --from-file=age.agekey=/dev/stdin`

`kubectl  create namespace flux-system --dry-run=client -o yaml | kubectl apply -f -`

`kubectl get secret db-user-pass -o json | jq '.data | map_values(@base64d)'`

Loki snippets. If you know you know.

```logql
{app=""traefik""} | json message_extracted=""message"" |  line_format ""{{.message_extracted}}"" | json | DownstreamStatus!=`200`
```

```logql
{app=""authentik""} | json message_extracted=""message""| line_format ""{{.message_extracted}}"" | json level=""level"",timestamp=""timestamp"",event=""event"" | level=`error`
```

`kubectl get namespace ""monitoring"" -o json   | tr -d ""\n"" | sed ""s/\""finalizers\"": \[[^]]\+\]/\""finalizers\"": []/""   | kubectl replace --raw /api/v1/namespaces/monitoring/finalize -f -`


Publicly available DoH server lists



https://github.com/curl/curl/wiki/DNS-over-HTTPS#publicly-available-servers

https://cln.io/blog/combined-list-of-dns-servers/

https://github.com/Sekhan/TheGreatWall

https://github.com/crypt0rr/public-doh-servers/tree/main




## :handshake:&nbsp; Community

Thanks to all the people who donate their time to the [Kubernetes @Home](https://github.com/k8s-at-home/) community.",VRAI
haacked/haacked.com,Application System,Documentations,2025-05-06T15:04:07Z,2025-02-24T21:21:59Z,0,0,0,0,0,1,0,0,2013-11-26T06:12:53Z,2025-03-18T13:26:42Z,284958,144,Shell,VRAI,181,FAUX,6,"blog,github-pages,jekyll",6,You've been haacked and you like it,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,535,"# Haacked.com

This is my blog. There are many like it, but this one is mine.

## Run it locally

The following command builds the site and runs it on http://localhost:4000/
It takes a while because I have a lot of posts.

```shell
jekyll serve
```

## Testing

[HTML::Proofer](https://github.com/gjtorikian/html-proofer) is set up to validate all links within the project.  You can run this locally to ensure that your changes are valid:

```shell
bundle install
bundle exec rake test
```",VRAI
haraldkoch/kochhaus-home,DevOPs,Documentations,2025-05-15T12:47:43Z,2025-05-10T19:16:38Z,0,0,0,0,9,0,0,0,2021-06-03T19:06:41Z,2025-04-07T13:20:29Z,17276,136,YAML,VRAI,14,FAUX,7,"ansible,gitops,helm,k8s-at-home,kubernetes",7,Experimental homelab configuration for running things on kubernetes,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,5,"<div>
<img src=""https://github.com/kubernetes/kubernetes/raw/master/logo/logo.svg"" align=""left"" width=""144px"" height=""144px""/>

#### kochhaus-home - a mono-repo for my homelab

_... automated via [Flux](https://fluxcd.io), [Renovate](https://github.com/renovatebot/renovate) and [GitHub Actions](https://github.com/features/actions)_ 🐟

</div>

<div align=""center"">

[![Discord](https://img.shields.io/discord/673534664354430999?style=for-the-badge&label&logo=discord&logoColor=white&color=blue)](https://discord.gg/home-operations)&nbsp;&nbsp;
[![Kubernetes](https://img.shields.io/badge/dynamic/yaml?url=https%3A%2F%2Fgithub.com%2Fharaldkoch%2Fkochhaus-home%2Fraw%2Fmain%2Fkubernetes%2Fmain%2Fapps%2Ftools%2Fsystem-upgrade-controller%2Fplans%2Fserver.yaml&query=%24.spec.version&style=for-the-badge&logo=kubernetes&logoColor=white&label=%20)](https://k3s.io/)&nbsp;&nbsp;
[![Renovate](https://img.shields.io/github/actions/workflow/status/haraldkoch/kochhaus-home/renovate.yaml?branch=main&label=&logo=renovatebot&style=for-the-badge&color=blue)](https://github.com/haraldkoch/kochhaus-home/actions/workflows/renovate.yaml)

</div>

<div align=""center"">

[![Age-Days](https://img.shields.io/endpoint?url=https%3A%2F%2Fkromgo.kochhaus.dev%2Fcluster_age_days&style=flat-square&label=Age)](https://github.com/kashalls/kromgo)&nbsp;&nbsp;
[![Uptime-Days](https://img.shields.io/endpoint?url=https%3A%2F%2Fkromgo.kochhaus.dev%2Fcluster_uptime_days&style=flat-square&label=Uptime)](https://github.com/kashalls/kromgo)&nbsp;&nbsp;
[![Node-Count](https://img.shields.io/endpoint?url=https%3A%2F%2Fkromgo.kochhaus.dev%2Fcluster_node_count&style=flat-square&label=Nodes)](https://github.com/kashalls/kromgo)&nbsp;&nbsp;
[![Pod-Count](https://img.shields.io/endpoint?url=https%3A%2F%2Fkromgo.kochhaus.dev%2Fcluster_pod_count&style=flat-square&label=Pods)](https://github.com/kashalls/kromgo)&nbsp;&nbsp;
[![CPU-Usage](https://img.shields.io/endpoint?url=https%3A%2F%2Fkromgo.kochhaus.dev%2Fcluster_cpu_usage&style=flat-square&label=CPU)](https://github.com/kashalls/kromgo)&nbsp;&nbsp;
[![Memory-Usage](https://img.shields.io/endpoint?url=https%3A%2F%2Fkromgo.kochhaus.dev%2Fcluster_memory_usage&style=flat-square&label=Memory)](https://github.com/kashalls/kromgo)&nbsp;&nbsp;
[![Power-Usage](https://img.shields.io/endpoint?url=https%3A%2F%2Fkromgo.kochhaus.dev%2Fcluster_power_usage&style=flat-square&label=Power)](https://github.com/kashalls/kromgo)&nbsp;&nbsp;
[![Alerts](https://img.shields.io/endpoint?url=https%3A%2F%2Fkromgo.kochhaus.dev%2Fcluster_alert_count&style=flat-square&label=Alerts)](https://github.com/kashalls/kromgo)

</div>

<div align=""center"">

[![volkswagen status](https://auchenberg.github.io/volkswagen/volkswargen_ci.svg?v=1)](https://github.com/auchenberg/volkswagen)

</div>

---

## Overview

This is a monorepository for my home Kubernetes clusters.
I try to adhere to Infrastructure as Code (IaC) and GitOps practices using tools like [Ansible](https://www.ansible.com/), [Terraform](https://www.terraform.io/), [Kubernetes](https://kubernetes.io/), [Flux](https://github.com/fluxcd/flux2), [Renovate](https://github.com/renovatebot/renovate), and [GitHub Actions](https://github.com/features/actions).

The purpose here is to learn Kubernetes, while practicing GitOps. I have two longer-term goals:

1. migrate many of the services that I currently run on Linode to my HomeLab.
2. Build a small Raspberry Pi cluster at home to run a infrastructure, with the intent of being able to run critical components from a UPS during power outages.

---

## ⛵ Kubernetes

There is a template over at [onedr0p/flux-cluster-template](https://github.com/onedr0p/flux-cluster-template) if you want to try and follow along with some of the practices I use here.

### Installation

My cluster is built using [k3s](https://k3s.io/), provisioned on bare-metal Arch Linux using the [Ansible](https://www.ansible.com/) galaxy role [ansible-role-k3s](https://github.com/PyratLabs/ansible-role-k3s). This is a hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes. I also have a separate NAS server with ZFS for NFS/SMB shares, bulk file storage and backups.

### Core Components

- [actions-runner-controller](https://github.com/actions/actions-runner-controller): self-hosted Github runners
- [calico](https://www.tigera.io/project-calico/): container networking with IPv6 support and policy enforcement.
- [cert-manager](https://cert-manager.io/docs/): Configured to create TLS certs for all ingress services automatically using LetsEncrypt.
- [external-dns](https://github.com/kubernetes-sigs/external-dns): monitors service and ingress resources, and automatically generates DNS updates for them. This lets me maintain DNS mappings and LetsEncrypt certificates without a cloudflare account or domain.
- [external-secrets](https://github.com/external-secrets/external-secrets/): managed Kubernetes secrets using [1Password](https://1password.com/).
- [ingress-nginx](https://github.com/kubernetes/ingress-nginx/): ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer
- [rook-ceph](https://rook.io/): Cloud native distributed block storage for Kubernetes
- [sops](https://toolkit.fluxcd.io/guides/mozilla-sops/): managed secrets for Kubernetes, Ansible, and Terraform which are committed to Git
- [volsync](https://github.com/backube/volsync): backup and recovery of persistent volume claims

### GitOps

[Flux](https://github.com/fluxcd/flux2) watches the clusters in my [kubernetes](./kubernetes/) folder (see Directories below) and makes the changes to my clusters based on the state of my Git repository.

The way Flux works for me here is it will recursively search the `kubernetes/${cluster}/apps` folder until it finds the most top level `kustomization.yaml` per directory and then apply all the resources listed in it. That aforementioned `kustomization.yaml` will generally only have a namespace resource and one or many Flux kustomizations. Those Flux kustomizations will generally have a `HelmRelease` or other resources related to the application underneath it which will be applied.

[Renovate](https://github.com/renovatebot/renovate) watches my **entire** repository looking for dependency updates, when they are found a PR is automatically created. When some PRs are merged Flux applies the changes to my cluster.

### Wow

Yes, this is a lot of infrastructure and heavy lifting - the point is to experiment with Kubernetes and GitOps in a safe space.

[![dexhorthy](assets/blog-on-kubernetes.png)](https://twitter.com/dexhorthy/)

### Directories

This Git repository contains the following directories under [Kubernetes](./kubernetes/).

```sh
📁 kubernetes
├── 📁 main            # main cluster
│   ├── 📁 apps           # applications
│   ├── 📁 bootstrap      # bootstrap procedures
│   ├── 📁 components     # re-useable components
│   └── 📁 flux           # core flux configuration
└── 📁 registry        # registry cluster (running harbor)
    ├── 📁 apps           # applications
    ├── 📁 bootstrap      # bootstrap procedures
    └── 📁 flux           # core flux configuration
```

---

## ☁️ Cloud Dependencies

While most of my infrastructure and workloads are self-hosted I do rely upon the cloud for certain key parts of my setup. This saves me from having to worry about two things. (1) Dealing with chicken/egg scenarios and (2) services I critically need whether my cluster is online or not.

The alternative solution to these two problems would be to host a Kubernetes cluster in the cloud and deploy applications like [HCVault](https://www.vaultproject.io/), [Vaultwarden](https://github.com/dani-garcia/vaultwarden), [ntfy](https://ntfy.sh/), and [Gatus](https://gatus.io/). However, maintaining another cluster and monitoring another group of workloads is a lot more time and effort than I am willing to put in.

| Service                                         | Use                                                               | Cost           |
|-------------------------------------------------|-------------------------------------------------------------------|----------------|
| [1Password](https://1password.com/)             | Secrets with [External Secrets](https://external-secrets.io/)     | ~$60/yr        |
| [Cloudflare](https://www.cloudflare.com/)       | Domain and S3                                                     | Free           |
| [GitHub](https://github.com/)                   | Hosting this repository and continuous integration/deployments    | Free           |
| [Linode](https://linode.com/)                   | servers hosting my email and public web                           | Free           |
| [Pushover](https://pushover.net/)               | Kubernetes Alerts and application notifications                   | $5 OTP         |
| [healthchecks.io](https://healthchecks.io)      | Monitoring internet connectivity and Prometheus status            | Free           |
|                                                 |                                                                   | Total: ~$5/mo  |

---

## 🌐 DNS

### Home DNS

On a pair of Raspberry Pi 3s, I have [Bind9](https://github.com/isc-projects/bind9) and [blocky](https://github.com/0xERR0R/blocky) deployed. In my cluster `external-dns` is deployed with the `RFC2136` provider which syncs DNS records to `bind9`. `blocky` is used by non-servers as ad-blocking and caching proxy, using `bind9` for local lookups.

### Public DNS

Outside the `external-dns` instance mentioned above another instance is deployed in my cluster and configured to sync DNS records to [Cloudflare](https://www.cloudflare.com/). The only ingress this `external-dns` instance looks at to gather DNS records to put in `Cloudflare` are ones that have an ingress class name of `external` and contain an ingress annotation `external-dns.alpha.kubernetes.io/target`.

---

## 🔧 Hardware

### Main Kubernetes Cluster

| Node                | CPU               |  RAM | Storage    | Function             | OS         |
|---------------------|-------------------|------|------------|----------------------|------------|
| HP EliteDesk 800 G2 | Intel i5-6500T    | 16GB | 240GB SSD  | control-plane        | Arch Linux |
| HP EliteDesk 800 G2 | Intel i5-6500T    | 16GB | 240GB SSD  | control-plane        | Arch Linux |
| HP EliteDesk 800 G2 | Intel i5-6500T    | 16GB | 240GB SSD  | control-plane        | Arch Linux |
| Lenovo M910q tiny   | Intel i5-6500T    | 16GB | 512GB NVMe | worker, ceph storage | Arch Linux |
| Lenovo M900q tiny   | Intel i5-6500T    | 16GB | 512GB SSD  | worker, ceph storage | Arch Linux |
| Lenovo M910q tiny   | Intel i5-6500T    | 16GB | 512GB NVMe | worker, ceph storage | Arch Linux |

### Registry Kubernetes Cluster

| Node        | CPU               | RAM | Storage   | Function            | OS         |
|-------------|-------------------|-----|-----------|---------------------|------------|
| libvirtd VM | AMD Ryzen 5 1600T | 8GB | 128GB HDD | single-node cluster | Arch Linux |

### Infrastructure Kubernetes Cluster (coming soon)

| Node       | CPU            | RAM  | Storage  | Function      | OS     |
|------------|----------------|------|----------|---------------|--------|
| Turing RK1 | Cortex A76/A55 | 16GB | 1TB NVMe | control-plane | Debian |
| Turing RK1 | Cortex A76/A55 | 16GB | 1TB NVMe | control-plane | Debian |
| Turing RK1 | Cortex A76/A55 | 16GB | 1TB NVMe | control-plane | Debian |

---

## ⭐ Stargazers

<div align=""center"">

[![Star History Chart](https://api.star-history.com/svg?repos=haraldkoch/kochhaus-home&type=Date)](https://star-history.com/#haraldkoch/kochhaus-home&Date)

</div>

---

## 🤝 Thanks

Big shout out to original [flux-cluster-template](https://github.com/onedr0p/flux-cluster-template), and the [Home Operations](https://discord.gg/home-operations) Discord community.

Be sure to check out [kubesearch.dev](https://kubesearch.dev/) for ideas on how to deploy applications or get ideas on what you may deploy.",VRAI
harness-community/harnesscd-example-apps,Documentations,Documentations,2025-04-05T08:11:23Z,2025-02-21T15:55:05Z,0,0,0,0,1,0,0,0,2023-04-27T19:23:35Z,2025-04-05T08:11:27Z,2006,13,CSS,FAUX,944,FAUX,6,,6,Example Apps to Demonstrate Harness CD & GitOps,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,22,"# Harness CD & GitOps Example Apps

This repository contains example applications for demoing Harness CD & GitOps functionality. Feel free
use this to learn more about Harness CD & GitOps in your own Harness account, or fork this repo and push your own commits!

| Application | Description |
|-------------|-------------|
| [guestbook](guestbook/) | A hello word guestbook app as plain YAML |
| [ksonnet-guestbook](ksonnet-guestbook/) | The guestbook app as a ksonnet app |
| [helm-guestbook](helm-guestbook/) | The guestbook app as a Helm chart |
| [jsonnet-guestbook](jsonnet-guestbook/) | The guestbook app as a raw jsonnet |
| [jsonnet-guestbook-tla](jsonnet-guestbook-tla/) | The guestbook app as a raw jsonnet with support for top level arguments |
| [kustomize-guestbook](kustomize-guestbook/) | The guestbook app as a Kustomize 2 app |
| [pre-post-sync](pre-post-sync/) | Demonstrates Harness CD PreSync and PostSync hooks |
| [sync-waves](sync-waves/) | Demonstrates Harness CD sync waves with hooks |
| [helm-dependency](helm-dependency/) | Demonstrates how to customize an OTS (off-the-shelf) helm chart from an upstream repo |
| [sock-shop](sock-shop/) | A microservices demo app (https://microservices-demo.github.io) |
| [plugins](plugins/) | Apps which demonstrate config management plugins usage |
| [blue-green](blue-green/) | Demonstrates how to implement blue-green deployment using Harness CD|
| [apps](apps/) | An app composed of other apps |",FAUX
harness-community/solutions-architecture,Documentations,Documentations,2024-10-28T21:02:06Z,2023-03-07T13:55:59Z,0,33,0,0,0,0,0,0,2023-03-07T13:48:16Z,2024-12-12T23:57:15Z,112,10,Open Policy Agent,VRAI,8,FAUX,7,,7,A collection of resources and best practices for operating Harness at scale,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,5,# solutions-architecture,FAUX
hashicorp/policy-library-aws-networking-terraform,Toolkit,Toolkit,2024-01-23T01:33:40Z,2022-07-22T19:07:58Z,12,0,0,0,0,0,0,0,2022-07-28T20:43:40Z,2024-11-27T05:12:37Z,45,11,HCL,VRAI,30,FAUX,2,,2,Prescriptive Sentinel policies that can be used to establish secure Terraform configuration for Amazon Web Services networking infrastructure.,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,13,"# AWS Networking Sentinel Policies for Terraform
This library, provides prescriptive Terraform policies that can be used to establish secure Terraform configuration for Amazon Web Services. The policies that are contained in this library are based on the [CIS Amazon Web Services Benchmarks](https://www.cisecurity.org/benchmark/amazon_web_services). Terraform Cloud/Enterprise users can use the policies in this library to establish a foundational level of security for the services that they are adopting in Amazon Web Services.

> **NOTE:**
>
> This Policy Library is not an exhaustive list of all of possible security configurations and architecture that is available in Amazon Web Services. If you have questions, comments, or have identified ways for us to improve this library, please create [a new GitHub issue](https://github.com/hashicorp/policy-library-aws-networking-terraform/issues/new/choose).
>
> Alternatively, We welcome any contributions that improve the quality of this library! To learn more about contributing and suggesting changes to this library, refer to the [contributing guide](https://github.com/hashicorp/policy-library-aws-networking-terraform/blob/main/CONTRIBUTING.md).

---

## Policies included

-  Ensure no security groups allow ingress from 0.0.0.0/0 to port 22 ([docs](https://github.com/hashicorp/policy-library-aws-networking-terraform/blob/main/docs/policies/deny-public-ssh-acl-rules.md) | [code](https://github.com/hashicorp/policy-library-aws-networking-terraform/blob/main/policies/deny-public-ssh-acl-rules/deny-public-ssh-acl-rules.sentinel))
-  Ensure no security groups allow ingress from 0.0.0.0/0 to port 3389 ([docs](https://github.com/hashicorp/policy-library-aws-networking-terraform/blob/main/docs/policies/deny-public-rdp-acl-rules.md) | [code](https://github.com/hashicorp/policy-library-aws-networking-terraform/blob/main/policies/deny-public-rdp-acl-rules/deny-public-rdp-acl-rules.sentinel))
-  Ensure the default security group of every VPC restricts all traffic ([docs](https://github.com/hashicorp/policy-library-aws-networking-terraform/blob/main/docs/policies/restrict-all-vpc-traffic-acl-rules.md) | [code](https://github.com/hashicorp/policy-library-aws-networking-terraform/blob/main/policies/restrict-all-vpc-traffic-acl-rules/restrict-all-vpc-traffic-acl-rules.sentinel))

---",FAUX
hashicorp/sentinel-sdk,Toolkit,Toolkit,2025-04-29T02:44:42Z,2024-05-17T07:48:06Z,3,0,0,0,0,0,0,0,2017-08-29T05:57:48Z,2025-03-18T01:50:03Z,319,52,Go,VRAI,13,FAUX,8,,8,This SDK allows developers to extend Sentinel to source external information for use in their policies.,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,16,"# Sentinel Plugin SDK

![Tests](https://github.com/hashicorp/sentinel-sdk/actions/workflows/test.yml/badge.svg)
[![GoDoc](https://godoc.org/github.com/hashicorp/sentinel-sdk?status.svg)](https://godoc.org/github.com/hashicorp/sentinel-sdk)
[![Heimdall](https://heimdall.hashicorp.services/api/v1/assets/sentinel-sdk/badge.svg?key=8a99f5a22605231081b7fb8be0453015916fb79441a73af371dc625373e4a919)](https://heimdall.hashicorp.services/site/assets/sentinel-sdk)

This repository contains the [Sentinel](https://www.hashicorp.com/sentinel)
plugin SDK. This SDK allows developers to extend Sentinel to source external
information for use in their policies.

Sentinel plugins can be written in any language, but the recommended language is
[Go](https://golang.org/). We provide a high-level framework to make writing
plugins in Go extremely easy. For other languages, plugins can be written by
implementing the
[protocol](https://github.com/hashicorp/sentinel-sdk/blob/main/proto/plugin.proto)
over gRPC.

To get started writing a Sentinel plugin, we recommend reading the [extending
Sentinel](https://docs.hashicorp.com/sentinel/extending/) guide.

You can also view the plugin API via
[GoDoc](https://godoc.org/github.com/hashicorp/sentinel-sdk).

## SDK Compatibility Matrix

Sentinel's plugin protocol is, at this time, _not_ backwards compatible.  This
means that a specific version of the Sentinel runtime is always coupled to a
specific version of the plugin protocol, and SDK. The following table can help
you determine which version of the SDK is necessary to work with which versions
of Sentinel.

Sentinel Version|Plugin Protocol Version|SDK Version
-|-|-
**Up to v0.10.4**|**1**|**Up to v0.1.1**
Up to v0.18.13|2|Up to v0.3.13
From v0.19.0|3|Since v0.4.0

## Development Info

The following tools are required to work with the Sentinel SDK:

* [The Sentinel runtime](https://docs.hashicorp.com/sentinel/downloads), usually
  at the most recent version. There are rare exceptions to this, such as when
  the protocol is in active development. Refer to the [SDK Compatibility
  Matrix](#sdk-compatibility-matrix) to locate the correct version of the SDK to
  work with the most current version of the runtime.
* [Google's Protocol
  Buffers](https://developers.google.com/protocol-buffers/docs/downloads).

After both of these are installed, you can use the following `make` commands:

* `make test` will run tests on the SDK. You can use the `TEST` and `TESTARGS`
  variables to control the packages and test arguments, respectively.
* `make tools` will install any necessary Go tools.
* `make generate` will generate any auto-generated code. Currently this includes
  the protocol, mockery files, and the code for the plugin testing toolkit.

The `modules`, `test-circle`, and `/usr/bin/sentinel` targets are only used in
Circle and are not necessary for interactive development.

## Help and Discussion

For issues specific to the SDK, please use the GitHub issue tracker (the
[Issues](https://github.com/hashicorp/sentinel-sdk/issues) tab).

For general Sentinel support and discussion, please use the [Sentinel Community
Forum](https://discuss.hashicorp.com/c/sentinel).",FAUX
hashicorp/terraform-sentinel-policies,Documentations,Documentations,2024-04-23T10:32:24Z,2022-01-31T20:51:08Z,386,0,0,0,0,0,0,0,2022-01-26T15:26:47Z,2025-04-03T01:24:26Z,806,162,HCL,VRAI,254,FAUX,12,,12,Example Sentinel Policies for use with Terraform Cloud and Terraform Enterprise,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,13,"# Example Third Generation Sentinel Policies for Terraform

NOTE: The content of this repository is in the process of being migrated to the [Terraform Registry](https://registry.terraform.io/browse/policies).

This directory and its sub-directories contain third-generation Sentinel policies and associated [Sentinel CLI](https://docs.hashicorp.com/sentinel/intro/getting-started/install) test cases and mocks which were created in 2020 for AWS, Microsoft Azure, Google Cloud Platform (GCP), and VMware. It also contains some some common, re-usable functions.

Additionally, it contains [Policy Set](https://www.terraform.io/docs/cloud/sentinel/manage-policies.html#the-sentinel-hcl-configuration-file) configuration files so that the cloud-specific and cloud-agnostic policies can easily be added to Terraform Cloud organizations using [VCS Integrations](https://www.terraform.io/docs/cloud/vcs/index.html) after forking this repository.

These policies and the Terraform Sentinel v2 imports they use can only be used with Terraform 0.12 and above.

These policies use the Terraform Sentinel v2 imports. They also use [Sentinel Modules](https://docs.hashicorp.com/sentinel/extending/modules) which allow Sentinel functions and rules to be defined in one file and used by Sentinel policies in other files.

To learn more about the Terraform Sentinel v2 imports, see this [blog post](https://www.hashicorp.com/blog/terraform-sentinel-v2-imports-now-in-technology-preview).

To learn more about Sentinel Modules, see this [blog post](https://discuss.hashicorp.com/t/sentinel-v0-15-0-introducing-modules/6579).

## Using These Policies with Terraform Cloud and Terraform Enterprise
These policies and the common functions they use can be used as organized with the current version of Terraform Cloud (TFC) and with Terraform Enterprise (TFE) v202011-1 and higher. That version was released on November 10, 2020. It added the Sentinel 0.16.0 runtime which introduced the option of using HCL instead of JSON configuration files.

## Important Characterizations of the Third Generation Policies
These third-generation policies have several important characteristics:
1. As mentioned above, they use the Terraform Sentinel v2 imports, which are more closely aligned with Terraform 0.12's data model and leverage the recently added [filter expression](https://docs.hashicorp.com/sentinel/language/collection-operations/#filter-expression), and make it easier to restrict policies to specific operations performed by Terraform against resources.
1. The policies use parameterized functions defined in four [Sentinel modules](https://docs.hashicorp.com/sentinel/extending/modules). Since they are defined in modules, their implementations do **not** need to be pasted into the policies. This is a **HUGE** improvement over the second-generation common functions!
1. A related benefit of using functions from modules is that the policies themselves do not have any `for` loops or `if/else` conditionals. This makes it easier for users to understand the sample policies and to write their own policies that copy them.
1. The policies have been written in a way that causes all violations to be reported. This means a user who violates a policy will be informed about all of their violations in a single shot without having to run multiple Sentinel CLI tests or TFC/TFE plans.
1. The policies print out the full address of each resource instance that does violate a rule in the same format that is used in plan and apply logs, namely `module.<A>.module.<B>.<type>.<name>[<index>]`. (Note that `index` will only be present if multiple instances of a resource are defined either with the `count` or the `for_each` meta-arguments.) This allows writers of Terraform code to quickly determine the resources they need to fix even if the violations occur in modules that they did not write.
1. They are written in a way that made Sentinel's older default output much less verbose. (But Sentinel's default output was improved in version 0.17.0.) Users looking at Sentinel policy violations that occur during their runs will get all the information they need from the messages explicitly printed from the policies using Sentinel's `print` function. (Sentinel's default output that reports `TRUE` or `FALSE` for various rules and boolean expressions used by them along with Sentinel policy line numbers is really only useful to the policy's author.)
1. The common function `evaluate_attribute`, which is in the tfplan-functions.sentinel and tfstate-functions.sentinel modules, can evaluate the values of any attribute of any resource even if it is deeply nested inside the resource. It does this by calling itself recursively.

## Common Functions
You can find most of the common functions used in the third-generation policies in the Sentinel modules in the [common functions](./common-functions) directory:
  * [tfplan-functions](./common-functions/tfplan-functions)
  * [tfstate-functions](./common-functions/tfstate-functions)
  * [tfconfig-functions](./common-functions/tfconfig-functions)
  * [tfrun-functions](./common-functions/tfrun-functions)
  * [report](./common-functions/report)

There are also some functions that can be used with the AWS, Azure, and GGP providers in [aws-functions](./aws/aws-functions), [azure-functions](./azure/azure-functions), and [gcp-functions](.gcp/gcp-functions) and some functions that can be used when talking to module registries in [registry-functions](./cloud-agnostic/http-examples/registry-functions).

All of the common functions that use any of the 4 Terraform Sentinel imports (tfplan/v2, tfstate/v2, tfconfig/v2, and tfrun) are defined in a single file. This makes it easier to import all of the functions that use one of those imports into the Sentinel CLI test cases and Terraform Cloud policy sets, since those only need a single stanza such as this one for each module:
```
""modules"": {
  ""tfplan-functions"": {
    ""path"": ""../../../common-functions/tfplan-functions/tfplan-functions.sentinel""
  }
}
```
Test cases that use the other modules would either change all three occurrences of ""tfplan"" in that stanza to ""tfstate"", ""tfconfig"", ""tfrun"", ""aws"", or ""azure"" or would add additional stanzas with those changes.

We have put each Sentinel module in its own directory which also contains Markdown files for each of the module's functions under a docs directory. The names of the functions at the top of these Markdown documentation files have hyperlinks that will take you directly to the function definition in the module itself. Each of these Markdown files describes the function, its declaration, its arguments, other common functions it uses, what it returns, and what it prints. It also gives examples of calling the function and sometimes lists some policies that call it.

While having multiple Sentinel functions in a single file does make examining the function code a bit harder, we think the reduced work associated with referencing the functions in the test cases and policy sets justifies this.

To use any of the functions in a new policy, be sure to include lines like these:
```
import ""tfplan-functions"" as plan
import ""tfstate-functions"" as state
import ""tfconfig-functions"" as config
import ""tfrun-functions"" as run
import ""aws-functions"" as aws
import ""azure-functions"" as azure
import ""registry-functions"" as registry
```
In this case, we are using `plan`, `state`, `config`, `run`, `aws`, `azure`, and `registry` as aliases for the seven imports to keep lines that use their functions shorter. Of course, you only need to import the modules that contain functions that your policy actually calls.

The `report` module contains necessary logic to pretty print policy evaluation results.

### The Functions of the tfplan-functions and tfstate-functions Modules
We discuss these two modules together because they are essentially identical except for their use of the tfplan/v2 and tfstate/v2 imports. (But note that the tfplan-functions module has some filter functions that the tfstate-functions module does not.)

Each of these modules has several types of functions:
  * `find_resources` and `find_datasources` functions that find resources or data sources of a specific type. Note that the tfplan versions of these functions only find resources that are being created or changed and data sources that are being created, changed, or read.
  * `find_resources_by_provider` and `find_datasources_by_provider` functions that find resources or data sources for a specific provider. Note that the tfplan versions of these functions only find resources that are being created or changed and data sources that are being created, changed, or read. Also note that the string that should be passed to these functions varies between Terraform 0.12 and 0.13.
  * `find_resources_being_destroyed` and `find_datasources_being_destroyed` function that find resources or data sources that are being destroyed but not re-created.
  * The `find_blocks` function finds all blocks of a specific type in a single resource.
  * `filter_*` functions that filter a collection of resources, data sources, or blocks to a sub-collection that violates some condition. (When we say resources below, we are including data sources which are really just read-only resources.) The filter functions all accept a collection of resource changes (for tfplan/v2) or resources (for tfstate/v2), an attribute, a value or a list of values, and a boolean, `prtmsg`, which can be `true` or `false` and indicates whether the filter function should print violation messages. The filter functions return a map consisting of 2 items:
    * `resources`: a map consisting of resource changes (for tfplan/v2) or resources (for tfstate/v2) or blocks that violate a condition.
    * `messages`: a map of violation messages associated with the resource changes, resources, or blocks.
  Note that both the `resources` and `messages` collections are indexed by the address of the resources, so they will have the same order and length. The filter functions all call the `evaluate_attribute` function to evaluate attributes of resources even if nested deep within them. After calling a filter function and assigning the results to a variable like `violatingResources`, you can test if there are any violations with this condition: `length(violatingResources[""messages""]) is 0`.
  * The `evaluate_attribute` function, which can evaluate the values of any attribute of any resource even if it is deeply nested inside the resource. It does this by calling itself recursively. The implementation in the tfplan-functions module will convert `rc` to `rc.change.after`. If you want it to examine previous values instead of planned values, pass it `rc.change.before` instead of `rc`.
  * The `to_string` function which can convert any Sentinel object to a string. It is used to build the messages in the `messages` collection returned by the filter functions.
  * The `print_violations` function which can be called after calling one of the filter function to print the violation messages. This would only be called if the `prtmsg` argument had been set to `false` when calling the filter function. This is sometimes desirable especially if processing blocks of resources since your policy can then print some other message that gives the address of the resource with block-level violations before printing them.

Documentation for each individual function can be found in these directories:
  * [tfplan-functions](./common-functions/tfplan-functions/docs)
  * [tfstate-functions](./common-functions/tfstate-functions/docs)

### The Functions of the tfconfig-functions Module
The `tfconfig-functions` module has several types of functions:
  * `find_all_*` functions find all resources, data sources, provisioners, providers, variables, outputs, and module calls of all types.
  * `find_*_by_type` functions that find resources, data sources, provisioners, or providers of a specific type.
  * `find_*_in_module` functions that find resources, data sources, variables, providers, outputs, or module calls in a specific module.
  * `find_*_by_provider` functions that find resources or data sources created by a specific provider.
  * The `find_outputs_by_sensitivity` function that finds outputs based on their `sensitive` setting.
  * The `find_descendant_modules` function that finds all module addresses called directly or indirectly by a specific module including that module itself. Calling `find_descendant_modules("""")` will return all module addresses within the Terraform configuration.
  * Various filter functions such as `filter_attribute_not_in_list` and `filter_attribute_in_list` that are similar to the filter functions in the tfplan-functions module. However, these can only be used against top-level attributes of the items in the collection passed to them or against items directly under the `config` map of items. Those collections can be any type of entity covered by the tfconfig/v2 import including resources, data sources, providers, provisioners, variables, outputs, and module calls. The filter functions return a map consisting of 2 items:
    * `items`: a map consisting of items that violate a condition.
    * `messages`: a map of violation messages associated with the items.
  * The same `to_string` and `print_violations` functions that are in the tfplan-functions module.
  * A `get_module_source` function that computes the source of a module from its address.
  * A `get_ancestor_module_source` function that computes the source of the first ancestor module that is not a local module of a module from its address. This is used in the [restrict-resources-by-module-source.sentinel](./cloud-agnostic/restrict-resources-by-module-source.sentinel) policy to restrict creation of resources based on the actual module sources.
  * A `get_parent_module_address` function that computes the address of the parent module of a module from its address.

Documentation for each individual function can be found in this directory:
  * [tfconfig-functions](./common-functions/tfconfig-functions/docs)

### The Functions of the tfrun-functions Module
The `tfrun-functions` module has the following functions:
  * The `limit_proposed_monthly_cost` function validates that the proposed monthly cost estimate is less than the given limit.
  * The `limit_cost_and_percentage_increase` function validates that the proposed monthly cost estimate and percentage increase over the previous cost estimate ar both less than limits.
  * The `limit_cost_by_workspace_name` function validates that the monthly cost estimate is less than the limit in a map associated with a workspace name prefix or suffix that the current workspace has.

Documentation for each individual function can be found in this directory:
  * [tfrun-functions](./common-functions/tfrun-functions/docs)

### The Functions of the report Module
The `report` module has the following functions:
  * The `generate_policy_report` function accepts a `summary` object and pretty prints the policy evaluation results. More information about this function can be found [here](./common-functions/report/docs)

### The Functions of the aws-functions Module
The `aws-functions` module (which is located in the aws/aws-functions directory) has the following functions:
  * The `find_resources_with_standard_tags` function finds all AWS resources of specified types that should have tags in the current plan that are not being permanently deleted.
  * The `determine_role_arn` function determines the ARN of a role set in the `role_arn` parameter of an AWS provider. It can only determine the role_arn if it is set to either a hard-coded value or to a reference to a single Terraform variable. It sets the role to ""complex"" if it finds a single non-variable reference or if it finds multiple references. It sets the role to ""none"" if no role arn is found.
  * The `get_assumed_roles` function gets all roles assumed by AWS providers in the current Terraform configuration. It calls the `determine_role_arn` function.
  * The `validate_assumed_roles_with_list` function validates assumed roles found by the `get_assumed_roles` function against a list of role ARNs.
  * The `validate_assumed_roles_with_map` function validates assumed roles found by the `get_assumed_roles` function against a map of role ARNs which are associated with regular expressions representing workspace names that are allowed to use them.

Documentation for each individual function can be found in this directory:
  * [aws-functions](./aws/aws-functions/docs)

### The Functions of the azure-functions Module
The `azure-functions` module (which is located in the azure/azure-functions directory) has the following functions:
  * The `find_resources_with_standard_tags` function finds all Azure resources of specified types that should have tags in the current plan that are not being permanently deleted.

Documentation for each individual function can be found in this directory:
  * [azure-functions](./azure/azure-functions/docs)

### The Functions of the registry-functions Module
The `registry-functions` module (which is located in the cloud-agnostic/http-examples/registry-functions directory) has the following functions:
  * The `get_recent_module_versions` function finds recent versions for private or public modules from a private module registry (PMR).
  * The `get_recent_module_versions_by_page` function finds recent versions for private or public modules from a private module registry (PMR) one page at a time. It is called by the `get_recent_module_versions` function. Having a separate function that deals with pagination keeps the interface for the `get_recent_module_versions` function cleaner.
  * The `find_most_recent_version` function finds the most recent versing string from a map of version strings.
  * The `is_module_in_public_registry` function determines if a module is in the public module registry.

Documentation for each individual function can be found in this directory:
    * [registry-functions](./cloud-agnostic/http-examples/registry-functions/docs)

## Mock Files and Test Cases
Sentinel [mock files](https://www.terraform.io/docs/enterprise/sentinel/mock.html) and [test cases](https://docs.hashicorp.com/sentinel/commands/config#test-cases) have been provided under the test directory of each cloud so that all the policies can be tested with the [Sentinel CLI](https://docs.hashicorp.com/sentinel/commands). The mocks were generated from actual Terraform 0.12 plans run against Terraform code that provisioned resources in these clouds. The pass and fail mock files were edited to respectively pass and fail the associated Sentinel policies. Some policies, including those that have multiple rules, have multiple fail mock files with names that indicate which condition or conditions they fail.

## Testing Policies
To test the policies of any of the clouds, please do the following:
1. Download the Sentinel CLI from the [Sentinel Downloads](https://docs.hashicorp.com/sentinel/downloads) page. (Be sure to use Sentinel 0.15.2 or higher.)
1. Unzip the zip file and place the sentinel binary in your path.
1. Clone this repository to your local machine.
1. Navigate to any of the cloud directories (aws, azure, gcp, or vmware) or to the cloud-agnostic directory.
1. Run `sentinel test` to test all policies for that cloud.
1. If you just want to test a single policy, run `sentinel test <policy_name>` where \<policy_name\> is the policy name.

Adding the `-verbose` flag to the above commands will show you the output that you would see if running the policies in TFC or TFE.

## Policy Set Configuration Files
As mentioned in the introduction of this file, this repository contains [Policy Set](https://www.terraform.io/docs/cloud/sentinel/manage-policies.html#the-sentinel-hcl-configuration-file) configuration files so that the cloud-specific and cloud-agnostic policies can easily be added to Terraform Cloud organizations using [VCS Integrations](https://www.terraform.io/docs/cloud/vcs/index.html) after forking this repository.

Each of these files is called ""sentinel.hcl"" and should list all policies in its directory with an [Enforcement Level](https://www.terraform.io/docs/cloud/sentinel/manage-policies.html#enforcement-levels) of ""advisory"". This means that registering these policy sets in a Terraform Cloud or Terraform Enterprise organization will not actually have any impact on provisioning of resources from those organizations even if some of the policy checks do report violations.

The `sentinel.hcl` files list the source of each policy like this: `source = ""./<policy>.sentinel""`. While including a source for a policy in the same directory as the `sentinel.hcl` file is not currently required, it will be required in the future. So, we added them now to avoid future problems.

The `sentinel.hcl` files should also include any Sentinel modules used by any of the policies they list.

Users who wish to actually enforce any of these policies should change the enforcement levels of them to ""soft-mandatory"" or ""hard-mandatory"" in their forks of this repository or in other VCS repositories that contain copies of the policies.

## Adding Policies
If you add a new third-generation policy to one of the cloud directories or the cloud-agnostic directory, please add a new stanza to that directory's sentinel.hcl file listing the name and source of your new policy.

The Sentinel Simulator expects test cases to be in a test/\<policy\> directory under the directory containing the policy being tested where \<policy\> is the name of the policy not including the "".sentinel"" extension. When you add new policies for any of the clouds, please be sure to create a new directory with the same name of the policy under that cloud's directory and then add test cases and mock files to that directory. For `tfplan/v2` mocks, we recommend you remove the `planned_values` and `raw` collections unless your policy uses them; doing this makes it easier to replace values of resource attributes in copied mocks since you will only have to search the `resource_changes` collection.

Ensure that the pass and fail mocks cause the policy to pass and fail respectively. If you add a policy with multiple conditions, add mock files that fail each condition and one that fails all of them. You can also add mocks under the cloud's mocks directory if your policy uses a resource for which no mocks currently exist.

## Adding Common Functions
If you add a new common function to any of the Sentinel modules in this repository, please also add a new documentation file (`*.md`) for it in the associated docs directory and put a hyperlink on the title of the document pointing to the line in the module file where the function starts. Please also reference example functions or policies that use the new function.",FAUX
hashintel/hash,Toolkit,Application System,2025-05-15T18:04:48Z,2025-05-10T11:53:48Z,0,0,0,7,0,0,0,0,2019-07-15T16:19:36Z,2025-04-08T09:03:42Z,350195,1171,TypeScript,VRAI,89,FAUX,32,"ai,database,graph,hash,rust,superapp,type-system,typescript",32,"🚀  The open-source, multi-tenant, self-building knowledge graph",FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,51,"[app.hash.ai]: https://app.hash.ai?utm_medium=organic&utm_source=github_readme_hash-repo_root
[create an account]: https://app.hash.ai/signup?utm_medium=organic&utm_source=github_readme_hash-repo_root
[development roadmap]: https://hash.dev/roadmap?utm_medium=organic&utm_source=github_readme_hash-repo_root
[hiring]: https://hash.ai/careers?utm_medium=organic&utm_source=github_readme_hash-repo_root
[running your own instance]: https://hash.dev/docs/get-started/setup#local-hash?utm_medium=organic&utm_source=github_readme_hash-repo_root
[sign in]: https://app.hash.ai/signin?utm_medium=organic&utm_source=github_readme_hash-repo_root

<!-- markdownlint-disable link-fragments -->

[awesome hash]: https://github.com/hashintel/awesome-hash
[github_banner]: #hash
[github_star]: https://github.com/hashintel/hash#
[gh-what-is-hash]: #--what-is-hash
[gh-getting-started]: #--getting-started
[gh-examples]: #--examples
[gh-roadmap]: #--roadmap
[gh-repo-structure]: #--about-this-repository
[gh-contributing]: #--contributing
[gh-license]: #--license
[gh-security]: #--security
[gh-contact]: #--contact

[![github_banner](https://hash.ai/cdn-cgi/imagedelivery/EipKtqu98OotgfhvKf6Eew/ec83e48d-5a46-4c3f-a603-5d9fc43ff400/github)][github_banner]

[![github_star](https://img.shields.io/github/stars/hashintel/hash?label=Star%20on%20GitHub&style=social)][github_star]

# HASH

This is HASH's _public monorepo_ which contains our public code, docs, and other key resources.

## [![a](/.github/assets/gh_icon_what-is-hash_20px-base.svg)][gh-what-is-hash] &nbsp; What is HASH?

**HASH is a self-buliding, open-source database which grows, structures and checks itself.** HASH integrates data in (near-)realtime, and provides a powerful set of interfaces so that information can be understood and used in any context. Intelligent, autonomous agents can be deployed to grow, check, and maintain the database, integrating and structuring information from the public internet as well as your own connected private sources. And users, including those who are non-technical, are able to visually browse and manage both entities (data) and types (schemas). HASH acts as a source of truth for critical data, no matter its source, and provides a platform for high-trust, safety-assured decision-making. [Read our blog post →](https://hash.ai/blog/self-building-database)

**In the future...** we plan on growing HASH into an all-in-one workspace, or complete operating system, with AI-generated interfaces known as ""blocks"" created at the point of need, on top of your strongly-typed data (addressing the data quality and integrity challenges inherent in today's current generation of generative AI interfaces).

## [![a](/.github/assets/gh_icon_getting-started_20px-base.svg)][gh-getting-started] &nbsp; Getting started

<details>
  <summary> &nbsp; 🚀 <strong>Quick-start (<5 mins):</strong> use the hosted app</summary>

### Create an account

The only current ""officially supported"" way of trying HASH right now is by signing up for and using the hosted platform at [app.hash.ai]

[Create an account] to get started.

### Sign in

[Sign in] to access your account.

### Skip the queue

When you first create an account you may be placed on a waitlist. To jump the queue, once signed in, follow the instructions shown in your HASH dashboard. All submissions are reviewed by a member of the team.

</details>

<details>
  <summary> &nbsp; Running HASH locally</summary>

### Running HASH locally

**Running HASH locally is not yet officially supported.** We plan on publishing a comprehensive guide to [running your own instance] of HASH shortly (2025Q2). In the meantime, you may try the instructions below.

#### Experimental instructions

1. Make sure you have, [Git](https://git-scm.com), [Rust](https://www.rust-lang.org), [Docker](https://docs.docker.com/get-docker/), and [Protobuf](https://github.com/protocolbuffers/protobuf). Building the Docker containers requires [Docker Buildx](https://docs.docker.com/build/install-buildx/).
   Run each of these version commands and make sure the output is expected:

   ```sh
   git --version
   ## ≥ 2.17

   rustup --version
   ## ≥ 1.27.1 (Required to match the toolchain as specified in `rust-toolchain.toml`, lower versions most likely will work as well)

   docker --version
   ## ≥ 20.10

   docker compose version
   ## ≥ 2.17.2

   docker buildx version
   ## ≥ 0.10.4
   ```

   If you have difficulties with `git --version` on macOS you may need to install Xcode Command Line Tools first: `xcode-select --install`.

   If you use Docker for macOS or Windows, go to _Preferences_ → _Resources_ and ensure that Docker can use at least 4GB of RAM (8GB is recommended).

2. [Clone](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) this repository and **navigate to the root of the repository folder** in your terminal.

3. We use [mise-en-place](https://mise.jdx.dev/) to manage tool versions consistently across our codebase. We recommend using `mise` to automatically install and manage the required development tools:

   ```sh
   mise install
   ```

   It's also possible to install them manually, use the correct versions for these tools as specified in `.config/mise`.

   After [installing mise](https://mise.jdx.dev/getting-started.html#installing-mise-cli) you will also need to set it to [automatically activate](https://mise.jdx.dev/getting-started.html#activate-mise) in your shell.

4. Install dependencies:

   ```sh
   yarn install
   ```

5. Ensure Docker is running.
   If you are on Windows or macOS, you should see app icon in the system tray or the menu bar.
   Alternatively, you can use this command to check Docker:

   ```sh
   docker run hello-world
   ```

6. If you need to test or develop AI-related features, you will need to create an `.env.local` file in the repository root with the following values:

   ```sh
   OPENAI_API_KEY=your-open-ai-api-key                                      # required for most AI features
   ANTHROPIC_API_KEY=your-anthropic-api-key                                 # required for most AI features
   HASH_TEMPORAL_WORKER_AI_AWS_ACCESS_KEY_ID=your-aws-access-key-id         # required for most AI features
   HASH_TEMPORAL_WORKER_AI_AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key # required for most AI features
   E2B_API_KEY=your-e2b-api-key                                             # only required for the question-answering flow action
   ```

   **Note on environment files:** `.env.local` is not committed to the repo – **put any secrets that should remain secret here.** The default environment variables are taken from `.env`, extended by `.env.development`, and finally by `.env.local`. If you want to overwrite values specified in `.env` or `.env.development`, you can add them to `.env.local`. Do **not** change any other `.env` files unless you intend to change the defaults for development or testing.

7. Launch external services (Postgres, the graph query layer, Kratos, Redis, and OpenSearch) as Docker containers:

   ```sh
   yarn external-services up -d
   ```

   1. You can optionally force a rebuild of the Docker containers by adding the `--build` argument(**this is necessary if changes have been made to the graph query layer). It's recommended to do this whenever updating your branch from upstream**.

   2. You can keep external services running between app restarts by adding the `--detach` argument to run the containers in the background. It is possible to tear down the external services with `yarn external-services down`.

   3. When using `yarn external-services:offline up`, the Graph services does not try to connect to `https://blockprotocol.org` to fetch required schemas. This is useful for development when the internet connection is slow or unreliable.

   4. You can also run the Graph API and AI Temporal worker outside of Docker – this is useful if they are changing frequently and you want to avoid rebuilding the Docker containers. To do so, _stop them_ in Docker and then run `yarn dev:graph` and `yarn workspace @apps/hash-ai-worker-ts dev` respectively in separate terminals.

8. Launch app services:

   ```sh
   yarn start
   ```

   This will start backend and frontend in a single terminal. Once you see http://localhost:3000, the frontend end is ready to visit there.
   The API is online once you see `localhost:5001` in the terminal. Both must be online for the frontend to function.

   You can also launch parts of the app in separate terminals, e.g.:

   ```sh
   yarn start:graph
   yarn start:backend
   yarn start:frontend
   ```

   See `package.json` → `scripts` for details and more options.

9. Log in

   When the HASH API is started, three users are automatically seeded for development purposes. Their passwords are all `password`.

   - `alice@example.com`, `bob@example.com` – regular users
   - `admin@example.com` – an admin

##### Running the browser plugin

If you need to run the browser plugin locally, see the `README.md` in the `apps/plugin-browser` directory.

##### Resetting the local database

If you need to reset the local database, to clear out test data or because it has become corrupted during development, you have two options:

1. The slow option – rebuild in Docker

   1. In the Docker UI (or via CLI at your preference), stop and delete the `hash-external-services` container
   2. In 'Volumes', search 'hash-external-services' and delete the volumes shown
   3. Run `yarn external-services up --wait` to rebuild the services

2. The fast option – reset the database via the Graph API

   1. Run the Graph API in test mode by running `yarn dev:graph:test-server`
   2. Run `yarn graph:reset-database` to reset the database
   3. **If you need to use the frontend**, you will also need to delete the rows in the `identities` table in the `dev_kratos` database, or signin will not work. You can do so via any Postgres UI or CLI. The db connection and user details are in `.env`

##### External services test mode

The external services of the system can be started in 'test mode' to prevent polluting the development database.
This is useful for situations where the database is used for tests that modify the database without cleaning up afterwards.

To make use of this test mode, the external services can be started as follows:

```sh
yarn external-services:test up
```

</details>

<details>
  <summary> &nbsp; Deploying HASH to the cloud</summary>

##### Sending emails

Email-sending in HASH is handled by either Kratos (in the case of authentication-related emails) or through the HASH API Email Transport (for everything else).

To use `AwsSesEmailTransporter`, set `export HASH_EMAIL_TRANSPORTER=aws_ses` in your terminal before running the app. Valid AWS credentials are required for this email transporter to work.

Transactional emails templates are located in the following locations:

- Kratos emails in [`./../../apps/hash-external-services/kratos/templates/`](./../../apps/hash-external-services/kratos/templates/). This directory contains the following templates:
  - [`recovery_code`](./../../apps/hash-external-services/kratos/templates/recovery_code) - Email templates for the account recovery flow using a code for the UI.
    - When an email belongs to a registered HASH user, it will use the `valid` template, otherwise the `invalid` template is used.
  - [`verification_code`](./../../apps/hash-external-services/kratos/templates/verification_code) - Email verification templates for the account registration flow using a code for the UI.
    - When an email belongs to a registered HASH user, it will use the `valid` template, otherwise the `invalid` template is used.
- HASH emails in [`../hash-api/src/email/index.ts`](../hash-api/src/email/index.ts)

### Deploying HASH to the cloud

**Support for running HASH in the cloud is coming soon.** We plan on publishing a comprehensive guide to deploying HASH on AWS/GCP/Azure in the near future. In the meantime, instructions contained in the root [`/infra` directory](https://github.com/hashintel/hash/tree/main/infra) might help in getting started.

</details>

## [![a](/.github/assets/gh_icon_examples_20px-base.svg)][gh-examples] &nbsp; Examples

**Coming soon:** we'll be collecting examples in the _[Awesome HASH]_ repository.

## [![a](/.github/assets/gh_icon_roadmap_20px-base.svg)][gh-roadmap] &nbsp; Roadmap

Browse the HASH [development roadmap] for more information about currently in-flight and upcoming features.

## [![a](/.github/assets/gh_icon_repo-structure_20px-base.svg)][gh-repo-structure] &nbsp; About this repository

<details>
  <summary> &nbsp; Repository structure</summary>

### Repository structure

This repository's contents is divided across several primary sections:

- [**`/apps`**](/apps) contains the primary code powering our runnable [applications](https://github.com/hashintel/hash/tree/main/apps#applications)
  - The HASH application itself is divided into various different services which can be found in this directory.
- [**`/blocks`**](/blocks) contains our public _Block Protocol_ [blocks](https://github.com/hashintel/hash/tree/main/blocks#blocks)
- [**`/content`**](/content) contains our publicly-editable website [content](https://github.com/hashintel/hash/tree/main/content#content) (e.g. glossary definitions, user docs)
- [**`/infra`**](/infra) houses deployment scripts, utilities and other [infrastructure](https://github.com/hashintel/hash/tree/main/infra#infrastructure) useful in running our apps
- [**`/libs`**](/libs) contains [libraries](https://github.com/hashintel/hash/tree/main/libs#libraries) including npm packages and Rust crates
- [**`/tests`**](/tests) contains end-to-end and integration tests that span across one or more apps, blocks or libs

</details>

<details>
  <summary> &nbsp; Environment variables</summary>

### Environment variables

Here's a list of possible environment variables. Everything that's necessary already has a default value.

You **do not** need to set any environment variables to run the application.

#### General API server environment variables

- `NODE_ENV`: (""development"" or ""production"") the runtime environment. Controls
  default logging levels and output formatting.
- `PORT`: the port number the API will listen on.

#### AWS configuration

If you want to use AWS for file uploads or emails, you will need to have it configured:

- `AWS_REGION`: The region, eg. `us-east-1`
- `AWS_ACCESS_KEY_ID`: Your AWS access key
- `AWS_SECRET_ACCESS_KEY`: Your AWS secret key
- `AWS_S3_UPLOADS_BUCKET`: The name of the bucket to use for file uploads (if you want to use S3 for file uploads), eg: `my_uploads_bucket`
- `AWS_S3_UPLOADS_ACCESS_KEY_ID`: (optional) the AWS access key ID to use for file uploads. Must be provided along with the secret access key if the API is not otherwise authorized to access the bucket (e.g. via an IAM role).
- `AWS_S3_UPLOADS_SECRET_ACCESS_KEY`: (optional) the AWS secret access key to use for file uploads.
- `AWS_S3_UPLOADS_ENDPOINT`: (optional) the endpoint to use for S3 operations. If not, the AWS S3 default for the given region is used. Useful if you are using a different S3-compatible storage provider.
- `AWS_S3_UPLOADS_FORCE_PATH_STYLE`: (optional) set `true` if your S3 setup requires path-style rather than virtual hosted-style S3 requests.

For some in-browser functionality (e.g. document previewing), you must configure a Access-Control-Allow-Origin header on your bucket to be something other than '\*'.

#### File uploads

By default, files are uploaded locally, which is **not** recommended for production use. It is also possible to upload files on AWS S3.

- `FILE_UPLOAD_PROVIDER`: Which type of provider is used for file uploads. Possible values `LOCAL_FILE_SYSTEM`, or `AWS_S3`. If choosing S3, then you need to configure the `AWS_S3_UPLOADS_` variables above.
- `LOCAL_FILE_UPLOAD_PATH`: Relative path to store uploaded files if using the local file system storage provider. Default is `var/uploads` (the `var` folder is the folder normally used for application data)

#### Email

During development, the dummy email transporter writes emails to a local folder.

- `HASH_EMAIL_TRANSPORTER`: `dummy` or `aws`. If set to dummy, the local dummy email transporter will be used during development instead of aws (default: `dummy`)
- `DUMMY_EMAIL_TRANSPORTER_FILE_PATH`: Default is `var/api/dummy-email-transporter/email-dumps.yml`
- `DUMMY_EMAIL_TRANSPORTER_USE_CLIPBOARD`: `true` or `false` (default: `true`)

#### OpenSearch

**NOTE: OpenSearch is currently disabled by default, and is presently unmaintained.**

- `HASH_OPENSEARCH_ENABLED`: whether OpenSearch is used or not. `true` or `false`. (default: `false`).
- `HASH_OPENSEARCH_HOST`: the hostname of the OpenSearch cluster to connect to. (default: `localhost`)
- `HASH_OPENSEARCH_PASSWORD`: the password to use when making the connection. (default: `admin`)
- `HASH_OPENSEARCH_PORT`: the port number that the cluster accepts (default: `9200`)
- `HASH_OPENSEARCH_USERNAME`: the username to connect to the cluster as. (default: `admin`)
- `HASH_OPENSEARCH_HTTPS_ENABLED`: (optional) set to ""1"" to connect to the cluster
  over an HTTPS connection.

#### Postgres

- `POSTGRES_PORT` (default: `5432`)

Various services also have their own configuration.

The Postgres superuser is configured through:

- `POSTGRES_USER` (default: `postgres`)
- `POSTGRES_PASSWORD` (default: `postgres`)

The Postgres information for Kratos is configured through:

- `HASH_KRATOS_PG_USER` (default: `kratos`)
- `HASH_KRATOS_PG_PASSWORD` (default: `kratos`)
- `HASH_KRATOS_PG_DATABASE` (default: `kratos`)

The Postgres information for Temporal is configured through:

- `HASH_TEMPORAL_PG_USER` (default: `temporal`)
- `HASH_TEMPORAL_PG_PASSWORD` (default: `temporal`)
- `HASH_TEMPORAL_PG_DATABASE` (default: `temporal`)
- `HASH_TEMPORAL_VISIBILITY_PG_DATABASE` (default: `temporal_visibility`)

The Postgres information for the graph query layer is configured through:

- `HASH_GRAPH_PG_USER` (default: `graph`)
- `HASH_GRAPH_PG_PASSWORD` (default: `graph`)
- `HASH_GRAPH_PG_DATABASE` (default: `graph`)

#### Redis

- `HASH_REDIS_HOST` (default: `localhost`)
- `HASH_REDIS_PORT` (default: `6379`)

#### Statsd

If the service should report metrics to a StatsD server, the following variables must be set.

- `STATSD_ENABLED`: Set to ""1"" if the service should report metrics to a StatsD server.
- `STATSD_HOST`: the hostname of the StatsD server.
- `STATSD_PORT`: (default: 8125) the port number the StatsD server is listening on.

#### Snowplow telemetry

- `HASH_TELEMETRY_ENABLED`: whether Snowplow is used or not. `true` or `false`. (default: `false`)
- `HASH_TELEMETRY_HTTPS`: set to ""1"" to connect to the Snowplow over an HTTPS connection. `true` or `false`. (default: `false`)
- `HASH_TELEMETRY_DESTINATION`: the hostname of the Snowplow tracker endpoint to connect to. (required)
- `HASH_TELEMETRY_APP_ID`: ID used to differentiate application by. Can be any string. (default: `hash-workspace-app`)

#### Others

- `FRONTEND_URL`: URL of the frontend website for links (default: `http://localhost:3000`)
- `NOTIFICATION_POLL_INTERVAL`: the interval in milliseconds at which the frontend will poll for new notifications, or 0 for no polling. (default: `10_000`)
- `HASH_INTEGRATION_QUEUE_NAME` The name of the Redis queue which updates to entities are published to
- `HASH_REALTIME_PORT`: Realtime service listening port. (default: `3333`)
- `HASH_SEARCH_LOADER_PORT`: (default: `3838`)
- `HASH_SEARCH_QUEUE_NAME`: The name of the queue to push changes for the search loader service (default: `search`)
- `API_ORIGIN`: The origin that the API service can be reached on (default: `http://localhost:5001`)
- `SESSION_SECRET`: The secret used to sign sessions (default: `secret`)
- `LOG_LEVEL`: the level of runtime logs that should be omitted, either set to `debug`, `info`, `warn`, `error` (default: `info`)
- `BLOCK_PROTOCOL_API_KEY`: the api key for fetching blocks from the [Þ Hub](https://blockprotocol.org/hub). Generate a key at https://blockprotocol.org/settings/api-keys.

</details>

## [![a](/.github/assets/gh_icon_contributing_20px-base.svg)][gh-contributing] &nbsp; Contributing

Please see [CONTRIBUTING](.github/CONTRIBUTING.md) if you're interested in getting involved in the design or development of HASH.

We're also [hiring] for a number of key roles. We don't accept applications for engineering roles like a normal company might, but exclusively headhunt (using HASH as a tool to help us find the best people). Contributing to our public monorepo, even in a small way, is one way of _guaranteeing_ you end up on our radar as every PR is reviewed by a human, as well as AI.

We also provide repo-specific [example configuration files](/.config/_examples) you can use for popular IDEs, including [VSCode](/.config/_examples/vscode) or [Zed](/.config/_examples/zed).

## [![a](/.github/assets/gh_icon_license_20px-base.svg)][gh-license] &nbsp; License

The vast majority of this repository is published as free, open-source software. Please see [LICENSE](LICENSE.md) for more information about the specific licenses under which the different parts are available.

## [![a](/.github/assets/gh_icon_security_20px-base.svg)][gh-security] &nbsp; Security

Please see [SECURITY](.github/SECURITY.md) for instructions around reporting issues, and details of which package versions we actively support.

## [![a](/.github/assets/gh_icon_contact_20px-base.svg)][gh-contact] &nbsp; Contact

Find us on 𝕏 at [@hashintel](https://x.com/hashintel), email [hey@hash.ai](mailto:hey@hash.ai), create a [discussion](https://github.com/orgs/hashintel/discussions), or open an [issue](https://github.com/hashintel/hash/issues/new/choose) for quick help and community support.

Project permalink: `https://github.com/hashintel/hash`",VRAI
hexikani/pulsechain-projects,Documentations,Documentations,2024-07-23T22:29:29Z,2023-12-09T04:09:27Z,0,0,0,0,0,2,0,0,2022-08-27T03:47:46Z,2025-01-28T12:11:38Z,9806,5,,VRAI,5,FAUX,0,,0,Open Source repository of PulseChain projects and tokens in machine processable form,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,2,"# PulseChain Projects Repository
Open Source repository of PulseChain and HEX-related products, projects, site and tokens in machine processable form (YAML).
This documentation uses for simplicity term _project_ do denote all types of entities described in this repository.

# Data Structure
Each project (product, site, or token) is described by a single file in the `data/` subdirectory.
Each file is in [YAML](https://yaml.org/) format, with `.yaml` extension, starting with `---` signature.

Allowed root keys:
  - `name` - (mandatory) a human readable name of the project.
  - `status` - (mandatory) enumeration describing stage of implementation and deployment, allowed values are:
    - `promise` - the project exists mostly on paper or community is building.
    - `active` - project is in active development with publicly available artifacts (application previews, progress reports, community discussion and polls, etc.).
    - `completed` - project was fully completed and waits for its final deployment.
    - `usable` - project turned into product and can be used of bought on market.
    - `dormant` - project community is quite silent, there are no real updates from the project team.
    - `abandoned` - project community is completely silent, project team does not respond in a timely manner.
    - `terminated` - project was terminated, without usable artifiacts.
    - `rugged` - there are clear signs that project was rug pulled (social media channel closed, liquidity suddenly removed, etc.).
    - `scam` - there are clear signs that project was a scam.
  - `description` - long description of the project.
  - `www` - fully qualified URL to main page of the project.
  - `categories` - describes detailed metadata about projects' ""sacrifice"" TBD.
  - `socials` - describes social media accounts related to this project, each social media is associated with a key.
    Please note, that values of this dictionary are not URLs but only account identifiers.  
    Allowed keys are:
    - `twitter` - account for Twitter.
    - `tg` - account for Telegram.
    - `fb` - account for Facebook.
    - `yt` - account for YouTube.
    - `ig` - account for Instagram.
    - `email` - primary contact email address.
    - `github` - account for GitHub.
    - `reddit` - account for Reddit.
    - `medium` - account for Medium.
    - `discord` - account for Discord.
  - `links` - array of links related to the project.
    Each link has structure `<link-type>: <link-url>`, where type can be potentially any identifier.
    Same type of link can be used multiple types.  
    Recommended link types are:
    - `app` - link to mobile app.
    - `dapp` - link to dApp web site.
    - `docs` - link to web site project documentation.
    - `audit` - link to projects audit report.
    - `kyc` - link to KYC certificate for project or project members.
  - `sacrifice` - describes detailed metadata about projects' ""sacrifice"" TBD.
  - `token` - describes detailed metadata about the token related to this project TBD.",FAUX
hitcherland/FoundryVTT-Heart,Application System,Documentations,2024-09-03T21:13:21Z,2024-07-23T19:11:56Z,0,0,0,0,0,1,0,0,2021-06-10T21:41:50Z,2024-10-12T07:06:38Z,2772,8,JavaScript,VRAI,16,FAUX,18,,18,Basic FoundryVTT System for Heart - The City Beneath,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,7,[Error reading file],VRAI
hmrc/address-lookup,Application System,Documentations,2024-10-29T14:45:49Z,2024-02-19T07:03:59Z,0,0,0,0,0,0,0,0,2017-02-07T20:11:46Z,2024-10-29T14:45:58Z,3814,6,Scala,VRAI,9,FAUX,3,,3,,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,23,"# Address Lookup

## Overview
REST microservice that implements the lookup of postal and BFPO (British Forces Post Office) address details within the UK.

## Documentation
Please read the [**address-lookup API definition**](public/api/conf/1.0/docs/address-lookup-api.md) for more information.

## Dependencies
* SBT
* Java 8
* Play Framework
* `address-search-api` 

## Running / testing locally
Run `address-lookup` services using `sm2`:
```bash
sm2 start ADDRESS_LOOKUP_SERVICES
```

## Development
The unit tests can be run as follows:
```bash
sbt test
```

The integration tests can be run as follows:
```bash
sbt it:test
```

## Local Demo Mode
```bash
sbt ""run 9022""
```

## License
This code is open source software licensed under the [Apache 2.0 License](""http://www.apache.org/licenses/LICENSE-2.0.html"").",VRAI
hogeschool/webdev-semester,Documentations,Documentations,2024-11-25T12:31:41Z,2024-07-03T12:06:01Z,2,0,0,0,0,0,0,0,2024-01-29T13:50:14Z,2025-03-15T19:10:19Z,2317,42,JavaScript,VRAI,9,FAUX,0,,0,,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,5,,FAUX
huaweicloud/cloud-custodian,Toolkit,Toolkit,2025-05-16T03:46:52Z,2025-04-10T06:14:30Z,0,0,0,0,0,93,5,0,2025-02-06T01:22:26Z,2025-04-07T11:54:49Z,55192,8,Python,FAUX,29,FAUX,4,,4,"Rules engine for cloud security, cost optimization, and governance, DSL in yaml for policies to query, filter, and take actions on resources",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,528,"Cloud Custodian (c7n)
=================

<p align=""center""><img src=""https://cloudcustodian.io/img/logo_capone_devex_cloud_custodian.svg"" alt=""Cloud Custodian Logo"" width=""200px"" height=""200px"" /></p>

---

[![slack](https://img.shields.io/badge/slack-chat-yellow)](https://communityinviter.com/apps/cloud-custodian/c7n-chat)
[![CI](https://github.com/cloud-custodian/cloud-custodian/workflows/CI/badge.svg?event=push)](https://github.com/cloud-custodian/cloud-custodian/actions?query=workflow%3ACI+branch%3Amaster+event%3Apush)
[![](https://img.shields.io/badge/license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0)
[![](https://codecov.io/gh/cloud-custodian/cloud-custodian/branch/master/graph/badge.svg)](https://codecov.io/gh/cloud-custodian/cloud-custodian)
[![](https://requires.io/github/cloud-custodian/cloud-custodian/requirements.svg?branch=master)](https://requires.io/github/cloud-custodian/cloud-custodian/requirements/?branch=master)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3402/badge)](https://bestpractices.coreinfrastructure.org/projects/3402)

Cloud Custodian, also known as c7n, is a rules engine for managing
public cloud accounts and resources. It allows users to define
policies to enable a well managed cloud infrastructure, that\'s both
secure and cost optimized. It consolidates many of the adhoc scripts
organizations have into a lightweight and flexible tool, with unified
metrics and reporting.

Custodian can be used to manage AWS, Azure, and GCP environments by
ensuring real time compliance to security policies (like encryption and
access requirements), tag policies, and cost management via garbage
collection of unused resources and off-hours resource management.

Custodian also supports running policies on infrastructure as code assets
to provide feedback directly on developer workstations or within CI pipelines.

Custodian policies are written in simple YAML configuration files that
enable users to specify policies on a resource type (EC2, ASG, Redshift,
CosmosDB, PubSub Topic) and are constructed from a vocabulary of filters
and actions.

It integrates with the cloud native serverless capabilities of each
provider to provide for real time enforcement of policies with builtin
provisioning. Or it can be run as a simple cron job on a server to
execute against large existing fleets.

Cloud Custodian is a CNCF Incubating project, lead by a community of hundreds
of contributors.

Features
--------

-   Comprehensive support for public cloud services and resources with a
    rich library of actions and filters to build policies with.
-   Run policies on infrastructure as code (terraform, etc) assets.	
-   Supports arbitrary filtering on resources with nested boolean
    conditions.
-   Dry run any policy to see what it would do.
-   Automatically provisions serverless functions and event sources (
    AWS CloudWatchEvents, AWS Config Rules, Azure EventGrid, GCP
    AuditLog & Pub/Sub, etc)
-   Cloud provider native metrics outputs on resources that matched a
    policy
-   Structured outputs into cloud native object storage of which
    resources matched a policy.
-   Intelligent cache usage to minimize api calls.
-   Supports multi-account/subscription/project usage.
-   Battle-tested - in production on some very large cloud environments.

Links
-----

-   [Homepage](http://cloudcustodian.io)
-   [Docs](http://cloudcustodian.io/docs/index.html)
-   [Project Roadmap](https://github.com/orgs/cloud-custodian/projects/1)
-   [Developer Install](https://cloudcustodian.io/docs/developer/installing.html)
-   [Presentations](https://www.google.com/search?q=cloud+custodian&source=lnms&tbm=vid)
-   [YouTube Channel](https://www.youtube.com/channel/UCdeXCdFLluylWnFfS0-jbDA)

Quick Install
-------------

Custodian is published on pypi as a series of packages with the `c7n`
prefix, its also available as a docker image.

```shell
$ python3 -m venv custodian
$ source custodian/bin/activate
(custodian) $ pip install c7n
```


Usage
-----

The first step to using Cloud Custodian (c7n) is writing a YAML file
containing the policies that you want to run. Each policy specifies
the resource type that the policy will run on, a set of filters which
control resources will be affected by this policy, actions which the policy
with take on the matched resources, and a mode which controls which
how the policy will execute.

The best getting started guides are the cloud provider specific tutorials.

 - [AWS Getting Started](https://cloudcustodian.io/docs/aws/gettingstarted.html)
 - [Azure Getting Started](https://cloudcustodian.io/docs/azure/gettingstarted.html)
 - [GCP Getting Started](https://cloudcustodian.io/docs/gcp/gettingstarted.html)

As a quick walk through, below are some sample policies for AWS resources.

  1. will enforce that no S3 buckets have cross-account access enabled.
  1. will terminate any newly launched EC2 instance that do not have an encrypted EBS volume.
  1. will tag any EC2 instance that does not have the follow tags
     ""Environment"", ""AppId"", and either ""OwnerContact"" or ""DeptID"" to
     be stopped in four days.

```yaml
policies:
 - name: s3-cross-account
   description: |
     Checks S3 for buckets with cross-account access and
     removes the cross-account access.
   resource: aws.s3
   region: us-east-1
   filters:
     - type: cross-account
   actions:
     - type: remove-statements
       statement_ids: matched

 - name: ec2-require-non-public-and-encrypted-volumes
   resource: aws.ec2
   description: |
    Provision a lambda and cloud watch event target
    that looks at all new instances and terminates those with
    unencrypted volumes.
   mode:
    type: cloudtrail
    role: CloudCustodian-QuickStart
    events:
      - RunInstances
   filters:
    - type: ebs
      key: Encrypted
      value: false
   actions:
    - terminate

 - name: tag-compliance
   resource: aws.ec2
   description: |
     Schedule a resource that does not meet tag compliance policies to be stopped in four days. Note a separate policy using the`marked-for-op` filter is required to actually stop the instances after four days.
   filters:
    - State.Name: running
    - ""tag:Environment"": absent
    - ""tag:AppId"": absent
    - or:
      - ""tag:OwnerContact"": absent
      - ""tag:DeptID"": absent
   actions:
    - type: mark-for-op
      op: stop
      days: 4
```

You can validate, test, and run Cloud Custodian with the example policy with these commands:

```shell
# Validate the configuration (note this happens by default on run)
$ custodian validate policy.yml

# Dryrun on the policies (no actions executed) to see what resources
# match each policy.
$ custodian run --dryrun -s out policy.yml

# Run the policy
$ custodian run -s out policy.yml
```

You can run Cloud Custodian via Docker as well:

```shell
# Download the image
$ docker pull cloudcustodian/c7n
$ mkdir output

# Run the policy
#
# This will run the policy using only the environment variables for authentication
$ docker run -it \
  -v $(pwd)/output:/home/custodian/output \
  -v $(pwd)/policy.yml:/home/custodian/policy.yml \
  --env-file <(env | grep ""^AWS\|^AZURE\|^GOOGLE"") \
  cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml

# Run the policy (using AWS's generated credentials from STS)
#
# NOTE: We mount the ``.aws/credentials`` and ``.aws/config`` directories to
# the docker container to support authentication to AWS using the same credentials
# credentials that are available to the local user if authenticating with STS.

$ docker run -it \
  -v $(pwd)/output:/home/custodian/output \
  -v $(pwd)/policy.yml:/home/custodian/policy.yml \
  -v $(cd ~ && pwd)/.aws/credentials:/home/custodian/.aws/credentials \
  -v $(cd ~ && pwd)/.aws/config:/home/custodian/.aws/config \
  --env-file <(env | grep ""^AWS"") \
  cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml
```

The [custodian cask
tool](https://cloudcustodian.io/docs/tools/cask.html) is a go binary
that provides a transparent front end to docker that mirors the regular
custodian cli, but automatically takes care of mounting volumes.

Consult the documentation for additional information, or reach out on gitter.

Cloud Provider Specific Help
----------------------------

For specific instructions for AWS, Azure, and GCP, visit the relevant getting started page.

- [AWS](https://cloudcustodian.io/docs/aws/gettingstarted.html)
- [Azure](https://cloudcustodian.io/docs/azure/gettingstarted.html)
- [GCP](https://cloudcustodian.io/docs/gcp/gettingstarted.html)

Get Involved
------------

-   [GitHub](https://github.com/cloud-custodian/cloud-custodian) - (This page)
-   [Slack](https://communityinviter.com/apps/cloud-custodian/c7n-chat) - Real time chat if you're looking for help or interested in contributing to Custodian! 
    - [Gitter](https://gitter.im/cloud-custodian/cloud-custodian) - (Older real time chat, we're likely migrating away from this)
-   [Linen.dev](https://www.linen.dev/s/cloud-custodian/c/general) - Follow our discussions on Linen
-   [Mailing List](https://groups.google.com/forum/#!forum/cloud-custodian) - Our project mailing list, subscribe here for important project announcements, feel free to ask questions
-   [Reddit](https://reddit.com/r/cloudcustodian) - Our subreddit
-   [StackOverflow](https://stackoverflow.com/questions/tagged/cloudcustodian) - Q&A site for developers, we keep an eye on the `cloudcustodian` tag
-   [YouTube Channel](https://www.youtube.com/channel/UCdeXCdFLluylWnFfS0-jbDA/) - We're working on adding tutorials and other useful information, as well as meeting videos

Community Resources
-------------------

We have a regular community meeting that is open to all users and developers of every skill level.
Joining the [mailing list](https://groups.google.com/forum/#!forum/cloud-custodian) will automatically send you a meeting invite. 
See the notes below for more technical information on joining the meeting. 

- [Community Meeting Videos](https://www.youtube.com/watch?v=qy250y0UT-4&list=PLJ2Un8H_N5uBeAAWK95SnWvm_AuNJ8q2x)
- [Community Meeting Notes Archive](https://github.com/orgs/cloud-custodian/discussions/categories/announcements)
- [Upcoming Community Events](https://cloudcustodian.io/events/)
- [Cloud Custodian Annual Report 2021](https://github.com/cncf/toc/blob/main/reviews/2021-cloud-custodian-annual.md) - Annual health check provided to the CNCF outlining the health of the project
- [Ada Logics Third Party Security Audit](https://ostif.org/cc-audit-complete/)


Additional Tools
----------------

The Custodian project also develops and maintains a suite of additional
tools here
<https://github.com/cloud-custodian/cloud-custodian/tree/master/tools>:

- [**_Org_:**](https://cloudcustodian.io/docs/tools/c7n-org.html) Multi-account policy execution.

- [**_ShiftLeft_:**](https://cloudcustodian.io/docs/tools/c7n-left.html) Shift Left ~ run policies against Infrastructure as Code assets like terraform.

- [**_PolicyStream_:**](https://cloudcustodian.io/docs/tools/c7n-policystream.html) Git history as stream of logical policy changes.

- [**_Salactus_:**](https://cloudcustodian.io/docs/tools/c7n-salactus.html) Scale out s3 scanning.

- [**_Mailer_:**](https://cloudcustodian.io/docs/tools/c7n-mailer.html) A reference implementation of sending messages to users to notify them.

- [**_Trail Creator_:**](https://cloudcustodian.io/docs/tools/c7n-trailcreator.html) Retroactive tagging of resources creators from CloudTrail

- **_TrailDB_:** Cloudtrail indexing and time series generation for dashboarding.

- [**_LogExporter_:**](https://cloudcustodian.io/docs/tools/c7n-logexporter.html) Cloud watch log exporting to s3

- [**_Cask_:**](https://cloudcustodian.io/docs/tools/cask.html) Easy custodian exec via docker

- [**_Guardian_:**](https://cloudcustodian.io/docs/tools/c7n-guardian.html) Automated multi-account Guard Duty setup

- [**_Omni SSM_:**](https://cloudcustodian.io/docs/tools/omnissm.html) EC2 Systems Manager Automation

- [**_Mugc_:**](https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/ops#mugc) A utility used to clean up Cloud Custodian Lambda policies that are deployed in an AWS environment.

Contributing
------------

See <https://cloudcustodian.io/docs/contribute.html>

Security
--------

If you've found a security related issue, a vulnerability, or a
potential vulnerability in Cloud Custodian please let the Cloud
[Custodian Security Team](mailto:security@cloudcustodian.io) know with
the details of the vulnerability. We'll send a confirmation email to
acknowledge your report, and we'll send an additional email when we've
identified the issue positively or negatively.

Code of Conduct
---------------

This project adheres to the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md)

By participating, you are expected to honor this code.",FAUX
Hyperledger-TWGC/tape,Toolkit,Toolkit,2025-01-08T07:28:11Z,2024-09-20T07:29:31Z,0,2,0,0,0,0,0,0,2019-09-04T13:29:56Z,2025-03-07T01:51:09Z,1446,220,Go,VRAI,69,FAUX,6,"bottleneck,fabric,hyperledger-fabric,performance,test,tool,traffic-generator",6,A Simple Traffic Generator for Hyperledger Fabric,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,16,"# Tape
<div align=""center"">
<img src=""logo.svg"" width=""100"">
</div>
Tape 是一款轻量级 Hyperledger Fabric 性能测试工具

[![Go doc](https://img.shields.io/badge/go.dev-reference-brightgreen?logo=go&logoColor=white&style=flat)](https://pkg.go.dev/github.com/hyperledger-twgc/tape)
[![Github workflow test](https://github.com/Hyperledger-TWGC/tape/actions/workflows/test.yml/badge.svg)](https://github.com/Hyperledger-TWGC/tape/actions/workflows/test.yml)

## 项目背景

Tape 项目原名 Stupid，最初由 超级账本中国技术工作组成员[郭剑南](https://github.com/guoger)开发，目的是提供一款轻量级、可以快速测试 Hyperledger Fabric TPS 值的工具。Stupid 取自[KISS](https://en.wikipedia.org/wiki/KISS_principle) 原则 Keep it Simple and Stupid，目前已正式更名为Tape，字面含义卷尺，寓意测量，测试。

目前 Tape 已贡献到超级账本中国技术社区，由[TWGC 性能优化小组](https://github.com/Hyperledger-TWGC/fabric-performance-wiki)负责维护。

## 项目特点

1. **轻量级**， Tape 实现过程中没有使用 SDK，直接使用 gRPC 向 Fabric 节点发送和接收请求；
2. **易操作**，通过简单的配置文件和命令即可快速启动测试；
3. **结果准确**，Tape 直接使用 gRPC 发送交易，并且对交易和区块处理的不同阶段单独拆分，使用协程及通道缓存的方式并行处理，大幅度提升了 Tape 自身的处理效率，从而可以准确的测试出 Fabric 的真实性能。
4. **参考标准** 其设计和功能参考[性能测试白皮书](https://github.com/Hyperledger-TWGC/fabric-performance-wiki/blob/master/performance-whitepaper.md)。

Tape由负载生成器客户端和观察者客户端组成。因此Tape仅可以用来对已经完成部署的Fabric网络进行测试。
- 负载生成器客户端
  - 直接使用了GRPC链接到被测网络而不使用任何SDK。因此避免了connection profile的配置， 减少了SDK的其他功能，如服务发现，可能带来的性能损耗。
- 观察者客户端会观察在多个peer节点上的提交，但不会进行资源的实时监控。

## 文档索引

如果你想快速使用 Tape 测试 TPS，请参考[快速开始](docs/zh/gettingstarted.md)；

如果你想了解配置文件中各项参数的具体含义，请参考[配置文件说明](docs/zh/configfile.md)；

如果你想详细了解 Tape 工作流程，请参考[工作流程](docs/zh/workflow.md)；

如果你在使用过程中遇到了问题请参考[FAQ](https://github.com/Hyperledger-TWGC/tape/wiki/FAQ)，如果 FAQ 还不能解决你的问题，请在 github 中提 issue，或者发邮件咨询项目维护者。


## [如何贡献](CONTRIBUTING.md)

## [维护者信息](MAINTAINERS.md)

## 使用许可

Tape 遵守 [Apache 2.0 开源许可](LICENSE)。

## Credits
Icons made by <a href=""https://www.flaticon.com/authors/good-ware"" title=""Good Ware"">Good Ware</a> from <a href=""https://www.flaticon.com/"" title=""Flaticon"">www.flaticon.com</a>",FAUX
iann0036/former2,Toolkit,Application System,2025-04-19T01:03:22Z,2023-12-15T02:26:15Z,0,0,1,0,0,0,1,0,2019-03-01T12:06:47Z,2025-04-08T06:27:12Z,68654,2303,JavaScript,VRAI,277,FAUX,178,"aws,cdk,cdktf,cloudformation,iac,pulumi,terraform,troposphere",178,Generate CloudFormation / Terraform / Troposphere templates from your existing AWS resources.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,31,"# Former2

[![CloudFormation](https://img.shields.io/badge/CloudFormation-84%25-orange.svg)](RESOURCE_COVERAGE.md#cloudformation-resource-coverage) [![Terraform](https://img.shields.io/badge/Terraform-49%25-blue.svg)](RESOURCE_COVERAGE.md#terraform-coverage)

> Generate CloudFormation / Terraform / Troposphere templates from your existing AWS resources


![Screenshot](img/screen1.png)

![Screenshot](img/screen2.png)

## Overview

Former2 allows you to generate Infrastructure-as-Code outputs from your existing resources within your AWS account. By making the relevant calls using the AWS JavaScript SDK, Former2 will scan across your infrastructure and present you with the list of resources for you to choose which to generate outputs for.

## Installation

Though [some AWS services](https://github.com/aws/aws-sdk-js/blob/master/SERVICES.md) do not require it, you will need to install the Former2 Helper browser extension in order to have support for all AWS services. The extension exists to bypass a lack of CORS on some services, such as S3 and IAM.

[Install Former2 Helper for Google Chrome](https://chrome.google.com/webstore/detail/former2-helper/fhejmeojlbhfhjndnkkleooeejklmigi)

[Install Former2 Helper for Mozilla Firefox](https://addons.mozilla.org/en-US/firefox/addon/former2-helper/)

[Install Former2 Helper for Microsoft Edge](https://microsoftedge.microsoft.com/addons/detail/okkjnfohglnomdbpimkcdkiojbeiedof)

Alternatively, you can [download and install](https://github.com/iann0036/former2-helper) the extension yourself.

## Usage

Visit [former2.com](https://former2.com/) to start.

You will need an [IAM key pair](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) to authenticate your requests. If you are not planning on importing resources directly, it is recommended that you provide only read access with these credentials and suggest you assign the [ReadOnlyAccess](https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/ReadOnlyAccess) policy. If you intend to use the Import feature, you should grant appropriate permissions to create the stack.

Once authenticated you can navigate via the dashboard or sidebar to specific services, select the resources to add to your outputs and finally click the ""Generate"" button at the top of the screen.

The following outputs are currently supported:

* CloudFormation
* Terraform
* Troposphere
* CDK V1 (Cfn Primitives) - TypeScript, Python, Java, C#
* CDK V2 (Cfn Primitives) - TypeScript, Python, Java, C#
* CDK for Terraform - TypeScript
* Pulumi - TypeScript
* Diagram - embedded version of [draw.io](https://github.com/jgraph/drawio)

### Former2 CLI

A command-line version of Former2 with limited functionality is available for installation. For more information, see the [CLI instructions](cli/README.md).

### LocalStack Support

To enable support for use against [LocalStack](https://docs.localstack.cloud/references/network-troubleshooting/endpoint-url/) endpoints, enable the setting in the Settings page, ensure you are using the Former2 Helper extension/add-on, and add the following [configuration](https://docs.localstack.cloud/references/configuration/) to LocalStack:

_For Google Chrome:_
```
EXTRA_CORS_ALLOWED_ORIGINS=chrome-extension://fhejmeojlbhfhjndnkkleooeejklmigi
```

_For Mozilla Firefox:_
```
EXTRA_CORS_ALLOWED_ORIGINS=moz-extension://853c673f-1bd8-4226-a5ff-f1473f7b3d90
```

_For Microsoft Edge:_
```
EXTRA_CORS_ALLOWED_ORIGINS=extension://okkjnfohglnomdbpimkcdkiojbeiedof
```

## Security

Former2 does not create any resources within your AWS account.

Calls to the AWS service API endpoints are made either directly with the JavaScript SDK or via the browser extension (which also hits endpoints directly). Resource data and your credentials are kept entirely in memory and is never sent over the internet or anywhere else. The credentials are only used to sign requests to AWS endpoints. You should take care to remove any sensitive data (passwords etc.) when sharing your generated code/templates with others.

If you prefer not to use a publicly hosted site to use Former2, you can [host your own version](HOSTING.md) by running a HTTP server from the root of the repository. Extension support will also be available if you host on 127.0.0.1 or localhost, otherwise you can [modify the extension](HOSTING.md) as needed.

## Pricing

Though Former2 is free to access or use locally, some [AWS services](https://aws.amazon.com/secrets-manager/pricing/) have small charges associated with API calls so usage may attract an extra couple cents to your AWS bill.

## FAQ

**Does this replace Console Recorder?**

[Console Recorder](https://github.com/iann0036/AWSConsoleRecorder) still fills a certain gap and has features this tool does not support (such as Get/List outputs). I'll be maintaining both for as long as I can.

**I found an issue / I'm missing an output / Something's wrong. Can you help?**

If you find a bug or want to raise a feature request, please do so via [the issues page](https://github.com/iann0036/former2/issues).",VRAI
iann0036/iamlive,Toolkit,Application System,2025-04-28T10:43:29Z,2024-06-28T22:47:19Z,0,0,0,0,0,0,6,0,2021-02-04T10:57:02Z,2025-04-02T09:01:14Z,21523,3227,Go,VRAI,111,FAUX,41,"aws,aws-iam,aws-iam-policies,azure,azure-rbac,gcp,gcp-iam,iam,least-privilege",41,"Generate an IAM policy from AWS, Azure, or Google Cloud (GCP) calls using client-side monitoring (CSM) or embedded proxy",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,12,"# iamlive

> Generate an IAM policy from AWS, Azure, or Google Cloud (GCP) calls using client-side monitoring (CSM) or embedded proxy

![](https://raw.githubusercontent.com/iann0036/iamlive/assets/iamlive.gif)

> [!IMPORTANT]  
> The Azure and Google Cloud providers are in preview and may produce incorrect outputs at this time

## Installation

### Pre-built binaries

Pre-built binaries for Windows, macOS and Linux are available for download in the project [releases](https://github.com/iann0036/iamlive/releases).

Once downloaded, place the extracted binary in your $PATH (or execute in-place). For macOS users, you may need to allow the application to run via System Preferences.

### Build with Go

To build and install this application, clone this repository and execute the following from it's base:

```
go install
```

You must have Go 1.16 or later installed for the build to work.

### Homebrew

You may also install this application using a Homebrew tap with the following command:

```
brew install iann0036/iamlive/iamlive
```

### Other Methods

* [Lambda Extension](https://github.com/iann0036/iamlive-lambda-extension) _(AWS only)_
* [Docker](https://meirg.co.il/2021/04/23/determining-aws-iam-policies-according-to-terraform-and-aws-cli/)
* [GitHub Action (with Terraform)](https://github.com/scott-doyland-burrows/gha-composite-terraform-iamlive)
* [LocalStack](https://github.com/rulio/iamlive-localstack/)

## Usage

To start the listener, simply run `iamlive` in a separate window to your CLI / SDK application. You can use Ctrl+C to exit when you are done.

### CLI Arguments

You can optionally also include the following arguments to the `iamlive` command:

**--provider:** the cloud service provider to intercept calls for (`aws`,`azure`,`gcp`) (_default: aws_)

**--set-ini:** when set, the `.aws/config` file will be updated to use the CSM monitoring or CA bundle and removed when exiting (_default: false_) (_AWS only_)

**--profile:** use the specified profile when combined with `--set-ini` (_default: default_) (_AWS only_)

**--fails-only:** when set, only failed AWS calls will be added to the policy, csm mode only (_default: false_) (_AWS only_)

**--output-file:** specify a file that will be written to on SIGHUP or exit (_default: unset_)

**--refresh-rate:** instead of flushing to console every API call, do it this number of seconds (_default: 0_)

**--sort-alphabetical:** sort actions alphabetically (_default: false for AWS, otherwise true_)

**--host:** host to listen on for CSM (_default: 127.0.0.1_)

**--background:** when set, the process will return the current PID and run in the background without output (_default: false_)

**--force-wildcard-resource:** when set, the Resource will always be a wildcard (_default: false_) (_AWS only_)

**--mode:** the listening mode (`csm`,`proxy`) (_default: csm for aws, otherwise proxy_)

**--bind-addr:** the bind address for proxy mode (_default: 127.0.0.1:10080_)

**--ca-bundle:** the CA certificate bundle (PEM) to use for proxy mode (_default: ~/.iamlive/ca.pem_)

**--ca-key:** the CA certificate key to use for proxy mode (_default: ~/.iamlive/ca.key_)

**--account-id:** the AWS account ID to use in policy outputs within proxy mode (_default: 123456789012 unless detected_) (_AWS only_)

**--override-aws-map:** overrides the embedded AWS mapping JSON file with the filepath provided (_AWS only_)

**--debug:** dumps associated HTTP requests when set in proxy mode (_default: false_)

_Basic Example (CSM Mode)_

```
iamlive --set-ini
```

_Basic Example (Proxy Mode)_

```
iamlive --set-ini --mode proxy
```

_Basic Example (Azure)_

```
iamlive --provider azure
```

_Basic Example (Google Cloud)_

```
iamlive --provider gcp
```

_Comprehensive Example (CSM Mode)_

```
iamlive --set-ini --profile myprofile --fails-only --output-file policy.json --refresh-rate 1 --sort-alphabetical --host 127.0.0.1 --background
```

_Comprehensive Example (Proxy Mode)_

```
iamlive --set-ini --mode proxy --profile myprofile --output-file policy.json --refresh-rate 1 --sort-alphabetical --bind-addr 127.0.0.1:10080 --ca-bundle ~/.iamlive/ca.pem --ca-key ~/.iamlive/ca.key --account-id 123456789012 --background --force-wildcard-resource
```

The arguments may also be specified in an INI file located at `~/.iamlive/config`.

### CSM Mode

Client-side monitoring mode is the default behaviour for AWS and will use [metrics](https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/metrics.html) delivered locally via UDP to capture policy statements with the `Action` key only (`Resource` is only available in proxy mode).

CSM mode is only available for the AWS provider.

#### CLI

To enable CSM in the AWS CLI, you should either use the `--set-ini` option or add the following to the relevant profile in `.aws/config`:

```
csm_enabled = true
```

Alternatively, you can run the following in the window executing your CLI commands:

```
export AWS_CSM_ENABLED=true
```

#### SDKs

To enable CSM in the various AWS SDKs, you can run the following in the window executing your application prior to it starting:

```
export AWS_CSM_ENABLED=true
export AWS_CSM_PORT=31000
export AWS_CSM_HOST=127.0.0.1
```

### Proxy Mode

Proxy mode will serve a local HTTP(S) server (by default at `http://127.0.0.1:10080`) that will inspect requests sent to the AWS endpoints before forwarding on to generate IAM policy statements. The CA key/certificate pair will be automatically generated and stored within `~/.iamlive/` by default.

#### AWS CLI

To set the appropriate CA bundle in the AWS CLI, you should either use the `--set-ini` option or add the following to the relevant profile in `.aws/config`:

```
ca_bundle = ~/.iamlive/ca.pem
```

Alternatively, you can run the following in the window executing your CLI commands:

```
export AWS_CA_BUNDLE=~/.iamlive/ca.pem
```

You must also set the proxy settings for your session by running the following in the window executing your CLI commands:

```
export HTTP_PROXY=http://127.0.0.1:10080
export HTTPS_PROXY=http://127.0.0.1:10080
```

#### AWS SDKs

To enable proxy mode in the various AWS SDKs, you can run the following in the window executing your application prior to it starting:

For AWS SDKs:

```
export HTTP_PROXY=http://127.0.0.1:10080
export HTTPS_PROXY=http://127.0.0.1:10080
export AWS_CA_BUNDLE=~/.iamlive/ca.pem
```

Check the [official docs](https://docs.aws.amazon.com/credref/latest/refdocs/setting-global-ca_bundle.html) for further details on setting the CA bundle.

#### Azure CLI and SDKs

To enable proxy mode in the Azure CLI or SDK, you can run the following in the window executing your application prior to it starting:

```
export HTTP_PROXY=http://127.0.0.1:10080
export HTTPS_PROXY=http://127.0.0.1:10080
export REQUESTS_CA_BUNDLE=~/.iamlive/ca.pem
```

#### Google Cloud CLI and SDKs

To enable proxy mode in the Google Cloud CLI or SDKs, you can run the following in the window executing your application prior to it starting:

```
gcloud config set proxy/type http
gcloud config set proxy/address 127.0.0.1
gcloud config set proxy/port 10080
gcloud config set core/custom_ca_certs_file ~/.iamlive/ca.pem
```

## FAQs

_I get a message ""package embed is not in GOROOT"" when attempting to build myself_

This project requires Go 1.16 or above to be built correctly (due to embedding feature).

## Acknowledgements

This project makes use of [Parliament](https://github.com/duo-labs/parliament) and was assisted by Scott Piper's [CSM explainer](https://summitroute.com/blog/2020/05/25/client_side_monitoring/). Thanks also to Noam Dahan's [research](https://ermetic.com/whats-new/blog/auditing-passrole-a-problematic-privilege-escalation-permission/) into missing `iam:PassRole` dependant actions.",VRAI
IBM/portieris,Toolkit,Application System,2025-05-07T12:14:59Z,2025-01-02T17:34:06Z,0,0,0,0,0,0,0,0,2018-04-20T15:18:35Z,2025-04-04T09:19:31Z,1094,333,Go,VRAI,77,FAUX,51,,51,A Kubernetes Admission Controller for verifying image trust.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,53,"![Portieris logo](./logos/text_and_logo.svg)

[![Go](https://github.com/IBM/portieris/actions/workflows/go-portieris.yaml/badge.svg)](https://github.com/IBM/portieris/actions/workflows/go-portieris.yaml)

Portieris is a Kubernetes admission controller for the enforcement of image security policies. You can create image security policies for each Kubernetes namespace or at the cluster level, and enforce different rules for different images.

## How it works

Portieris uses a Kubernetes mutating admission webhook to modify your Kubernetes resources, at the point of creation, to ensure that Kubernetes runs only policy compliant images. When configured to do so, Portieris can enforce Docker Content Trust with optional trust pinning, or can verify signatures that are created by using Red Hat's simple signing model and prevents the creation of resources that use untrusted or unverified images.

If your cloud provider provides a [Notary](https://github.com/notaryproject/notary) server (sometimes referred to as Content Trust), Portieris accesses trust data in that Notary server that corresponds to the image that you are deploying. To verify Red Hat simple signatures, the signatures must be accessible by using registry extension APIs or a configured signature store.

When you create or edit a workload, the Kubernetes API server sends a request to Portieris. The AdmissionRequest contains the content of your workload. For each image in your workload, Portieris finds a matching security policy.

If trust enforcement is enabled in the policy, Portieris pulls signature information for your image from the corresponding Notary server and, if a signed version of the image exists, creates a JSON patch to edit the image name in the workload to the signed image by digest. If a signer is defined in the policy, Portieris additionally checks that the image is signed by the specified role, and verifies that the specified key was used to sign the image.

If simple signing is specified by the policy, Portieris verifies the signature by using the public key and identity rules that are supplied in the policy and, if verified, mutates the image name to a digest reference to ensure that concurrent tag changes can't influence the image that is being pulled.

While it is possible to require both Notary trust and simple signing, the two methods must agree on the signed digest for the image. If the two methods return different signed digests, the image is denied. Alternative signing methods are not allowed.

If any image in your workload does not satisfy the policy, the entire workload is prevented from deploying.

Portieris receives AdmissionRequests for the creation of, or edits to, all types of workload. To prevent Portieris from impacting auto-recovery, it approves requests where a known parent exists.

Portieris' Admission Webhook is configured to fail closed. Three instances of Portieris ensure that it is able to approve its own upgrades and auto-recovery. If all instances of Portieris are unavailable, Kubernetes doesn't auto-recover it, and you must delete the MutatingAdmissionWebhook to allow Portieris to recover.

## Image mutation option

The default behavior is to mutate the image reference on successful admission so that it references an image immutably by using a digest and not a tag. This behavior ensures that there is little possibility that an image can be substituted after verification by making changes in the image registry. Without this safeguard there is a time window between admission and image pull, and also between admission and any subsequent re-pull of the image due to rescheduling where an image under a tag can change. Switching the image to the digest form ensures that the image content is guaranteed to be the one admitted. 

Some closed loop deployment technologies verify that the image that is running is the expected one by looking at the image reference in the running container and seeing a mutated image reference as different, then attempt to correct the discrepancy, with no effect. In this case, an infinite reconciliation loop driving the Kubernetes API, Portieris, and registry traffic can start. To avoid this scenario, you can specify `mutateImage: false` in the policy behavior setting, which does not change the image that is admitted under this policy and prevents the undesirable consequences. The benefit is at the expense of reintroducing the window between admission and pull for other images to be substituted and run without verification.

Use this option only where absolutely necessary and don't use it alongside `trust` requirements because `notaryV1` tags aren't directly linked to registry tags and  unexpected images can run with trust verification even in the steady state (without registry changes).  
See, [Image mutation option](POLICIES.md#image-mutation-option).

## Portieris metrics

Portieris exposes two metrics for monitoring the policy decisions made for workload images, these metrics are available on port 8080, and are exposed by annotations to [Prometheus](https://prometheus.io/). The metrics are:

```
portieris_pod_admission_decision_allow_count
portieris_pod_admission_decision_deny_count
```

The metrics are counters that increment each time a decision is made.

These metrics are available to view locally on the `:8080/metrics` path for each pod that is running.

## Installing Portieris

Portieris is installed by using a Helm chart. Before you begin, ensure that you have Kubernetes 1.16, or later, on your cluster and Helm 3.0, or later, installed on your workstation.

* To install Portieris in the default namespace (`portieris`), complete the following steps:

  1. Find the release you want to run, see [Releases](https://github.com/IBM/portieris/releases), and download the Helm chart package. 
  2. Unpack the charts, for example, run: 
  
     ```
     tar xzvf portieris-0.9.4.tgz
     ```
     
  3. Generate your CA certificate as well as a certificate/key pair for your webhook server, signed by the CA you created. Add these to the `values.yaml` file under `UseGeneratedCerts`. Be sure to also set `enabled` to true. For more detailed instructions, see the README under `helm/portieris`.
  
  4. Run:
  
     ```
     helm install portieris --create-namespace --namespace portieris ./portieris
     ``` 

* To install Portieris in a different namespace, including an existing one, omit the `--create-namespace` option. Because the namespace forms part of the webhook certificate common name, you must generate the certificate for the target namespace.

  1. Run:
     
     ```
     sh portieris/gencerts <namespace>
     ```
     
  2. Run:
  
     ```
     helm install portieris --create-namespace --namespace <namespace> ./portieris
     ```

* To manage certificates through an installed [cert-manager](https://cert-manager.io/), you don't need to unpack the charts. 

  1. Run:

     ```
     helm install portieris --set UseCertManager=true portieris-0.9.4.tgz
     ```

By default, Portieris' admission webhook runs in all namespaces including its own install namespace, so that Portieris is able to review all the pods in the cluster. However, this can prevent the cluster from self-healing in the event that Portieris becomes unavailable. 

Portieris also supports skipping namespaces with a certain label set. You can enable this by adding `--set AllowAdmissionSkip=true` to your installation command, but ensure that you control who can add labels to namespaces and who can access namespaces with this label so that a malicious party can't use this label to bypass Portieris.

Another way to avoid update deadlock is to specify `--set webHooks.failurePolicy=Ignore`. 

## Uninstalling Portieris

**Note**: When you uninstall Portieris, all your image security policies are deleted.

To uninstall Portieris run: 

```
helm delete portieris --namespace <namespace>
```

**Note**:

* If you no longer require the namespace, you must delete it manually by running: 
  
  ```
  kubectl delete namespace/<namespace>
  ```
  
* If you have issues uninstalling Portieris by using Helm, try running the clean-up script: 
  
  ```
  helm/cleanup.sh portieris <namespace>
  ```

## Image security policies

Image security policies define Portieris' behavior in your cluster. You must configure your own policies in order for Portieris to enforce your required security posture. [Policies](POLICIES.md) are described separately.

## Configuring access controls for your security policies

You can configure Kubernetes role-based access control (RBAC) rules to define which users and applications can modify your security policies. For more information, see the [Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) docs and [Controlling user access with IBM Cloud IAM and Kubernetes RBAC](https://cloud.ibm.com/docs/containers?topic=containers-users) in the IBM Cloud Kubernetes Service docs.

If Portieris is installed with `AllowAdmissionSkip=true`, you can prevent Portieris' admission webhook from being called in specific namespaces by labelling the namespace with `securityenforcement.admission.cloud.ibm.com/namespace: skip`. This action allows pods in that namespace to recover when the admission webhook is down, but note that no policies are applied in that namespace. For example, the Portieris installation namespace is configured with this label to allow Portieris itself to recover when it is down. Ensure that you control who can add labels to namespaces and who can access namespaces with this label so that a malicious party can't use this label to bypass Portieris.

## Reporting security issues

To report a security issue, don't open an issue. Instead, send your report by email to `alchreg@uk.ibm.com` privately.",VRAI
ietf-tools/relaton-data-ids,Documentations,Database,2025-05-16T03:40:32Z,2025-05-01T16:32:43Z,0,0,0,0,0,28,0,0,2021-12-05T13:04:09Z,2025-04-08T03:55:03Z,14111592,9,Ruby,VRAI,12,FAUX,5,,5,Bibliographic data information for Internet-Drafts in Relaton format,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,5,### Bibliographic data information for Internet-Drafts in Relaton format,VRAI
igniterealtime/openfire-ofmeet-plugin,Application System,Toolkit,2024-10-30T11:49:01Z,2019-01-29T16:24:23Z,0,0,0,0,0,0,0,0,2017-06-01T11:13:32Z,2025-01-23T04:58:09Z,150217,47,Java,VRAI,44,FAUX,31,,31,Provides an HTTP Online Meeting solution for Openfire using Jitsi Meet.,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,7,"<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.0 Transitional//EN"">

<html>
<head>
    <title>Openfire Meetings Plugin Readme</title>
    <style type=""text/css"">
        BODY {
            font-size: 100%;
        }

        BODY, TD, TH {
            font-family: tahoma, verdana, arial, helvetica, sans-serif;
            font-size: 0.8em;
        }

        H2 {
            font-size: 10pt;
            font-weight: bold;
        }

        A:hover {
            text-decoration: none;
        }

        H1 {
            font-family: tahoma, arial, helvetica, sans-serif;
            font-size: 1.4em;
            font-weight: bold;
            border-bottom: 1px #ccc solid;
            padding-bottom: 2px;
        }

        TT {
            font-family: courier new;
            font-weight: bold;
            color: #060;
        }

        PRE {
            font-family: courier new;
            font-size: 100%;
        }
    </style>
</head>
<body>

<h1>
    Openfire Meetings Plugin Readme
</h1>

<div>
<p>An <a href=""http://www.igniterealtime.org/projects/openfire/"">Openfire</a> plugin that provides high quality, scalable video
conferences using Jitsi Videobridge.</p>
</body>
</html>",VRAI
iits-consulting/charts,Application System,Documentations,2025-05-14T13:09:53Z,2025-02-04T14:50:08Z,0,0,0,0,9,0,0,0,2023-04-27T15:30:25Z,2025-03-29T11:03:27Z,2520,16,Smarty,VRAI,7,FAUX,3,"argocd,charts,gitops,helm,iits-consulting,otc",3,Common helm charts we use,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,21,"# IITS helm charts

This project contains several public charts as used by the folks at [IITS](https://iits-consulting.de/).

## Requirements

These charts are running only with a OIDC Proxy upfront and traefik as a Ingress Controller

## Installation

Take a look at the specific README.md of the chart.

## Acceptance criteria

Any helm chart provided by iits-consulting needs to adhere to the following acceptance criteria:

* The `Chart.yaml` contains all required attributes as defined in https://helm.sh/docs/topics/charts/#the-chartyaml-file
* Document values in such a way that [helm-docs](https://github.com/norwoodj/helm-docs) may generate
  a nice `README.md`
* Enable custom annotations in `values.yaml`
* Define common labels for better separation of concerns
* Configuration changes have to cause a pod restart
* Whenever possible, sensitive information should be injected by something like
  a [mutating webhook](https://banzaicloud.com/docs/bank-vaults/mutating-webhook/) rather than be part of your chart
* Container health checks need to be present
* Use subcharts to manage dependencies whenever possible
* **Document** every values.yaml variable that is meant to be adjusted
* Specify a license
* Provide a default .helmignore
* HorizontalPodAutoscaler should be present
* Have a `NOTES.txt` that provides information about the deployment",VRAI
intelops/genval,DevOPs,DevOPs,2024-11-14T17:59:57Z,2024-08-07T13:56:11Z,0,26,0,0,1,0,0,0,2023-09-12T11:50:07Z,2024-11-14T18:00:02Z,333482,6,Go,VRAI,5,FAUX,45,,45,"Simplifies configuration management for a wide range of tools, including Dockerfile, Kubernetes manifests, and other infrastructure files.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,5,"[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/7843/badge)](https://www.bestpractices.dev/projects/7843)
[![Go Report Card](https://goreportcard.com/badge/github.com/intelops/genval)](https://goreportcard.com/report/github.com/intelops/genval)
[![Build Status](https://github.com/intelops/genval/actions/workflows/ci.yaml/badge.svg)](https://github.com/intelops/genval/actions?query=workflow%3Abuild)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)


# Genval: Simplifying Configuration Management



Genval is a versatile Go utility that simplifies configuration management for a wide range of tools, including Dockerfile, Kubernetes manifests, and other infrastructure files.



## Streamlining validation and Generation of Configurations



**Genval** is a robust utility written in Golang that streamlines the management of configuration files for various tools. Whether you need Dockerfiles, YAML/JSON manifests for Kubernetes, or custom resource definitions (CRDs), Genval simplifies the process of validation and generation for multiple configuration artifacts.



## Why Genval?

Managing configurations across different tools can be a daunting task. Ensuring that these configurations adhere to best practices is critical, at the same time it can be time-consuming and error-prone. Genval automates these processes, making configuration management more efficient.



## Key Features



### Dockerfile Management



-  **Dockerfile Generation**: After successful validation, Genval generates a Dockerfile based on your input, tailored to your specifications.


-  **Input Validation**: Genval validates your input for generation of Dockerfile, typically provided in JSON format, to ensure correct structure.


-  **Best Practice Validation**: Genval doesn't stop at generation; it checks your Dockerfile against predefined best practices for security and optimization.


-  **Informative Feedback**: If Genval detects issues during best practice validation, it provides informative feedback to help you improve your Dockerfile.



> Note: For Dockerfile validation and generation, `genval` expects a predefined structure for the `JSON` file provided to the `--reqinput` flag. Sample `.json` files can be found in the `./templates/dockerFile-samples` directory.



### Managing Kubernetes Manifests



- Genval validates input for required structure based on the tool in use. It can enforce best practices while authoring configuration files for tools like Kubernetes and various CRDs.

- Users can provide minimal and custom configurations for a given resource, with Genval populating all necessary fields based on security best practices recommended by the community.



- Genval generates YAML manifests according to the required format for the specified resource and tool.

### Validation of configuration file

Genval provides capabilities to validate configuration for different resources and technologies, including Dockerfile, Kubernetes manifests, and Terraform files using Rego and CEL policy languages.


## Getting Started

To use Genval:



- Download the `genval` binary for your platform from the official [release page](https://github.com/intelops/genval/releases).



## Verifying Binary Signatures



Genval's release process signs binaries using Cosign's keyless signing mode. To verify a specific binary, retrieve the release artifact, signature, and public certificate for your desired os/arch. Detailed instructions are available in the [Sigstore blog](https://blog.sigstore.dev/cosign-2-0-released/).

**Example to verify a binary for linux_amd64**

```shell


COMMIT=$(git rev-list --tags --max-count=1)
version=$(git describe --tags ${COMMIT})
version=""${version#v}""

# get the artifact
$ wget  https://github.com/intelops/genval/releases/download/v${version}/genval_$version}_linux_amd64.tar.gz
# get the signature
$ wget  https://github.com/intelops/genval/releases/download/v${version}/genval_${version}_linux_amd64.tar.gz.sig
# Get the certificate
$ wget  https://github.com/intelops/genval/releases/download/v${version}/genval_${version}_linux_amd64.tar.gz.crt



cosign  verify-blob  \
--certificate-identity  ""https://github.com/intelops/genval/.github/workflows/release.yaml@refs/tags/${version}""  \
--certificate-oidc-issuer  ""https://token.actions.githubusercontent.com""  \
--cert  ./genval_${version}_linux_amd64.tar.gz.crt  \
--signature  genval_${version}_linux_amd64.tar.gz.sig \
./genval_${version}_linux_amd64.tar.gz
```
If verification is successful, you'll see ""**Verified OK.**""

> For more details on signing/verifying container images and artifacts refer this [Sigstore blog](https://blog.sigstore.dev/cosign-2-0-released/)


## Installation



- Download the genval binary from the official [release page](https://github.com/intelops/genval/releases)

- Move the executable to `/usr/local/bin` for convenience.



## Quick Start


For a quick start, pre-built templates for Dockerfile generation for popular languages can be found in the `./templates/inputs/dockerfile_input` folder. We also maintain all the default policies and input templates in a dedicated [repository](https://github.com/intelops/policyhub).


## Building from Source

The easieast way to build the `genval` executable is using the `build` Makefile target.
`make build`.
This will build the binary from source and place the `genval` binary in the `./bin` folder that can be copied to `/usr/local/bin`.



To build genval from source:

- Clone the Genval repository: `git clone https://github.com/intelops/genval.git`

- Navigate to the project directory: `cd genval`

- Build Genval: `CGO_ENABLED=0 go build -o ./genval .`



The generated binary, genval, will be available in the current working directory. You can move it to your PATH or use it from the current directory.

# Welcome to Genval

Genval provides a range of powerful modes for generating and validating configuration files across various technologies. Each mode serves specific purposes and can be accessed through Genval's main commands:


- `dockerfile`: Generate and validate Dockerfiles, utilizing [Rego](https://www.openpolicyagent.org/docs/latest/policy-language/) for validation of Dockerfile
-  `regoval` Validate Dockerfiles, Kubernetes manifests, and Terraform files using Rego policies
-  `celval` Validate Dockerfiles, Kubernetes manifests, and Terraform files with [Common Expression Language (CEL)](https://cel.dev/overview/cel-overview) policies
-  `cue` Generate and validate Kubernetes and related config files leveraging [Cuelang aka CUE](https://cuelang.org/docs/)
-  `cuemod` Create a workspace for generating and validating Kubernetes and related config files.
- `artifact` Manage pushing and pulling built artifacts from OCI compliant container registries.

AAdditionally, a helpful command called `showjson` allows users to view the JSON representation of input files passed to Genval. By using this command, users can specify an input file, such as a **Dockerfile**, **Terraform file** and obtain its corresponding JSON representation. Since many policies are written based on JSON structured input, this feature facilitates users in developing custom policies in **Rego** and **CEL**.

> All commands accept inputs from both local files and remote URLs, such as those from a Git repository in raw format. If you wish to query files from https://github.com, authentication to GitHub via a Personal Access Token (PAT) is required. To set this up, create an environment variable named GITHUB_TOKEN and assign it your PAT. Here's how to do it: export GITHUB_TOKEN=<Your...PAT>.


### Dockerfile Validation and Generation:
To validate and generate Dockerfiles using Genval, use the `dockerfile` command. Provide the path to your input JSON or YAML file using the `--reqinput` flag. Specify the desired output path for the generated Dockerfile along with the `--inputpolicy` and `--outputpolicy` Rego policy files for validating the input JSON and the generated Dockerfile respectively. Genval will handle the validation process seamlessly.

Example:

```
$ genval dockerfile --reqinput=./templates/inputs/dockerfile_input/golang_input.json \
--output Dockerfile \
--inputpolicy ./templates/defaultpolicies/rego/input_policies \
--outputpolicy ./templates/defaultpolicies/rego/dockerfile_policies
 ```

> Customize the values provided in the flags according to your specific input file and Rego policies.
>
> You can supply all arguments to the --reqinput, inputpolicy, and outputpolicy flags from remote URLs, such as those hosted on GitHub (e.g., https://github.com).

> For authenticating with GitHub.com, set the env variable GITHUB_TOKEN:
`export GITHUB_TOKEN=<Your GitHub PAT>`

All the rego policies are housed in a hirearchy containing a `rego` policy and a `JSON` file containing all the metadata related to policy. A user needs to pass the directory containing boththe `.rego` and `.json` files. Genval also accpets a top leval directory containing multiple sub-directories containing multiple rego and accompanied JSON files for validating with more than one policy.

Users can use policies to validate input JSON as well as generated Dockerfile with policies stored in their OCI registries
or with Genval's default Rego policies. Behind the scenes, this action iteracts with OCI registries for pulling the policies.

To facilitate authentication with OCI compliant container registries,
Users can provide credentials through --credentials flag. The creds can be provided via <$USER:$PAT> or <REGISTRY_PAT> format.
If no credentials are provided, Genval searches for the ""./docker/config.json"" file in the user's $HOME directory.
If this file is found, Genval utilizes it for authentication.

**Validating with Default policies**

```shell
./genval dockerfile --reqinput https://github.com/intelops/genval-security-policies/blob/patch-1/input-templates/dockerfile_input/clang_input.json \
--output ./output/Dockefile-cobra
// No credntials provided, will default to $HOME/.docker/config.json for credentials
```

**Validating with policies stored in OCI compliant container registries**

```shell
./genval dockerfile --reqinput https://github.com/intelops/genval-security-policies/blob/patch-1/input-templates/dockerfile_input/clang_input.json \
--output ./output/Dockefile-cobra \
inputpolicy oci://ghcr.io/intelops/policyhub/genval/input_policies:v0.0.1 \
--outputpolicy oci://ghcr.io/intelops/policyhub/genval/dockerfile_policies:v0.0.1 \
--credentials <$GITHUB_PAT> or <$USER:$PAT> format
```
**Review Feedback**: Genval provides feedback based on best practice validation. Use this feedback to refine your Dockerfile.

### Validation of the Dockerfile, Kubernetes manifests and Terraform files using Rego policies

Users can leverage Genval's feature of Validating of resources using policies stored in OCI compliant registries or provide policies stored in their own OCI compliant registries.

To facilitate authentication with OCI compliant container registries, Users can provide credentials through `--credentials` flag while invoking a regoval subcommand. The credentials can
be provided via <$USER:$PAT> or <$REGISTRY_PAT> format. If no credentials are provided, Genval searches for the `./docker/config.json` file in the user's `$HOME` directory. If this file is found, Genval utilizes it for authentication.

**Example**:

./genval regoval dockerfileval --reqinput=Dockerfile \
--policy oci://ghcr.io/intelops/policyhub/genval/dockerfile_policies:v0.0.1
--credentials <GITHUB_PAT> or <USER:PAT>


Users can you use default policies maintained by the community stored in the https://github.com/intelops/policyhub repo

./genval regoval dockerfileval --reqinput <Path to Dockerfile>
// No credntials provided, will default to $HOME/.docker/config.json for credentials


#### Validation of Dockerfiles with Rego policies

```shell
genval regoval dockerfileval --reqinput ./templates/inputs/Dockerfile \
--policy ./templates/defaultpolicies/dockerfile_policies
```

#### Validation of Kubernetes manifests using Rego policies

```shell
genval regoval infrafile --reqinput ./templates/inputs/k8s/deployment.json \
--policy ./templates/defaultpolicies/k8s.rego
```

#### Validation of Terraform files using Rego policies

> Users can directly provide the `.tf` file to genval along with a policy written in Rego for validatin the Terraform file
```shell
genval regoval terraform --reqinput ./templates/inputs/terraform/sec_group.tf \
--policy ./templates/defaultpolicies/terraform.rego
```


### Validation of the Dockerfile, Kubernetes manifests and Terraform files using CEL policies

`celval` is the main command that manages validation of Dockerfiles, Kubernetes manifests, and Terraform files using Common Expression Language (CEL).
The structure for CEL policies is described below:

```yaml
policies:
- apiVersion: v1alpha1
  kind: CELPolicy
  metadata:
    name: Check image with latest tag
    description: Deny Images with latest tag
    severity: Critical
    benchmark: XYZ
  rule: |
    !input.spec.template.spec.containers[0].
    image.endsWith('latest')
```
The `metadata` block contains all the details about the policy, like its name, description, severity and its benchmark of the rule is based on. The `rules` field specifies the CEL expression to be evaluated. For above example, the `rule` will validate and ensure the image in a `Deployments` does not use the `latest ` tag.

#### Validation of Dockerfiles with CEL policies

```shell
genval celval dockerfileval --reqinput=input.json \
--policy=<'path/to/CEL policy file>
```

#### Validation of Kubernetes manifests using Rego policies

```shell
./genval celval infrafile --reqinput=./templates/inputs/k8s/deployment.json \
--policy=./templates/defaultpolicies/cel/k8s.yaml
```


#### Validation of Terraform files using Rego policies

```shell
./genval celval terraform --reqinput ./templates/inputs/terraform/sec_group.tf \
--policy=--policy ./templates/defaultpolicies/cel/terraform.yaml
```


### Validation and Generation of Kubernetes configurations

Genval leverages [cuelang](https://cuelang.org/docs/)for the validation and generation of Kubernetes and CRD manifests. When utilizing Genval for these tasks, employ the cue mode. This mode requires JSON/YAML input via the `--reqinput` flag. Additionally, specify a resource flag, indicating the top-level label defined in the Cue policies. Lastly, provide the Cue policies (Cue definitions) for validation and generation. The `--policy` argument accepts a directory containing a cue.mod directory, which holds the upstream APIs in .cue format for assisting in validating the provided config file. The same directory also contains a policy (Cue definition) for validation and generation.

To aid users in creating workspaces enriched with the required directory structure for the `--policy` argument, Genval offers the [`cuemod init` command](#creating-workspace-for-working-with-cue-mode). This command takes one argument, -`-tool`, which specifies the desired technology. Genval currently supports the following tools:

- `k8s`: For validating and/or generating manifests for Kubernetes.
- `argocd`: For validating and/or generating manifests for ArgoCD
- `tekton`: For validating and/or generating manifests for Tekton
- `crossplane`: TFor validating and/or generating manifests for Crossplane


Users can provide multiple `.cue` policies within the workspace directory, enabling the supply of distinct `.cue` definitions as needed. For example, a DevSecOps/Platform engineering team may provide a schema enforcing security best practices for a specific environment, while development teams can customize policies for validation and generation, tailoring configurations to their particular environments.

Example:

```shell
$ genval cue --reqinput https://github.com/santoshkal/cuemod-demo/tree/main/k8s \
--resource Application \
--policy ./policy
```

This command validates Deployment manifests using the provided `.cue` schema definitions from the `policy` directory and generates the final YAML manifest in the output directory in the current working directory.

>The `--resource` flag in **cue** mode labels the top-level flag in the Cue definitions. In the above example, ""Application"" defines a Kubernetes Deployment and a Service resource.


For a detailed workflow illustrating the capabilities of Cue and Genval for validating and generating Kubernetes configurations, you can refer to [this document](./cmd/cueval/example.md).
The workflow for adding a Cue schema for Kubernetes CRDs is failry easy, and demostrated in the [CONTRIBUTION.md document](./CONTRIBUTION.md/#contributing-by-adding-a-cue-schema-to-the-project).

### Creating workspace for working with cue mode

In order to utilize the **cue command** effectively, a directory must be provided to the `--policy` flag. This directory is essential and **must** contain a cue.mod directory with upstream configurations for the relevant tool being evaluated, along with one or more `.cue` definitions.

Genval streamlines the creation of such a workspace for several technologies, including Kubernetes, ArgoCD, TektonCD, and Crossplane.

To initiate a workspace, utilize the `cuemod init` command and specify the desired technology using the `--tool` flag

The `cuemod init` command acts as a helper command, facilitating the creation of all necessary files for working with the `cue` command. It validates and retrieves all required dir/files from the OCI registry, placing them on disk for use with the `cue` command.

Currently, the supported tools for initializing `cuemods` are `k8s`, `tektoncd`, `argocd`. `crossplane`, and `clusterAPI`.

```shell
$ genval cuemod init --tool=k8s
```

> Note: If a workspace for a tool that is not available in the above list of supported tools. Genval also supports pulling a custom workspace built and stored by users in OCI registries. The only requirement while building and pushing the workspace to OCI registry, is the the directory structure, which should exactly be in the following order:
```shell
.
├── cue.mod # This directory may contain all Kubernetes types in cue format, generated with ""cue get go k8s.io/apis/..."" cue command.
└── policy.cue # This is a .cue file containing the Cue definitions/policies
```
`genval cuemod init --tool k8s` command will create a new directory in users current working directory with name `cuemod-k8s:1.29` with following structure:

```shell
./k8s:1.29/
├── archive
└── extracted-content
```

In the above directory tree, the `archive` sub-directiry will contain the raw artifact in `tar.gz` format, and in the `extracted-content` sub-directory `cuemod-k8s:v1.29` directory will contain all the required files for working with `cue` command
following will be the structure of the `extracted-content` sub-directory:

```shell
./extracted-content
├── cue.mod
└── policy.cue
└── README.md
```

User needs to update the `policy.cue` file with relevant Cue definitions and if necessary add mode Cue definition files to this directory. This worksapcew would be now ready to be passed to the `--policy` argument when working with [`cue` command](#validation-and-generation-of-kubernetes-configurations).

For example:

```shell
$ genval cue --reqinput https://github.com/santoshkal/cuemod-demo/tree/main/k8s \
--resource Application \
--policy ./k8s:1.29/extracted-contents/cuemod-k8s:v1.29
```




### Managing the generated and Validated configuration files

Genval offers comprehensive management capabilities for the configuration files generated and validated. It allows users to build these files as OCI artifacts and store them in OCI-compliant container registries. Additionally, Genval supports pulling the same configuration files from the registry when needed.

To bolster supply chain security workflows, Genval enables users to sign the artifacts after storing them in the registry. Similarly, when pulling any artifact, Genval provides functionality to verify the signatures of the artifacts. This feature leverages **Sigstore's Cosign keyless mode** of signing and verifying artifacts. However, users can also utilize their own private and public keys for signing and verifying the artifacts respectively.

#### Building, pushing, and signing generated and/or verified config files and OCI artifacts

The following command demonstrates building and pushing the OCI artifact (genval:test) to GitHub Container Registry (ghcr.io) while signing the artifact with Cosign in Keyless mode:

```shell
$ ./genval artifact push --reqinput ./templates/defaultpolicies/rego \
--url --dest ghcr.io/santoshkal/artifacts/genval:test \
--sign true
```

#### Pulling tghe stored artifact from the container registry and verifying the signatures
The following command illustrates pulling an artifact (genval:test) stored in the container registry (ghcr.io), verifying the artifact's signatures, and finally storing the contents of the artifact in the ./output directory:

```shell
./genval artifact pull --dest ghcr.io/santoshkal/artifacts/genval:test \
--path ./output \
--verify true
```

Users can also verify artifacts signed with cosign private key and the verify the artifact using the associated cosign public key:

```shell
$ genval artifact pull --dest oci://ghcr.io/santoshkal/artifacts/genval:no-sign \
--path ./output \
--verify true \
--key ./cosign/cosign.pub
```

### Templates

The `./templates` folder holds some sample files to be used in Genval. the `./templates/inputs` holds JSON input templates for both generating Dockerfiles in `container` mode and Kubernetes manifests in `cue` mode. Similarly, all the sample policies for all the modes are stored in `./templates/defaultpolices` directory. User can use these template files to start with and as they go along they can build upon it and customize these policies to suite their specific use cases.",FAUX
intel/policy-library-intel-aws,Toolkit,Toolkit,2025-01-16T18:14:57Z,2023-04-14T14:36:36Z,49,0,0,0,0,0,0,0,2022-11-18T21:23:52Z,2025-01-16T18:14:59Z,182,16,HCL,VRAI,8,FAUX,0,,0,Intel Cloud Optimization Module - AWS Sentinel Policies,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,8,"<p align=""center"">
  <img src=""https://github.com/intel/policy-library-intel-aws/blob/main/images/logo-classicblue-800px.png?raw=true"" alt=""Intel Logo"" width=""250""/>
</p>

# Deprecation Notice
A new version of this library is now available at [github.com/intel/policy-library-intel](https://github.com/intel/intel-policy-library). The new library includes enhanced features to keep developers informed of optimal instance selection and provides notifications when Intel releases new recommended instance types. We encourage all users to transition to the new library to take advantage of these improvements.

# AWS Sentinel Policies - Intel Cloud Optimization Modules

© Copyright 2022, Intel Corporation

## Sentinel Policies for AWS Modules

This library, provides prescriptive Sentinel policies that optimize Terraform deployed resources on Amazon Web Services (AWS). The policies that are contained in this library are based on the latest [performance and benchmarking tests](https://www.intel.com/content/www/us/en/developer/topic-technology/cloud/cloud-performance.html?f:@stm_10381_en=%5BAmazon%20Web%20Services%5D). Terraform Cloud and Enterprise users can use the policies in this library to enable intelligent developer decisions when deploying cloud infrastructure by choosing the best price for performance instance types for their cloud resources. 

## Getting Started

**Required Versions**

Sentinel : [>=0.24.0](https://developer.hashicorp.com/sentinel/install)  
Terraform Cloud : Use Enhanced Policy Sets with a Sentinel version [>=0.24.0](https://developer.hashicorp.com/terraform/cloud-docs/policy-enforcement/manage-policy-sets)

**Set Up**

To get started using these policies fork the AWS Policy Library from the github repository. This will ensure that all necessary components for the policies are included and allows you to easily stay up to date when new versions of the recommended instance list are released. 

Note that all policies are set to a default of **advisory** mode and will NOT impact deployments until changed to either **soft-mandatory** or **hard-mandatory**. 
See https://developer.hashicorp.com/terraform/cloud-docs/policy-enforcement/manage-policy-sets#sentinel for more information regarding Sentinel enforcement modes.

1. Log in to github and browse to the policy library repository: https://github.com/intel/policy-library-intel-aws
2. Fork the repository: https://docs.github.com/en/get-started/quickstart/fork-a-repo
3. Log into Terraform Enterprise or Terraform Cloud for Business and add the newly forked repository as a Version Controlled policy set: https://developer.hashicorp.com/terraform/cloud-docs/policy-enforcement/manage-policy-sets#create-policy-sets
4. Apply the policy set to your workspaces or globally as desired
5. Update the `sentinel.hcl` to **soft-mandatory** or **hard-mandatory** policies to harden governance and force rather than inform developers
6. When new versions are released simply sync your forked repository to be updated to the latest recommended instance list: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork (Note this will set policies back to **advisory**)

## How to Use

**Sentinel Test**

When using sentinel test in a remote directory against a policy that contains a static import (like the `deny-unapproved-instance-type` policies) additional commands and arguments must be passed in order for the test to run successfully. Execute the following command or another like it to test policies at the root level of this repository :

`find . -name ""*.sentinel"" -type f -execdir sentinel test \;`

**Customize Instance Lists**

Intel policy libraries are designed by default to  use the latest list of recommended instance types provided by Intel for a given Terraform resource. It is possible to modify both the list of allowed instance types and the behavior of automatically retriving new recommended instance lists. 

In order to modify the list of allowed instance types for a resource you must first identify the relevant content within the `approved.json` file. Do this by browsing to the `intel-cloud-resource-unapproved-instance-type.sentinel` policy you would like to modify in the `policies` folder. For this example, note the value *awsautoscaling* for `doc.allowed` to identify the section of the `approved.json` to modify: 

```

doc = {
""allowed"":   approved.awsautoscaling,

```

This will correspond to the section `awsautoscaling` within the `approved.json` that this policy will use for it's list of allowed instance types.

## Automatic Approved Instance List Retrieval

It is  possible to automatically retrieve new recommended instance types from Intel. In order to automatically receive updates to the allowed instance types list modify your `sentinel.hcl` import for the json data to reflect Intel's github repository. 

**NOTE**

Automatic retrieval of allowed instance types is not an editable entity. Using a remote source for the JSON data reduces the administrative burden of keeping instance lists up to date and ensures the most performant, optimized and secure hardware from Intel will be used but may have unintended consequences as lists are updated without user intervention. Be sure to understand the behavior of automatically retrieving allowed instance lists before proceeding.

In order to automatically receive updates to the `approved.json` without syncing your repo fork you must modify the `sentinel.hcl` file and change the static import from a local source to a remote:

```

import ""static"" ""approved"" {
  source = ""http::http://raw.githubusercontent.com/intel/policy-library-intel-aws/main/approved.json""
  format = ""json""
}

```

---

## Policies included

- intel-aws-autoscaling-group-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-autoscaling-group-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-autoscaling-group-deny-unapproved-instance-types/intel-aws-autoscaling-group-deny-unapproved-instance-types.sentinel))
- intel-aws-databricks-cluster-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-databricks-cluster-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-databricks-cluster-deny-unapproved-instance-types/intel-aws-databricks-cluster-deny-unapproved-instance-types.sentinel))
- intel-aws-databricks-cluster-deny-unapproved-runtime-engines ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-databricks-cluster-deny-unapproved-runtime-engines.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-databricks-cluster-deny-unapproved-runtime-engines/intel-aws-databricks-cluster-deny-unapproved-runtime-engines.sentinel))
- intel-aws-databricks-cluster-enforce-spark-conf ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-databricks-cluster-enforce-spark-conf.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-databricks-cluster-enforce-spark-conf/intel-aws-databricks-cluster-enforce-spark-conf.sentinel))
- intel-aws-db-instance-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-db-instance-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-db-instance-deny-unapproved-instance-types/intel-aws-db-instance-deny-unapproved-instance-types.sentinel))
- intel-aws-db-instance-deny-unapproved-storage-types ([docs](https://github.com/intel/policy-library-intel-aws/tree/main/docs/policies/intel-aws-db-instance-deny-unapproved-storage-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-db-instance-deny-unapproved-storage-types/intel-aws-db-instance-deny-unapproved-storage-types.sentinel))
- intel-aws-db-instance-deny-unencrypted-database ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-db-instance-deny-unencrypted-database.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-db-instance-deny-unencrypted-database/intel-aws-db-instance-deny-unencrypted-database.sentinel))
- intel-aws-eks-node-group-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-eks-node-group-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-eks-node-group-deny-unapproved-instance-types/intel-aws-eks-node-group-deny-unapproved-instance-types.sentinel))
- intel-aws-elasticache-replication-group-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-elasticache-replication-group-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-elasticache-replication-group-deny-unapproved-instance-types/intel-aws-elasticache-replication-group-deny-unapproved-instance-types.sentinel))
- intel-aws-aws-emr-cluster-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-aws-emr-cluster-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-aws-emr-cluster-deny-unapproved-instance-types.sentinel))
- intel-aws-aws-emr-instance-fleet-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-aws-emr-instance-fleet-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-aws-emr-instance-fleet-deny-unapproved-instance-types.sentinel))
- intel-aws-instance-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-instance-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-instance-deny-unapproved-instance-types/intel-aws-instance-deny-unapproved-instance-types.sentinel))
- intel-aws-launch-template-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-launch-template-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-launch-template-deny-unapproved-instance-types/intel-aws-launch-template-deny-unapproved-instance-types.sentinel))
- intel-aws-aws-rds-cluster-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-aws-rds-cluster-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-aws-rds-cluster-deny-unapproved-instance-types.sentinel))
- intel-aws-aws-rds-cluster-instance-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-aws-rds-cluster-instance-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-aws-rds-cluster-instance-deny-unapproved-instance-types.sentinel))
- intel-aws-sagemaker-endpoint-configuration-deny-unapproved-instance-types ([docs](https://github.com/intel/policy-library-intel-aws/blob/main/docs/policies/intel-aws-sagemaker-endpoint-configuration-deny-unapproved-instance-types.md) | [code](https://github.com/intel/policy-library-intel-aws/blob/main/policies/intel-aws-sagemaker-endpoint-configuration-deny-unapproved-instance-types/intel-aws-sagemaker-endpoint-configuration-deny-unapproved-instance-types.sentinel))

---",FAUX
intel/SGXDataCenterAttestationPrimitives,Application System,Application System,2025-03-18T15:39:11Z,2023-10-20T14:37:24Z,0,1,0,0,0,0,0,0,2018-08-28T23:14:01Z,2025-03-23T08:07:44Z,114413,292,C++,VRAI,175,FAUX,111,,111,,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,45,"Intel(R) Software Guard Extensions Data Center Attestation Primitives
================================================

Introduction
-------
Intel(R) Software Guard Extensions (Intel(R) SGX) Data Center Attestation Primitives (Intel(R) SGX DCAP) provides SGX attestation support targeted for data centers, cloud services providers and enterprises. This attestation model leverages Elliptic Curve Digital Signature algorithm (ECDSA) versus the current client based SGX attestation model which is EPID based (Enhanced Privacy Identification).

License
-------
This project is BSD license. See [License.txt](License.txt)

But Linux driver code is dual licensed under BSD/GPL v2. See [License.txt](driver/linux/License.txt) 

Third-party code is also used in this project. See [ThirdPartyLicenses.txt](QuoteGeneration/ThirdPartyLicenses.txt) and [ThirdPartyLicenses.txt](driver/win/ThirdPartyLicenses.txt) for details.

Contributing
-------
See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

Instruction
-------
## Build and Install the Intel(R) SGX Driver
   `README.md` files are provided under [Windows driver](driver/win) folder and [Linux driver](driver/linux) folder individually. Please follow the instructions in the corresponding `README.md` to build and install the Intel(R) SGX driver based on your target OS.

## Build Intel(R) SGX DCAP Quote Generation and Intel(R) SGX DCAP Quote Verification projects
Intel(R) SGX DCAP Quote Generation and Intel(R) SGX DCAP Quote Verification can be built on Linux by running ``make`` from root directory. To build on Windows, please refer the README.md in subdirectories.

## Build and Install the Intel(R) SGX DCAP Quote Generation Library
A [README.md](QuoteGeneration/README.md) is provided under [QuoteGeneration](QuoteGeneration) folder. Please follow the instructions in the `README.md` to build and install Intel(R) SGX DCAP Quote Generation Library.

## Build and Install the Intel(R) SGX DCAP Quote Verification Library
A [README.md](QuoteVerification/README.md) is provided under [QuoteVerification](QuoteVerification) folder. Please follow the instructions in the `README.md` to build and install Intel(R) SGX DCAP Quote Verification Library.",FAUX
invinst/chicago-police-data,Application System,Documentations,2024-10-03T14:07:57Z,2019-06-13T22:48:00Z,0,0,0,0,0,0,0,0,2016-06-03T06:34:33Z,2025-03-08T21:43:09Z,1757393,162,HTML,VRAI,60,FAUX,12,,12,a collection of public data re: CPD officers involved in police encounters,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,24,"# Chicago Police Data

## What is this?

This is a living repository of public data about Chicago’s police officers and their interactions with the public. The  various datasets stored within cover complaints of misconduct, misconduct investigations, use of force reports, awards, promotions, salary, official rosters, unit assignment over time.

The [Invisible Institute](https://invisible.institute/introduction) maintains this repo and uses these datasets to inform [CPDP.co](https://beta.cpdp.co) and @CPDPbot.

## What can I find in here?

Data ready for download can be found in [data/](https://github.com/invinst/chicago-police-data/tree/master/data) zipped up for your convenience.
* cleaned_data contains all our core datasets after the cleaning process
* matched_data contains all our core datasets matched to unique IDs for each officer (see [Workflow](#workflow))
* complaints_nov2016 contains only the raw complaints dataset as it was originally produced in response to our FOIA request.

FOIA response letters can be found in [foia/](https://github.com/invinst/chicago-police-data/tree/master/foia)

### Complaints

This dataset lists complaints about Chicago Police officers between 2000 and mid-2016 as well as the outcomes of those complaints. The data includes complaints made by civilians, which comprise the bulk of the records, as well as complaints made by other officers. Some were investigated by CPD’s Bureau of Internal Affairs. Others were investigated by a succession of civilian review agencies (formerly OPS, then IPRA, now COPA). The names of the officers accused are included.

### Awards

This dataset lists Chicago Police awards between 2005 and mid-2017. The Chicago Police publishes a list of current types of awards in their department directives [here](http://directives.chicagopolice.org/directives/data/a7a57bf0-12d101cb-ad612-d102-2b6676509a070b87.html) and describes the process [here](http://directives.chicagopolice.org/directives/data/a7a57bf0-12cad953-0e212-cada-aafce1abc7fcf520.html). The data includes awards that have been requested but are still in process, awards that were rejected, and awards to non-police employees of the CPD, in addition to awards given to sworn Chicago police officers.

### Salary

This dataset includes salary data for Chicago Police employees by year, spanning from 2002 to 2017. The data is kept by the City of Chicago’s Human Resources Department. Every officer has only one row per year, unless they changed positions in a year.

### Unit History

This dataset tracks the unit assignments of Chicago Police officers over time. The data includes entries dating back to the 1940s, though more recent data appears to be more reliable. Each row includes an officer’s name and date of appointment, plus the units they are leaving and joining. A list of unit numbers and their respective names can be found here.

### Matching Officers

In the raw data, officer identities had no unique identifier. In order to link between data sources, the officers in a single file were deduplicated, and these unique profiles were used to match against officer profiles from other deduplicated data sets. We use an iterative pairwise joining method, which identifies the strongest matches between unique officers in different data sets, removes these matched officers, then repeats the process until the permitted joins are exhausted or there are no unmatched officers left.

## Where did it come from?

The datasets and documents are all sourced originally from the Chicago Police Department (CPD), Civilian Office of Police Accountability (COPA), the Independent Police Review Authority (IPRA), or the City of Chicago. Most of these datasets were released in response to FOIA requests submitted by the Invisible Institute and its partners. Some information was also extracted from public web sites, for example, the [COPA Case Portal](http://www.chicagocopa.org/data-cases/case-portal/), [published reports](http://www.chicagocopa.org/news-publications/publications/) on the COPA web site, and the [City of Chicago Data Portal](https://data.cityofchicago.org/Public-Safety/COPA-Cases-Summary/mft5-nfa8).

# Using the data

This repo aims to be consistent with [Patrick Ball’s Principled Data Processing](https://youtube.com/watch?v=ZSunU9GQdcI).
For a detailed layout of the folder structure and data processing workflow, see [Workflow](#workflow).

Please join us here. Ask questions and share your own work. We have collected these datasets in this repository so that everyone can analyze and investigate police accountability in Chicago using a common body of evidence. If you would like to start a new project using our data, we encourage you to create a [new ticket](https://github.com/invinst/chicago-police-data/issues/new) and use the yellow ""independent project"" label to [collaborate with others](https://github.com/invinst/chicago-police-data/issues?q=is%3Aopen+is%3Aissue+label%3A%22independent+project%22) here on GitHub.

## Contributing to this repository

Browse the [""repo issues"" label on the Issue Tracker](https://github.com/invinst/chicago-police-data/issues?q=is%3Aopen+is%3Aissue+label%3A%22repo+issue%22) to see where help is needed.

If you discover a problem in this repo or if you find something that is inadequately explained, please open a [new ticket](https://github.com/invinst/chicago-police-data/issues/new) in the issue tracker and use the red ""repo issue"" label.

## I have a question

If you have a question about the information in this repo, where it comes from, or anything else related, open up a [new ticket](https://github.com/invinst/chicago-police-data/issues/new) in the tracker and use the purple ""question"" label.

# Workflow

The central goal of this repository is to make our data processing workflow transparent, reproducible, and auditable. While some output information may be redacted or removed from final publishing due to privacy concerns, the code will remain in this public repository so that the full process is in public view.

There are two main data processing directories in this repository are [individual/](###individual/) and [merge/](###merge/). Additionally, the [share/](###share/) directory contains files used across multiple tasks. The [frozen/](###frozen/) directory contains raw data and documents.

## Naming Conventions

Data files and directories which are related to a single FOIA or subset of a FOIA (including individual/, merge/, and frozen/ directories) follow a simple naming format: [data description]\_[data start year - data end year]\_[data received year - data received month]\_[FOIA number] (FOIA numbers are only included in directory names).

For example: the directory named individual/complaints-accused\_2000-2016\_2016-11\_p046957/ contains the data processing workflow for accused officers in complaints data, from 2000 to 2016, and this FOIA (number p046957) was received in November of 2016. As multiple types of complaint information came from the same FOIA, there are other directories with similar names, such as individual/complaints-victims\_2000-2016\_2016-11\_p046957/ which refers to the same data but only covers information about victims.

If some information is nonexistent or irrelevant (for example, the unit history dataset has no explicit “start year”), then that part will be left blank.

## Task Folders

The lowest level of directories in both individual/ and merge/ trees contain input/, output/, src/, and sometimes hand/ directories.

* **input**/ contains the data files that will be used for the current task. Tasks often take the previous tasks output as input.
* **src**/ contains all scripts used in the task: the helper functions, the task specific script (ex: if the task directory is clean/ then the main script will be clean.py), and the makefile (always named Makefile) which runs the task.
* **output**/ contains all files created in the task, which may include the processed input files, .log files, and .yaml files.
* **hand**/ contains reference files that are commonly shared between tasks and FOIAs, which may be needed for standardizing race, gender, column names.

### individual/

In **individual**/, there are multiple directories named according to their FOIA number and the month of receipt (if applicable) and the topic and date rage of the data. For some of these FOIAs (for example, the complaint and TRR data), there are multiple types of data contained in a single FOIA. The data description for these is complaints-[specific data] and TRR-[specific data]. The workflow goes as follows: import/ -> clean/ -> assign-unique-ids/ (if there are identifiable individuals) -> export/. All individual/ subdirectories can be run independently of each other.

* **import**/ takes the data in whatever format it has been received in (.csv, .xlsx, etc.), does minor formatting, standardizes column names, and collects initial metadata. Then it writes the data to a .csv.gz file.
* **clean**/ pushes the imported data through various cleaning functions that will standardize names, race, genders, columns that must be integers or dates, etc.
* **assign-unique-ids**/ identifies unique individuals with intra-file identification numbers based on file-specific characteristics. Two files will be written, one \_profiles file which contains one row per unique individual with all relevant demographic information and unique intra-file id, and another file identical to the ingested cleaned data with the addition of intra-file unique identifiers.
* **export**/ performs any final processing and exporting of the dataset that is needed. This may involve identifying individuals for merging from the \_profiles file before it is to be used in the merging process, or removing redundant columns from the main dataset.

### merge/

In merge/, exported files from the individual/ directories are brought together to be unified into the main relational dataset. While there cannot be a direct link between some files (for example awards and complaints) these files are linked through common officers identified within. The main output of each of these tasks (beginning with a number e.x. 01_roster...) is to produce the officer-reference.csv.gz file, which is a collection of \_profiles files combined with a unique ID unifying the same officer. This file is used to compare/collect potentially differing information about the same officer in different files.

Each subdirectory beginning with a number indicating the order in which the merges are run. Generally, the files are merged in order of number of unique officers in the data, beginning with the newest roster data set. Each directory's input contains the relevant 'full' file and the relevant \_profiles file, as well as the officer-reference file from the previous step. After the numbered tasks are run, the non-numbered task can run (in any order).

The `final-profiles` only takes in the officer-reference file from the last merge, and outputs a condensed profiles file that contains the ""best"" profile for every individual.

`generate_TRR_flags` does not utilize any officer information, but rather generates columns in the main TRR data set using information aggregated from weapon-discharge data and actions-response data.

These files in merge/ destory/change information and are only used for CPDP:

`fill_salary_ranks` fills the salary data with missing observations (year x rank x officer) for missing Lieutenants (output as salary-filled...) and then aggregates the filled data to 1 row per year x rank x officer (output as salary-ranks...)

`resolve_complaints` take core complaints files (complaints and accused) and generates/aggregates the pre-2000, 2016, and 2018 files to determine which data should be utilized for display purposes. Specifically, this ensures 1 row per CR in the complaints data ('complaints-complaints.csv.gz'), and 1 row per CR x officer in the accused data ('complaints-accused.csv.gz').

`resolve_complaints-supplementary` takes non-core complaints files (complainants, witnesses, investigators, victims) and creates resolved/aggregated files, e.g. information from either 2016 or 2018 complaints files. Outputs 'complaints-victims.csv.gz', etc.

`resolve_unit-history` takes all/both unit-history files and resolves conflicts and generates the aggregated 'unit-history.csv.gz'

### share/

share/ contains three sub-directories: src/ , hand/ , and tests/.

* **src**/ contains all helper functions/scripts that are used across multiple datasets or tasks, generally starting with the task to which they are relevant (merge\_functions.py or clean\_functions.py), or utils scripts that contain auxiliary helper functions (clean\_name\_utils.py or general\_utils.py). The file setup.py is used in all main scripts, regardless of dataset or task, because it initializes a logger and yaml file to store important information and it initializes namedtuples for all of the variables used in the main script.

* **hand**/ contains all reference data that is used across multiple datasets or tasks, generally stored in .yaml files. column\_names.yaml contains file specific column name standardization data for the import/ step, and column\_types.yaml stores the types of specific columns designated for cleaning in the clean/ step.

* **tests**/ contains pytest files with the format test_[file from src/].py and a hand/ directory (for cleaning tests).",VRAI
ioggstream/python-course,Documentations,Documentations,2025-04-27T19:43:32Z,2021-09-22T09:33:52Z,0,1,0,0,0,0,0,0,2014-02-15T09:11:27Z,2025-04-02T20:27:55Z,3699,64,HTML,VRAI,30,FAUX,31,,31,Stuff for python courses,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,7,"# python-course

Python exercises and materials for various courses.

## basic python

An introductory course for people knowing other languages.

- lessons: 2
- duration: 2 hours

## python for system administrator

A course for system administrators (Linux, Windows, Mac)

## MySQL 101 - 5.7

A MySQL course featuring data management basics

## Ansible 101

The training presented ad EuroPython 2017, with further revisions.

## Git 101

A fastrack for learning git.

## Docker 101

A fast track for learning docker and IoC basics.

## Further ideas

- intermediate python
- python for performance testing

---

## Developing

Many courses are written in markdown and converted in jupyter notebooks.
This is done to allow for a better versioning of the materials.

The docker image is provided via ghcr.io. You can test it locally with
`act`. See https://github.com/nektos/act.",FAUX
islet-project/islet,Application System,Application System,2025-04-21T02:35:54Z,2025-02-07T05:54:03Z,0,2,0,0,0,0,0,0,2022-06-22T06:59:15Z,2025-04-04T13:30:04Z,83867,114,Rust,VRAI,30,FAUX,29,"confidential-computing,rust",29,An on-device confidential computing platform,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,23,"<p align=""center""><img src=""https://github.com/islet-project/islet/blob/main/doc/res/logo-title.jpg?raw=true"" height=""100px""></p>

[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/8915/badge)](https://www.bestpractices.dev/projects/8915)

Islet is an open-source software project written in Rust that enables confidential computing
on ARM architecture devices using the ARMv9 CCA.
The primary objective of Islet is to enable on-device confidential computing
and protect user privacy on end user devices.

While current confidential computing solutions mainly focus on server-side
protection,  it is equally important to safeguard user information at the user
device level  since that is where private data collection initially occurs.
Furthermore, as more and more users rely on privacy apps such as private
messengers,  secure emails, password managers, and web browsers with privacy
settings,  there is a growing need to ensure privacy on user devices.
Islet, an open-source project, addresses this need by providing a platform
 for ARM-based confidential computing.

Enabling CC on user devices will not only establish end-to-end CC throughout
the entire data processing path,
but it will also help create a secure computation model
that enables processing of user private data on the user device
using the same components that previously were employed at the server side
without disclosing business logic.
Furthermore, on-device confidential computing will be a key enabler for
machine-to-machine computing without the need for server intervention

## Feature Overview
- Realm Management Monitor
- Hardware Enforced Security
- Confidential Computing API Standardization
- Automated Verification
- Use case : Confidential Machine Learning

## Overall Architecture

Islet provides a platform for running virtual machines (VMs)
confidentially, with standard SDKs for easy integration with other confidential
computing frameworks at upper layers.
The platform consists of two key components:
the Islet Realm Management Monitor (Islet-RMM) and Islet Hardware Enforced Security (Islet-HES).

- `Islet RMM` operates at EL2 in the Realm world on the application processor cores
and manages the confidential VMs, known as realms.
- On the other hand, `Islet HES` performs device boot measurement, generates
platform attestation reports, and manages sealing key functionality within a secure
hardware IP apart from the main application processor.

![islet-overview](doc/res/overview.png)

In designing Islet, we aim to to address the current security challenges in confidential
computing technologies right from the very beginning.
To ensure that our software is built with safety in mind, we have chosen to use the
Rust programming language, known for its unique security model that ensures memory
safety and concurrency safety.
Moving forward, we also plan to incorporate formal
verification techniques to further enhance the security of our design and implementation.

For more information, please visit our [developer site](https://islet-project.github.io/islet/).

## A demo video (Confidential ML)

![this page](https://github.com/islet-project/islet/raw/main/examples/confidential-ml/video/confidential_ml.gif)

- This video shows how Islet achieves an end-to-end confidential machine learning with a chat-bot scenario.
- This video flows as follows.
  1. It starts with a slide that describes all components involved in this demo. All components will run on confidential computing platforms.
  2. (*feed an ML model*) The model provider feeds the ML model into the ML server. This is done through a secure channel established with the aid of the certifier framework.
  3. (*run a coding assistant*) A mobile device user asks a chat-bot application that runs on Islet for generating a function. And then, that request is passed on to the ML server through a secure channel. Finally, the user can see the result (i.e., function).
  4. (*launch a malicious server*) This time, we launch a malicious server to show a failure case. When it attempts to join the certifier service (on the right side of the screen), it will not pass authentication as it results in a different measurement. Therefore, the malicious server cannot interact with the mobile device user in the first place.
- To download this video, click [here](https://github.com/islet-project/islet/raw/main/examples/confidential-ml/video/confidential_ml.mp4).",FAUX
jixiuf/vmacs,Application System,Documentations,2025-05-15T06:32:40Z,2025-05-12T07:28:41Z,0,0,0,0,0,0,0,0,2010-11-16T12:13:34Z,2025-04-08T08:57:15Z,85720,54,Emacs Lisp,VRAI,10,FAUX,1,,1,my .emacs.d&dotfiles,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,3,,VRAI
JJGadgets/Biohazard,DevOPs,Documentations,2025-05-16T01:36:31Z,2025-05-13T09:09:55Z,0,0,0,0,7,0,0,0,2022-04-11T20:24:00Z,2025-04-08T01:36:06Z,7115,58,Lua,VRAI,14,FAUX,112,"cilium,fluxcd,gitops,homelab,infrastructure-as-code,k8s-at-home,kubernetes,networkpolicy,sops,talhelper,talos,yaml",112,Glorifying jank that works | JJGadgets' HomeLab monorepo,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,4,"# Biohazard - JJ's Homelab Monorepo

**<ins>Glorifying jank that *works*.</ins>**

Powered by Flux, Kubernetes, Cilium, Talos, and jank. Amongst others.

<!--![Biohazard - CPU](https://img.shields.io/endpoint?url=https%3A%2F%2Fbiohazard-metrics.jjgadgets.tech%2Fquery%3Fformat%3Dendpoint%26metric%3Dcluster_cpu_usage&style=flat&label=Biohazard%20-%20CPU)
![Biohazard - Memory](https://img.shields.io/endpoint?url=https%3A%2F%2Fbiohazard-metrics.jjgadgets.tech%2Fquery%3Fformat%3Dendpoint%26metric%3Dcluster_memory_usage&style=flat&label=Biohazard%20-%20Memory)
![Blackhawk - Battery Charge](https://img.shields.io/endpoint?url=https%3A%2F%2Fbiohazard-metrics.jjgadgets.tech%2Fquery%3Fformat%3Dendpoint%26metric%3Dblackhawk_battery_percent&style=flat&label=Blackhawk%20-%20Battery%20Charge&link=https%3A%2F%2Fgithub.com%2Fkashalls%2Fkromgo)
![Blackhawk - Battery Health](https://img.shields.io/endpoint?url=https%3A%2F%2Fbiohazard-metrics.jjgadgets.tech%2Fquery%3Fformat%3Dendpoint%26metric%3Dblackhawk_battery_health&style=flat&label=Blackhawk%20-%20Battery%20Health&link=https%3A%2F%2Fgithub.com%2Fkashalls%2Fkromgo)
![Blackhawk - Battery Cycles](https://img.shields.io/endpoint?url=https%3A%2F%2Fbiohazard-metrics.jjgadgets.tech%2Fquery%3Fformat%3Dendpoint%26metric%3Dblackhawk_battery_cycles&style=flat&label=Blackhawk%20-%20Battery%20Health&link=https%3A%2F%2Fgithub.com%2Fkashalls%2Fkromgo)-->
<div align=""center"">

![Biohazard - Talos](https://biohazard-metrics.jjgadgets.tech/talos_build_version?format=badge)
![Biohazard - Kubernetes](https://biohazard-metrics.jjgadgets.tech/kubernetes_build_version?format=badge)
![Biohazard - Cilium](https://biohazard-metrics.jjgadgets.tech/cilium_version?format=badge)
<br><br>
![Biohazard - CPU](https://biohazard-metrics.jjgadgets.tech/cluster_cpu_usage?format=badge)
![Biohazard - Memory](https://biohazard-metrics.jjgadgets.tech/cluster_memory_usage?format=badge)
![Biohazard - Net TX](https://biohazard-metrics.jjgadgets.tech/cluster_network_transmit_usage?format=badge)
![Biohazard - Net RX](https://biohazard-metrics.jjgadgets.tech/cluster_network_receive_usage?format=badge)
<br><br>
![Biohazard - Cluster Age](https://biohazard-metrics.jjgadgets.tech/cluster_age_days?format=badge)
![Biohazard - Uptime](https://biohazard-metrics.jjgadgets.tech/cluster_uptime_days?format=badge)
![Biohazard - Nodes](https://biohazard-metrics.jjgadgets.tech/cluster_node_count?format=badge)
![Biohazard - Pods Running](https://biohazard-metrics.jjgadgets.tech/cluster_pods_running?format=badge)
![Biohazard - Pods Unhealthy](https://biohazard-metrics.jjgadgets.tech/cluster_pods_unhealthy?format=badge)
![Biohazard - Active Alerts](https://biohazard-metrics.jjgadgets.tech/prometheus_active_alerts?format=badge)
![Biohazard - Cilium Endpoints Unhealthy](https://biohazard-metrics.jjgadgets.tech/cilium_endpoints_unhealthy?format=badge)
![Biohazard - Cilium BPF Map Pressure](https://biohazard-metrics.jjgadgets.tech/cilium_bpf_map_pressure?format=badge)<br><br>
![Darkhawk - Battery](https://biohazard-metrics.jjgadgets.tech/darkhawk_battery_percent?format=badge)

</div>

---

## Overview

This is a mono repository for all the machines in my home infrasturcture, mainly focused around Kubernetes. The main goal is automation and being as hands-off as possible in manual labour and repeated tasks, while remaining agile in making changes to the cluster.

I also explore security solutions within my homelab, due to having my own PII and personal data on my infrastructure, as well as implementing security practices in a practical home ""production"" environment so that I can understand how things work and how each ""security positive"" change may impact the end user experience, resource usage, maintenance burden and other factors.

---

## Kubernetes

### Biohazard

This is my production home Kubernetes cluster. It is powered by Talos Linux, which allows for a Kubernetes-centric and appliance-like admin experience. This is a hyperconverged setup, with most of the compute handled here, as well as highly available (HA) application storage and critical data storage in the form of Rook-Ceph, backed up in a 3-2-1 fashion using VolSync running Restic and rclone. Network routing and security is handled by Cilium, which provides powerful NetworkPolicy capabilities while having relatively low maintenance burden.

Some VMs are also run in Biohazard using KubeVirt, which allows integration of Kubernetes-centric abstractions and principles such as NetworkPolicy, DNS service discovery and GitOps, and allows Kubernetes and Rook to manage failover and lifecycle of the VMs.

### Nuclear

This is my test cluster, however it is currently not running. This cluster is used when I want to test a major change involving mass migrations and/or potential prolonged outage, such as moving from Talos VMs on Proxmox VE consuming Proxmox-managed Ceph for storage to baremetal Talos + Rook-managed Ceph.

### GitOps

Flux and Renovate provide a mostly hands-off GitOps experience, where I can push the Kubernetes resources needed to deploy a new app to this Git repo as well as update the Kustomization.yaml used by Flux to control what a given cluster should deploy. From there, Flux will automatically reconcile the changes, and Renovate will ensure updates are either automerged or proposed in Pull Requests for me to review.

## Core Components

These can be found under the `./kube/deploy/core` folder, allowing for clear separation of components that are essential for the cluster to operate to serve apps.

- **Cilium**: Provides network routing, network security, exposing apps via LoadBalancers and other networking functionality.
- **Multus**: Provides secondary network connectivity aside from Cilium, such as connecting pods and KubeVirt VMs to specific VLANs' L2 domains directly.
- **Rook-Ceph**: Provides and manages highly available networked persistent storage within the Kubernetes cluster itself.
- **VolSync**: Provides and manages automated backups and restores of persistent storage.
- **democratic-csi**: Wildcard storage CSI driver supporting multiple backends like local-hostpath.
- **Flux**: Provides GitOps automation for syncing desired state of resources.
- **external-secrets**: Syncs secrets from external sources like 1Password as Kubernetes secrets, with a templating engine.
- **k8s-gateway**: Internal DNS resolver for exposing apps via Ingress, Gateway API and LoadBalancer Services.
- **external-dns**: Syncs DNS records against upstream resolvers' records, such as Cloudflare DNS.
- **cert-manager**: Automated TLS management for generating and rotating signed and trusted TLS certificates stored as Kubernetes secrets.
- **Ingress-NGINX**: Kubernetes Ingress controller for automated configuration of NGINX to reverse proxy HTTP/S apps with automated TLS from cert-manager.
- **cloudflared**: Expose specific apps publicly via Cloudflare Zero Trust tunnel.
- **Crunchy Postgres Operator**: Automated HA Postgres cluster and backups management.
- **VictoriaMetrics**: Pull-based monitoring platform.
- **kube-prometheus-stack + prometheus-operator**: Automated configuration and service discovery for Prometheus (and thus VictoriaMetrics), with shipped defaults for Kubernetes-focused monitoring and alerting.
- **Kyverno**: Kubernetes API webhook policy engine, for validating, mutating and generating resources. Also abused as The Jank Engine.
- **Spegel**: Transparent container registry cache mirror within cluster.
- **system-upgrade-controller**: Auto-update of cluster components' versions such as Talos OS and Kubernetes versions. Combined with GitOps + Renovate for a PR-based auto-updating workflow.

## Networking

My ""production"" home network is currently primarily powered by Fortinet.

- Firewall: **FortiGate 40F**
- 1GbE switch: **FortiSwitch 108E**, managed, using NAC for VLAN assignment.
- 10GbE switch: **TP-Link TL-ST1008F**, unmanaged, downstream of FortiSwitch so its NAC handles VLAN assignment.
- WiFi Access Point: **FortiAP 221E**

I also tinker with and have previously used other platforms, such as OPNsense firewall, Brocade ICX6450 switch, Aruba S1500-12p switch, Cisco Catalyst 3750G, etc.",VRAI
jouve/charts,Application System,Application System,2025-04-08T17:47:57Z,2025-01-29T13:10:00Z,0,0,0,0,0,0,0,47,2023-05-16T22:17:28Z,2025-03-26T21:42:38Z,1583,16,Mustache,VRAI,22,FAUX,4,,4,,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,84,"# usage

```console
helm repo add jouve https://jouve.github.io/charts/
```

# charts

| chart                                     | description                                                                                                                                          |
| ----------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| [jouve/extra](charts/extra)               | Deploy a list of Kubernetes resources as a release with [bitnami/common][bitnami/common] as a dependency |
| [jouve/hnc](charts/hnc)                   | [Hierarchical Namespace Controller][hnc]. Policies and delegated creation to Kubernetes namespaces       |
| [jouve/mailpit](charts/mailpit)           | Deploy [mailpit][mailpit]: An email and SMTP testing tool with API for developers                        |
| [jouve/postgresql](charts/postgresql)     | Deploy postgresql using [cloudnative-pg][cloudnative-pg] operator                                        |
| [jouve/subnamespace](charts/subnamespace) | SubnamespaceAnchor is the Schema for the [subnamespace][hnc] API                                         |

[bitnami/common]: https://github.com/bitnami/charts/tree/main/bitnami/common
[cloudnative-pg]: https://cloudnative-pg.io/
[hnc]: https://github.com/kubernetes-sigs/hierarchical-namespaces
[mailpit]: https://github.com/axllent/mailpit",FAUX
jpetazzo/container.training,Documentations,Documentations,2025-05-11T16:34:11Z,2025-01-15T18:48:47Z,0,0,0,0,10,0,0,0,2015-06-07T04:03:19Z,2025-04-07T05:55:24Z,38539,3757,Shell,VRAI,1610,FAUX,20,"compose,docker,dockerfiles,elk,helm,kubernetes,labs,slides,stern,swarm,training,workshop",20,"Slides and code samples for training, tutorials, and workshops about Docker, containers, and Kubernetes.",FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,109,"# Container Training

This repository (formerly known as `orchestration-workshop`)
contains materials (slides, scripts, demo app, and other
code samples) used for various workshops, tutorials, and
training sessions around the themes of Docker, containers,
and orchestration.

For the moment, it includes:

- Introduction to Docker and Containers,
- Container Orchestration with Docker Swarm,
- Container Orchestration with Kubernetes.

These materials have been designed around the following
principles:

- they assume very little prior knowledge of Docker,
  containers, or a particular programming language;
- they can be used in a classroom setup (with an
  instructor), or self-paced at home;
- they are hands-on, meaning that they contain lots
  of examples and exercises that you can easily
  reproduce;
- they progressively introduce concepts in chapters
  that build on top of each other.

If you're looking for the materials, you can stop reading
right now, and hop to http://container.training/, which
hosts all the slides decks available.

The rest of this document explains how this repository
is structured, and how to use it to deliver (or create)
your own tutorials.


## Why a single repository?

All these materials have been gathered in a single repository
because they have a few things in common:

- some [shared slides](slides/shared/) that are re-used
  (and updated) identically between different decks;
- a [build system](slides/) generating HTML slides from
  Markdown source files;
- a [semi-automated test harness](slides/autopilot/) to check
  that the exercises and examples provided work properly;
- a [PhantomJS script](slides/slidechecker.js) to check
  that the slides look good and don't have formatting issues;
- [deployment scripts](prepare-vms/) to start training
  VMs in bulk;
- a fancy pipeline powered by
  [Netlify](https://www.netlify.com/) and continuously
  deploying `master` to http://container.training/.


## What are the different courses available?

**Introduction to Docker** is derived from the first
""Docker Fundamentals"" training materials. For more information,
see [jpetazzo/intro-to-docker](https://github.com/jpetazzo/intro-to-docker).
The version in this repository has been adapted to the Markdown
publishing pipeline. It is still maintained, but only receives
minor updates once in a while.

**Container Orchestration with Docker Swarm** (formerly
known as ""Orchestration Workshop"") is a workshop created by Jérôme
Petazzoni in June 2015. Since then, it has been continuously updated
and improved, and received contributions from many others authors.
It is actively maintained.

**Container Orchestration with Kubernetes** was created by
Jérôme Petazzoni in October 2017, with help and feedback from
a few other contributors. It is actively maintained.


## Repository structure

- [bin](bin/)
  - A few helper scripts that you can safely ignore for now.
- [dockercoins](dockercoins/)
  - The demo app used throughout the orchestration workshops.
- [efk](efk/), [elk](elk/), [prom](prom/), [snap](snap/):
  - Logging and metrics stacks used in the later parts of
    the orchestration workshops.
- [prepare-local](prepare-local/), [prepare-machine](prepare-machine/):
  - Contributed scripts to automate the creation of local environments.
    These could use some help to test/check that they work.
- [prepare-vms](prepare-vms/):
  - Scripts to automate the creation of AWS instances for students.
    These are routinely used and actively maintained.
- [slides](slides/):
  - All the slides! They are assembled from Markdown files with
    a custom Python script, and then rendered using [gnab/remark](
    https://github.com/gnab/remark). Check this directory for more details.
- [stacks](stacks/):
  - A handful of Compose files (version 3) allowing to easily
    deploy complex application stacks.


## Course structure

(This applies only for the orchestration workshops.)

The workshop introduces a demo app, ""DockerCoins,"" built
around a micro-services architecture. First, we run it
on a single node, using Docker Compose. Then, we pretend
that we need to scale it, and we use an orchestrator
(SwarmKit or Kubernetes) to deploy and scale the app on
a cluster.

We explain the concepts of the orchestrator. For SwarmKit,
we setup the cluster with `docker swarm init` and `docker swarm join`.
For Kubernetes, we use pre-configured clusters.

Then, we cover more advanced concepts: scaling, load balancing,
updates, global services or daemon sets.

There are a number of advanced optional chapters about
logging, metrics, secrets, network encryption, etc.

The content is very modular: it is broken down in a large
number of Markdown files, that are put together according
to a YAML manifest. This allows to re-use content
between different workshops very easily.


### DockerCoins

The sample app is in the `dockercoins` directory.
It's used during all chapters
for explaining different concepts of orchestration.

To see it in action:

- `cd dockercoins && docker-compose up -d`
- this will build and start all the services
- the web UI will be available on port 8000


## Running the Workshop

If you want to deliver one of these workshops yourself,
this section is for you!

> *This section has been mostly contributed by
> [Bret Fisher](https://twitter.com/bretfisher), who was
> one of the first persons to have the bravery of delivering
> this workshop without me. Thanks Bret! 🍻
>
> Jérôme.*


### General timeline of planning a workshop

- Fork repo and run through slides, doing the hands-on to be sure you
  understand the different `dockercoins` repo's and the steps we go through to
  get to a full Swarm Mode cluster of many containers. You'll update the first
  few slides and last slide at a minimum, with your info.
- ~~Your docs directory can use GitHub Pages.~~
- This workshop expects 5 servers per student. You can get away with as little
  as 2 servers per student, but you'll need to change the slide deck to
  accommodate. More servers = more fun.
- If you have more then ~20 students, try to get an assistant (TA) to help
  people with issues, so you don't have to stop the workshop to help someone
  with ssh etc.
- AWS is our most tested process for generating student machines. In
  `prepare-vms` you'll find scripts to create EC2 instances, install docker,
  pre-pull images, and even print ""cards"" to place at each students seat with
  IP's and username/password.
- Test AWS Scripts: Be sure to test creating *all* your needed servers a week
  before workshop (just for a few minutes). You'll likely hit AWS limits in the
  region closest to your class, and it sometimes takes days to get AWS to raise
  those limits with a support ticket.
- Create a https://gitter.im chat room for your workshop and update slides
  with url. Also useful for TA to monitor this during workshop. You can use it
  before/after to answer questions, and generally works as a better answer then
  ""email me that question"".
- If you can send an email to students ahead of time, mention how they should
  get SSH, and test that SSH works. If they can `ssh github.com` and get
  `permission denied (publickey)` then they know it worked, and SSH is properly
  installed and they don't have anything blocking it. SSH and a browser are all
  they need for class.
- Typically you create the servers the day before or morning of workshop, and
  leave them up the rest of day after workshop. If creating hundreds of servers,
  you'll likely want to run all these `workshopctl` commands from a dedicated
  instance you have in same region as instances you want to create. Much faster
  this way if you're on poor internet. Also, create 2 sets of servers for
  yourself, and use one during workshop and the 2nd is a backup.
- Remember you'll need to print the ""cards"" for students, so you'll need to
  create instances while you have a way to print them.


### Things That Could Go Wrong

- Creating AWS instances ahead of time, and you hit its limits in region and
  didn't plan enough time to wait on support to increase your limits. :(
- Students have technical issues during workshop. Can't get ssh working,
  locked-down computer, host firewall, etc.
- Horrible wifi, or ssh port TCP/22 not open on network! If wifi sucks you
  can try using MOSH https://mosh.org which handles SSH over UDP. TMUX can also
  prevent you from losing your place if you get disconnected from servers.
  https://tmux.github.io
- Forget to print ""cards"" and cut them up for handing out IP's.
- Forget to have fun and focus on your students!


### Creating the VMs

`prepare-vms/workshopctl` is the script that gets you most of what you need for
setting up instances. See
[prepare-vms/README.md](prepare-vms)
for all the info on tools and scripts.


### Content for Different Workshop Durations

With all the slides, this workshop is a full day long. If you need to deliver
it in shorter timelines, here's some recommendations on what to cut out. You
can replace `---` with `???` which will hide slides. Or leave them there and
add something like `(EXTRA CREDIT)` to title so students can still view the
content but you also know to skip during presentation.


#### 3 Hour Version

- Limit time on debug tools, maybe skip a few. *""Chapter 1:
  Identifying bottlenecks""*
- Limit time on Compose, try to have them building the Swarm Mode by 30
  minutes in
- Skip most of Chapter 3, Centralized Logging and ELK
- Skip most of Chapter 4, but keep stateful services and DAB's if possible
- Mention what DAB's are, but make this part optional in case you run out
  of time


#### 2 Hour Version

- Skip all the above, and:
- Skip the story arc of debugging dockercoins all together, skipping the
  troubleshooting tools. Just focus on getting them from single-host to
  multi-host and multi-container.
- Goal is first 30min on intro and Docker Compose and what dockercoins is,
  and getting it up on one node in docker-compose.
- Next 60-75 minutes is getting dockercoins in Swarm Mode services across
  servers. Big Win.
- Last 15-30 minutes is for stateful services, DAB files, and questions.


### Pre-built images

There are pre-built images for the 4 components of the DockerCoins demo app: `dockercoins/hasher:v0.1`, `dockercoins/rng:v0.1`, `dockercoins/webui:v0.1`, and `dockercoins/worker:v0.1`. They correspond to the code in this repository.

There are also three variants, for demo purposes:

- `dockercoins/rng:v0.2` is broken (the server won't even start),
- `dockercoins/webui:v0.2` has bigger font on the Y axis and a green graph (instead of blue),
- `dockercoins/worker:v0.2` is 11x slower than `v0.1`.


## Past events

Since its inception, this workshop has been delivered dozens of times,
to thousands of people, and has continuously evolved. This is a short
history of the first times it was delivered. Look also in the ""tags""
of this repository: they all correspond to successive iterations of
this workshop. If you attended a past version of the workshop, you
can use these tags to see what has changed since then.

- QCON, New York City (2015, June)
- KCDC, Kansas City (2015, June)
- JDEV, Bordeaux (2015, July)
- OSCON, Portland (2015, July)
- StrangeLoop, Saint Louis (2015, September)
- LISA, Washington D.C. (2015, November)
- SCALE, Pasadena (2016, January)
- Zenika, Paris (2016, February)
- Container Solutions, Amsterdam (2016, February)
- ... and much more!


# Problems? Bugs? Questions?

If there is a bug and you can fix it: submit a PR.
Make sure that I know who you are so that I can thank you
(because you're the real MVP!)

If there is a bug and you can't fix it, but you can
reproduce it: submit an issue explaining how to reproduce.

If there is a bug and you can't even reproduce it:
sorry. It is probably an Heisenbug. We can't act on it
until it's reproducible, alas.


# “Please teach us!”

If you have attended one of these workshops, and want
your team or organization to attend a similar one, you
can look at the list of upcoming events on
http://container.training/.

You are also welcome to reuse these materials to run
your own workshop, for your team or even at a meetup
or conference. In that case, you might enjoy watching
[Bridget Kromhout's talk at KubeCon 2018 Europe](
https://www.youtube.com/watch?v=mYsp_cGY2O0), explaining
precisely how to run such a workshop yourself.

Finally, you can also contact the following persons,
who are experienced speakers, are familiar with the
material, and are available to deliver these workshops
at your conference or for your company:

- jerome dot petazzoni at gmail dot com
- bret at bretfisher dot com

(If you are willing and able to deliver such workshops,
feel free to submit a PR to add your name to that list!)

**Thank you!**",FAUX
juspay/hyperswitch-card-vault,Toolkit,Application System,2025-04-23T11:34:38Z,2024-10-04T09:34:55Z,0,0,0,0,0,2,0,0,2023-10-19T16:46:22Z,2025-03-28T21:12:25Z,782,44,Rust,VRAI,13,FAUX,21,"beginner-friendly,finance,open-source,postgresql,rust,security",21,Hyperswitch Card Vault is an open-source sensitive information storage system built on Rust.,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,6,"# Tartarus - Rust Locker


## Overview

The Hyperswitch Card Vault (Tartarus) is a highly performant and a secure vault to save sensitive data such as payment card details, bank account details etc.

It is designed in an polymorphic manner to handle and store any type of sensitive information making it highly scalable with extensive coverage of payment methods and processors.

Tartarus is built with a GDPR compliant personal identifiable information (PII) storage and secure encryption algorithms to be fully compliant with PCI DSS requirements.

Here's a quick guide to [Get Started](./docs/guides/setup.md) with setting up Tartarus.

### How does Tartarus work?

- Your application will communicate with Tartarus via a middleware.
- All requests and responses to and from the middleware are signed and encrypted with the JWS and JWE algorithms.
- The locker supports CRD APIs on the /data and /cards endpoints - <API Reference to be linked>
- Cards are stored against the combination of merchant and customer identifiers.
- Internal hashing checks are in place to avoid data duplication.

![General Work Flow](./docs/imgs/general-block-diagram.png)

### Key Hierarchy

- Master Key - AES generated key to that is encrypted/decrypted by the custodian keys to run the locker and associated configurations.
- Custodian Keys - AES generated key that is used to encrypt and decrypt the master key. It broken into two keys (key 1 and key 2) and available with two custodians to enhance security.

![Key Hierarchy](./docs/imgs/locker-key-hierarchy.png)

### Setup Guide

Follow this guide to setup Tartarus - [Get Started](./docs/guides/setup.md)",VRAI
k8s-operatorhub/community-operators,DevOPs,DevOPs,2025-05-16T00:38:42Z,2025-05-13T07:27:11Z,0,0,0,0,12,0,0,0,2021-06-23T09:23:18Z,2025-04-08T04:57:01Z,465224,237,Dockerfile,VRAI,622,FAUX,8,,8,The canonical source for Kubernetes Operators that are published on OperatorHub.io and part of the default catalog of the Operator Lifecycle Manager.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,949,"# Kubernetes Community Operators
[![License](http://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)

## About this repository

This repo is the canonical source for Kubernetes Operators that appear on [OperatorHub.io](https://operatorhub.io).
The solutions merged on this repository are distributed via the [OLM][olm] index catalog [quay.io/operatorhubio/catalog][quay.io].
Users can install [OLM][olm] in any Kubernetes or vendor such as Openshift to consume this content by adding a new CatalogSource for the index image `quay.io/operatorhubio/catalog`. [(more info)][catalog]

**NOTE** If you are looking to distribute solutions on Openshift/OKD catalog then, you also should publish them 
into the repository [Community Operators](https://github.com/redhat-openshift-ecosystem/community-operators-prod).

## Documentation

Full documentation is generated via [mkdoc](https://www.mkdocs.org/) and is located at [https://k8s-operatorhub.github.io/community-operators/](https://k8s-operatorhub.github.io/community-operators/)

## IMPORTANT NOTICE

Some APIs versions are deprecated and are OR will no longer be served on the Kubernetes version 
`1.22/1.25/1.26` and consequently on vendors like Openshift `4.9/4.12/4.13`.

**What does it mean for you?**

Operator bundle versions using the removed APIs can not work successfully from the respective releases. 
Therefore, it is recommended to check if your solutions are failing in these scenarios to let its users be aware. 
Note that you can inform via the CSV the minimal (`spec.minKubeVersion`) and the max Kubernetes 
version (metadata.annotation `operatorhub.io/ui-metadata-max-k8s-version`) where your solution can 
successfully work. This information can be checked on the details of each release on [OperatorHub.io](https://operatorhub.io).

Please, make sure you check the following announcements:
- [How to deal with removal of v1beta1 CRD removals in Kubernetes 1.22](https://github.com/k8s-operatorhub/community-operators/discussions/468)
- [Kubernetes API removals on 1.25/1.26 might impact your Operator. How to deal with it?](https://github.com/k8s-operatorhub/community-operators/discussions/1194)

> **NOTE** _If you have been distributing solutions on Openshift you might be aware of the 
property `""olm.properties"": '[{""type"": ""olm.maxOpenShiftVersion"", ""value"": ""<OCP version>""}]'` 
which can be used to block cluster admins upgrades when they have Operator versions installed that can **not** 
work well in OCP versions higher than the value informed. Nothing prevents you from using this property here, 
however, be aware that it is ignored on [OperatorHub.io](https://operatorhub.io) and that the index catalog built from 
this repository is not part of the Openshift catalog. So that, it can be useful only for those 
who are creating a new Source Catalog on Openshift using the index image: `quay.io/operatorhubio/catalog:latest`._ 

## Reporting Bugs

Use the issue tracker in this repository to report bugs.

[k8s-deprecated-guide]: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22
[olm]: https://github.com/operator-framework/operator-lifecycle-manager
[quay.io]: https://quay.io/repository/operatorhubio/catalog?tag=latest&tab=tags
[catalog]: https://k8s-operatorhub.github.io/community-operators/testing-operators/#1-create-the-catalogsource",FAUX
kata-containers/kata-containers,DevOPs,Application System,2025-05-15T18:59:56Z,2025-03-12T19:11:22Z,0,5,0,0,0,0,0,0,2017-12-07T05:01:12Z,2025-04-08T02:15:53Z,77872,5999,Rust,VRAI,1086,FAUX,1579,"acrn,containers,cri,cri-o,docker,firecracker,k8s,kubernetes,kvm,oci,qemu,security,virtual-machine,virtualization",1579,"Kata Containers is an open source project and community working to build a standard implementation of lightweight Virtual Machines (VMs) that feel and perform like containers, but provide the workload isolation and security advantages of VMs. https://katacontainers.io/",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,435,"<img src=""https://object-storage-ca-ymq-1.vexxhost.net/swift/v1/6e4619c416ff4bd19e1c087f27a43eea/www-images-prod/openstack-logo/kata/SVG/kata-1.svg"" width=""900"">

[![CI | Publish Kata Containers payload](https://github.com/kata-containers/kata-containers/actions/workflows/payload-after-push.yaml/badge.svg)](https://github.com/kata-containers/kata-containers/actions/workflows/payload-after-push.yaml) [![Kata Containers Nightly CI](https://github.com/kata-containers/kata-containers/actions/workflows/ci-nightly.yaml/badge.svg)](https://github.com/kata-containers/kata-containers/actions/workflows/ci-nightly.yaml)

# Kata Containers

Welcome to Kata Containers!

This repository is the home of the Kata Containers code for the 2.0 and newer
releases.

If you want to learn about Kata Containers, visit the main
[Kata Containers website](https://katacontainers.io).

## Introduction

Kata Containers is an open source project and community working to build a
standard implementation of lightweight Virtual Machines (VMs) that feel and
perform like containers, but provide the workload isolation and security
advantages of VMs.

## License

The code is licensed under the Apache 2.0 license.
See [the license file](LICENSE) for further details.

## Platform support

Kata Containers currently runs on 64-bit systems supporting the following
technologies:

| Architecture | Virtualization technology |
|-|-|
| `x86_64`, `amd64` | [Intel](https://www.intel.com) VT-x, AMD SVM |
| `aarch64` (""`arm64`"")| [ARM](https://www.arm.com) Hyp |
| `ppc64le` | [IBM](https://www.ibm.com) Power |
| `s390x` | [IBM](https://www.ibm.com) Z & LinuxONE SIE |

### Hardware requirements

The [Kata Containers runtime](src/runtime) provides a command to
determine if your host system is capable of running and creating a
Kata Container:

```bash
$ kata-runtime check
```

> **Notes:**
>
> - This command runs a number of checks including connecting to the
>   network to determine if a newer release of Kata Containers is
>   available on GitHub. If you do not wish this to check to run, add
>   the `--no-network-checks` option.
>
> - By default, only a brief success / failure message is printed.
>   If more details are needed, the `--verbose` flag can be used to display the
>   list of all the checks performed.
>
> - If the command is run as the `root` user additional checks are
>   run (including checking if another incompatible hypervisor is running).
>   When running as `root`, network checks are automatically disabled.

## Getting started

See the [installation documentation](docs/install).

## Documentation

See the [official documentation](docs) including:

- [Installation guides](docs/install)
- [Developer guide](docs/Developer-Guide.md)
- [Design documents](docs/design)
  - [Architecture overview](docs/design/architecture)
  - [Architecture 3.0 overview](docs/design/architecture_3.0/)

## Configuration

Kata Containers uses a single
[configuration file](src/runtime/README.md#configuration)
which contains a number of sections for various parts of the Kata
Containers system including the [runtime](src/runtime), the
[agent](src/agent) and the [hypervisor](#hypervisors).

## Hypervisors

See the [hypervisors document](docs/hypervisors.md) and the
[Hypervisor specific configuration details](src/runtime/README.md#hypervisor-specific-configuration).

## Community

To learn more about the project, its community and governance, see the
[community repository](https://github.com/kata-containers/community). This is
the first place to go if you wish to contribute to the project.

## Getting help

See the [community](#community) section for ways to contact us.

### Raising issues

Please raise an issue
[in this repository](https://github.com/kata-containers/kata-containers/issues).

> **Note:**
> If you are reporting a security issue, please follow the [vulnerability reporting process](https://github.com/kata-containers/community#vulnerability-handling)

## Developers

See the [developer guide](docs/Developer-Guide.md).

### Components

### Main components

The table below lists the core parts of the project:

| Component | Type | Description |
|-|-|-|
| [runtime](src/runtime) | core | Main component run by a container manager and providing a containerd shimv2 runtime implementation. |
| [runtime-rs](src/runtime-rs) | core | The Rust version runtime. |
| [agent](src/agent) | core | Management process running inside the virtual machine / POD that sets up the container environment. |
| [`dragonball`](src/dragonball) | core | An optional built-in VMM brings out-of-the-box Kata Containers experience with optimizations on container workloads |
| [documentation](docs) | documentation | Documentation common to all components (such as design and install documentation). |
| [tests](tests) | tests | Excludes unit tests which live with the main code. |

### Additional components

The table below lists the remaining parts of the project:

| Component | Type | Description |
|-|-|-|
| [packaging](tools/packaging) | infrastructure | Scripts and metadata for producing packaged binaries<br/>(components, hypervisors, kernel and rootfs). |
| [kernel](https://www.kernel.org) | kernel | Linux kernel used by the hypervisor to boot the guest image. Patches are stored [here](tools/packaging/kernel). |
| [osbuilder](tools/osbuilder) | infrastructure | Tool to create ""mini O/S"" rootfs and initrd images and kernel for the hypervisor. |
| [kata-debug](tools/packaging/kata-debug/README.md) | infrastructure | Utility tool to gather Kata Containers debug information from Kubernetes clusters. |
| [`agent-ctl`](src/tools/agent-ctl) | utility | Tool that provides low-level access for testing the agent. |
| [`kata-ctl`](src/tools/kata-ctl) | utility | Tool that provides advanced commands and debug facilities. |
| [`trace-forwarder`](src/tools/trace-forwarder) | utility | Agent tracing helper. |
| [`runk`](src/tools/runk) | utility | Standard OCI container runtime based on the agent. |
| [`ci`](.github/workflows) | CI | Continuous Integration configuration files and scripts. |
| [`ocp-ci`](ci/openshift-ci/README.md) | CI | Continuous Integration configuration for the OpenShift pipelines. |
| [`katacontainers.io`](https://github.com/kata-containers/www.katacontainers.io) | Source for the [`katacontainers.io`](https://www.katacontainers.io) site. |
| [`Webhook`](tools/testing/kata-webhook/README.md) | utility | Example of a simple admission controller webhook to annotate pods with the Kata runtime class |

### Packaging and releases

Kata Containers is now
[available natively for most distributions](docs/install/README.md#packaged-installation-methods).

## General tests

See the [tests documentation](tests/README.md).

## Metrics tests

See the [metrics documentation](tests/metrics/README.md).

## Glossary of Terms

See the [glossary of terms](https://github.com/kata-containers/kata-containers/wiki/Glossary) related to Kata Containers.",VRAI
kcl-lang/modules,DevOPs,Database,2025-05-13T14:54:48Z,2025-03-30T07:43:42Z,0,0,0,0,2,0,0,3,2023-10-08T08:07:59Z,2025-04-06T19:41:23Z,25923,26,Roff,VRAI,27,FAUX,6,,6,KCL Community Modules. View on Artifact Hub: https://artifacthub.io/packages/search?org=kcl&sort=relevance&page=1,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,24,"<h1 align=""center"">KCL Modules</h1>

<p align=""center"">
<a href=""./README.md"">English</a> | <a href=""./README-zh.md"">简体中文</a>

这个仓库负责保存已经发布的 KCL 模块，并且您可以在 [artifacthub.io (AH)](https://artifacthub.io/) 上找到这些包。

## 快速开始

在下一节中，我们通过 `helloworld` 示例向您展示如何发布您的 KCL 包并且在 AH 上找到他们。

### 准备工作

- 安装 [KCL](https://kcl-lang.io/docs/user_docs/getting-started/install)
- 安装 [git](https://git-scm.com/book/zh/v2/%E8%B5%B7%E6%AD%A5-%E5%AE%89%E8%A3%85-Git)
- [注册一个 Github 账户(可选)](https://docs.github.com/zh/get-started/signing-up-for-github/signing-up-for-a-new-github-account)

### 代码仓库

注意：如果您希望将您的 KCL 包发布到 `kcl-lang` 官方的 Registry 中，那么您的 KCL 包的源代码将以开源的形式保存在当前仓库中，您需要将您的包的源代码通过 PR 提交到这个仓库中。

### 准备您的 KCL 包

通过 `kcl mod init <module_name>` 命令, 您可以创建一个合法的 KCL 程序模块。

目前，仓库能够识别的合法的程序的目录结构如下：

```
<module_name>
    |- kcl.mod (必选的)
    |- kcl.mod.lock (可选的)
    |- artifacthub-pkg.yaml （可选的）
    |- README.md （可选的）
    |- (*.k) kcl program files
```

- kcl.mod : 作为 KCL 程序包的标识文件，这个文件**必选的**，包含 kcl.mod 文件的目录会被标识为文件的根目录。
- kcl.mod.lock : 自动生成的用来固定依赖版本的文件，这个文件**可选的**，不需要手动改动。
- artifacthub-pkg.yaml : 这个文件是**可选的**，因为我们的仓库目前通过 artifacthub 展示所有的包，通过 artifacthub-pkg.yaml 来配置您想要包的信息，这里我们采取的策略是**如果在您的包的 kcl.mod 文件所在目录中有一个名为 artifacthub-pkg.yaml 的配置文件，那么，我们将使用您提供 artifacthub-pkg.yaml 来展示您的包的信息，否则，我们将会使用一些默认的信息生成对应的 artifacthub-pkg.yaml 文件。**
- README.md : 一个 markdown 文件作为您的包的文档，这个文件是**可选的**，**如果您不提供这个文件，artifacthub 上将不会展示您的包的文档。**
- (*.k) kcl program files: 您的 KCL 程序的源代码。

### 通过 PR 发布您的包

#### 1. 下载代码仓库

首先，您需要使用 git 将仓库 https://github.com/kcl-lang/modules 下载到您的本地 

```shell
git clone https://github.com/kcl-lang/modules --depth=1
```

#### 2. 为您的包创建一个分支

我们推荐您的分支名为：publish-pkg-<module_name>, <module_name> 为您包的名称。

以包 helloworld 为例

进入您下载的 modules 目录中

```shell
cd modules
```

为包 helloworld 创建一个分支 `publish-pkg-helloworld`

```shell
git checkout -b publish-pkg-helloworld
```

#### 3. 添加您的包

您需要将您的包移动到当前目录下，在我们的例子中，我们使用 `kcl mod init` 命令创建包 helloworld

```shell
kcl mod init helloworld
```

您可以为 helloworld 包增加一个 README.md 文件保存在包的根目录下，用来展示在 AH 的首页中。

```shell
echo ""## Introduction"" >> helloworld/README.md
echo ""This is a kcl module named helloworld."" >> helloworld/README.md
```

#### 4. 提交您的包

您可以使用如下命令提交您的包

使用 `git add .` 命令将您的包添加到 git 的暂存区中

```shell
git add .
```

使用 `git commit -s` 命令提交您的包, 我们推荐您的 commit message 遵循  “publish module <module_name>” 的格式。

```shell
git commit -m ""publish module helloworld"" -s
```

使用 `git push` 命令将您的包提交到您的分支 publish-pkg-<module_name> 中

```shell
git push
```

#### 5. 提交 PR

将您的分支 publish-pkg-<module_name> 向仓库的 main 分支提交 PR。

- [如何创建 PR](https://docs.github.com/zh/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request)

### 通过 PR 升级您的包
完成包的内容上传后，您可以通过 PR 升级您的包。

注意：**我们没有提供任何改变包的内容但是不改变版本号的升级策略。** 如果您想要升级您的包，并希望您升级后的包被展示在 AH 上，您需要修改您的包的版本号。即在 kcl.mod 文件的 module 章节中的 version 字段。

```toml
[package]
name = ""my_module""
edition = ""*""
version = ""0.1.0"" # 改变这个字段来升级您的包
description = ""This is my module.""
```

同样，**您无法多次上传同一个版本号的 KCL 包**，一旦您的包的版本号已经被使用，您将无法再次使用这个版本号，再次上传这个包的方式就只有升级版本号。",VRAI
kdash-rs/kdash,Toolkit,Application System,2025-03-12T07:47:41Z,2024-07-30T10:15:20Z,0,0,0,0,0,0,0,0,2021-04-13T05:38:42Z,2025-04-06T21:38:18Z,9946,2229,Rust,VRAI,85,FAUX,40,"dashboard,hacktoberfest,k8s,kubernetes,monitoring,rust,tui",40,A simple and fast dashboard for Kubernetes,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,16,"# KDash - A fast and simple dashboard for Kubernetes

![ci](https://github.com/kdash-rs/kdash/actions/workflows/ci.yml/badge.svg)
![cd](https://github.com/kdash-rs/kdash/actions/workflows/cd.yml/badge.svg)
![License](https://img.shields.io/badge/license-MIT-blueviolet.svg)
![LOC](https://tokei.rs/b1/github/kdash-rs/kdash?category=code)
[![crates.io link](https://img.shields.io/crates/v/kdash.svg)](https://crates.io/crates/kdash)
![Docker Release](https://img.shields.io/docker/v/deepu105/kdash?label=Docker%20version)
![Release](https://img.shields.io/github/v/release/kdash-rs/kdash?color=%23c694ff)
[![Coverage](https://coveralls.io/repos/github/kdash-rs/kdash/badge.svg?branch=main)](https://coveralls.io/github/kdash-rs/kdash?branch=main)
[![GitHub Downloads](https://img.shields.io/github/downloads/kdash-rs/kdash/total.svg?label=GitHub%20downloads)](https://github.com/kdash-rs/kdash/releases)
![Docker pulls](https://img.shields.io/docker/pulls/deepu105/kdash?label=Docker%20downloads)
![Crate.io downloads](https://img.shields.io/crates/d/kdash?label=Crate%20downloads)

[![Follow Deepu K Sasidharan (deepu105)](https://img.shields.io/twitter/follow/deepu105?label=Follow%20Deepu%20K%20Sasidharan%20%28deepu105%29&style=social)](https://twitter.com/intent/follow?screen_name=deepu105)

![logo](artwork/logo.png)

A simple terminal dashboard for Kubernetes built with Rust [![Follow @kdashrs](https://img.shields.io/twitter/follow/kdashrs?label=Follow%20kdashrs&style=social)](https://twitter.com/intent/follow?screen_name=kdashrs)

![UI](screenshots/ui.gif)

## Sponsors

Thanks to the sponsors of [@deepu105](https://github.com/sponsors/deepu105) who makes maintaining projects like KDash sustainable. Consider [sponsoring](https://github.com/sponsors/deepu105) if you like the work.

<!-- ### Gold

### Silver

### Bronze

- [Robusta - Kubernetes monitoring](https://home.robusta.dev/)

Gold and Silver tiers are open for [Sponsors](https://github.com/sponsors/deepu105)  -->

## Installation

### Homebrew (Mac & Linux)

```bash
brew tap kdash-rs/kdash
brew install kdash

# If you need to be more specific, use:
brew install kdash-rs/kdash/kdash
```

To upgrade

```bash
brew upgrade kdash
```

### Scoop (Windows - Recommended way)

```bash
scoop bucket add kdash-bucket https://github.com/kdash-rs/scoop-kdash

scoop install kdash
```

### Chocolatey (Windows)

Chocolatey package is located [here](https://chocolatey.org/packages/kdash).
Since validation of the package takes forever, it may take a long while to become available after a release. I would recommend using Scoop instead for Windows.

```bash
choco install kdash

# Version number may be required for newer releases, if available:
choco install kdash --version=0.4.3
```

To upgrade

```bash
choco upgrade kdash --version=0.4.3
```

### Cargo

If you have Cargo installed then you install KDash from crates.io

```bash
cargo install kdash

# if you face issues with k8s-openapi crate try the below
cargo install --locked kdash
```

You can also clone the repo and run `cargo run` or `make` to build and run the app

### Nix (Maintained by third party)

Try out kdash via `nix run nixpkgs#kdash` or add `kdash` to your
`configuration.nix` for permanent installation.

### Install script

Run the below command to install the latest binary. Run with sudo if you don't have write access to `/usr/local/bin`. Else the script will install to the current directory

```sh
curl https://raw.githubusercontent.com/kdash-rs/kdash/main/deployment/getLatest.sh | bash
```

### Manual

Binaries for macOS (x86_64, arm64), Linux GNU/MUSL(x86_64, armv6, armv7, aarch64) and Windows (x86_64, aarch64) are available on the [releases](https://github.com/kdash-rs/kdash/releases) page

1. Download the latest [binary](https://github.com/kdash-rs/kdash/releases) for your OS.
1. For Linux/macOS:
   1. `cd` to the file you just downloaded and run `tar -C /usr/local/bin -xzf downloaded-file-name`. Use sudo if required.
   1. Run with `kdash`
1. For Windows:
   1. Use 7-Zip or TarTool to unpack the tar file.
   1. Run the executable file `kdash.exe`

### Docker

Run KDash as a Docker container by mounting your `KUBECONFIG`. For example the below command for the default path

```bash
docker run --rm -it -v ~/.kube/config:/root/.kube/config deepu105/kdash
# If you want localhost access from the container
docker run --network host --rm -it -v ~/.kube/config:/root/.kube/config deepu105/kdash
```

You can also clone this repo and run `make docker` to build a docker image locally and run it using the above command

## Troubleshooting

**Note**: This may not work properly if you run Kubernetes locally using Minikube or Kind

> Note: On Debian/Ubuntu you might need to install `libxcb-xfixes0-dev` and `libxcb-shape0-dev`. On Fedora `libxcb` and `libxcb-devel` would be needed.

> Note: On Linux you might need to have package `xorg-dev` (Debian/Ubuntu) or `xorg-x11-server-devel` (Fedora) or equivalent installed for the copy to clipboard features to work

> Note: If you are getting compilation error from openSSL. Make sure perl and perl-core are installed for your OS.

## USAGE:

```bash
kdash
```

Press `?` while running the app to see keybindings

## FLAGS:

- `-h, --help`: Prints help information
- `-V, --version`: Prints version information
- `-t, --tick-rate <tick-rate>`: Set the tick rate (milliseconds): the lower the number the higher the FPS.
- `-p, --poll-rate <poll-rate>`: Set the network call polling rate (milliseconds, should be multiples of tick-rate): the lower the number the higher the network calls.
- `-d, --debug[=<debug>]`: Enables debug mode and writes logs to `kdash-debug-<timestamp>.log` file in the current directory. Default behavior is to write INFO logs. Pass a log level to overwrite the default [possible values: info, debug, trace, warn, error]

## Limitations/Known issues

- **[Linux/Docker]** Copy to clipboard feature is OS/arch dependent and might crash in some Linux distros and is not supported on `aarch64` and `arm` machines.
- **[macOS]** KDash looks better on iTerm2 since macOS's default Terminal app makes the colors render weird.
- **[Windows]** KDash looks better on CMD since Powershell's default theme makes the colors look weird.
- **[Windows]** If using k3d for local clusters, set the server URL to 127.0.0.1 as 0.0.0.0 doesn't work with kube-rs. You can use `k3d cluster create --api-port 127.0.0.1:6550` or change the `cluster.server` value in your `.kube/config` for the k3d cluster to `127.0.0.1:<port>`.

## Features

- CLI info
- Node metrics
- Resource watch (configurable polling interval with `-p` flag)
- Custom resource definitions
- Describe resources & copy the output
- Get YAML for resources & copy the output
- Stream container logs
- Context
  - Context info
  - Context watch
  - Change namespace
  - Context switch
- Resources utilizations for nodes, pods and namespaces based on metrics server. Requires [metrics-server](https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server) to be deployed on the cluster.
- Dark/Light themes
- Sensible keyboard shortcuts
- Global glob filtering for resource names

## Screenshots

### Overview screen

![UI](screenshots/overview.png)

### Container logs screen (light theme)

![UI](screenshots/logs.png)

### Pod describe screen (light theme)

![UI](screenshots/describe.png)

### Contexts screen

![UI](screenshots/contexts.png)

### Utilization screen

![UI](screenshots/utilization.png)

## Libraries used

- [ratatui](https://github.com/ratatui-org/ratatui)
- [crossterm](https://github.com/crossterm-rs/crossterm)
- [clap](https://github.com/clap-rs/clap)
- [tokio](https://github.com/tokio-rs/tokio)
- [duct.rs](https://github.com/oconnor663/duct.rs)
- [kube-rs](https://github.com/clux/kube-rs)
- [serde](https://github.com/serde-rs/serde)
- [kubectl-view-allocations](https://github.com/davidB/kubectl-view-allocations)
- [copypasta](https://github.com/alacritty/copypasta)

## How does this compare to K9S?

[K9S](https://github.com/derailed/k9s) is a beast compared to this as it offers way more features including CRUD actions.

KDash only offers a view of the resources with a focus on speed and UX. Really, if something is slow or has bad UX then please raise a bug. Hence the UI/UX is designed to be more user-friendly and easier to navigate with contextual help everywhere and a tab system to switch between different resources easily.

At least for now, there are no plans to add full CRUD for resources but in the future, we might.

## Licence

MIT

## Creator

- [Deepu K Sasidharan](https://deepu.tech/)

## [Contributors](https://github.com/kdash-rs/kdash/graphs/contributors)",VRAI
killer-sh/cks-course-environment,Documentations,Documentations,2025-04-12T07:51:20Z,2022-11-18T14:40:46Z,0,3,0,0,0,0,0,5,2020-11-18T12:26:00Z,2025-03-22T16:39:08Z,163,428,Shell,FAUX,520,FAUX,3,,3,,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,11,"# Kubernetes CKS Course Environment

This is the repository of the [CKS FULL COURSE](https://youtu.be/d9xfB5qaOfg) on Youtube

There is also the [CKS SIMULATOR](https://killer.sh/cks)


## Course Resources

Many sections reference commands or links in their resources.

[RESOURCES](Resources.md)

## Killercoda Scenarios

Many topics have interactive in-browser Killercoda scenarios at the end. Solve these to test and harden your knowledge!

[SCENARIOS](Scenarios.md)

## Setup Cluster in Gcloud

### Setup cks-master

#### Create VM
```
1. create VM:
name: cks-master
family: e2-medium (2vCPU, 4GB)
image: ubuntu20.04 LTS focal
disk: 50GB
```

Like:
```
gcloud compute instances create cks-master --zone=europe-west3-c \
--machine-type=e2-medium \
--image=ubuntu-2004-focal-v20220419 \
--image-project=ubuntu-os-cloud \
--boot-disk-size=50GB
```

#### Configure
```
sudo -i
bash <(curl -s https://raw.githubusercontent.com/killer-sh/cks-course-environment/master/cluster-setup/latest/install_master.sh)
```

### Setup cks-worker

#### Create VM
```
1. create VM:
name: cks-worker
family: e2-medium (2vCPU, 4GB)
image: ubuntu20.04 LTS focal
disk: 50GB
```

Like:
```
gcloud compute instances create cks-worker --zone=europe-west3-c \
--machine-type=e2-medium \
--image=ubuntu-2004-focal-v20220419 \
--image-project=ubuntu-os-cloud \
--boot-disk-size=50GB
```

#### Configure
```
sudo -i
bash <(curl -s https://raw.githubusercontent.com/killer-sh/cks-course-environment/master/cluster-setup/latest/install_worker.sh)
```

### Connect to cluster
```
# install ""gcloud"" command

# connect ""gcloud"" to your GCP
gcloud auth login
gcloud projects list
gcloud config set project YOUR_PROJECT

# connect to instance
gcloud compute instances list
gcloud compute ssh cks-master
```

### Open ports
```
gcloud compute firewall-rules create nodeports --allow tcp:30000-40000
```",FAUX
komodorio/komoplane,Toolkit,Application System,2025-02-03T10:08:21Z,2023-10-20T11:02:41Z,0,0,0,0,1,0,0,0,2023-05-22T20:47:28Z,2025-04-03T13:37:24Z,724,307,TypeScript,VRAI,27,FAUX,19,"crossplane,gui,kubernetes,troubleshooting,ui",19,🍨 Crossplane Troubleshooting Tool by Komodor,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,13,"# <img src=""pkg/frontend/src/assets/logo_dark_text.svg"" style=""height: 3rem;"" alt=""komoplane""> 

[Komodor's](https://komodor.com) Crossplane Tool is a project to experiment with
visualizing [Crossplane](https://www.crossplane.io/) resources. The goal is to help Crossplane users to understand the
structure of their control plane resources and speed up troubleshooting.

<kbd>[<img src=""examples/screenshot1.png"" style=""width: 100%; border: 1px solid silver;"" border=""1"" alt=""Screenshot"">](examples/screenshot1.png)</kbd>

## Installation

The primary way of installing _komoplane_ is by installing the corresponding Helm chart:

```shell
helm repo add komodorio https://helm-charts.komodor.io \
  && helm repo update komodorio \
  && helm upgrade --install komoplane komodorio/komoplane
```

After installing, publish port `8090` from _komoplane_ pod and open it in your web browser.

By default, _komoplane_ works on port `8090`, you can change that via `extraArgs` Helm value.

### Running Without Installing

It is possible to run _komoplane_ locally as a binary process. To do so, download standalone binary
from [Releases](https://github.com/komodorio/komoplane/releases). Use `KUBECONTEXT` env variable to point to different context of your kubeconfig.

## Support & Community

We have two main channels for supporting the _komoplane_ users: 
[Slack community](https://komodorkommunity.slack.com) for general conversations (`#komoplane` channel)
and [GitHub issues](https://github.com/komodorio/komoplane/issues) for real bugs.

If you want to contribute some code to the project, consider looking at [roadmap](Roadmap.md) document with some of the ideas for improvements. Also, you may search for `TODO` and `FIXME` marks in the source code for smaller technical issues to solve. The [GitHub issues list](https://github.com/komodorio/komoplane/issues) might also have some items for potential contribution.

[Contributing doc](Contributing.md) contains instructions on how to setup dev environment.

## More Kubernetes Tooling

The _komoplane_ is an Open Source project founded by [Komodor](https://komodor.com). There are some more k8s-related tools by Komodor for you:
 - [Helm Dashboard](https://github.com/komodorio/helm-dashboard) - a GUI-based operations with Helm charts
 - [validkube](https://validkube.com/) - a service to validate Kubernetes manifests for security and best practices
 - [Kubernetes Operations Platform](https://app.komodor.com/#mode=signUp) - freemium all-in-one platform to operate k8s clusters with builtin monitoring & costs analysis",VRAI
kondukto-io/kntrl,DevOPs,DevOPs,2025-05-13T08:06:24Z,2025-03-14T07:33:43Z,0,11,0,0,0,0,0,0,2024-01-17T21:41:06Z,2025-04-08T04:09:43Z,6610,73,C,VRAI,5,FAUX,3,,3,"kntrl is an eBPF based runtime agent that monitors and prevents anomalous behaviour defined by you on your pipeline. kntrl achieves this by monitoring kernel calls, and denying access as soon as your defined behaviour is detected. For more: https://kntrl.dev",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,4,"![kntrl logo](./docs/img/kntrl_logo.png) <!-- markdownlint-disable-line first-line-heading -->

`kntrl` is an eBPF based runtime agent that monitors and prevents anomalous behaviour defined by you on your pipeline. kntrl achieves this by monitoring kernel calls, and denying access as soon as your defined behaviour is detected. Refer to this [presentation](https://docs.google.com/presentation/d/1nmbqGfIxp9UyxlfT5EJyQsEWtQaXVoWD9Qjj1MJevuk/edit?usp=sharing) to dive deeper into how we are achieving what kntrl does.

It can work as a single binary (`kntrl`) or with a docker runner (`docker.io/kondukto/kntrl:0.1.2`).

## Installation
### Linux 
`kntrl` is available as downloadable binaries from the releases page. Download the pre-compiled binary from the `releases` page and copy to the desired location. 


### Container Images
We provide ready to use Docker container images. To pull the latest image:
```
docker pull kondukto/kntrl:latest
```

To pull a specific version:
```
docker pull kondukto/kntrl:0.1.2
```

## Using kntrl

You can start using kntrl agent by simply running the following command:

```yaml
- name: kntrl agent
  run: sudo ./kntrl run --mode=monitor --allowed-hosts=download.kondukto.io,${{ env.GITHUB_ACTIONS_URL }} --allowed-ips=10.0.2.3  
```

OR with the docker:

```yaml
- name: kntrl agent
  run: sudo docker run --privileged \
    --pid=host \
    --network=host \
    --cgroupns=host \
    --volume=/sys/kernel/debug:/sys/kernel/debug:ro \
    --volume /tmp:/tmp \
    --rm docker.io/kondukto/kntrl:0.1.2 run --mode=trace --allowed-hosts=kondukto.io,download.kondukto.io 
```

This action will deploy kntrl into any GitHub Actions build.

## Usage
The `kntrl` agent is self explanatory and it comes with a help command. Simply run `--help` flag after each command/subcommand.

```
 ./kntrl --help
Runtime security tool to control and monitor egress/ingress traffic in CI/CD runners

Usage:
  tracer [command]

Available Commands:
  completion  Generate the autocompletion script for the specified shell
  help        Help about any command
  run         Starts the TCP/UDP tracer

Flags:
  -h, --help      help for tracer
  -v, --verbose   more logs

Use ""tracer [command] --help"" for more information about a command.
```

The agent supports the following parameters:

| Name                     | Default               | Description                                                                                                                                                                                                                                                                                                                                                               |
| ------------------------ | --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `mode`                   |   monitor                    | kntrl for detected behaviours (monitor or prevent/trace)                                                                                                                                                                                                                                                                                                                  |
| `allowed-hosts`                  |                       | allowed host list. (example.com, .github.com)                                                                                                                                                                                                                                                                                                                                                         |
| `allowed-ips`                  |                       | allowed IP list. (192.168.0.100, 1.1.1.1)                                                                                                                                                                                                                                                                                                                                                         |
| `allow-local-ranges`                  |  true              | allow access to local IP ranges                                                                                                                                                                                                                                                                                                                               |
| `allow-github-meta`                  |  false              | allow access to GitHub meta IP ranges (https://api.github.com/meta)                                                                                                                                                                                                                                                                                                                               |
| `output-file`                  | `/tmp/kntrl.out`                       | report file |                                                                                                                                                                                                                                     |

### Running kntrl on monitoring mode

```yaml
- name: kntrl agent
  run: sudo docker run --privileged \
  --pid=host \
  --network=host \
  --cgroupns=host \
  --volume=/sys/kernel/debug:/sys/kernel/debug:ro \
  --volume /tmp:/tmp \
  --rm docker.io/kondukto/kntrl:0.1.2 \
  --mode=monitor 
```

### Running kntrl on prevent mode

```yaml
- name: kntrl agent
  run: sudo docker run --privileged \
  --pid=host \
  --network=host \
  --cgroupns=host \
  --volume=/sys/kernel/debug:/sys/kernel/debug:ro \
  --volume /tmp:/tmp \
  --rm docker.io/kondukto/kntrl:0.1.2 \
  --mode=trace --allowed-hosts=download.kondukto.io, .github.com  
```

## Open Policy Agent (OPA) Rules
`kntrl` supports an OPA-based policy engine to determine whether the event should be blocked or not. All the policy rules are stored under the bundle/kntrl/ directory.

An example rego rule:
```
package kntrl.network[""is_local_ip_addr""]

import rego.v1

policy if {
        ipaddr := input.daddr
        local_ranges := [""192.168.0.0/16"", ""172.16.0.0/12"", ""10.0.0.0/8"", ""0.0.0.0/32""]
        net.cidr_contains(local_ranges[_], ipaddr)
        data.allow_local_ip_ranges == true
}
```

## Reporting

Each event will be logged in the output file. The default report file location is `/tmp/kntrl.out`.

Here is an example report:
```
{
  ""pid"": 2806,
  ""task_name"": ""curl"",
  ""proto"": ""tcp"",
  ""daddr"": ""140.82.114.22"",
  ""dport"": 443,
  ""domains"": [
    ""lb-140-82-114-22-iad.github.com.""
  ],
  ""policy"": ""pass""
}
{
  ""pid"": 2806,
  ""task_name"": ""curl"",
  ""proto"": ""tcp"",
  ""daddr"": ""142.251.167.95"",
  ""dport"": 443,
  ""domains"": [
    ""ww-in-f95.1e100.net.""
  ],
  ""policy"": ""block""
}
{
  ""pid"": 2806,
  ""task_name"": ""curl"",
  ""proto"": ""udp"",
  ""daddr"": ""127.0.0.1"",
  ""dport"": 53,
  ""domains"": [
    ""localhost""
  ],
  ""policy"": ""pass""
}
```

or 

```
Pid  | Comm    | Proto | Domain                          | Destination Addr   | Policy
------------------------------------------------------------------------------------
2806 | curl    | tcp   | lb-140-82-114-22-iad.github.com | 140.82.114.22:443  | pass
------------------------------------------------------------------------------------
2806 | curl    | tcp   | ww-in-f95.1e100.net             | 142.251.167.95:443 | block
------------------------------------------------------------------------------------
2806 | curl    | udp   | localhost                       | 127.0.0.1:53       | pass
------------------------------------------------------------------------------------
```

## Contribution

Contributions to kntrl are welcome.
Feel free to join our slack channel [https://kntrl.slack.com](https://kntrl.slack.com)

## License
Except for the eBPF code, all components are distributed under the [Apache License (version 2.0)](./LICENSE.md).

## More about Kondukto
`kntrl` is an open source project maintained by [Kondukto](https://kondukto.io).",FAUX
konfig-dev/konfig,Toolkit,Toolkit,2024-12-25T00:33:54Z,2024-09-15T02:31:05Z,0,0,0,0,0,0,0,0,2022-11-12T09:39:54Z,2025-04-07T03:40:26Z,555560,1531,TypeScript,VRAI,416,FAUX,7,"api,documentation,sdks",7,SDK & API Docs Generator. Sunset as of December 2024,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,9,"# [Konfig](https://konfigthis.com)

The monorepo that holds everything...
## Getting started

Get the repository on your local machine. **Takes a minute.**

```shell
git clone https://github.com/konfig-dev/konfig --recursive
cd konfig
```

This repository has submodules so pull all of them. **Also take a few minutes.**

```shell
git submodule update --init --recursive --remote --merge
```

## Environment Setup

1. Run Postgres as a background process

   ```shell
   # in /konfig
   brew install postgresql
   mkdir -p postgres/data
   initdb -D ./postgres/data
   pg_ctl -D ./postgres/data start
   ```

1. Setup `.env` file in `generator/konfig-dash` to something like:

   ```
   DATABASE_URL=""postgresql://dylanhuang@localhost:5432/konfig_dev?connection_limit=1""
   TEST_DATABASE_URL=""postgresql://dylanhuang@localhost:5432/konfig_test?connection_limit=1""

   AWS_ACCESS_KEY_ID=XXXXXX
   AWS_SECRET_ACCESS_KEY=XXXXXX

   # Used to encrypt/decrypt session cookies. Change this value and re-deploy to log out all users of your app at once.
   SESSION_SECRET=ZUWpQ9pB4fB5FFpjHLi8Z2qadzXkdTKhHBsXmGmjNdxtrZbevaCYWSpw7G7cHBhh
   ```

Paste this into your `~/.zshrc` or `~/.bashrc`

```bash
if [ -f $HOME/.envvars ]; then
    . $HOME/.envvars
else
    print ""404: ~/.envvars not found.""
fi
```

Then create `~/.envvars` with values from Dylan.

## How to run Konfig

1. Make sure `node_modules` is initiated in `konfig-dash`
   ```shell
   cd generator/konfig-dash
   yarn # takes some time
   yarn rw prisma migrate dev # setup the DB
   ```
1. Start the server with `yarn dev`

   ```shell
   # inside generator/konfig-dash
   yarn dev
   ```

1. Start `generator/konfig-generator-api` w/ IntelliJ
1. `cd` into an SDK repo and run `konfig generate -d`

## Making Changes

See [Changesets](https://github.com/changesets/changesets)",FAUX
konflux-ci/konflux-test,DevOPs,Database,2025-05-14T02:40:03Z,2025-04-23T00:24:51Z,0,19,0,0,0,0,0,0,2022-03-15T11:31:24Z,2025-04-03T18:46:15Z,590,6,Shell,VRAI,26,FAUX,13,,13,Konflux-test repository,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,29,"# Konflux-test
This is Konflux-test repository for building the image of the same name.
Purpose of this repo is to store resources, currently for tekton tasks and pipelines.

## Prerequisites

Install the following list of packages to run the policies locally.

  1. [Conftest](https://www.conftest.dev/install/)
  2. [jq](https://snapcraft.io/jq)
  3. [Skopeo](https://github.com/containers/skopeo/blob/main/install.md)

## Policy Unit Testing

In addition to [prerequisites](https://github.com/konflux-ci/konflux-test#prerequisites) install the packages below to run unit testing.

  1. [OPA](https://www.openpolicyagent.org/docs/latest/#running-opa)

Running command `opa test <path> [path [...]]` executes unit testing for policy, `path` points to the policy folder and unit testing fixtures data folder.

Running policy unit testing with code coverage report `opa test <path> [path [...]] -c`

Running policy unit testing and exit with non-zero status if coverage is less than threshold % `policy test <path> [path [...]] --threshold float`

In this repository, we run `opa test policies unittests unittests/test_data -c > /tmp/policy_unittest_result.json` to get unit testing code coverage and then analyze.

## Policy Integration Testing

In addition to [prerequisites](https://github.com/konflux-ci/konflux-test#prerequisites) install the packages below to run integration testing.

  1. [BATS](https://github.com/bats-core/bats-core/releases)

Run Integration tests locally by export the policies directory path to `policies` folder as shown below.

`export POLICY_PATH=policies`

`sh test/entrypoint.sh`",VRAI
kro-run/kro,DevOPs,DevOPs,2025-05-14T20:11:35Z,2025-04-22T13:37:55Z,0,0,0,0,2,0,0,0,2024-09-12T17:34:32Z,2025-04-08T06:57:24Z,9300,1669,Go,VRAI,171,FAUX,98,,98,kro | Kube Resource Orchestrator,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,59,"# kro | Kube Resource Orchestrator


[![Go Report Card](https://goreportcard.com/badge/github.com/kro-run/kro)](https://goreportcard.com/report/github.com/kro-run/kro)
[![unit tests](https://github.com/kro-run/kro/actions/workflows/unit-tests.yaml/badge.svg)](https://github.com/kro-run/kro/actions/workflows/unit-tests.yaml)
![GitHub go.mod Go version](https://img.shields.io/github/go-mod/go-version/kro-run/Kro)
![GitHub License](https://img.shields.io/github/license/kro-run/kro)
[![Build and Publish](https://github.com/kro-run/kro/actions/workflows/build-push-image.yaml/badge.svg?branch=main)](https://github.com/kro-run/kro/actions/workflows/build-push-image.yaml)
![GitHub Repo stars](https://img.shields.io/github/stars/kro-run/kro)

This project aims to simplify the creation and management of complex custom resources for Kubernetes.

Kube Resource Orchestrator (**kro**) helps you to define complex multi-resource constructs as reusable components in your applications and systems. It does this by providing a Kubernetes-native, vendor agnostic way to define groupings of Kubernetes resources. 

kro's fundamental custom resource is the *ResourceGraphDefinition*. A ResourceGraphDefinition defines collections of underlying Kubernetes resources. It can define any Kubernetes resources, either native or custom, and can specify the dependencies between them. This lets you define complex custom resources, and include default configurations for their use.

The kro controller will determine the dependencies between resources, establish the correct order of operations to create and configure them, and then dynamically create and manage all of the underlying resources for you.

kro is Kubernetes native and integrates seamlessly with existing tools to preserve familiar processes and interfaces.

## Documentation

| Title                                  | Description                     |
| -------------------------------------- | ------------------------------- |
| [Introduction][kro-overview]           | An introduction to kro          |
| [Installation][kro-installation]       | Install kro in your cluster     |
| [Getting started][kro-getting-started] | Deploy your first ResourceGraphDefinition |
| [Concepts][kro-concepts]               | Learn more about kro concepts   |
| [Examples][kro-examples]               | Example resources               |
| [Contributions](./CONTRIBUTING.md)       | How to get involved             |

[kro-overview]: https://kro.run/docs/overview
[kro-installation]: https://kro.run/docs/getting-started/Installation
[kro-getting-started]: https://kro.run/docs/getting-started/deploy-a-resource-graph-definition
[kro-concepts]: https://kro.run/docs/concepts/resource-group-definitions
[kro-examples]: https://kro.run/examples/

## FAQ

1. **What is kro?**

    Kube Resource Orchestrator (**kro**) is a new operator for Kubernetes that simplifies the creation of complex Kubernetes resource configurations.
    kro lets you create and manage custom groups of Kubernetes resources by defining them as a *ResourceGraphDefinition*, the project's fundamental custom resource.
    ResourceGraphDefinition specifications define a set of resources and how they relate to each other functionally.
    Once defined, ResourceGraphDefinitions can be applied to a Kubernetes cluster where the kro controller is running.
    Once validated by kro, you can create instances of your ResourceGraphDefinition.
    kro translates your ResourceGraphDefinition instance and its parameters into specific Kubernetes resources and configurations which it then manages for you.

2. **How does kro work?**

    kro is designed to use core Kubernetes primitives to make resource grouping, customization, and dependency management simpler.
    When a ResourceGraphDefinition is applied to the cluster, the kro controller verifies its specification, then dynamically creates a new CRD and registers it with the API server.
    kro then deploys a dedicated controller to respond to instance events on the CRD. This microcontroller is responsible for managing the lifecycle of resources defined in the ResourceGraphDefinition for each instance that is created.

3. **How do I use kro?**

    To create your custom resource groupings, you create *ResourceGraphDefinition* specifications. These specify one or more Kubernetes resources, and can include specific configurations for each resource.

    For example, you can define a *WebApp* ResourceGraphDefinition that defines a *WebAppCRD* CRD that is composed of a *Deployment*, pre-configured to deploy your web server backend, and a *Service* configured to run on a specific port.
    You can just as easily create a more complex *WebAppWithDB* ResourceGraphDefinition by combining the existing *WebApp* ResourceGraphDefinition with a *Table* custom resource to provision a cloud managed database instance for your web app to use.

    Once you have defined a ResourceGraphDefinition, you can apply it to a Kubernetes cluster where the kro controller is running. kro will take care of the heavy lifting of creating CRDs and deploying dedicated controllers in order to manage instances of your new custom resource group.

    To create an instance of your custom resource groupings, you create an instance of the CRD that your ResourceGraphDefinition specifies. In the WebApp ResourceGraphDefinition example, this would be an instance of the *WebAppCRD* CRD. kro will respond by dynamically creating, configuring, and managing the underlying Kubernetes resources for you. 

4. **Why did you build this project?**

    We want to help streamline and simplify building with Kubernetes. Building with Kubernetes means dealing with resources that need to operate and work together, and orchestrating this can get complex and difficult at scale.
   With this project, we're taking a step in reducing the complexity of resource dependency management and customization, paving the way for a simple and scalable way to create complex custom resources for Kubernetes.

5. **Can I use this in production?**

   This project is in active development and not yet intended for production use.
   The *ResourceGraphDefinition* CRD and other APIs used in this project are not yet solidified and highly subject to change.

## Community Participation

Development and discussion is coordinated in the [Kubernetes Slack (invite link)][k8s-slack] channel [#kro][kro-channel] channel.

[k8s-slack]: https://communityinviter.com/apps/kubernetes/community
[kro-channel]: https://kubernetes.slack.com/archives/C081TMY9D6Y

Please join our community meeting.
* Every other Wednesday at 9AM PT (Pacific Time). [Convert to local timezone][tz-help] 
* Agenda: [Public doc][agenda]
* Join us: [Zoom meeting][zoom]
* Community meeting recordings:  [YouTube channel][youtube]


[tz-help]: http://www.thetimezoneconverter.com/?t=9%3A00&tz=PT%20%28Pacific%20Time%29
[agenda]: https://docs.google.com/document/d/1GqeHcBlOw6ozo-qS4TLdXSi5qUn88QU6dwdq0GvxRz4/edit?tab=t.0
[zoom]: https://us06web.zoom.us/j/85388697226?pwd=9Xxz1F0FcNUq8zFGrsRqkHMhFZTpuj.1
[youtube]: https://www.youtube.com/channel/UCUlcI3NYq9ehl5wsdfbJzSA

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

[Apache 2.0](LICENSE)",FAUX
kubearmor/KubeArmor,Toolkit,DevOPs,2025-05-15T14:13:15Z,2025-05-06T06:10:10Z,0,0,0,0,26,0,0,0,2020-11-26T01:59:16Z,2025-04-08T06:24:39Z,61314,1702,Go,VRAI,367,FAUX,256,"bpf,containers,ebpf,hacktoberfest,kernel,kubernetes,lsm,policy,sandbox,security,system,tool",256,"Runtime Security Enforcement System. Workload hardening/sandboxing and implementing least-permissive policies made easy leveraging LSMs (BPF-LSM, AppArmor).",FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,109,"![](.gitbook/assets/logo.png)

[![Build Status](https://github.com/kubearmor/KubeArmor/actions/workflows/ci-go.yml/badge.svg)](https://github.com/kubearmor/KubeArmor/actions/workflows/ci-go.yml/)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5401/badge)](https://bestpractices.coreinfrastructure.org/projects/5401)
[![CLOMonitor](https://img.shields.io/endpoint?url=https://clomonitor.io/api/projects/cncf/kubearmor/badge)](https://clomonitor.io/projects/cncf/kubearmor)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/kubearmor/kubearmor/badge)](https://securityscorecards.dev/viewer/?uri=github.com/kubearmor/KubeArmor)
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fkubearmor%2FKubeArmor.svg?type=shield&issueType=license)](https://app.fossa.com/projects/git%2Bgithub.com%2Fkubearmor%2FKubeArmor?ref=badge_shield)
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fkubearmor%2FKubeArmor.svg?type=shield&issueType=security)](https://app.fossa.com/projects/git%2Bgithub.com%2Fkubearmor%2FKubeArmor?ref=badge_shield)
[![Slack](https://img.shields.io/badge/Join%20Our%20Community-Slack-blue)](https://cloud-native.slack.com/archives/C02R319HVL3)
[![Discussions](https://img.shields.io/badge/Got%20Questions%3F-Chat-Violet)](https://github.com/kubearmor/KubeArmor/discussions)
[![Docker Downloads](https://img.shields.io/docker/pulls/kubearmor/kubearmor)](https://hub.docker.com/r/kubearmor/kubearmor)
[![ArtifactHub](https://img.shields.io/badge/ArtifactHub-KubeArmor-blue?logo=artifacthub&labelColor=grey&color=green)](https://artifacthub.io/packages/search?kind=19)

KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior \(such as process execution, file access, and networking operations\) of pods, containers, and nodes (VMs) at the system level.

KubeArmor leverages [Linux security modules \(LSMs\)](https://en.wikipedia.org/wiki/Linux_Security_Modules) such as [AppArmor](https://en.wikipedia.org/wiki/AppArmor), [SELinux](https://en.wikipedia.org/wiki/Security-Enhanced_Linux), or [BPF-LSM](https://docs.kernel.org/bpf/prog_lsm.html) to enforce the user-specified policies. KubeArmor generates rich alerts/telemetry events with container/pod/namespace identities by leveraging eBPF.

|  |   |
|:---|:---|
| :muscle: **[Harden Infrastructure](getting-started/hardening_guide.md)** <hr>:chains: Protect critical paths such as cert bundles <br>:clipboard: MITRE, STIGs, CIS based rules <br>:left_luggage: Restrict access to raw DB table | :ring: **[Least Permissive Access](getting-started/least_permissive_access.md)** <hr>:traffic_light: Process Whitelisting <br>:traffic_light: Network Whitelisting <br>:control_knobs: Control access to sensitive assets |
| :telescope: **[Application Behavior](getting-started/workload_visibility.md)** <hr>:dna: Process execs, File System accesses <br>:compass: Service binds, Ingress, Egress connections <br>:microscope: Sensitive system call profiling | :snowflake: **[Deployment Models](getting-started/deployment_models.md)** <hr>:wheel_of_dharma: Kubernetes Deployment<br>:whale2: Containerized Deployment<br>:computer: VM/Bare-Metal Deployment |

## Architecture Overview

![KubeArmor High Level Design](.gitbook/assets/kubearmor_overview.png)

## Documentation :notebook:

* :point_right: [Getting Started](getting-started/deployment_guide.md)
* :dart: [Use Cases](getting-started/use-cases/hardening.md)
* :heavy_check_mark: [KubeArmor Support Matrix](getting-started/support_matrix.md)
* :chess_pawn: [How is KubeArmor different?](getting-started/differentiation.md)
* :scroll: Security Policy for Pods/Containers [[Spec](getting-started/security_policy_specification.md)] [[Examples](getting-started/security_policy_examples.md)]
* :scroll: Cluster level security Policy for Pods/Containers [[Spec](getting-started/cluster_security_policy_specification.md)] [[Examples](getting-started/cluster_security_policy_examples.md)]
* :scroll: Security Policy for Hosts/Nodes [[Spec](getting-started/host_security_policy_specification.md)] [[Examples](getting-started/host_security_policy_examples.md)]<br>
... [detailed documentation](https://docs.kubearmor.io/kubearmor/)

### Contributors :busts_in_silhouette:

* :blue_book: [Contribution Guide](contribution/contribution_guide.md)
* :technologist: [Development Guide](contribution/development_guide.md), [Testing Guide](contribution/testing_guide.md)
* :raised_hand: [Join KubeArmor Slack](https://cloud-native.slack.com/archives/C02R319HVL3)
* :question: [FAQs](getting-started/FAQ.md)

### Biweekly Meeting

- :speaking_head: [Zoom Link](http://zoom.kubearmor.io)
- :page_facing_up: Minutes: [Document](https://docs.google.com/document/d/1IqIIG9Vz-PYpbUwrH0u99KYEM1mtnYe6BHrson4NqEs/edit)
- :calendar: Calendar invite: [Google Calendar](http://www.google.com/calendar/event?action=TEMPLATE&dates=20220210T150000Z%2F20220210T153000Z&text=KubeArmor%20Community%20Call&location=&details=%3Ca%20href%3D%22https%3A%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1IqIIG9Vz-PYpbUwrH0u99KYEM1mtnYe6BHrson4NqEs%2Fedit%22%3EMinutes%20of%20Meeting%3C%2Fa%3E%0A%0A%3Ca%20href%3D%22%20http%3A%2F%2Fzoom.kubearmor.io%22%3EZoom%20Link%3C%2Fa%3E&recur=RRULE:FREQ=WEEKLY;INTERVAL=2;BYDAY=TH&ctz=Asia/Calcutta), [ICS file](getting-started/resources/KubeArmorMeetup.ics)

## Notice/Credits :handshake:

- KubeArmor uses [Tracee](https://github.com/aquasecurity/tracee/)'s system call utility functions.

## CNCF

KubeArmor is [Sandbox Project](https://www.cncf.io/projects/kubearmor/) of the Cloud Native Computing Foundation.
![CNCF SandBox Project](.gitbook/assets/cncf-sandbox.png)

## ROADMAP

KubeArmor roadmap is tracked via [KubeArmor Projects](https://github.com/orgs/kubearmor/projects?query=is%3Aopen)",VRAI
kubermatic/kubermatic,DevOPs,DevOPs,2025-05-15T11:22:11Z,2025-05-05T12:23:56Z,0,0,0,0,0,0,0,5,2016-01-30T17:05:09Z,2025-04-08T10:52:17Z,172638,1138,Go,VRAI,170,FAUX,356,"cluster-api,kubermatic-kubernetes-platform,kubernetes",356,Kubermatic Kubernetes Platform - the Central Kubernetes Management Platform For Any Infrastructure ,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,145,"<p align=""center"">
  <img src=""docs/kkp-logo.png#gh-light-mode-only"" width=""700px"" />
  <img src=""docs/kkp-logo-dark.png#gh-dark-mode-only"" width=""700px"" />
</p>

<p align=""center"">
  <img src=""https://img.shields.io/github/v/release/kubermatic/kubermatic"" alt=""last stable release"">

  <a href=""https://goreportcard.com/report/k8c.io/kubermatic/v2"">
    <img src=""https://goreportcard.com/badge/k8c.io/kubermatic/v2"" alt=""go report card"">
  </a>

  <a href=""https://pkg.go.dev/k8c.io/kubermatic/v2"">
    <img src=""https://pkg.go.dev/badge/k8c.io/kubermatic/v2"" alt=""godoc"">
  </a>
</p>

## Overview / User Guides

Kubermatic Kubernetes Platform is in an open source project to centrally manage the global automation of thousands of Kubernetes clusters across multicloud, on-prem and edge with unparalleled density and resilience.

All user documentation is available at the [Kubermatic Kubernetes Platform docs website][21].

## Editions

There are two editions of Kubermatic Kubernetes Platform:

Kubermatic Kubernetes Platform Community Edition (CE) is available freely under the Apache License, Version 2.0.
Kubermatic Kubernetes Platform Enterprise Edition (EE) includes premium features that are most useful for organizations with large-scale Kubernetes installations with more than 50 clusters. To access the Enterprise Edition and get official support please become a subscriber.

## Licensing

See the [LICENSE](LICENSE) file for licensing information as it pertains to files in this repository.

## Installation

We strongly recommend that you use an official release of Kubermatic Kubernetes Platform. Follow the instructions under the **Installation** section of [our documentation][21] to get started.

_The code and sample YAML files in the main branch of the kubermatic repository are under active development and are not guaranteed to be stable. Use them at your own risk!_

## More information

[The documentation][21] provides a getting started guide, plus information about building from source, architecture, extending kubermatic, and more.

Please use the version selector at the top of the site to ensure you are using the appropriate documentation for your version of kubermatic.

## Troubleshooting

If you encounter issues [file an issue][1] or talk to us on the [#kubermatic channel][12] on the [Kubermatic Community Slack][15] ([click here to join][16]).

## Contributing

Thanks for taking the time to join our community and start contributing!

### Before you start

* Please familiarize yourself with the [Code of Conduct][4] before contributing.
* See [CONTRIBUTING.md][2] for instructions on the developer certificate of origin that we require.

### Repository layout

```
├── addons    # Default Kubernetes addons
├── charts    # The Helm charts we use to deploy
├── cmd       # Various Kubermatic binaries for the controller-managers, operator etc.
├── codegen   # Helper programs to generate Go code and Helm charts
├── docs      # Some basic developer-oriented documentation
├── hack      # scripts for development and CI
└── pkg       # most of the actual codebase
```

### Development environment

```bash
git clone git@github.com:kubermatic/kubermatic.git
cd kubermatic
```

There are a couple of scripts in the `hacks` directory to aid in running the components locally
for testing purposes.

#### Running components locally

##### user-cluster-controller-manager

In order to instrument the seed-controller to allow for a local user-cluster-controller-manager, you need to add a `worker-name` label with your local machine's name as its value. Additionally, you need to scale down the already running deployment.

```sh
# Using a kubeconfig, which points to the seed-cluster
export cluster_id=""<id-of-your-user-cluster>""
kubectl label cluster ${cluster_id} worker-name=$(uname -n)
kubectl scale deployment -n cluster-${cluster_id} usercluster-controller --replicas=0
```

Afterwards, you can start your local user-cluster-controller-manager.

```sh
# Using a kubeconfig, which points to the seed-cluster
./hack/run-user-cluster-controller-manager.sh
```

##### seed-controller-manager

```bash
./hack/run-seed-controller-manager.sh
```

##### master-controller-manager

```bash
./hack/run-master-controller-manager.sh
```

#### Run linters

Before every push, make sure you run:

```bash
make lint
```
#### Run tests

```bash
make test
```

#### Update code generation

The Kubernetes code-generator tool does not work outside of `GOPATH`
([upstream issue](https://github.com/kubernetes/kubernetes/issues/86753)), so the script
below will automatically run the code generation in a Docker container.

```bash
hack/update-codegen.sh
```

### Pull requests

* We welcome pull requests. Feel free to dig through the [issues][1] and jump in.

## Changelog

See [the list of releases][3] to find out about feature changes.

[1]: https://github.com/kubermatic/kubermatic/issues
[2]: https://github.com/kubermatic/kubermatic/blob/main/CONTRIBUTING.md
[3]: https://github.com/kubermatic/kubermatic/releases
[4]: https://github.com/kubermatic/kubermatic/blob/main/CODE_OF_CONDUCT.md

[12]: https://kubermatic-community.slack.com/messages/kubermatic
[15]: http://kubermatic-community.slack.com
[16]: https://join.slack.com/t/kubermatic-community/shared_invite/zt-vqjjqnza-dDw8BuUm3HvD4VGrVQ_ptw

[21]: https://docs.kubermatic.com/kubermatic/",FAUX
kubernetes-sigs/wg-policy-prototypes,Application System,Documentations,2025-01-13T02:44:31Z,2023-08-12T09:30:28Z,0,0,0,0,8,0,0,0,2020-04-02T10:09:05Z,2025-04-04T04:20:09Z,42312,67,Go,VRAI,39,FAUX,1,k8s-sig-auth,1,A place for policy work group related proposals and prototypes.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,26,"# Policy Prototypes

A place for policy work group related proposals and prototypes.

:warning: **Warning: Code and other artifacts in this repository are prototypes and proposals, work-in-progress, not endorsed by any Kubernetes SIG, and not recommended for production use.**

## Community, discussion, contribution, and support

Learn how to engage with the Kubernetes community on the [community page](http://kubernetes.io/community/).

You can reach the maintainers of this project at:

- [Slack](https://kubernetes.slack.com/messages/wg-policy)
- [Mailing List](https://groups.google.com/forum/#!forum/kubernetes-wg-policy)

## Join this repo

File a request at https://github.com/kubernetes/org to be added to @kubernetes-sigs, using the Template.

Once you've been a member, when you are ready to become a reviewer of other people's code, file a PR on our OWNERS file and an approver will need to approve you.

Once you've been a reviewer, you can request to become an approver by filing a PR on our OWNERS file and another approver will need to approve you.

## Code of conduct

Participation in the Kubernetes community is governed by the [Kubernetes Code of Conduct](code-of-conduct.md).

## Projects - also see [Project Board](https://github.com/kubernetes-sigs/wg-policy-prototypes/projects/1)

* [Policy Report CRD](policy-report/README.md)
* [Kubernetes Policy Management Paper](https://github.com/kubernetes/sig-security/tree/main/sig-security-docs/papers/policy) - [PDF](https://github.com/kubernetes/sig-security/blob/main/sig-security-docs/papers/policy/CNCF_Kubernetes_Policy_Management_WhitePaper_v1.pdf), [markdown](https://github.com/kubernetes/sig-security/blob/main/sig-security-docs/papers/policy/kubernetes-policy-management.md)

### Backlog or Retired

* [Guardian](guardian/README.md) - Formal verification of policy

## Additional Information

* [Policy WG Community Page](https://github.com/kubernetes/community/tree/master/wg-policy)",VRAI
kubescape/regolibrary,Toolkit,Toolkit,2025-04-14T12:13:33Z,2024-11-26T09:43:42Z,0,346,0,0,0,0,0,0,2021-09-01T05:54:13Z,2025-04-07T19:25:43Z,7658,124,Open Policy Agent,VRAI,51,FAUX,15,"compliance,kubernetes,kubescape,opa,security",15,The regolibrary package contains the controls Kubescape uses for detecting misconfigurations in Kubernetes manifests.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,41,"<!-- markdown-link-check-disable -->
[![Version](https://img.shields.io/github/v/release/kubescape/regolibrary)](releases)
[![release-date](https://img.shields.io/github/release-date/kubescape/regolibrary)](releases)
<!-- markdown-link-check-enable-->
[![GitHub](https://img.shields.io/github/license/kubescape/kubescape)](https://github.com/kubescape/kubescape/blob/master/LICENSE)
<!-- markdown-link-check-enable-->
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/kubescape/regolibrary/badge)](https://securityscorecards.dev/viewer/?uri=github.com/kubescape/regolibrary)

# Kubescape Regolibrary

This repository contains a library of security controls that codify Kubernetes best practices derived from the most prevalent security frameworks in the industry. [Kubescape](https://github.com/kubescape/kubescape) uses these controls to scan again running clusters or manifest files under development. They’re written in Rego, the purpose-built declarative policy language that supports Open Policy Agent (OPA).


## Terminology

- **Framework** - a group of controls to test against

- **Control** - a potential vulnerability to check, can include multiple rules

- **Rule** - a single specific test

## Contributing

### Add a framework

Add `frameworkName.json` file in the `/frameworks` directory

Example of a framework:
```json
{
    ""name"": ""DevOpsBest"",
    ""description"": ""This framework is recommended for use by devops."",
    ""attributes"": {
    },
    ""scanningScope"": {
        ""matches"": [
            ""cluster"",
            ""file""
        ]
    },
    ""controlsNames"": [
        ""Naked pods"",
        ""Container runtime socket mounted"",
        ""Image pull policy on latest tag"",
        ""Label usage for resources"",
        ""K8s common labels usage"",
        ""Pods in default namespace"",
        ""Container hostPort"",
        ""Resources CPU limit and request"",
        ""Resources memory limit and request"",
        ""Configured liveness probe"",
        ""Configured readiness probe""
    ]
}
```
* controlNames - List of controls to run, must be exact name. Use copy-paste to be sure.
* `scanningScope` - this framework will run just if kubescape scan process match to the scope in the list.(for example the framework above will run if the running kubescape scan is for scanning cluster or file) - list of allowed scanning scope ``` [[""cluster"", ""file""], [""cluster""], [""cloud""], [""GKE""], [""EKS""], [""AKS""]] ```. `cloud` meaning - will run just on managed cluster


### Add a control

Add `controlName.json` file in the `/controls` directory.

Example of a control:
```json
{
    ""name"": ""Pods in default namespace"",
    ""attributes"": {
    },
    ""description"": ""It is recommended to avoid running pods in cluster without explicit namespace assignment. This control identifies all the pods running in the default namespace."",
    ""remediation"": ""Create necessary namespaces and move all the pods from default namespace there."",
    ""rulesNames"": [
        ""pods-in-default-namespace""
    ],
    ""long_description"": ""It is recommended to avoid running pods in cluster without explicit namespace assignment. This may lead to wrong capabilities and permissions assignment and potential compromises. This control identifies all the pods running in the default namespace."",
    ""test"": ""Check that there are no pods in the 'default' namespace"",
    ""id"": ""C-0061"",
    ""controlID"": ""C-0061"",
    ""baseScore"": 3, 
    ""scanningScope"": {
        ""matches"": [
            ""cluster"",
            ""file""
        ]
    },
     ""category"": {
        ""name"" : ""Workload"",
        ""subCategory"": {
            ""name"": ""Resource management""
        }
   }
}
```
* `rulesNames` -  List of rules to run, must be exact name. Use copy-paste to be sure.
* `scanningScope` - this control will run just if kubescape scan process match to the scope in the list.(for example the control above will run if the running kubescape scan is for scanning cluster or file) - list of allowed scanning scope ``` [[""cluster"", ""file""], [""cluster""], [""cloud""], [""GKE""], [""EKS""], [""AKS""]] ```. `cloud` meaning - will run just on managed cluster
* `category` - The category the control belongs to. Some controls may also define a `subCategory`. The available categories/sub categories are listed under the `mapCategoryNameToID.json` file, mapped to their respective IDs
* `subCategory` - A sub category for a `category` (optional). Must be listed under the  `mapCategoryNameToID.json` file


* `long_description`, `test` and other control fields are used mainly in the [documentation](https://hub.armosec.io/docs)

* See [control go struct](https://github.com/kubescape/opa-utils/blob/master/reporthandling/datastructures.go#L56) for more control fields

### Add a rule:

1. Add to `/rules` a new directory with the rule name

2. Add to the rule directory file - `rule.metadata.json`:

Example of rule.metadata.json:
```json
{
    ""name"": ""resources-cpu-limit-and-request"",
    ""attributes"": {
    },
    ""ruleLanguage"": ""Rego"",
    ""match"": [
      {
        ""apiGroups"": [
          """"
        ],
        ""apiVersions"": [
          ""v1""
        ],
        ""resources"": [
          ""Pod""
        ]
      }
    ],
    ""ruleDependencies"": [
    ],
    ""controlConfigInputs"": [
      {
        ""path"": ""settings.postureControlInputs.cpu_request_max"",
        ""name"": ""cpu_request_max"",
        ""description"": ""Ensure a CPU resource request is set and is under this defined maximum value.""
      }
    ],
    ""description"": ""CPU limits and requests are not set."",
    ""remediation"": ""Ensure CPU limits and requests are set."",
    ""ruleQuery"": ""armo_builtins""
}
```


* See [rule go struct](https://github.com/kubescape/opa-utils/blob/master/reporthandling/datastructures.go#L37) for further explanations of rule fields
* Optional attributes :
  * `""hostSensorRule"": ""true""` - indicates the rule gets information from the host scanner

  * `""useFromKubescapeVersion""` - add if rule is only supported from a certain Kubescape version. Inclusive.

  * `""useUntilKubescapeVersion""` - add if a newer version exists so the control doesn’t run both. Inclusive. 

  * `""imageScanRelated"": true` - indicates that rule uses information from image scanning.

  * `""controlConfigInputs""` - A list the rule uses and can be configured by the user. See example above.


3. Add to the new rule directory a new file - `raw.rego`

    This is where the logic of the rule is. 
    Example of `raw.rego`:
    ```rego
    package armo_builtins

    deny[msga] {

        pod := input[_]
        pod.kind == ""Pod""
        container := pod.spec.containers[i]
        container.securityContext.privileged == true
        path := sprintf(""containers[%d].securityContext.privileged"", [i])

        msga := {
            ""alertMessage"": sprintf(""pod: %v is defined as privileged"", [pod.metadata.name]),
            ""packagename"": ""armo_builtins"",
            ""fixPaths"": [],
            ""failedPaths"": path,
            ""alertObject"": {
                ""k8sApiObjects"": [pod]
            }
        }
    }
    ```
    Use [opa rego reference](https://www.openpolicyagent.org/docs/latest/policy-reference/) for help with syntax

    See structure of a [rule response](https://github.com/kubescape/opa-utils/blob/master/reporthandling/datastructuresv1.go#L23)


4. Add a test for the new rule (and run it!). Learn how to add a test [here](testrunner/README.md#adding-new-rules) and how to run it [here](testrunner/README.md).

5. Add `filter.rego` if needed - If it exists, the filter is run by Kubescape to calculate ‘all resources’ = the number of potential resources to fail. It affects the risk score. This is needed in cases where a rule asks for resources that wil not potentially fail. Example: if a rule asks for pods and service accounts to see if they are connected but only fails the pods, we would create a filter rego that returns only pods.

**N.B.** To speed up the rule creation, we provided the script `scripts/init-rule.py`. This tool for scaffolding and code generation can be used to bootstrap a new rule fast. Let's see an example. To create a new rule, type the command:

```shell
python3 scripts/init-rule.py \
    --name ""ensure-something-is-set"" \
    --fix-command ""chmod 700 /tmp/file"" \
    --rule-description ""this is an example description"" \
    --rule-remediation ""this is an example remediation"" \
    --alert-message ""found something weird"" \
    --test-list ""success,failed_1,failed_2""
```

This command will create the following directory structure in the **regolibrary** repository.

```shell
rules/ensure-something-is-set/
├── raw.rego
├── rule.metadata.json
└── test
    ├── failed_1
    │   ├── expected.json
    │   └── input
    ├── failed_2
    │   ├── expected.json
    │   └── input
    └── success
        ├── expected.json
        └── input
```

To have a complete overview about the script, type this command: `python3 scripts/init-rule.py --help`.

## OPA bundles
The Kubescape regolibrary is [available](https://github.com/kubescape/regolibrary/releases/latest) as an [OPA bundle](https://www.openpolicyagent.org/docs/latest/management-bundles), for both targets, WASM and Rego. 

### Using the bundles
> Endpoint names are normalized to be used as a Rego package name. Here are some examples:
> ```
> host-pid -> host_pid
> Host_Ipc -> Host_Ipc
> foobar -> foobar
> ```
> To be sure, you can use the following regex to validate the endpoint name:
> ```python
> import re
> def normalize_rule_name(name) -> str:
>      return re.sub(r'[^a-zA-Z0-9_]', '_', name)
> ```

#### Rules
Rules endpoints uses the following naming convention:
```
data.armo_builtins.rules.<rule_name>.raw.deny
```
If there is a filter rule, it's available at the following endpoint:
```
data.armo_builtins.rules.<rule_name>.filter.deny
```
#### Controls
Controls endpoints uses the following naming convention:
```
data.armo_builtins.controls.<control_id>.deny
```
#### Frameworks
Frameworks endpoints uses the following naming convention:
```
data.armo_builtins.frameworks.<framework_name>.deny
```

### Settings
When evaluating frameworks or controls, you can control the amount of metadata the results will contain by using the `data.settings`.

Available settings:
- `data.settings.verbose`: If set to `true`, the evaluation will return a list with an entry for each rule response. Each rule response includes the rule response itself, the control metadata (if evaluated as part of a control), and the framework metadata (if evaluated as part of a framework).

- `data.settings.metadata`: If set to `true`, the evaluation will return a json object with the metadata of the rule, the control (if evaluated as part of a control), or the framework (if evaluated as part of a framework). This json object will have a field named `""results""`, with all the lower level results.

  When `data.settings.verbose` was set to `true`, it takes precedence over `data.settings.metadata`.

  Here is an example of a framework evaluation using the `data.settings.metadata` setting:
  ```json5
  {
    ""name"": ""ArmoBest"",
    ""controlsNames"": [...],
    ""description"": """",
    // Other framework metadata ...
    ""results"": {
      ""C-0005"": {
        ""name"": ""API server insecure port is enabled"",
        ""controlID"": ""C-0005"",
        // Other control metadata ...
        ""results"": [
          {
            ""alertMessage"": ""API server insecure port is enabled"",
            // Other rule response fields ...
          }
        ]
      }
    }
  }
  ```
- No settings: If no settings were set, the evaluation will return a list with an entry for each rule response. Each rule response will include only the rule response itself.
> The default setting in the released bundles is `data.settings.metadata`.

### Build
To build the OPA bundles, use the python script `/scripts/bundle.py`.

For example:
```bash
python3 scripts/bundle.py . -o release
```

### Unsupported rules and controls
Some rules and controls are not supported in the OPA bundles, because they require extra customized Rego built-in functions (you can always use Kubescape to evaluate them :wink:).

#### Rules
The following rules are not supported in the OPA bundles:
<!-- Start of OPA bundles removed rules -->
- deny-RCE-vuln-image-pods
- exposed-rce-pods
- has-critical-vulnerability
- deny-vuln-image-pods
- rule-can-bash-cmd-inside-container
- excessive_amount_of_vulnerabilities_pods
- exposed-critical-pods
<!-- End of OPA bundles removed rules -->

## Support & Communication
Reach out if you have any questions:

* [Open an issue](https://github.com/kubescape/regolibrary/issues/new/choose)
* [Slack Community ](https://cloud-native.slack.com/archives/C04EY3ZF9GE) For any Q&A or support you can reach us at our CNCF Slack channels


## Learn more: 
- [NSA Framework](https://www.nsa.gov/Press-Room/News-Highlights/Article/Article/2716980/nsa-cisa-release-kubernetes-hardening-guidance/)

- [MITRE ATT&CK® Framework](https://www.microsoft.com/security/blog/wp-content/uploads/2021/03/Matrix-1536x926.png)

- [CIS Framework](https://workbench.cisecurity.org/benchmarks/8973)

## Contributions

Thanks to all our contributors! Check out our [CONTRIBUTING](https://github.com/kubescape/kubescape/blob/master/CONTRIBUTING.md) file to learn how to join them.

* Feel free to pick a task from the [issues](https://github.com/kubescape/regolibrary/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22), roadmap or suggest a feature of your own.
* [Open an issue](https://github.com/kubescape/regolibrary/issues/new/choose): we aim to respond to all issues within 48 hours.
* [Join the CNCF Slack](https://slack.cncf.io/) and then our [users](https://cloud-native.slack.com/archives/C04EY3ZF9GE) or [developers](https://cloud-native.slack.com/archives/C04GY6H082K) channel.",FAUX
kubeshield/bpf-opa-demo,Documentations,Application System,2025-02-07T06:35:23Z,2019-11-29T06:07:09Z,0,4,0,0,0,0,0,0,2019-08-02T09:51:21Z,2025-02-18T12:20:36Z,16141,33,Go,VRAI,5,FAUX,8,,8,,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,3,"[![Go Report Card](https://goreportcard.com/badge/kubeshield.dev/bpf-opa-demo)](https://goreportcard.com/report/kubeshield.dev/bpf-opa-demo)
[![Build Status](https://github.com/kubeshield/bpf-opa-demo/workflows/CI/badge.svg)](https://github.com/kubeshield/bpf-opa-demo/actions?workflow=CI)
[![Slack](https://slack.appscode.com/badge.svg)](https://slack.appscode.com)
[![Twitter](https://img.shields.io/twitter/follow/kubeshield.svg?style=social&logo=twitter&label=Follow)](https://twitter.com/intent/follow?screen_name=Kubeshield)

# bpf-opa-demo

## Motivation

Securing Kubernetes cluster is a multi-faceted task. Runtime security is one aspect of it. It ensures that the workloads deployed in the cluster doesn't do any malicious behaviors. For runtime instrumentation, we wanted to use Extended Berkeley Packet Filter (eBPF), a core technology in the Linux kernel.

There are already many tools available in this space, but each project has its own custom components. We want to use a set of common set of tools and techniques for binding these different components.

For runtime analysis, we already have [Falco](http://falco.org/) project. Falco was initially dependent on kernel modules, but now they are using eBPF technology.

Falco is written in C++, but Kubernetes and its associated libraries are written in Go. Having a tool that is written in Go helps us to be more integrated with kubernetes.

Falco also has its own custom rule engine. We wanted to replace Falco's rule engine with with [Open Policy Agent (OPA)](https://www.openpolicyagent.org/), an open-source, general purpose policy engine.

## Architecture

The central component of this project is the Open Policy Agent (OPA) and Extended Berkeley Packet Filter (eBPF). eBPF is a modern and powerful technology in the Linux kernel. eBPF allows a user to attach programs to certain parts of the Linux kernel. When the code path is traversed, the attached eBPF programs are executed.

The open policy agent (OPA) is an open-source, general-purpose policy engine. OPA provides a high-level declarative language called Rego. Using rego, one can write policies. The application queries OPA and supplies JSON data as input for any policy violations for the given input.

Any application running in a Linux system communicates with the kernel with the system call interface. Everything the application is doing is done through the system call interface. So if we can monitor what system calls the application is doing, we can detect if the application is doing any kind of malicious activity.

To instrument on the system call interface, we're leveraging eBPF technology. We're attaching eBPF programs to the system call interface and extracting the parameters by which the system call is invoked. Then this information is passed to the kernel perf ring buffer. Our userspace program retrieves the information from the perf ring buffer, parses the raw syscall arguments and then finally queries to the OPA server for rule violations. Users can also write their own rules using rego and supply them to the OPA server.

![a](img/arch.svg)
Fig: Diagram showing the major components of bpf-opa-demo

## Usage

> Make sure that you're running a linux machine with kernel version `4.17` or higher. We're attaching eBPF programs to `raw_tracepoints`, and this feature is available from linux kernel `4.17`. For more information, you can see [here](https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md).

Download `Open Policy Agent` from [github release page](https://github.com/open-policy-agent/opa/releases). Start the OPA server

```bash
opa run -s
```

Download `bpf-opa-demo` binary from [github release page](https://github.com/kubeshield/bpf-opa-demo/releases). Run the binary with root permissions.

```bash
sudo ./bpf-opa-demo
```

You can use the open policy agent's [rest-API](https://www.openpolicyagent.org/docs/latest/rest-api/) to update the default rules and add your own rules.

When you run something that violates the rules, the program will print to the stdout the input with which it is queried

```json
{
  ""modify_shell_configuration_file"": {
    ""event"": {
      ""len"": 87,
      ""name"": ""openat"",
      ""nparams"": 6,
      ""params"": {
        ""dev"": 2049,
        ""dirfd"": 18446744073709552000,
        ""fd"": 3,
        ""flags"": 14,
        ""mode"": 438,
        ""name"": ""/home/tahsin/.bashrc""
      },
      ""tid"": 30828,
      ""ts"": 27562555577941,
      ""type"": 307
    },
    ""process"": {
      ""args"": [
        ""/home/tahsin/.bashrc""
      ],
      ""cgroup"": [
        ""cpuset=/"",
        ""cpu=/user.slice"",
        ""cpuacct=/user.slice"",
        ""io=/user.slice"",
        ""memory=/user.slice/user-1001.slice/session-2.scope""
      ],
      ""command"": ""nano"",
      ""executable"": ""nano"",
      ""name"": """",
      ""parent"": {
        ""args"": [],
        ""cgroup"": [
          """"
        ],
        ""command"": ""/usr/bin/fish"",
        ""executable"": ""/usr/bin/fish"",
        ""name"": ""fish"",
        ""parent"": null,
        ""pid"": 30573,
        ""ppid"": 0
      },
      ""pid"": 30828,
      ""ppid"": 30573
    }
  }
}
```

This indicates that the process `nano` did the system call `openat` to edit the file `.bashrc`. Here `process.cgroup` is empty as we're running this from our host machine. If we run time same thing from the container, we get the following output

```json
{
  ""read_shell_configuration_file"": {
    ""event"": {
      ""len"": 80,
      ""name"": ""openat"",
      ""nparams"": 6,
      ""params"": {
        ""dev"": 102,
        ""dirfd"": 18446744073709552000,
        ""fd"": 3,
        ""flags"": 1,
        ""mode"": 0,
        ""name"": ""/root/.bashrc""
      },
      ""tid"": 384,
      ""ts"": 29654919403736,
      ""type"": 307
    },
    ""process"": {
      ""args"": [
        ""/root/.bashrc""
      ],
      ""cgroup"": [
        ""cpuset=/docker/41e5c07cc31073cfcad80176bf5195d04d4511cfdaba0205ac149112579a03c9"",
        ""cpu=/docker/41e5c07cc31073cfcad80176bf5195d04d4511cfdaba0205ac149112579a03c9"",
        ""cpuacct=/docker/41e5c07cc31073cfcad80176bf5195d04d4511cfdaba0205ac149112579a03c9"",
        ""io=/docker/41e5c07cc31073cfcad80176bf5195d04d4511cfdaba0205ac149112579a03c9"",
        ""memory=/docker/41e5c07cc31073cfcad80176bf5195d04d4511cfdaba0205ac149112579a03c9""
      ],
      ""command"": ""cat"",
      ""executable"": ""cat"",
      ""name"": """",
      ""parent"": {
        ""args"": null,
        ""cgroup"": null,
        ""command"": """",
        ""executable"": """",
        ""name"": """",
        ""parent"": null,
        ""pid"": 0,
        ""ppid"": 0
      },
      ""pid"": 384,
      ""ppid"": 32735
    }
  }
}
```

You can see that the `process.cgroup` now has the docker container ID from which the command was run on.

## Findings

When doing this project, we found some limitations of rego.

- You can not write complex rules inside a single rule body. When evaluating rule bodies, OPA searches for variable bindings that make all of the expressions true. There may be multiple sets of bindings that make the rule body true. The rule body can be understood intuitively as: `expression-1 AND expression-2 AND ... AND expression-N `. In order to use `OR`, you have to define a rule multiple times. An incrementally defined rule can be intuitively understood as `<rule-1> OR <rule-2> OR ... OR <rule-N>`. So writing complex rules that involve multiple nested `AND` and `OR` expressions are very painful. For example, this following rule checks if `ncat` process was run with some arguments, so we had to write rules for each argument separately.

```
Netcat_Remote_Code_Execution = input {
	nc_process
}
Netcat_Remote_Code_Execution = input {
	ncat_process
}
```

```
ncat_arg_contains_exe {
     contains(input.process.args[_], ""--sh-exec"")
}
ncat_arg_contains_exe {
     contains(input.process.args[_], ""--exec"")
}
ncat_arg_contains_exe {
     contains(input.process.args[_], ""-e "")
}
ncat_arg_contains_exe {
     contains(input.process.args[_], ""-c "")
}
ncat_arg_contains_exe {
     contains(input.process.args[_], ""--lua-exec"")
}
```

- You can not do bitwise operations in rego. For example, to check if `O_RDONLY` flag set on `open` syscalls, we had to do the following

```
O_RDONLY := 1

is_open_read {
	round((input.event.params.flags-0.1) / O_RDONLY) % 2 > 0
}
```

- Rego doesn't support any kind of loops, if else blocks etc. We can write more complex rules if it has support for these.


## Acknowledgements

- We're using eBPF code from the [sysdig project](https://github.com/draios/sysdig/tree/master/driver/bpf).
Follow [this guide](https://falco.org/docs/source/) from the falco project for building the eBPF codes.
Then copy the compiled elf file to `bpf/probe.o`

- We're using default rules from the [falco project](https://github.com/falcosecurity/falco/blob/master/rules/falco_rules.yaml) and rewriting them in rego, the policy language from Open Policy Agent(OPA).",VRAI
kubesphere/ks-installer,DevOPs,DevOPs,2024-03-20T00:20:37Z,2023-08-25T03:18:01Z,0,0,0,0,0,0,0,2,2019-07-15T08:24:39Z,2025-04-01T08:00:58Z,142413,541,Jinja,VRAI,749,FAUX,281,"dashboard,existing-kubernetes-cluster,hacktoberfest,installer,k8s,kubernetes,kubernetes-dashboard",281,Install KubeSphere on existing Kubernetes cluster,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,110,"# Install KubeSphere on Existing Kubernetes Cluster

> English | [中文](README_zh.md)

In addition to supporting deploying on VM and BM, KubeSphere also supports installing on cloud-hosted and on-premises existing Kubernetes clusters.

## Prerequisites

> - Kubernetes Version: 1.20.x, 1.21.x, 1.22.x, 1.23.x (experimental);
> - CPU > 1 Core, Memory > 2 G;
> - An existing default Storage Class in your Kubernetes clusters.
> - The CSR signing feature is activated in kube-apiserver when it is started with the `--cluster-signing-cert-file` and `--cluster-signing-key-file` parameters, see [RKE installation issue](https://github.com/kubesphere/kubesphere/issues/1925#issuecomment-591698309).

1. Make sure your Kubernetes version is compatible by running `kubectl version` in your cluster node. The output looks as the following:

```bash
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.8"", GitCommit:""fd5d41537aee486160ad9b5356a9d82363273721"", GitTreeState:""clean"", BuildDate:""2021-02-17T12:41:51Z"", GoVersion:""go1.15.8"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.8"", GitCommit:""fd5d41537aee486160ad9b5356a9d82363273721"", GitTreeState:""clean"", BuildDate:""2021-02-17T12:33:08Z"", GoVersion:""go1.15.8"", Compiler:""gc"", Platform:""linux/amd64""}
```

> Note: Pay attention to `Server Version` line, if `GitVersion` is greater than `v1.19.0`, it's good to go. Otherwise you need to upgrade your Kubernetes first.

2. Check if the available resources meet the minimal prerequisite in your cluster.

```bash
$ free -g
              total        used        free      shared  buff/cache   available
Mem:              16          4          10           0           3           2
Swap:             0           0           0
```

3. Check if there is a default Storage Class in your cluster. An existing Storage Class is the prerequisite for KubeSphere installation.

```bash
$ kubectl get sc
NAME                      PROVISIONER               AGE
glusterfs (default)               kubernetes.io/glusterfs   3d4h
```

If your Kubernetes cluster environment meets all requirements mentioned above, then you can start to install KubeSphere.

## To Start Deploying KubeSphere

### Minimal Installation

```bash
kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.3.0/kubesphere-installer.yaml
kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.3.0/cluster-configuration.yaml
```

Then inspect the logs of installation.

```bash
kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -f
```

When all Pods of KubeSphere are running, it means the installation is successful. Check the port (30880 by default) of the console service by the following command. Then you can use `http://IP:30880` to access the console with the default account `admin/P@88w0rd`.

```bash
kubectl get svc/ks-console -n kubesphere-system
```
### Enable Pluggable Components

> Attention:
> - KubeSphere supports enable the pluggable components before or after the installation, you can refer to the [cluster-configuration.yaml](deploy/cluster-configuration.yaml) for more details.
> - Make sure there is enough CPU and memory available in your cluster.

1. [Optional] Create the secret of certificate for Etcd in your Kubernetes cluster. This step is only needed when you want to enable Etcd monitoring.

> Note: Create the secret according to the actual Etcd certificate path of your cluster; If the Etcd has not been configured certificate, an empty secret needs to be created.

- If the Etcd has been configured with certificates, refer to the following step (The following command is an example that is only used for the cluster created by `kubeadm`):

```bash
$ kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  \
--from-file=etcd-client-ca.crt=/etc/kubernetes/pki/etcd/ca.crt  \
--from-file=etcd-client.crt=/etc/kubernetes/pki/etcd/healthcheck-client.crt  \
--from-file=etcd-client.key=/etc/kubernetes/pki/etcd/healthcheck-client.key
```

- If the Etcd has not been configured with certificates.

```bash
kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs
```

2. If you already have a minimal KubeSphere setup, you still can enable the pluggable components by editing the ClusterConfiguration of ks-installer using the following command.

> Note: Please make sure there is enough CPU and memory available in your cluster.

```bash
kubectl edit cc ks-installer -n kubesphere-system
```
> Note: When you're enabling KubeEdge, please set advertiseAddress as below and expose corresponding ports correctly before you run or restart ks-installer. Please refer to [KubeEdge Guide](https://kubesphere.io/docs/pluggable-components/kubeedge/) for more details.
```yaml
kubeedge:
    cloudCore:
      cloudHub:
        advertiseAddress:
        - xxxx.xxxx.xxxx.xxxx
```

3. Inspect the logs of installation.

```bash
kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -f
```

## Upgrade

Deploy the new version of ks-installer:
```bash
# Notice: ks-installer will automatically migrate the configuration. Do not modify the cluster configuration by yourself.

kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.3.0/kubesphere-installer.yaml --force
```

> Note: If your KubeSphere version is v3.1.0 or eariler, please upgrade to v3.2.x first.",FAUX
kubev2v/forklift,Toolkit,Toolkit,2025-05-05T14:58:07Z,2025-05-14T10:45:51Z,0,170,0,0,0,0,0,0,2022-09-08T10:27:58Z,2025-04-07T15:50:44Z,55387,70,Go,VRAI,49,FAUX,78,"kubernetes,kubevirt,migration,openstack,ova,ovf,ovirt,virt-v2v,vmware,vsphere",78,"Toolkit for migrating VMs from VMware, OVA, oVirt and OpenStack to KubeVirt",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,35,"![Build](https://github.com/kubev2v/forklift/workflows/Build%20and%20push%20images/badge.svg)&nbsp;![CI](https://github.com/kubev2v/forklift/workflows/CI/badge.svg)&nbsp;[![Code Coverage](https://codecov.io/gh/kubev2v/forklift/branch/main/graph/badge.svg?token=VV6EBWKJGB)](https://codecov.io/gh/kubev2v/forklift)

# Forklift
Migrates virtual machines at scale to Kubernetes KubeVirt.
Migrations are performed in a few simple steps, first by providing source and destination credentials,
then mapping the source and destination infrastructure and creating a choreographed plan, and finally,
executing the migration effort.
![diagram.png](docs/diagram.png)

## Features
- **Warm migration** using Change Block Tracking/Incremental Backup to reduce the downtime, supported in VMware and oVirt migrations.
- For VMware migrations, the Forklift uses [virt-v2v](https://libguestfs.org/virt-v2v.1.html) **guest conversions** which installs the virtio drivers and edits the guest to run on QEMU-KVM.
- Migrating to **remote clusters**, user can install the Forklift on one cluster and orchestrate other cluster to do the migration.
- Migrating VMs **between clusters** using the KubeVirt [Export API](https://kubevirt.io/user-guide/storage/export_api/).
- **Validations** of the Virtual Machines to let users know if migration plan has issues that need to be addressed before running.
---

## Deploy
Deploy the latest Forklift operator index to the cluster
```bash
make deploy-operator-index REGISTRY_TAG=latest
```


## Build
Custom build of the controller, bundle and index which will be deployed to the cluster
```bash
export REGISTRY_ORG=user
make push-controller-image \
     push-operator-bundle-image \
     push-operator-index-image \
     deploy-operator-index
```
Note: The order of targets is important as the bundle needs to be created after controller and index after bundle.

### Configuration

| Name                       | Default value                                  | Description                                                            |
|----------------------------|------------------------------------------------|------------------------------------------------------------------------|
| REGISTRY_TAG               | devel                                          | The tag with which the image will be built and pushed to the registry. |
| REGISTRY_ORG               | kubev2v                                        | The registry organization to which the built image should be pushed.   |
| REGISTRY                   | quay.io                                        | The registry address to which the images should be pushed.             |
| CONTAINER_CMD              | autodetected                                   | The container runtime command (e.g.: /usr/bin/podman)                  |
| VERSION                    | 99.0.0                                         | The version with which the forklift should be built.                   |
| NAMESPACE                  | konveyor-forklift                              | The namespace in which the operator should be installed.               |
| CHANNELS                   | development                                    | The olm channels.                                                      |
| DEFAULT_CHANNEL            | development                                    | The default olm channel.                                               |
| OPERATOR_IMAGE             | quay.io/kubev2v/forklift-operator:latest       | The forklift operator image with the ansible-operator role.            |
| CONTROLLER_IMAGE           | quay.io/kubev2v/forklift-controller:latest     | The forklift controller image.                                         |
| MUST_GATHER_IMAGE          | quay.io/kubev2v/forklift-must-gather:latest    | The forklift must gather an image.                                     |
| UI_PLUGIN_IMAGE            | quay.io/kubev2v/forklift-console-plugin:latest | The forklift OKD/OpenShift UI plugin image.                            |
| VALIDATION_IMAGE           | quay.io/kubev2v/forklift-validation:latest     | The forklift validation image.                                         |
| VIRT_V2V_IMAGE             | quay.io/kubev2v/forklift-virt-v2v:latest       | The forklift virt v2v image for cold migration.                        |
| VDDK_IMAGE                 |                                                | The default Virtual Disk Development Kit (VDDK) image, default empty.  |
| POPULATOR_CONTROLLER_IMAGE | quay.io/kubev2v/populator-controller:latest    | The forklift volume-populator controller image.                        |
| OVIRT_POPULATOR_IMAGE      | quay.io/kubev2v/ovirt-populator:latest         | The oVirt populator image.                                             |",FAUX
kubewarden/helm-charts,Application System,Documentations,2025-05-15T17:40:03Z,2025-03-17T06:56:30Z,0,0,0,0,1,0,0,0,2021-03-03T11:18:23Z,2025-04-07T12:45:50Z,1995,27,Smarty,VRAI,16,FAUX,15,hacktoberfest,15,Helm charts for the Kubewarden project,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,23,"# Kubewarden helm-charts
[![Kubewarden Core Repository](https://github.com/kubewarden/community/blob/main/badges/kubewarden-core.svg)](https://github.com/kubewarden/community/blob/main/REPOSITORIES.md#core-scope)
[![Stable](https://img.shields.io/badge/status-stable-brightgreen?style=for-the-badge)](https://github.com/kubewarden/community/blob/main/REPOSITORIES.md#stable)

[![E2E](https://github.com/kubewarden/helm-charts/actions/workflows/e2e-tests.yml/badge.svg?event=schedule)](https://github.com/kubewarden/helm-charts/actions/workflows/e2e-tests.yml?query=event%3Aschedule)

This repository contains the helm charts used to deploy the Kubewarden stack.",VRAI
kubewarden/policy-evaluator,Toolkit,Application System,2025-05-16T04:44:49Z,2025-04-18T13:02:03Z,0,10,0,0,0,0,0,0,2021-04-22T09:10:53Z,2025-04-02T06:50:41Z,1894,16,Rust,VRAI,9,FAUX,22,hacktoberfest,22,"Crate used by Kubewarden that is able to evaluate policies with a given input, request to evaluate and settings.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,12,"# policy-evaluator
[![Kubewarden Core Repository](https://github.com/kubewarden/community/blob/main/badges/kubewarden-core.svg)](https://github.com/kubewarden/community/blob/main/REPOSITORIES.md#core-scope)
[![Stable](https://img.shields.io/badge/status-stable-brightgreen?style=for-the-badge)](https://github.com/kubewarden/community/blob/main/REPOSITORIES.md#stable)

Crate used by Kubewarden that is able to evaluate policies with a
given input, request to evaluate and settings.",VRAI
kyma-project/test-infra,Toolkit,Database,2025-05-15T14:30:41Z,2025-05-08T06:41:00Z,0,0,0,0,0,0,0,14,2018-09-05T09:44:20Z,2025-04-08T05:23:23Z,48240,37,Go,VRAI,179,FAUX,103,,103,,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,209,"# Test Infra
<!-- markdown-link-check-disable-next-line -->
[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fkyma-project%2Ftest-infra.svg?type=shield)](https://app.fossa.io/projects/git%2Bgithub.com%2Fkyma-project%2Ftest-infra?ref=badge_shield)
<!-- markdown-link-check-disable-next-line -->
[![REUSE status](https://api.reuse.software/badge/github.com/kyma-project/test-infra)](https://api.reuse.software/info/github.com/kyma-project/test-infra)

## Overview

The purpose of the `test-infra` repository is to store configuration and scripts for the test infrastructure used in the `kyma-project` organization.
See also the internally available [test-infra onboarding](https://github.tools.sap/kyma/test-infra/blob/main/onboarding.md).

### Project Documentation

Please see the [index page](/docs/index.md) for the Test Infra documentation. It lists all the documentation available in the `test-infra` repository.

### Prow

The `test-infra` repository contains the whole configuration of Prow. Its purpose is to replace the internal Continuous Integration (CI) tool in the `kyma-project` organization.

For more detailed documentation on installation, configuration, development, and testing, go to the [`docs/prow`](./docs/prow) subfolder.


## License
<!-- markdown-link-check-disable-next-line -->
[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fkyma-project%2Ftest-infra.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2Fkyma-project%2Ftest-infra?ref=badge_large)

## Contributing
<!--- mandatory section - do not change this! --->

See the [Contributing Rules](CONTRIBUTING.md).

## Code of Conduct
<!--- mandatory section - do not change this! --->

See the [Code of Conduct](CODE_OF_CONDUCT.md) document.

## Licensing
<!--- mandatory section - do not change this! --->

See the [license](./LICENSE) file.",VRAI
kyverno/demos,Documentations,Database,2024-05-06T07:00:24Z,2023-03-02T00:18:04Z,0,0,0,0,79,0,0,0,2022-04-01T17:04:04Z,2024-07-17T19:04:01Z,8254,5,Shell,VRAI,11,FAUX,0,,0,demo policies and resources,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,8,"# Kyverno demos

This repository contains Kyverno demo policies and resources. Select a folder to view details.",VRAI
kyverno/kyverno,Toolkit,Application System,2025-05-16T05:07:30Z,2025-05-07T14:17:20Z,0,0,0,0,952,0,0,0,2019-02-04T16:25:48Z,2025-04-08T11:57:58Z,142570,6200,Go,VRAI,985,FAUX,407,"kubernetes,policy-management",407,Cloud Native Policy Management,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,381,"<!--
Copyright 2024 The Kyverno Authors

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Kyverno [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Cloud%20Native%20Policy%20Management.%20No%20new%20language%20required%1&url=https://github.com/kyverno/kyverno/&hashtags=kubernetes,devops)

**Cloud Native Policy Management 🎉**

[![Go Report Card](https://goreportcard.com/badge/github.com/kyverno/kyverno)](https://goreportcard.com/report/github.com/kyverno/kyverno)
![License: Apache-2.0](https://img.shields.io/github/license/kyverno/kyverno?color=blue)
[![GitHub Repo stars](https://img.shields.io/github/stars/kyverno/kyverno)](https://github.com/kyverno/kyverno/stargazers)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5327/badge)](https://bestpractices.coreinfrastructure.org/projects/5327)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/kyverno/kyverno/badge)](https://securityscorecards.dev/viewer/?uri=github.com/kyverno/kyverno)
[![SLSA 3](https://slsa.dev/images/gh-badge-level3.svg)](https://slsa.dev)
[![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/kyverno)](https://artifacthub.io/packages/search?repo=kyverno)
[![codecov](https://codecov.io/gh/kyverno/kyverno/branch/main/graph/badge.svg)](https://app.codecov.io/gh/kyverno/kyverno/branch/main)
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fkyverno%2Fkyverno.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Fkyverno%2Fkyverno?ref=badge_shield)


<a href=""https://kyverno.io"" rel=""kyverno.io"">![logo](img/Kyverno_Horizontal.png)</a>

<p class=""callout info"" style=""font-size: 100%;"">
Kyverno is a policy engine designed for cloud native platform engineering teams. It enables security, automation, compliance, and governance using policy-as-code. Kyverno can validate, mutate, generate, and cleanup configurations using Kubernetes admission controls, background scans, and source code respository scans. Kyverno policies can also be used to verify OCI images, for software supply chain security. Kyverno policies can be managed as Kubernetes resources and do not require learning a new language. Kyverno is designed to work nicely with tools you already use like kubectl, kustomize, and Git.
</p>

<a href=""https://opensourcesecurityindex.io/"" target=""_blank"" rel=""noopener""> <img
        style=""width: 282px; height: 56px""
        src=""https://opensourcesecurityindex.io/badge.svg""
        alt=""Open Source Security Index - Fastest Growing Open Source Security Projects""
        width=""282""
        height=""56""
    />
</a>

## 📙 Documentation

Kyverno installation and reference documents are available at [kyverno.io](https://kyverno.io).

👉 **[Quick Start](https://kyverno.io/docs/introduction/#quick-start)**

👉 **[Installation](https://kyverno.io/docs/installation/)**

👉 **[Sample Policies](https://kyverno.io/policies/)**

## 🎯 Popular Use Cases

Kyverno helps platform teams enforce best practices and security policies. Here are some common use cases:

1. **Security & Compliance**
   - Enforce pod security standards
   - Require specific security contexts
   - Validate image sources and signatures
   - Ensure resource limits and requests

2. **Operational Excellence**
   - Automatically add labels and annotations
   - Enforce naming conventions
   - Generate default network policies
   - Validate resource configurations

3. **Cost Optimization**
   - Enforce resource quotas
   - Require cost allocation labels
   - Clean up unused resources
   - Validate instance types

4. **Developer Guardrails**
   - Enforce ingress/egress rules
   - Require liveness/readiness probes
   - Validate container images
   - Auto-mount configuration

Each use case includes ready-to-use policies in our [policy library](https://kyverno.io/policies/).

## 🙋‍♂️ Getting Help

We are here to help!

👉 For feature requests and bugs, file an [issue](https://github.com/kyverno/kyverno/issues).

👉 For discussions or questions, join the [Kyverno Slack channel](https://slack.k8s.io/#kyverno).

👉 For community meeting access, see [mailing list](https://kyverno.io/community/#community-meetings).

👉 To get follow updates ⭐️ [star this repository](https://github.com/kyverno/kyverno/stargazers).

## ➕ Contributing

Thanks for your interest in contributing to Kyverno! Here are some steps to help get you started:

✔ Read and agree to the [Contribution Guidelines](/CONTRIBUTING.md).

✔ Browse through the [GitHub discussions](https://github.com/kyverno/kyverno/discussions).

✔ Read Kyverno design and development details on the [GitHub Wiki](https://github.com/kyverno/kyverno/wiki).

✔ Check out the [good first issues](https://github.com/kyverno/kyverno/labels/good%20first%20issue) list. Add a comment with `/assign` to request assignment of the issue.

✔ Check out the Kyverno [Community page](https://kyverno.io/community/) for other ways to get involved.

## Software Bill of Materials

All Kyverno images include a Software Bill of Materials (SBOM) in [CycloneDX](https://cyclonedx.org/) JSON format. SBOMs for Kyverno images are stored in a separate repository at `ghcr.io/kyverno/sbom`. More information on this is available at [Fetching the SBOM for Kyverno](https://kyverno.io/docs/security/#fetching-the-sbom-for-kyverno). 

## Contributors

Kyverno is built and maintained by our growing community of contributors!

<a href=""https://github.com/kyverno/kyverno/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=kyverno/kyverno"" />
</a>

Made with [contributors-img](https://contrib.rocks).

## License

Copyright 2025, the Kyverno project. All rights reserved. Kyverno is licensed under the [Apache License 2.0](LICENSE).

Kyverno is a [Cloud Native Computing Foundation (CNCF) Incubating project](https://www.cncf.io/projects/) and was contributed by [Nirmata](https://nirmata.com/?utm_source=github&utm_medium=repository).",VRAI
kyverno/playground,Documentations,Application System,2025-05-08T11:41:38Z,2025-04-05T07:09:22Z,0,0,0,0,13,0,0,0,2023-05-03T14:01:42Z,2025-04-08T09:56:42Z,44495,27,Vue,VRAI,9,FAUX,10,"kyverno,playground",10,Powers the Kyverno playground,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,7,"# Kyverno Playground

![release](https://github.com/kyverno/playground/workflows/release/badge.svg)
![ci](https://github.com/kyverno/playground/workflows/ci/badge.svg)
![image](https://github.com/kyverno/playground/workflows/image/badge.svg)
[![Go Report Card](https://goreportcard.com/badge/github.com/kyverno/playground/backend)](https://goreportcard.com/report/github.com/kyverno/playground/backend)
![License: Apache-2.0](https://img.shields.io/github/license/kyverno/playground?color=blue)
[![GitHub Repo stars](https://img.shields.io/github/stars/kyverno/playground)](https://github.com/kyverno/playground/stargazers)

The **public version of the Playground** is available at https://playground.kyverno.io.

## About

The Kyverno Playground is a web service that simulates [Kyverno](https://github.com/kyverno/kyverno) behaviour, you can experiment and play with Kyverno policies directly in your browser

The service receives a configuration, resource and policies definitions, runs the Kyverno engine, and returns the results of evaluating policies against resources.

The playground currently supports:
- Validation rules
- Mutation rules
- Image verification rules
- Generate rules

**NOTES:**
- This tool only works with public image registries
- No data is gathered, stored, or shared

## Features

The playground frontend offers a rich feature set:
- Supports admission information like `username`, `groups`, `roles` and `cluster roles`
- Saving and loading state from the local storage
- Loading policies and resources from the Kyverno catalog
- Sharing state with simple links
- Comes with a tutorial to learn Kyverno easily

### Context and Variables

It is currently not possible to add variables from external resources or do actual API calls.

It is only possible to mock variables using the variables configuration in the context input.

### Multiple Manifests

It is supported to define multiple policies and/or resources as inputs.

Context and variables will be shared for all executions.

### Load Manifests

The ""File"" Button loads a local YAML file as input.

The ""URL"" Button loads a manifest from an external URL, example: https://raw.githubusercontent.com/kyverno/policies/main/best-practices/disallow-latest-tag/disallow-latest-tag.yaml

## Install

Kyverno Playground releases are available at https://github.com/kyverno/playground/releases.

Additionaly we publish docker images at [ghcr.io/kyverno/playground](https://github.com/kyverno/playground/pkgs/container/playground) and an helm chart repository is available at https://kyverno.github.io/playground.

### Install with Helm

Add `kyverno-playground` Helm repository:

```shell
helm repo add kyverno-playground https://kyverno.github.io/playground/
```

Install `kyverno-playground` Helm chart:

```shell
helm upgrade --install kyverno-playground --namespace kyverno --create-namespace --wait kyverno-playground/kyverno-playground
```

Install `kyverno-playground` Helm chart (without configuring an Helm repository):
```shell
helm upgrade --install kyverno-playground --namespace kyverno --create-namespace --wait --repo https://kyverno.github.io/playground kyverno-playground
```

Install `kyverno-playground` local Helm chart:
```shell
helm upgrade --install kyverno-playground --namespace kyverno --create-namespace --wait ./charts/kyverno-playground
```

### Install and run locally

Alternatively, you can install and run the Playground locally. This will allow you to connect the Playground to a real cluster.

Please read the [Cluster connected docs](./docs/CLUSTER.md).

## Custom resources

The Playground uses openapi schemas to load resources from yaml content. To load a resource correctly the Playground needs the corresponding openapi schema.

By default, all Kubernetes builtin resources are supported. To work with custom resources you need to provide the custom resource definition.

Providing custom resource definitions can be done in different ways:
* Using the `--engine-builtin-crds` flag in the backend (see the list of [supported built-in CRDs](#supported-built-in-custom-resource-definitions))
* Using the `--engine-local-crds` flag in the backend, pointing to a directory containing yaml CRD definitions
* Paste your CRD yaml definitions directly in the frontend

### Supported built-in custom resource definitions

The following CRDs are embedded in the Playground backend and can be enabled with the `--builtin-crds` flag:

| Name | Flag |
| --- | --- |
| ArgoCD | `--builtin-crds=argocd` |
| Cert Manager | `--builtin-crds=cert-manager` |
| Tekton Pipeline | `--builtin-crds=tekton-pipeline` |
| Prometheus Operator | `--builtin-crds=prometheus-operator` |
| (Cluster)PolicyReports | `--builtin-crds=wgpolicyk8s` |

## Build

Instructions for building and running the Playground from source code is available in the [docs](./docs) section.

## Screenshots

![Kyverno Playground - Layout](./frontend/screens/dashboard.png?raw=true)

<hr />

![Kyverno Playground - Examples](./frontend/screens/tutorials.png?raw=true)

<hr />

![Kyverno Playground - Validation Results](./frontend/screens/results.png?raw=true)

<hr />

![Kyverno Playground - DarkMode](./frontend/screens/context.png?raw=true)

<hr />

![Kyverno Playground - DarkMode](./frontend/screens/share.png?raw=true)",VRAI
kyverno/policies,Documentations,Documentations,2025-05-09T07:51:37Z,2024-10-31T04:50:52Z,0,0,0,0,1184,0,0,0,2020-12-02T20:44:13Z,2025-04-07T19:36:15Z,4193,373,Shell,VRAI,267,FAUX,78,,78,Kyverno policies for security and best practices ,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,105,"## Contributors
<a href=""https://github.com/kyverno/policies/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=kyverno/policies"" />
</a>

Made with [contributors-img](https://contrib.rocks).

# Policies

This repository contains Kyverno policies for a wide array of usage on various Kubernetes and ecosystem resources and subjects. For the optimal searching and browsing experience, please see [Usage and Documentation](#usage-and-documentation). For guidance on how you can contribute your own, please see [Contribution](#contribution). To request a Kyverno policy be created which doesn't exist, please see [Policy Requests](#policy-requests).

## Usage and Documentation

See https://kyverno.io/policies/ for a list of all the policies represented here in a simplified list with easy filtering abilities.

## Contribution

Anyone and everyone is welcome to write and contribute Kyverno policies! We have standardized on several practices to ensure these policies are effective, descriptive, and assist in easy location on the website. Please follow these guidelines when contributing or modifying a policy.

* As a CNCF project, Kyverno requires all contributors to abide by the DCO guidelines published [here](https://github.com/cncf/foundation/blob/main/dco-guidelines.md). This entails signing off on all git commits.

* Use the [Kyverno annotations](https://github.com/kyverno/policies/wiki/Kyverno-annotations) to mark your policy with descriptive metadata. This is not only important to explain your policy, but to allow the filtering logic on the [policies page](https://kyverno.io/policies/) to work effectively.

* Name your policy something descriptive which matches its function. Either dashes or underscores are permitted.

* Provide test resources (where possible) which allow your policy to be validated using the Kyverno CLI. See an example of a complete policy, resource, and test [here](https://github.com/kyverno/policies/tree/main/pod-security/baseline/disallow-capabilities). If unfamiliar with the Kyverno CLI and its test ability, please see the documentation [here](https://kyverno.io/docs/testing-policies/).

* For `validate` rules, please set `validationFailureAction: Audit` so that should a user download and apply the policy without having a yet full understanding of Kyverno, it will not cause unintended harm to their environment by blocking resources.

* String values do not need to be quoted nor do values which contain JMESPath expressions such as `{{request.operation}}`. The exception is if a field's value is *only* such an expression. In those cases, the JMESPath expression needs to be double quoted.

* Since Kyverno policies are made available on [Artifact Hub](https://artifacthub.io/), each new policy requires a separate metadata file. Create the `artifacthub-pkg.yml` file in the same directory as your policy. See the [Artifact Hub](#artifact-hub) section below for more details on its contents.

* A dedicated folder must be created for each policy.

* The folder must be named the same as the policy.

* Use dashes for folder name and policy name instead of underscores.

* When updating a policy already in the library, calculate the new sha256 sum of the changed policy and update the `artifacthub-pkg.yml` file's `digest` field with this value. This is to ensure Artifact Hub picks up the changes once merged. Note that because of validation checks in Kyverno's CI processes, it expects the digest to have been generated on a Linux system. Due to the differences of control characters, a digest generated from a Windows system may be different from that generated in Linux.

Once your policy is written within these guidelines and tested, please open a standard PR against the `main` branch of kyverno/policies. In order for a policy to make it to the website's [policies page](https://kyverno.io/policies/), it must first be committed to the `main` branch in this repo. Following that, an administrator will render these policies to produce Markdown files in a second PR. You do not need to worry about this process, however.

In order to streamline the process, the beginning ""stub"" of a ClusterPolicy resource is provided below with an example of how especially the annotations should be completed. Be sure to check the documentation and other sample policies as there is no guarantee this below stub is up to date.

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: disallow-capabilities
  annotations:
    policies.kyverno.io/title: Disallow Capabilities
    policies.kyverno.io/category: Pod Security Standards (Baseline)
    policies.kyverno.io/severity: medium
    kyverno.io/kyverno-version: 1.6.0
    policies.kyverno.io/minversion: 1.6.0
    kyverno.io/kubernetes-version: ""1.22-1.23""
    policies.kyverno.io/subject: Pod
    policies.kyverno.io/description: >-
      Adding capabilities beyond those listed in the policy must be disallowed.
spec:
  validationFailureAction: Audit
  background: true
  rules:
  - name: my-rule-name
    match:
      any:
      - resources:
          kinds:
            - Resource
```

### Artifact Hub

Add an `artifacthub-pkg.yml` metadata file to the folder. See an example metadata file for Kyverno policies below and customize per the comments.

```yaml
---
name: backup-all-volumes # The name of the package (only alphanum, no spaces, dashes allowed)
version: 1.0.0 # Version of the policy
displayName: Backup All Volumes  # Display name of the policy
createdAt: ""2023-03-29T00:00:00.000Z"" # The date this package was created (RFC3339 layout)
description: >-
# The description value should be taken from the relevant annotation policies.kyverno.io/description
      In order for Velero to backup volumes in a Pod using an opt-in approach, it
      requires an annotation on the Pod called `backup.velero.io/backup-volumes` with the
      value being a comma-separated list of the volumes mounted to that Pod. This policy
      automatically annotates Pods (and Pod controllers) which refer to a PVC so that
      all volumes are listed in the aforementioned annotation if a Namespace with the label
      `velero-backup-pvc=true`.
install: |- # The installation instructions for the package
    ```shell
    kubectl apply -f https://raw.githubusercontent.com/kyverno/policies/main/velero/backup-all-volumes/backup-all-volumes.yaml
    ```   
keywords: # Keywords should always have ""kyverno"" and whatever the value of the policies.kyverno.io/category annotation. 
  - velero
  - kyverno
readme: | # readme should be same as policies.kyverno.io/description annotation plus the last sentence as a static value.
  In order for Velero to backup volumes in a Pod using an opt-in approach, it
  requires an annotation on the Pod called `backup.velero.io/backup-volumes` with the
  value being a comma-separated list of the volumes mounted to that Pod. This policy
  automatically annotates Pods (and Pod controllers) which refer to a PVC so that
  all volumes are listed in the aforementioned annotation if a Namespace with the label
  `velero-backup-pvc=true`.

  Refer to the documentation for more details on Kyverno annotations: https://artifacthub.io/docs/topics/annotations/kyverno/
annotations: # See the annotations guide on Artifact Hub here: https://artifacthub.io/docs/topics/annotations/kyverno/
  kyverno/category: ""Velero""
  kyverno/kubernetesVersion: ""1.25""
  kyverno/subject: ""Pod, Annotation""
digest: 795012387c2755c61fa802fea900011c45520c2cffb27238210933ebb9a7f2c0 # The SHA256 hash String that uniquely identifies this package version
```

## Policy Requests

If you're not yet comfortable with Kyverno and would like to see a policy that may not presently exist, or if you're having trouble crafting that perfect policy, a couple resources exist. The most expedient way to get help may be to post on [Kyverno Slack](https://kyverno.io/community/). Kyverno has a rich and active community with its members and maintainers ready to assist. You may also [open an issue](https://github.com/kyverno/policies/issues) to request a certain policy be created to satisfy your needs. If going this route, do keep a few things in mind.

* Clearly explain in detail your use case for *why* you need a policy which isn't present on the [policies page](https://kyverno.io/policies/).

* Explain what you want this policy to do and on what resources.

* If applicable, explain what other policies and/or steps you have taken yourself that have been unsuccessful.

* Be responsive to the GitHub issue if further follow-up is required by the contributors or maintainers.

Having this information up front will assist others in crafting a policy to meet your needs.",FAUX
kyverno/policy-reporter,Toolkit,Application System,2025-05-16T05:11:40Z,2025-04-16T21:06:47Z,0,0,0,0,4,0,0,0,2021-02-19T23:55:09Z,2025-04-08T08:07:37Z,52445,307,Go,VRAI,96,FAUX,18,"grafana,kubernetes,kyverno,metrics,observability,prometheus-metrics",18,Monitoring and Observability Tool for the PolicyReport CRD with an optional UI.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,73,"# Policy Reporter 3.x
[![CI](https://github.com/kyverno/policy-reporter/actions/workflows/ci.yaml/badge.svg)](https://github.com/kyverno/policy-reporter/actions/workflows/ci.yaml) [![Go Report Card](https://goreportcard.com/badge/github.com/kyverno/policy-reporter)](https://goreportcard.com/report/github.com/kyverno/policy-reporter) [![Coverage Status](https://coveralls.io/repos/github/kyverno/policy-reporter/badge.svg?branch=main)](https://coveralls.io/github/kyverno/policy-reporter?branch=main)


![Screenshot Policy Reporter UI v2](https://github.com/kyverno/policy-reporter/blob/main/docs/images/screen.png)

## Documentation

The documentation for Policy Reporter v3 can be found here: [https://kyverno.github.io/policy-reporter-docs/](https://kyverno.github.io/policy-reporter-docs/)

## Getting Started

## Installation with Helm v3

Installation via Helm Repository

### Add the Helm repository
```bash
helm repo add policy-reporter https://kyverno.github.io/policy-reporter
helm repo update
```

### Installation with Policy Reporter UI and Kyverno Plugin enabled
```bash
helm install policy-reporter policy-reporter/policy-reporter --create-namespace -n policy-reporter --set ui.enabled=true --set kyverno-plugin.enabled=true
kubectl port-forward service/policy-reporter-ui 8082:8080 -n policy-reporter
```

Open `http://localhost:8082/` in your browser.",VRAI
Legit-Labs/legitify,Toolkit,Toolkit,2024-09-25T09:42:27Z,2024-01-07T12:38:47Z,0,15,0,0,0,0,0,0,2022-07-14T15:10:54Z,2025-03-31T08:53:57Z,4494,799,Go,VRAI,64,FAUX,16,"ci,devops,devsecops,github,gitlab,golang,sdlc-security,security,security-scanner,supply-chain-security",16,Detect and remediate misconfigurations and security risks across all your GitHub and GitLab assets,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,28,"<div align=""center"">
 
[![Build & Test](https://github.com/Legit-Labs/legitify/actions/workflows/build_and_test.yaml/badge.svg)](https://github.com/Legit-Labs/legitify/actions/workflows/build_and_test.yaml)
[![Code Analysis](https://github.com/Legit-Labs/legitify/actions/workflows/code_analysis.yaml/badge.svg)](https://github.com/Legit-Labs/legitify/actions/workflows/code_analysis.yaml)
[![Version Releaser](https://github.com/Legit-Labs/legitify/actions/workflows/release.yaml/badge.svg)](https://github.com/Legit-Labs/legitify/actions/workflows/release.yaml)
[![Build Docs](https://github.com/Legit-Labs/legitify/actions/workflows/build_docs.yaml/badge.svg)](https://github.com/Legit-Labs/legitify/actions/workflows/build_docs.yaml)
[![Go Report Card](https://goreportcard.com/badge/github.com/Legit-Labs/legitify)](https://goreportcard.com/report/github.com/Legit-Labs/legitify)
<img referrerpolicy=""no-referrer-when-downgrade"" src=""https://static.scarf.sh/a.png?x-pxid=6f4cbb25-54f4-4c47-b611-9b741732bb86"" />
<br/>
 <img width=""250"" alt=""Legitify Logo"" src=""https://user-images.githubusercontent.com/74864790/174815311-746a0c98-9a1f-44a9-808c-035788edfd4d.png"">

Strengthen the security posture of your source-code management! <br/>
Detect and remediate misconfigurations, security and compliance issues across all your GitHub and GitLab assets with ease 🔥 <br/>
by [Legit Security](https://www.legitsecurity.com/).

<b>
Wonder what Legit Security does?
</b>

Legit Security is an application security posture management (ASPM) and software supply chain security solution.<br/>
For more information check out the [comparison table](#legitify-vs-the-legit-security-platform)

</div>


https://user-images.githubusercontent.com/107790206/210602039-2d022692-87ea-4005-b9c6-f091158de3ce.mov

## Installation

Installation is possible in several ways:

- For macOS (or linux) using homebrew:

```
brew install legitify
```

- You can download the latest legitify release from https://github.com/Legit-Labs/legitify/releases, each archive contains:

  - Legitify binary for the desired platform
  - Built-in policies provided by Legit Security

- From source with the following steps:

```
git clone git@github.com:Legit-Labs/legitify.git
go run main.go analyze ...
```

- As a GitHub CLI extension (check out https://github.com/Legit-Labs/gh-legitify for more information)

```
gh extension install legit-labs/gh-legitify
gh legitify
```

## CI - Legitify Custom GitHub Action

You can run legitify as part of a CI process with the legitify Custom GitHub Actions:

```
name: Legitify Analyze
on:
    workflow_dispatch:
    schedule:
      - cron: '0 11 * * 1-5'

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - name: Legitify Action
        uses: Legit-Labs/legitify@main
        with:
          github_token: ${{ secrets.PAT_FOR_LEGITIFY }}
          ignore-policies: |
             non_admins_can_create_public_repositories
             requires_status_checks
```

Checkout the [action file](https://github.com/Legit-Labs/legitify/blob/main/action.yml) for additional parameters
and configuration.

## Provenance

To enhance the software supply chain security of legitify's users, as of v0.1.6, every legitify release contains a [SLSA Level 3 Provenance](https://github.com/slsa-framework/slsa-github-generator/blob/main/internal/builders/generic/README.md) document.  
The provenance document refers to all artifacts in the release, as well as the generated docker image.  
You can use [SLSA framework's official verifier](https://github.com/slsa-framework/slsa-verifier) to verify the provenance.  
Example of usage for the darwin_arm64 architecture for the v0.1.6 release:

```
VERSION=0.1.6
ARCH=darwin_arm64
./slsa-verifier verify-artifact --source-branch main --builder-id 'https://github.com/slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@refs/tags/v1.2.2' --source-uri ""git+https://github.com/Legit-Labs/legitify"" --provenance-path multiple.intoto.jsonl ./legitify_${VERSION}_${ARCH}.tar.gz
```

## Commands

### analyze

```
SCM_TOKEN=<your_token> legitify analyze
```

By default, legitify will check the policies against all your resources (organizations, repositories, members, actions). Archived repositories are skipped.

You can control which resources will be analyzed with command-line flags namespace and org:

- `--namespace (-n)`: will analyze policies that relate to the specified resources
- `--org`: will limit the analysis to the specified GitHub organizations or GitLab group, excluding archived repositories
- `--repo`: will limit the analysis to the specified GitHub repositories or GitLab projects
- `--scm`: specify the source code management platform. Possible values are: `github` or `gitlab`. Defaults to `github`. Please note: when running on GitLab, `--scm gitlab` is required.
- `--enterprise`: will specify which enterprises should be analyzed. Please note: in order to analyze an enterprise, an enterprise slug must be provided.

```
SCM_TOKEN=<your_token> legitify analyze --org org1,org2 --namespace organization,member
```

The above command will test organization and member policies against org1 and org2.

### gpt-analysis

```
SCM_TOKEN=<your_token> OPENAI_TOKEN=<token> ./legitify gpt-analysis --repo org1/repo1 --org org1
```

GPT-3 based analysis of the security posture of the provided repository or organization.

**NOTE: The repository/organization metadata is sent to openai servers.**

Flags:

- `--org`: will limit the analysis to the specified GitHub organizations or GitLab group
- `--repo`: will limit the analysis to the specified GitHub repositories or GitLab projects
- `--scm`: specify the source code management platform. Possible values are: `github` or `gitlab`. Defaults to `github`.
- `--token`: token for the SCM (or set the SCM_TOKEN environment variable)
- `--openai-token`: token for openai API (or set OPENAI_TOKEN environment variable)

Must provide either `--org` or `--repo` or both.

Generating openai token:

1. Go to https://beta.openai.com/signup and create an openai account
2. Under https://platform.openai.com/account/api-keys press ""Create new secret key""

## GitHub Action Usage

You can also run legitify as a GitHub action in your workflows, see the **action_examples** directory for concrete examples.

## Requirements

### GitHub (Cloud and Enterprise Server)

1. To get the most out of legitify, you need to be an owner of at least one GitHub organization. Otherwise, you can still use the tool if you're an admin of at least one repository inside an organization, in which case you'll be able to see only repository-related policies results.
2. legitify requires a GitHub personal access token (PAT) to analyze your resources successfully, which can be either provided as an argument (`-t`) or as an environment variable (`SCM_TOKEN`).
   The PAT needs the following scopes for full analysis:

```
admin:org, read:enterprise, admin:org_hook, read:org, repo, read:repo_hook
```

See [Creating a Personal Access Token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token) for more information.  
Fine-grained personal access tokens are currently not supported.

### GitHub Enterprise Server

You can run legitify against a GitHub Enterprise Server instance if you set the endpoint URL in the environment variable `SERVER_URL`:

```sh
export SERVER_URL=""https://github.example.com/""
SCM_TOKEN=<your_token> legitify analyze --org org1,org2 --namespace organization,member
```

### GitLab Cloud/Server

1. As mentioned in the previous section, you need to be an owner of at least one GitLab group. Otherwise, you can still use the tool if you're an admin of at least one project inside a group, in which case you'll be able to see only project-related policies results.
2. legitify requires a GitLab personal access token (PAT) to analyze your resources successfully, which can be either provided as an argument (`-t`) or as an environment variable (`SCM_TOKEN`).
   The PAT needs the following scopes for full analysis:
   `   read_api, read_user, read_repository, read_registry`
   See [Creating a Personal Access Token](https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html) for more information.  
   To run legitify against GitLab Cloud set the scm flag to gitlab `--scm gitlab`, to run against GitLab Server you need to provide also a SERVER_URL:

```sh
export SERVER_URL=""https://gitlab.example.com/""
SCM_TOKEN=<your_token> legitify analyze --namespace organization --scm gitlab
```

> **_NOTE 1:_** To ignore invalid server certificate, please pass the `ignore-invalid-certificate` flag

> **_NOTE 2:_** For non-premium GitLab accounts some policies (such as branch protection policies) will be skipped

## Namespaces

Namespaces in legitify are resources that are collected and run against the policies.
Currently, the following namespaces are supported:

1. `organization` - GitHub organization (or GitLab group) level policies (e.g., ""Two-Factor Authentication Is Not Enforced for the Organization"")
2. `actions` - organization GitHub Actions policies (e.g., ""GitHub Actions Runs Are Not Limited To Verified Actions"")
3. `member` - contributor level policies (e.g., ""Stale Admin Found"")
4. `repository` - GitHub repository (or GitLab Project) level policies (e.g., ""Code Review By At Least Two Reviewers Is Not Enforced""). Note: Archived repositories are ignored unless specified directly via the `--repo` argument.
5. `runner_group` - runner group policies (e.g, ""runner can be used by public repositories"")

By default, legitify will analyze all namespaces. You can limit only to selected ones with the `--namespace` flag, and then a comma separated list of the selected namespaces.

## Output Options

By default, legitify will output the results in a human-readable format.
This includes the list of policy violations listed by severity,
as well as a summary table that is sorted by namespace.

### Output Formats

Using the `--output-format (-f)` flag, legitify supports outputting the results in the following formats:

1. `human-readable` - Human-readable text (default).
2. `json` - Standard JSON.
3. `sarif` - SARIF format ([info](https://sarifweb.azurewebsites.net/)).

### Output Schemes

Using the `--output-scheme` flag, legitify supports outputting the results in different grouping schemes.
Note: `--output-format=json` must be specified to output non-default schemes.

1. `flattened` - No grouping; A flat listing of the policies, each with its violations (default).
2. `group-by-namespace` - Group the policies by their namespace.
3. `group-by-resource` - Group the policies by their resource e.g. specific organization/repository.
4. `group-by-severity` - Group the policies by their severity.

### Output Destinations

- `--output-file` - full path of the output file (default: no output file, prints to stdout).
- `--error-file` - full path of the error logs (default: ./error.log).

### Coloring

When outputting in a human-readable format, legitify support the conventional `--color[=when]` flag, which has the following options:

- `auto` - colored output if stdout is a terminal, uncolored otherwise (default).
- `always` - colored output regardless of the output destination.
- `none` - uncolored output regardless of the output destination.

### Misc

- Use the `--failed-only` flag to filter-out passed/skipped checks from the result.
- Use the `--ignore-policies-path $PATH` and provide a file with the policies you want to ignore to skip specific policies.
  One policy per line, e.g.
  `no_conversation_resolution
requires_status_checks                                                     ─╯`

## Scorecard Support - Only for GitHub server/cloud repositories

[Scorecard](https://github.com/ossf/scorecard) is an OSSF's open-source project:

> Scorecards is an automated tool that assesses a number of important heuristics (""checks"") associated with software security and assigns each check a score of 0-10. You can use these scores to understand specific areas to improve in order to strengthen the security posture of your project. You can also assess the risks that dependencies introduce, and make informed decisions about accepting these risks, evaluating alternative solutions, or working with the maintainers to make improvements.

legitify supports running scorecard for all of the organization's repositories, enforcing score policies and showing the results using the `--scorecard` flag:

- `no` - do not run scorecard (default).
- `yes` - run scorecard and employ a policy that alerts on each repo score below 7.0.
- `verbose` - run scorecard, employ a policy that alerts on each repo score below 7.0, and embed its output to legitify's output.

legitify runs the following scorecard checks:
|Check|Public Repository|Private Repository|
|--|--|--|
|Security-Policy|V||
|CII-Best-Practices|V||
|Fuzzing|V||
|License|V||
|Signed-Releases|V||
|Branch-Protection|V|V|
|Code-Review|V|V|
|Contributors|V|V|
|Dangerous-Workflow|V|V|
|Dependency-Update-Tool|V|V|
|Maintained|V|V|
|Pinned-Dependencies|V|V|
|SAST|V|V|
|Token-Permissions|V|V|
|Vulnerabilities|V|V|
|Webhooks|V|V|

## Policies

legitify comes with a set of policies for each SCM in the `policies/` directory.

These policies are documented [here](https://legitify.dev).

## Contribution

Thank you for considering contributing to Legitify! We encourage and appreciate any kind of contribution.
Here are some resources to help you get started:

- [Contribution Guide](https://github.com/Legit-Labs/legitify/blob/main/CONTRIBUTING.md)
- [Code of Conduct](https://github.com/Legit-Labs/legitify/blob/main/CODE_OF_CONDUCT.md)
- [Open an Issue](https://github.com/Legit-Labs/legitify/issues/new/choose)
- [Open a Pull Request](https://github.com/Legit-Labs/legitify/compare)

## Support

If you have questions about legitify or need any assistance with its operation, don't hesitate to [reach out](mailto:legitify@legitsecurity.com). Our team is committed to providing support and ensuring a smooth experience.

## Legitify vs. the Legit Security platform

If you liked Legitify, you are going to love the Legit Security Platform!

- It automates Legitify checks for the entire environments, discovers more systems and shows all results in a simple web app to manage at scale.
- Legit security is a complete CI/CD security solution together with Application Security Posture Management (ASPM) that covers application security end-to-end.
- It is a SaaS platform, built for engineering, DevOps and security teams and trusted by many leading organizaitons around the world.

Below is a comfeature parison between Legitify and Legit:

| **Capability** | **Legitify** | **Legit Security Platform** |
|---|---|---|
| Supported platforms | GitHub <br> GitLab | ALL major SCMs (incl. Azure DevOps, Bitbucket and more)<br> CI/CD systems (e.g. Jenkins) <br> Package registries (e.g. JFrog Artifactory)<br>Cloud providers (e.g. AWS) |
| Risk detection | SCM Misconfigurations only | SCMs Misconfigurations <br> CI Misconfigurations <br> CD Misconfigurations <br> Package Registries Misconfigurations <br> Pipeline risks <br> Secrets <br> IaC <br> Security Incidents <br> And more... |
| Compliance report | [OSSF SCM Best Practices](https://best.openssf.org/SCM-BestPractices/) | SSDF <br> SLSA <br> SOC2 <br> ISO 27001 <br> FedRAMP <br> And more... |
| Policy drifts detection | Can be detected periodically though Legitify's GitHub Action | Get real-time alerts when a misconfiguration is introduced |
| SDLC assets management | - | Yes |
| Issue & policy management | - | Yes |
| Code To Cloud context | - | Yes (contextualized information enables smarter prioritization) |
| Workspaces & product groups | - | Yes |
| Ticketing & alerting | - | Jira, Slack, and more |
| Ingest risk | - | Import APIs and integrations with SAST, SCA and other testing solutions |
| Rest APIs | - | Yes |

To check out Legit, visit our [website](https://www.legitsecurity.com/) or directly [book a demo](https://info.legitsecurity.com/book-a-demo)

<div align=""center"">
 <a href=""https://www.legitsecurity.com"">
  <img width=""250"" alt=""Legitify Logo"" src=""https://github.com/Legit-Labs/legitify/assets/74864790/c76dc765-e8fd-498e-ab92-1228eb5a1f2d"">
 </a>
</div>",FAUX
loft-sh/jspolicy,Toolkit,Application System,2024-06-07T15:52:25Z,2023-12-06T20:07:17Z,0,0,0,0,1,0,0,0,2021-05-03T00:24:46Z,2025-04-05T10:40:42Z,157649,387,Go,VRAI,39,FAUX,19,,19,jsPolicy - Easier & Faster Kubernetes Policies using JavaScript or TypeScript,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,26,"<br>
<a href=""https://www.jspolicy.com""><img src=""docs/static/media/jspolicy-logo-dark.svg""></a>

### **[Website](https://www.jspolicy.com)** • **[Getting Started Guide](https://www.jspolicy.com/docs/getting-started/installation)** • **[Documentation](https://www.jspolicy.com/docs/why-jspolicy)** • **[Blog](https://loft.sh/blog)** • **[Twitter](https://twitter.com/loft_sh)** • **[Slack](https://slack.loft.sh/)**

![Latest Release](https://img.shields.io/github/v/release/loft-sh/jspolicy?style=for-the-badge&label=Latest%20Release&color=%23007ec6)
![License: Apache-2.0](https://img.shields.io/github/license/loft-sh/jspolicy?style=for-the-badge&color=%23007ec6)


### jsPolicy - Easier & Faster Kubernetes Policies using JavaScript or TypeScript
- **Lightning Fast & Secure Policy Execution** - jsPolicy runs policies with Google's super fast V8 JavaScript engine in a pool of pre-heated sandbox environments. Most policies do not even take a single millisecond to execute
- **Great Language For Policies** - JavaScript is made for handling and manipulating JSON objects (short for: JavaScript Object Notation!) and Kubernetes uses JSON by converting your YAML to JSON during every API request
- **3 Policy Types** for anything you need:
  - **Validating Policies** - Request validation that is as easy as calling `allow()`, `deny(""This is not allowed"")`, or `warn(""We'll let this one slip, but upgrade to the new ingress controller"")`
  - **Mutating Policies** - Simple mutations of the kubectl request payload via `mutate(modifiedObj)`
  - **Controller Policies** - Run custom JavaScript controllers that react to any changes to the objects in your cluster (controller policies are reactive, so they are not webhooks and part of a Kubernetes API server request but instead react to `Events` in your cluster after they have happened). With controller policies you can write resource sync mechanisms, enforce objects in namespaces, garbage collectors or fully functional CRD controllers
- **Simple yet Powerful** - Create a functional webhook with a single line of JavaScript or write your own fully blown custom StatefulSet controller in TypeScript with jsPolicy. There are no limits and the possibilities are endless
- **Easy Cluster Access** - Control cluster state with built-in functions such as `get(""Pod"", ""v1"", ""my-namespace/my-pod"")`, `list(""Namespace"", ""v1"")`, `create(limitRange)`, `update(mySecret)` or `remove(configMap)`
- **Focus on Policy Logic** - Jump right in and only focus on writing your own policy logic or simply reuse existing policies. Let jsPolicy do the rest and don't worry about high-availability, performance tuning, auditing, certificate management, webhook registration, prometheus metrics, shared resource caches, controller boilerplate, dynamic policy management etc. anymore
- **Turing Complete Policy Language** - Use `loops`, `Promises`, `generator` functions, `?` operators, TypeScript Type-Safe practices, hot reloaders, linting, test frameworks and all other modern JS language features and development best practices for writing clean and easy to maintain policy code
- **Huge Ecosystem of Libraries** - Use any CommonJS JavaScript or TypeScript library from npmjs or from your private registry
- **Easy Policy Sharing & Reuse** - Share entire policies or reusable functions via npmjs or via your private registry
- **Efficient Policy Development** - Use any of the dev tools available in JavaScript or TypeScript for a highly efficient workflow


Learn more on [www.jspolicy.com](https://www.jspolicy.com).


[![Join us on Slack!](docs/static/media/slack.svg)](https://slack.loft.sh/)

<br>

## Architecture 
[![jsPolicy Architecture](docs/static/media/diagrams/jspolicy-architecture.svg)](https://www.jspolicy.com)

![jsPolicy Compatibility](docs/static/media/cluster-compatibility.png)


Learn more in the [documentation](https://www.jspolicy.com/docs/why-jspolicy).

<br>

<p align=""center"">
⭐️ <strong>Do you like jsPolicy? Support the project with a star</strong> ⭐️
</p>

<br>

## Quick Start
To learn more about jspolicy, [**open the full getting started guide**](https://www.jspolicy.com/docs/getting-started/installation).

### 1. Install jsPolicy
Install jsPolicy to your Kubernetes cluster via Helm v3:
```bash
helm install jspolicy jspolicy -n jspolicy --create-namespace --repo https://charts.loft.sh
```


### 2. Create a Policy
Create the file `policy.yaml`:
```bash
# policy.yaml
apiVersion: policy.jspolicy.com/v1beta1
kind: JsPolicy
metadata:
  name: ""deny-default-namespace.company.tld""
spec:
  operations: [""CREATE""]
  resources: [""*""]
  scope: Namespaced
  javascript: |
    if (request.namespace === ""default"") {
      deny(""Creation of resources within the default namespace is not allowed!"");
    }
```

### 3. Apply The Policy
Apply the policy in your cluster:
```bash
kubectl apply -f policy.yaml
```


### 4. See Policy In Action
```bash
kubectl create deployment nginx-deployment -n default --image=nginx
```

## Contributing

Thank you for your interest in contributing! Please refer to
[CONTRIBUTING.md](https://github.com/loft-sh/jspolicy/blob/main/CONTRIBUTING.md) for guidance.

<br>

---

This project is open-source and licensed under Apache 2.0, so you can use it in any private or commercial projects.",VRAI
Loongson-Cloud-Community/dockerfiles,Toolkit,Documentations,2025-04-24T07:36:11Z,2025-03-14T09:33:25Z,1,0,0,0,0,0,0,4,2022-07-20T02:10:33Z,2025-04-07T07:37:48Z,121205,18,Go,VRAI,17,FAUX,5,,5,,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,15,"# dockerfiles

*dockerfiles* 是龙芯容器镜像的源码仓库，您可以直接通过本项目向 [cr.loongnix.cn]
提交镜像，感谢您为龙芯生态作出的贡献。

## 仓库说明

* 仓库按照 *ORGANIZATION/REPOSITORY/VERSION* 的形式组织，例如 *rook/ceph/1.8.4*
* 仓库组织与 [Docker Hub] 保持一致，*library* 表示进入 [Docker Hub] 官方仓库的镜像
* 每一个叶子目录表示一个*项目*，单次提交仅能包含一个项目
* 对于 *a.b.c* 形式的版本，不加前缀 *v*

## 使用方法

为了规范和自动化，所有项目框架均从模板中生成，默认提供了两种
模板，*generate.sh* 和 *generate-new.sh*。

### generate.sh

__使用方法__

``` bash
./generate.sh ORGANIZATION REPOSITORY VERSION
```

该模板适用于构建和源码分离的项目，例如 *kubernetes*，可以提取
项目的 *Dockerfile* 文件，并通过 *Makefile* 构建镜像。阅读
[Makefile.template](Makefile.template) 了解详情。


### generate-new.sh

__使用方法__

``` bash
./generate-new.sh ORGANIZATION REPOSITORY VERSION
```

该模板适用于构建与源码一体的项目，使用该模板可以将源码修改
以补丁的形式打入，并通过 *Makefile* 构建镜像。阅读
[Makefile-new.template](Makefile-new.template) 了解详情。

### 注意事项

__镜像修改意见__

本仓库的ci流水线会根据生成的镜像提出修改意见，类似于：

- cr.loongnix.cn/grafana/promtail:2.8.2

|    | code        | level   | alerts                                                                                                                                          |
|---:|:------------|:--------|:------------------------------------------------------------------------------------------------------------------------------------------------|
|  0 | DKL-DI-0005 | FATAL   | Use 'rm -rf /var/lib/apt/lists' after 'apt-get install|update' : |0 /bin/sh -c apt-get update &&   apt-get install -qy   tzdata ca-certificates |
|  1 | CIS-DI-0001 | WARN    | Last user should not be root                                                                                                                    |
|  2 | CIS-DI-0005 | INFO    | export DOCKER_CONTENT_TRUST=1 before docker pull/build                                                                                          |
|  3 | CIS-DI-0006 | INFO    | not found HEALTHCHECK statement                                                                                                                 |
|  4 | CIS-DI-0008 | INFO    | setuid file: urwxr-xr-x usr/bin/chsh                                                                                                            |

您需要根据修改意见对于源码或者补丁进行修改，消除修改意见中`FATAL`级别的告警。

__镜像存在多个tag__

如果构建的镜像需要添加多个`tag`，请务必在`make image`阶段进行添加。

## 如何贡献

欢迎贡献 [CONTRIBUTING.md](CONTRIBUTING.md)


<!-- footer -->
[Docker Hub]: https://hub.docker.com
[cr.loongnix.cn]: https://cr.loongnix.cn",VRAI
luminartech/helm-charts-public,DevOPs,Documentations,2025-05-07T06:56:00Z,2025-04-15T17:48:36Z,0,0,0,0,1,0,0,0,2023-01-02T07:27:26Z,2025-03-31T22:37:00Z,1412,10,Mustache,VRAI,5,FAUX,2,"crossplane,crossplane-provider-aws,helm-chart-repository,helm-charts,infrastructure,infrastructure-as-code,infrastructure-modules,kubernetes",2,Luminar Technologies public repository for kubernetes helm charts,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,13,"# Helm-charts-public

Luminar public helm charts.

## How do I make a chage?

1. Update or create a new chart (module) in `/charts` and send a PR. Make sure that `version` field in Chart.yaml is updated accordingly since GitHub Action relies on it (see [Helm chrats version pattern](#helm-chrats-version-pattern)).
2. Once PR is merged, the GitHub Action will release chart to GitHub Packages - https://github.com/orgs/luminartech/packages?visibility=pubilc. Usually it takes about a minute.
3. Update any helm chart reference(s).

## Helm chrats version pattern

### appVersion field

It equals to the version of the main upstream dependency chart.
E.g. for karpenter chart with:

```
	...
	dependencies:
	  - name: karpenter
	    version: ""v0.33.2""
	    repository: ""oci://public.ecr.aws/karpenter""
	    condition: karpenter.enabled
	...
```

it will be `0.33.2`.

Upstream application version may be used as well, though it's not always possible. For instance, corssplane-aws-* charts may be used with multiple different versions of Crossplane provider and in such case latest tested version is used.

### version field

Contains the value of `appVersion` with helm chart release number suffix - `appVersion-X`, i.e. `0.33.2-0`.

Release number always starts with 0.

Version increment is required to trigger a pipeline that releases chart to repository on GitHub Container Registry. In other words, any change to helm chart should pair up with update of `version` field in Chart.yaml.",VRAI
m-mizutani/ghnotify,Toolkit,Application System,2024-12-14T01:21:18Z,2022-04-15T00:09:04Z,0,6,0,0,0,0,0,0,2022-02-27T01:48:45Z,2024-12-14T01:21:23Z,184,21,Go,VRAI,6,FAUX,6,"github-actions,go,slack",6,General GitHub event notification tool to Slack with Open Policy Agent and Rego,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,3,"# ghnotify

`ghnotify` is a general GitHub event notification tool to Slack with [Open Policy Agent](https://github.com/open-policy-agent/opa) and [Rego](https://www.openpolicyagent.org/docs/latest/policy-language/). There are a lot of notification tools from GitHub to Slack. However, in most case, notification rules are deeply integrated with source code implementation and customization of notification rule by end-user is limited.

`ghnotify` uses generic policy language Rego and OPA as runtime to separate implementation and policy completely. Therefore, `ghnotify` can handle and notify **all type of GitHub event**, not only issue/PR comments but also such as following events according to your Rego policy.

- GitHub Actions success/failure
- deploy key creation
- push to specified branch
- add/remove a label
- repository creation, archived, transferred
- team modification
- a new GitHub App installation

## Setup

### 1) Retrieve Bot User OAuth Token of Slack

Create your Slack bot and keep *OAuth Tokens for Your Workspace* in *OAuth & Permissions* page.

### 2) Creating Rego policy

`ghnotify` evaluates received GitHub event one by one. If `notify` variable exists in evaluation results, `ghnotify` notifies a message to Slack according to the results.

Policy rules are following.

**Input**: What data will be provided
- `input.name`: Event name. It comes from `X-GitHub-Event` header.
- `input.event`: Webhook events. See [docs](https://docs.github.com/en/developers/webhooks-and-events/webhooks) for more detail and schema.

**Result**: What data should be returned
- `notify`: Set of notification messages
    - `notify[_].channel`: Destination channel of Slack. It can be used by only API token
    - `notify[_].text`: Custom message of slack notification
    - `notify[_].body`: Custom message body
    - `notify[_].color`: Message bar color
    - `notify[_].fields`: Set of custom message fields.
        - `notify[_].fields[_].name`: Field name
        - `notify[_].fields[_].value`: Field value
        - `notify[_].fields[_].url`: Link assigned to the field

#### Example 1) Notification of ""call me"" in issue comment

```rego
package github.notify

notify[msg] {
    input.name == ""issue_comment""
    contains(input.event.comment.body, ""mizutani"")
    msg := {
        ""channel"": ""#notify-mizutani"",
        ""text"": ""Hello, mizutani"",
        ""body"": input.event.comment.body,
    }
}
```

Then, you shall get a message like following.

![](https://user-images.githubusercontent.com/605953/155864886-c9c8ccbb-809c-44df-8925-fe69a0d820f4.png)


#### Example 2) Notification of workflow (actions) failed

```rego
package github.notify

notify[msg] {
    input.name == ""workflow_run""
    input.event.action == ""completed""
    input.event.conclusion == ""failure""

    msg := {
        ""channel"": ""#notify-failure"",
        ""text"": ""workflow failed"",
        ""color"": ""#E01E5A"", # red
    }
}
```

#### Example 3) Assigned ""breaking-change"" label to PR

```rego
package github.notify

notify[msg] {
    input.name == ""pull_request""
    input.event.action == ""labeled""
    input.event.label.name == ""breaking-change""
    labels := { name | name := input.event.pull_request.labels[_].name }

    msg := {
        ""channel"": ""#notify-mizutani"",
        ""text"": ""breaking change assigned"",
        ""fields"": [
            {
                ""name"": ""All labels"",
                ""value"": concat("", "", labels),
            },
        ],
    }
}
```

## Run

### Use Case 1: As GitHub Actions

- Pros: Easy to install
- Cons: GitHub Actions can receive events from only the repository

Create GitHub Actions workflow as following.

```yaml
name: Build and publish container image

on:
  push:
  issue:
  issue_comment:

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      GHNOTIFY_SLACK_API_TOKEN: ${{ secrets.GHNOTIFY_SLACK_API_TOKEN }}
    steps:
      - name: checkout
        uses: actions/checkout@v2
      - name: dump event
        run: echo '${{ toJSON(github.event) }}' > /tmp/event.json
      - uses: docker://ghcr.io/m-mizutani/ghnotify:latest
        with:
          args: ""emit -f /tmp/event.json -t ${{ github.event_name }} --local-policy ./policy""
```

### Use Case 2: As GitHub App server

- Pros: Easy to install
- Cons: GitHub Actions can receive event on each repository, can not watch organization wide

#### Deploy `ghnotify`

Deploy `ghnotify` to your environment and prepare URL that can be accessed from public internet. I recommend [Cloud Run](https://cloud.google.com/run) of Google Cloud in the use case.

When deploying `ghnotify`, I recommend to generate and use *Webhook secret* value. Please prepare random token and provide it to `--webhook-secret`.

Callback endpoint will be `http://{hostname}:4080/webhook/github`. You can change port number by `--addr` option.

#### Create a new GitHub App

1. Go to https://github.com/settings/apps and click `New GitHub App`
2. Grant permissions and check events you want to subscribe in `Subscribe to events`.
3. Check `Active` in `Webhook` section
4. Set URL of deployed `ghnotify` to `Webhook URL`
5. Set *Webhook secret* to `Webhook secret` if you configured
6. Then click `Create GitHub App`

#### Example

## Options

- Server
    - `--addr`: Server address and port to listen webhook. e.g. `0.0.0.0:8080`
    - `--webhook-secret`: Webhook secret
- Policy (either one of `--local-policy` and `--remote-url` is required)
    - `--local-policy`: Policy files or directory.
    - `--local-package`: Package name of policy file
    - `--remote-url`: URL of OPA server
    - `--remote-header`: HTTP header to query OPA server
- Notification (either one of following is required)
    - `--slack-api-token`: API token retrieved in Step 1 (Recommended)
    - `--slack-webhook`: Incoming webhook URL of Slack

## License

Apache License 2.0",VRAI
m9sweeper/m9sweeper,Toolkit,Application System,2024-03-11T19:46:11Z,2023-10-26T18:11:51Z,0,0,0,0,0,0,0,29,2022-10-28T15:31:47Z,2025-03-28T16:22:14Z,24214,257,TypeScript,VRAI,26,FAUX,15,"containers,docker,falco,kube-bench,kube-hunter,kubernetes,kubesec,security,trivy",15,m9sweeper is a free and easy kubernetes security platform. ,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,16,"<!--
Hey, thanks for using the awesome-readme-template template.
If you have any enhancements, then fork this project and create a pull request
or just open an issue with the label ""enhancement"".

Don't forget to give this project a star for additional support ;)
Maybe you can mention me or this repo in the acknowledgements too
-->
<div align=""center"">

  <img src=""assets/logo.png"" alt=""logo"" width=""200"" height=""auto"" />
  <h1>m9sweeper</h1>

  <p>
    Kubernetes Security for Everyone!
  </p>

  <p>

</p>

<!-- Badges -->
<p>
  <a href=""https://github.com/m9sweeper/m9sweeper/graphs/contributors"">
    <img src=""https://img.shields.io/github/contributors/m9sweeper/m9sweeper"" alt=""contributors"" />
  </a>
  <a href="""">
    <img src=""https://img.shields.io/github/last-commit/m9sweeper/m9sweeper"" alt=""last update"" />
  </a>
  <a href=""https://github.com/m9sweeper/m9sweeper/network/members"">
    <img src=""https://img.shields.io/github/forks/m9sweeper/m9sweeper"" alt=""forks"" />
  </a>
  <a href=""https://github.com/m9sweeper/m9sweeper/stargazers"">
    <img src=""https://img.shields.io/github/stars/m9sweeper/m9sweeper"" alt=""stars"" />
  </a>
  <a href=""https://github.com/m9sweeper/m9sweeper/issues/"">
    <img src=""https://img.shields.io/github/issues/m9sweeper/m9sweeper"" alt=""open issues"" />
  </a>

  <a href=""https://github.com/m9sweeper/m9sweeper/blob/main/LICENSE"">
    <img src=""assets/license.svg"" alt=""license"" />
  </a>
</p>

<h4>
    <a href=""https://youtu.be/5sIqnYSZWAc/"">View Demo</a>
  <span> · </span>
    <a href=""https://m9sweeper.io/docs/latest/docs/"">Documentation</a>
  <span> · </span>
    <a href=""https://github.com/m9sweeper/m9sweeper/issues/"">Report Bug</a>
  <span> · </span>
    <a href=""https://github.com/m9sweeper/m9sweeper/issues/"">Request Feature</a>
  </h4>
</div>

<br />

<!-- Table of Contents -->

# :notebook_with_decorative_cover: Table of Contents

- [About the Project](#star2-about-the-project)

  - [Features in Action](#camera-features-in-action)
  - [Features List](#dart-features)
  - [Prerequisites](#bangbang-prerequisites)
  - [Tool Box](#toolbox-toolbox)
  - [Quick Install](#gear-installation)
  - [Code of Conduct](#scroll-code-of-conduct)

- [Contributors](#wave-contributing)
- [License](#warning-license)
- [Contact](#handshake-contact)

<!-- About the Project -->

## :star2: About the Project

m9sweeper is a free and easy kubernetes security platform. It integrates industry standard open source utilities into a one-stop-shop kubernetes security tool that can walk most kubernetes adminstrators through securing a kubernetes cluster as well as the apps running on the cluster.

<!-- Screenshots -->

### :camera: Features in Action

<div align=""center"">
  <img src=""assets/m9sweeper-features.gif"" alt=""m9sweeper-features-gif"" />
</div>

<!-- Features -->

### :dart: Features

m9sweeper makes securing a cluster easy with:

- CVE Scanning
- Enforcement of CVE Scanning Rules
- Reports and Dashboards, including historical reporting to see how your security posture has changed over time
- CIS Security Benchmarking
- Pen Testing
- Deployment Coaching
- Intrusion Detection
- Gatekeeper Policy Management

<!-- Getting Started -->

## :toolbox: Toolbox

m9sweeper makes it easy to orchestrate the implementation of a number of free security tools:

[Trivy](https://github.com/aquasecurity/trivy): CVE Scanner

[Kubesec](https://github.com/controlplaneio/kubesec): Deployment Best Practices

[kube-bench](https://github.com/aquasecurity/kube-bench): CIS Benchmarks

[OPA Gatekeeper](https://github.com/open-policy-agent/gatekeeper): Compliance and Security Policies

[kube-hunter](https://github.com/aquasecurity/kube-hunter): Cluster Penetration Testing

[Project Falco](https://falco.org/): Intrusion Detection

<!-- Prerequisites -->

### :bangbang: Prerequisites

This project requires a Kubernetes Cluster and uses helm as package manager

```bash
Installing Kubernetes Locally with Minikube
Mac: Install from terminal
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64
sudo install minikube-darwin-amd64 /usr/local/bin/minikube

Windows: Install with chocolatey and install a bash client
choco install minikube
choco install git

Both: Start Kubernetes
minikube start --cni calico --kubernetes-version=v1.23.16
kubectl get pods --all-namespaces

More at  https://minikube.sigs.k8s.io/docs/start/

```

<!-- Installation -->

### :gear: Quick Installation

While our documentation has more details, installing m9sweeper can be as simple
as running a few CLI commands to install it into your own kubernetes cluster
with helm.

    helm repo add m9sweeper https://m9sweeper.github.io/m9sweeper && \
    helm repo update && \
    helm upgrade m9sweeper m9sweeper/m9sweeper --install --wait \
      --create-namespace --namespace m9sweeper-system \
      --set-string dash.init.superAdminEmail=""super.admin@m9sweeper.io"" \
      --set-string dash.init.superAdminPassword=""password"" \
      --set-string global.jwtSecret=""changeme"" \
      --set-string global.apiKey=""YOUR-API-KEY""

<!-- Contributing -->

## :wave: Contributing

<a href=""https://github.com/vellankikoti/awesome-readme-template/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=vellankikoti/awesome-readme-template"" />
</a>

The initial project was created by team members at Intelletive Consulting at times when projects were slow or to train new members, but we hope others will contribute as well. Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jacobbeasley""><img src=""https://avatars.githubusercontent.com/u/433581?v=4?s=100"" width=""100px;"" alt=""Jacob Beasley""/><br /><sub><b>Jacob Beasley</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=jacobbeasley"" title=""Code"">💻</a> <a href=""https://github.com/m9sweeper/m9sweeper/commits?author=jacobbeasley"" title=""Documentation"">📖</a> <a href=""#research-jacobbeasley"" title=""Research"">🔬</a> <a href=""#projectManagement-jacobbeasley"" title=""Project Management"">📆</a> <a href=""#infra-jacobbeasley"" title=""Infrastructure (Hosting, Build-Tools, etc)"">🚇</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.linkedin.com/in/jason-woodman-8604476/""><img src=""assets/jason.jpeg"" width=""100px;"" alt=""jasonWoodman""/><br /><sub><b>jasonWoodman</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=jasonWoodman"" title=""Code"">💻</a> <a href=""https://github.com/m9sweeper/m9sweeper/commits?author=jasonWoodman"" title=""Documentation"">📖</a> <a href=""#research-jasonWoodman"" title=""Research"">🔬</a> <a href=""#projectManagement-jasonWoodman"" title=""Project Management"">📆</a> <a href=""#infra-jasonWoodman"" title=""Infrastructure (Hosting, Build-Tools, etc)"">🚇</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/brandan-schmitz""><img src=""https://avatars.githubusercontent.com/u/6267549?v=4?s=100"" width=""100px;"" alt=""Brandan Schmitz""/><br /><sub><b>Brandan Schmitz</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=brandan-schmitz"" title=""Code"">💻</a> <a href=""#infra-brandan-schmitz"" title=""Infrastructure (Hosting, Build-Tools, etc)"">🚇</a> <a href=""https://github.com/m9sweeper/m9sweeper/commits?author=brandan-schmitz"" title=""Documentation"">📖</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/KBerndt10""><img src=""https://avatars.githubusercontent.com/u/64435961?v=4?s=100"" width=""100px;"" alt=""KBerndt10""/><br /><sub><b>KBerndt10</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=KBerndt10"" title=""Code"">💻</a> <a href=""#infra-KBerndt10"" title=""Infrastructure (Hosting, Build-Tools, etc)"">🚇</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.linkedin.com/in/becky-saunders/""><img src=""assets/becky-saunders.jpeg"" width=""100px;"" alt=""beckysaunders94""/><br /><sub><b>beckysaunders94</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=beckysaunders94"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/sunny1304int""><img src=""https://avatars.githubusercontent.com/u/55746651?v=4?s=100"" width=""100px;"" alt=""Farhan Tanvir""/><br /><sub><b>Farhan Tanvir</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=sunny1304int"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jshoberg""><img src=""https://avatars.githubusercontent.com/u/47117895?v=4?s=100"" width=""100px;"" alt=""jshoberg""/><br /><sub><b>jshoberg</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=jshoberg"" title=""Code"">💻</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/charisprose""><img src=""assets/generic-female-icon.jpg"" width=""100px;"" alt=""charisprose""/><br /><sub><b>charisprose</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=charisprose"" title=""Code"">💻</a> <a href=""https://github.com/m9sweeper/m9sweeper/commits?author=charisprose"" title=""Tests"">⚠️</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://www.tariquemahmud.net/""><img src=""https://avatars.githubusercontent.com/u/2160924?v=4?s=100"" width=""100px;"" alt=""Gazi Tarique Mahmud""/><br /><sub><b>Gazi Tarique Mahmud</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=tarique313"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://www.intelletive-bd.com/""><img src=""https://avatars.githubusercontent.com/u/15213083?v=4?s=100"" width=""100px;"" alt=""Shibly""/><br /><sub><b>Shibly</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=sforkani"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/sabbirali""><img src=""https://avatars.githubusercontent.com/u/1538742?v=4?s=100"" width=""100px;"" alt=""sabbirali""/><br /><sub><b>sabbirali</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=sabbirali"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/GrantWK""><img src=""https://avatars.githubusercontent.com/u/17361213?v=4?s=100"" width=""100px;"" alt=""Grant Keiner""/><br /><sub><b>Grant Keiner</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=GrantWK"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/grantoenges""><img src=""https://avatars.githubusercontent.com/u/91495894?v=4?s=100"" width=""100px;"" alt=""grantoenges""/><br /><sub><b>grantoenges</b></sub></a><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author=grantoenges"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.linkedin.com/in/maggie-tian-cs/""><img src=""assets/maggie-tian.jpeg"" width=""100px;"" alt=""Maggie Tian""/><br /><sub><b>Maggie Tian</b></sub><br /><a href=""https://www.linkedin.com/in/maggie-tian-cs"" title=""Code"">💻</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><img src=""?s=100"" width=""100px;"" alt=""Rakibul Rushel""/><br /><sub><b>Rakibul Rushel</b></sub><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author="" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><img src=""?s=100"" width=""100px;"" alt=""Jobayer Ahmed""/><br /><sub><b>Jobayer Ahmed</b></sub><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author="" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><img src=""?s=100"" width=""100px;"" alt=""Steve Gagnon""/><br /><sub><b>Steve Gagnon</b></sub><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author="" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.linkedin.com/in/abm-khorshed-alam-rifat-a9945b126""><img src=""assets/khorshed-alam.jpeg"" width=""100px;"" alt=""Khorshed Alam""/><br /><sub><b>Khorshed Alam</b></sub><br /><a href=""https://www.linkedin.com/in/abm-khorshed-alam-rifat-a9945b126/"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.linkedin.com/in/vellankikoti/""><img src=""https://avatars.githubusercontent.com/u/38071840?s=100"" width=""100px;"" alt=""Koti Vellanki""/></a><br /><sub><b>Koti Vellanki</b></sub><br /><a href=""https://www.linkedin.com/in/vellankikoti/"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.linkedin.com/in/sahil-narang-bab6a2135/""><img src=""assets/sahil-narang.jpeg"" width=""100px;"" alt=""Sahil Narang""/><br /><sub><b>Sahil Narang</b></sub><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author="" title=""Code"">💻</a> <a href=""#infra"" title=""Infrastructure (Hosting, Build-Tools, etc)"">🚇</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.linkedin.com/in/shahriyasiddique/""><img src=""assets/shahriya-siddique.jpeg"" width=""100px;"" alt=""Shahriya Siddique""/><br /><sub><b>Shahriya Siddique</b></sub><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author="" title=""Code"">💻</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.linkedin.com/in/rrprodhan/""><img src=""assets/raiyan-rashid.jpeg"" width=""100px;"" alt=""Raiyan Prodhan""/><br /><sub><b>Raiyan Prodhan</b></sub><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author="" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.linkedin.com/in/kristin-sandness/""><img src=""assets/generic-female-icon.jpg"" width=""100px;"" alt=""Kristin Sandness""/><br /><sub><b>Kristin Sandness</b></sub><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author="" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.linkedin.com/in/samer-chandra-sarker-khokon-a2191b59/""><img src=""assets/samer-sarker.jpeg"" width=""100px;"" alt=""Samer Sarker""/><br /><sub><b>Samer Sarker</b></sub><br /><a href=""https://github.com/m9sweeper/m9sweeper/commits?author="" title=""Code"">💻</a></td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!

<!-- Feature Requests -->

## Feature Requests

If you have feature requests, please submit them as github issues and prefix the request with ""Feature Request:"". If you find the feature has already been requested, then please upvote that feature so we know it is a feature that others are looking for.

This helps us to prioritize further feature development based upon the needs of our users.

<!-- License -->

## :warning: License

Distributed under the Apache License v2. See LICENSE.txt for more information.

<!-- Contact -->

## :handshake: Contact

Official website - [@official_website](https://m9sweeper.io/)

Project Link: [https://github.com/m9sweeper/m9sweeper](https://github.com/m9sweeper/m9sweeper)",VRAI
magda-io/magda,Toolkit,Database,2025-05-04T11:46:24Z,2025-03-08T05:49:23Z,0,74,0,0,0,1,0,0,2016-08-23T07:27:25Z,2025-04-08T01:02:37Z,235959,531,JavaScript,VRAI,94,FAUX,369,"elasticsearch,kubernetes,nodejs,open-data,postgresql,scala",369,"A federated, open-source data catalog for all your big data and small data",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,41,"# Magda

[![GitHub release](https://img.shields.io/github/release/magda-io/magda.svg)](https://github.com/magda-io/magda/releases)
[![pipeline status](https://gitlab.com/magda-data/magda/badges/master/pipeline.svg)](https://gitlab.com/magda-data/magda/commits/master)
[![Try it out](https://img.shields.io/badge/try%20it%20out-demo.dev.magda.io-blue.svg)](https://demo.dev.magda.io)
[![GitHub Discussions](https://img.shields.io/github/discussions/magda-io/magda?label=Get%20help%20or%20discuss%20&style=plastic)](https://github.com/magda-io/magda/discussions)

Magda is a data catalog system that will provide a single place where all of an organization's data can be catalogued, enriched, searched, tracked and prioritized - whether big or small, internally or externally sourced, available as files, databases or APIs. Magda is designed specifically around the concept of _federation_ - providing a single view across all data of interest to a user, regardless of where the data is stored or where it was sourced from. The system is able to quickly crawl external data sources, track changes, make automatic enhancements and make notifications when changes occur, giving data users a one-stop shop to discover all the data that's available to them.

![Magda Search Demo](docs/assets/searchdemo420p.gif)

## Current Status

Magda is under active development by a small team - we often have to prioritise between making the open-source side of the project more robust and adding features to our own deployments, which can mean newer features aren't documented well, or require specific configuration to work. If you run into problems using Magda, we're always happy to help on [GitHub Discussions](https://github.com/magda-io/magda/discussions).

### As an open data search engine

Magda has been used in production for over a year by [data.gov.au](https://data.gov.au), and is relatively mature for use in this use case.

### As a data catalogue

Over the past 18 months, our focus has been to develop Magda into a more general-purpose data catalogue for use within organisations. If you want to use it as a data catalog, please do, but expect some rough edges! If you'd like to contribute to the project with issues or PRs, we love to recieve them.

## Features

- Powerful and scalable search based on [OpenSearch](https://opensearch.org/)
- Quick and reliable aggregation of external sources of datasets
- An unopinionated central store of metadata, able to cater for most metadata schemas
- Federated authentication via passport.js - log in via Google, Facebook, WSFed, AAF, CKAN, and easily create new providers.
- Based on Kubernetes for cloud agnosticism - deployable to nearly any cloud, on-premises, or on a local machine.
- Easy (as long as you know Kubernetes) installation and upgrades
- Extensions are based on adding new docker images to the cluster, and hence can be developed in any language

### Currently Under Development

- A heavily automated, quick and easy to use data cataloguing process intended to produce high-quality metadata for discovery
- A robust, policy-based authorization system built on Open Policy Agent - write flexible policies to restrict access to datasets and have them work across the system, including by restricting search results to what you're allowed to see.
- Storage of datasets

Our current roadmap is available at https://magda.io/docs/roadmap

## Architecture

Magda is built around a collection of microservices that are distributed as docker containers. This was done to provide easy extensibility - Magda can be customised by simply adding new services using any technology as docker images, and integrating them with the rest of the system via stable HTTP APIs. Using Helm and Kubernetes for orchestration means that configuration of a customised Magda instance can be stored and tracked as plain text, and instances with identical configuration can be quickly and easily reproduced.

![Magda Architecture Diagram](docs/assets/marketecture.svg)

If you are interested in the architecture details of Magda, you might want to have a look at [this doc](./docs/docs/architecture/Guide%20to%20Magda%20Internals.md).

### Registry

Magda revolves around the _Registry_ - an unopinionated datastore built on top of Postgres. The Registry stores _records_ as a set of JSON documents called _aspects_. For instance, a dataset is represented as a record with a number of aspects - a basic one that records the name, description and so on as well as more esoteric ones that might not be present for every dataset, like temporal coverage or determined data quality. Likewise, distributions (the actual data files, or URLs linking to them) are also modelled as records, with their own sets of aspects covering both basic metadata once again, as well as more specific aspects like whether the URL to the file worked when last tested.

Most importantly, aspects are able to be declared dynamically by other services by simply making a call with a name, description and JSON schema. This means that if you have a requirement to store extra information about a dataset or distribution you can easily do so by declaring your own aspect. Because the system isn't opinionated about what a record is beyond a set of aspects, you can also use this to add new entities to the system that link together - for instance, we've used this to store projects with a name and description that link to a number of datasets.

### Connectors

Connectors go out to external datasources and copy their metadata into the Registry, so that they can be searched and have other aspects attached to them. A connector is simply a docker-based microservice that is invoked as a [job](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/). It scans the target datasource (usually an open-data portal), then completes and shuts down. We have connectors for a number of existing open data formats, otherwise you can easily write and run your own.

### Minions

A minion is a service that listens for new records or changes to existing records, performs some kind of operation and then writes the result back to the registry. For instance, we have a broken link minion that listens for changes to distributions, retrieves the URLs described, records whether they were able to be accessed successfully and then writes that back to the registry in its own aspect.

Other aspects exist that are written to by many minions - for instance, we have a ""quality"" aspect that contains a number of different quality ratings from different sources, which are averaged out and used by search.

### Search

Datasets and distributions in the registry are ingested into an [OpenSearch](https://opensearch.org/) cluster, which indexes a few core aspects of each and exposes an API.

> Please note: since v4.0.0, we replaced the internal [ElasticSearch](https://www.elastic.co/elasticsearch) cluster with latest [OpenSearch](https://opensearch.org/) cluster. As OpenSearch is a fork of open source Elasticsearch 7.10. As such, it provides backwards REST APIs for ingest, search, and management. The query syntax and responses are also the same.

### User Interface

Magda provides a user interface, which is served from its own microservice and consumes the APIs. We're planning to make the UI itself extensible with plugins at some point in the future.

## To try the last version (with prebuilt images)

If you just want to install a local testing version, installing Magda using [Helm](https://helm.sh/) is relatively easier (you can use [minikube](https://minikube.sigs.k8s.io/docs/) to install a local k8s test cluster):

```bash
# create a namespace ""magda"" in your cluster
kubectl create namespace magda

# install Magda version v5.0.1 to namespace ""magda"", turn off openfass function and expose the service via loadBalancer
helm upgrade --namespace magda --install --version 5.0.1 --timeout 9999s --set magda-core.gateway.service.type=LoadBalancer magda oci://ghcr.io/magda-io/charts/magda
```

> Since v2, we release our helm charts to Github container registry: `oci://ghcr.io/magda-io/charts`

You can find out the load balancer IP and access it:

```bash
echo $(kubectl get svc --namespace magda gateway --template ""{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}"")
```

If you are interested in playing more, you might find useful docs from [here](./docs/docs/). Particularly:

- [Magda Charts Docs Index](./docs/docs/helm-charts-docs-index.md)
- [How to create a local testing user](./docs/docs/how-to-create-local-users.md)
- [How to set a user as admin user](./docs/docs/how-to-set-user-as-admin-user.md)

You might also want to have a look at this tutorial repo:

https://github.com/magda-io/magda-brown-bag

Or find out more on: https://magda.io/docs/building-and-running if you are interested in the development or play with the code.

## To build and run from source

https://magda.io/docs/building-and-running

## To get help with developing or running Magda

Start a discussion at https://github.com/magda-io/magda/discussions. There's not a lot on there yet, but we monitor it closely :).

## Want to get help deploying it into your organisation?

Email us at contact@magda.io.

## Want to contribute?

Great! Take a look at https://github.com/magda-io/magda/blob/master/.github/CONTRIBUTING.md :).

## Documentation links

- [Magda API Reference](https://demo.dev.magda.io/api/v0/apidocs/index.html)
- [Magda Helm Chart Reference](docs/docs/helm-charts-docs-index.md)
- [Migration & Upgrade Documents](docs/docs/migration)
- [Other documentations](docs/docs/index.md)

More documents can be found from the folder [docs/docs/](./docs/docs/).",VRAI
materialsproject/custodian,Toolkit,Toolkit,2025-05-15T22:00:35Z,2025-04-14T14:31:43Z,0,0,0,0,0,3,0,0,2013-02-04T18:33:16Z,2025-04-02T04:30:10Z,104803,152,Python,VRAI,110,FAUX,30,,30,"A simple, robust and flexible just-in-time job management framework in Python.",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,60,"![Custodian logo](https://raw.githubusercontent.com/materialsproject/custodian/master/docs/assets/custodian_logo.png)
[![GitHub license](https://img.shields.io/github/license/materialsproject/custodian)](https://github.com/materialsproject/custodian/blob/main/LICENSE)
[![Linting](https://github.com/materialsproject/custodian/workflows/Linting/badge.svg)](https://github.com/materialsproject/custodian/workflows/Linting/badge.svg)
[![Testing](https://github.com/materialsproject/custodian/actions/workflows/test.yml/badge.svg)](https://github.com/materialsproject/custodian/actions/workflows/test.yml)
[![Downloads](https://pepy.tech/badge/custodian)](https://pepy.tech/project/custodian)
[![codecov](https://codecov.io/gh/materialsproject/custodian/branch/master/graph/badge.svg?token=OwDQVJnghu)](https://codecov.io/gh/materialsproject/custodian)

Custodian is a simple, robust and flexible just-in-time (JIT) job management
framework written in Python. Using custodian, you can create wrappers that
perform error checking, job management and error recovery. It has a simple
plugin framework that allows you to develop specific job management workflows
for different applications.

Error recovery is an important aspect of many *high-throughput* projects that
generate data on a large scale. When you are running on the order of hundreds
of thousands of jobs, even an error-rate of 1% would mean thousands of errored
jobs that would be impossible to deal with on a case-by-case basis.

The specific use case for custodian is for long running jobs, with potentially
random errors. For example, there may be a script that takes several days to
run on a server, with a 1% chance of some IO error causing the job to fail.
Using custodian, one can develop a mechanism to gracefully recover from the
error, and restart the job with modified parameters if necessary.

The current version of Custodian also comes with several sub-packages for error
handling for Vienna Ab Initio Simulation Package (VASP), NwChem, QChem, FEFF, Lobster and CP2K
calculations.

# Getting custodian

## Stable version

The version at the Python Package Index (PyPI) is always the latest stable release that will be hopefully, be
relatively bug-free. Install as follows:

```sh
pip install custodian
```

Some plugins (e.g., VASP management) require additional setup (please see [pymatgen docs]).

## Developmental version

The bleeding edge developmental version is at the custodian's [Github repo](https://github.com/materialsproject/custodian). The developmental
version is likely to be more buggy, but may contain new features. The Github version include test files as well for
complete unit testing. After cloning the source, you can type

```sh
pip install -e .
```

to install the package in editable mode.

# Requirements

Custodian has no required dependencies. However, if you wish to use many of the built-in error handlers and Jobs for
VASP, NWChem, QChem, etc., you will likely need pymatgen to be installed as well.

# Usage

Please refer to the official [custodian docs] for details on how to use
custodian.

# How to cite custodian

If you use custodian in your research, especially the VASP component, please
consider citing the following work:

```txt
Shyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier,
Michael Kocher, Shreyas Cholia, Dan Gunter, Vincent Chevrier, Kristin A.
Persson, Gerbrand Ceder. *Python Materials Genomics (pymatgen) : A Robust,
Open-Source Python Library for Materials Analysis.* Computational
Materials Science, 2013, 68, 314-319. doi:10.1016/j.commatsci.2012.10.028
```

# License

Custodian is released under the MIT License. The terms of the license are as
follows:

```txt
The MIT License (MIT)
Copyright (c) 2011-2012 MIT & LBNL

Permission is hereby granted, free of charge, to any person obtaining a
copy of this software and associated documentation files (the ""Software""),
to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense,
and/or sell copies of the Software, and to permit persons to whom the
Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.
```

[pymatgen docs]: http://pymatgen.org/
[custodian docs]: https://materialsproject.github.io/custodian/",FAUX
mattclay/aws-terminator,DevOPs,Toolkit,2024-12-06T16:38:03Z,2023-04-03T18:36:07Z,0,0,0,0,1,0,0,0,2019-05-07T13:33:30Z,2024-12-06T16:38:08Z,608,25,Python,VRAI,50,FAUX,8,,8,An AWS Lambda function for cleaning up AWS resources.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,57,"# AWS Terminator

An AWS Lambda function for cleaning up AWS resources.

## Run the sanity tests

We use `tox` to run the sanity tests:

```console
$ tox
```

You can run one specific test with a `-e foo` parameter. Use `tox -av` to list them all:

```console
$ tox -e pylint
```",VRAI
Mellanox/network-operator,Toolkit,DevOPs,2025-05-14T10:50:00Z,2025-05-06T16:54:39Z,0,0,0,0,17,0,0,0,2020-06-29T11:20:04Z,2025-04-07T19:04:33Z,4809,246,Go,VRAI,60,FAUX,44,,44,NVIDIA Network Operator,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,32,"[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0)
[![Go Report Card](https://goreportcard.com/badge/github.com/Mellanox/network-operator)](https://goreportcard.com/report/github.com/Mellanox/network-operator)
[![Coverage Status](https://coveralls.io/repos/github/Mellanox/network-operator/badge.svg)](https://coveralls.io/github/Mellanox/network-operator)

- [NVIDIA Network Operator](#nvidia-network-operator)
  - [Documentation](#documentation)
  - [Prerequisites](#prerequisites)
    - [Kubernetes Node Feature Discovery (NFD)](#kubernetes-node-feature-discovery-nfd)
  - [Resource Definitions](#resource-definitions)
    - [NICClusterPolicy CRD](#nicclusterpolicy-crd)
      - [NICClusterPolicy spec:](#nicclusterpolicy-spec)
        - [Example for NICClusterPolicy resource:](#example-for-nicclusterpolicy-resource)
      - [NICClusterPolicy status](#nicclusterpolicy-status)
        - [Example Status field of a NICClusterPolicy instance](#example-status-field-of-a-nicclusterpolicy-instance)
    - [MacvlanNetwork CRD](#macvlannetwork-crd)
      - [MacvlanNetwork spec:](#macvlannetwork-spec)
        - [Example for MacvlanNetwork resource:](#example-for-macvlannetwork-resource)
    - [HostDeviceNetwork CRD](#hostdevicenetwork-crd)
      - [HostDeviceNetwork spec:](#hostdevicenetwork-spec)
        - [Example for HostDeviceNetwork resource:](#example-for-hostdevicenetwork-resource)
    - [IPoIBNetwork CRD](#ipoibnetwork-crd)
      - [IPoIBNetwork spec:](#ipoibnetwork-spec)
        - [Example for IPoIBNetwork resource:](#example-for-ipoibnetwork-resource)
  - [System Requirements](#system-requirements)
  - [Tested Network Adapters](#tested-network-adapters)
  - [Compatibility Notes](#compatibility-notes)
  - [Deployment Example](#deployment-example)
  - [Docker image](#docker-image)
  - [Driver Containers](#driver-containers)
  - [Upgrade](#upgrade)
  - [Externally Provided Configurations For Network Operator Sub-Components](#externally-provided-configurations-for-network-operator-sub-components)

<small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>Table of contents generated with markdown-toc</a></i></small>

# NVIDIA Network Operator
NVIDIA Network Operator leverages [Kubernetes CRDs](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
and [Operator SDK](https://github.com/operator-framework/operator-sdk) to manage Networking related Components in order to enable Fast networking,
RDMA and GPUDirect for workloads in a Kubernetes cluster.

The Goal of Network Operator is to manage _all_ networking related components to enable execution of
RDMA and GPUDirect RDMA workloads in a kubernetes cluster including:
* Mellanox Networking drivers to enable advanced features
* Kubernetes device plugins to provide hardware resources for fast network
* Kubernetes secondary network for Network intensive workloads

## Documentation
For more information please visit the official [documentation](https://docs.nvidia.com/networking/software/cloud-orchestration/index.html).


## Prerequisites
### Kubernetes Node Feature Discovery (NFD)
NVIDIA Network operator relies on Node labeling to get the cluster to the desired state.
[Node Feature Discovery](https://github.com/kubernetes-sigs/node-feature-discovery) `v0.13.2` or newer is deployed by default via HELM chart installation.
NFD is used to label nodes with the following labels:

- PCI vendor and device information
- RDMA capability
- GPU features*

>__NOTE__: We use [nodeFeatureRules](https://kubernetes-sigs.github.io/node-feature-discovery/v0.13/usage/custom-resources.html#nodefeaturerule) to label PCI vendor and device.This is enabled via `nfd.deployNodeFeatureRules` chart parameter.

__Example NFD worker configurations:__

```yaml
    config:
      sources:
        pci:
          deviceClassWhitelist:
          - ""0300""
          - ""0302""
          deviceLabelFields:
          - vendor
```

>\* Required for GPUDirect driver container deployment

>__NOTE__: If NFD is already deployed in the cluster, make sure to pass `--set nfd.enabled=false` to the helm install command to avoid conflicts,
and if NFD is deployed from this repo the `enableNodeFeatureApi` flag is enabled by default to have the ability to create NodeFeatureRules.

## Resource Definitions
The Operator Acts on the following CRDs:

### NICClusterPolicy CRD
CRD that defines a Cluster state for Mellanox Network devices.

>__NOTE__: The operator will act on a NicClusterPolicy instance with a predefined name ""nic-cluster-policy"", instances with different names will be ignored.

#### NICClusterPolicy spec:
NICClusterPolicy CRD Spec includes the following sub-states:
- `ofedDriver`: [OFED driver container](https://github.com/Mellanox/ofed-docker) to be deployed on Mellanox supporting nodes.
- `rdmaSharedDevicePlugin`: [RDMA shared device plugin](https://github.com/Mellanox/k8s-rdma-shared-dev-plugin)
and related configurations.
- `sriovDevicePlugin`: [SR-IOV Network Device Plugin](https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin)
    and related configurations.
- `ibKubernetes`: [InfiniBand Kubernetes](https://github.com/Mellanox/ib-kubernetes/) and related configurations. 
- `secondaryNetwork`: Specifies components to deploy in order to facilitate a secondary network in Kubernetes. It consists of the following optionally deployed components:
    - [Multus-CNI](https://github.com/intel/multus-cni): Delegate CNI plugin to support secondary networks in Kubernetes
    - CNI plugins: Currently only [containernetworking-plugins](https://github.com/containernetworking/plugins) is supported
    - [IP Over Infiniband (IPoIB) CNI Plugin](https://github.com/Mellanox/ipoib-cni): Allow users to create an IPoIB child link and move it to the pod.
    - IPAM CNI: [Whereabouts IPAM CNI](https://github.com/k8snetworkplumbingwg/whereabouts) and related configurations
- `nvIpam`: [NVIDIA Kubernetes IPAM](https://github.com/Mellanox/nvidia-k8s-ipam) and related configurations.
- `nicConfigurationOperator`: [NVIDIA NIC Configuration Operator](https://github.com/Mellanox/nic-configuration-operator) and related configuration.

>__NOTE__: Any sub-state may be omitted if it is not required for the cluster.

>__NOTE__: NVIDIA IPAM and Whereabouts IPAM plugin can be deployed simultaneously in the same cluster


##### Example for NICClusterPolicy resource:
In the example below we request OFED driver to be deployed together with RDMA shared device plugin.

```
apiVersion: mellanox.com/v1alpha1
kind: NicClusterPolicy
metadata:
  name: nic-cluster-policy
spec:
  ofedDriver:
    image: mofed
    repository: nvcr.io/nvidia/mellanox
    version: 23.04-0.5.3.3.1
    startupProbe:
      initialDelaySeconds: 10
      periodSeconds: 10
    livenessProbe:
      initialDelaySeconds: 30
      periodSeconds: 30
    readinessProbe:
      initialDelaySeconds: 10
      periodSeconds: 30
  rdmaSharedDevicePlugin:
    image: k8s-rdma-shared-dev-plugin
    repository: ghcr.io/mellanox
    version: sha-fe7f371c7e1b8315bf900f71cd25cfc1251dc775
    # The config below directly propagates to k8s-rdma-shared-device-plugin configuration.
    # Replace 'devices' with your (RDMA capable) netdevice name.
    config: |
      {
        ""configList"": [
          {
            ""resourceName"": ""rdma_shared_device_a"",
            ""rdmaHcaMax"": 63,
            ""selectors"": {
              ""vendors"": [""15b3""],
              ""deviceIDs"": [""1017""],
              ""ifNames"": [""ens2f0""]
            }
          }
        ]
      }
  secondaryNetwork:
    cniPlugins:
      image: plugins
      repository: ghcr.io/k8snetworkplumbingwg
      version: v1.2.0-amd64
    multus:
      image: multus-cni
      repository: ghcr.io/k8snetworkplumbingwg
      version: v3.9.3
      # if config is missing or empty then multus config will be automatically generated from the CNI configuration file of the master plugin (the first file in lexicographical order in cni-conf-dir)
      # config: ''
    ipamPlugin:
      image: whereabouts
      repository: ghcr.io/k8snetworkplumbingwg
      version: v0.6.1-amd64
```

Can be found at: `example/crs/mellanox.com_v1alpha1_nicclusterpolicy_cr.yaml`

NicClusterPolicy with [NVIDIA Kubernetes IPAM](https://github.com/Mellanox/nvidia-k8s-ipam) configuration

```
apiVersion: mellanox.com/v1alpha1
kind: NicClusterPolicy
metadata:
  name: nic-cluster-policy
spec:
  ofedDriver:
    image: mofed
    repository: nvcr.io/nvidia/mellanox
    version: 23.04-0.5.3.3.1
    startupProbe:
      initialDelaySeconds: 10
      periodSeconds: 10
    livenessProbe:
      initialDelaySeconds: 30
      periodSeconds: 30
    readinessProbe:
      initialDelaySeconds: 10
      periodSeconds: 30
  rdmaSharedDevicePlugin:
    image: k8s-rdma-shared-dev-plugin
    repository: ghcr.io/mellanox
    version: sha-fe7f371c7e1b8315bf900f71cd25cfc1251dc775
    # The config below directly propagates to k8s-rdma-shared-device-plugin configuration.
    # Replace 'devices' with your (RDMA capable) netdevice name.
    config: |
      {
        ""configList"": [
          {
            ""resourceName"": ""rdma_shared_device_a"",
            ""rdmaHcaMax"": 63,
            ""selectors"": {
              ""vendors"": [""15b3""],
              ""deviceIDs"": [""101b""]
            }
          }
        ]
      }
  secondaryNetwork:
    cniPlugins:
      image: plugins
      repository: ghcr.io/k8snetworkplumbingwg
      version: v1.2.0-amd64
    multus:
      image: multus-cni
      repository: ghcr.io/k8snetworkplumbingwg
      version: v3.9.3
  nvIpam:
    image: nvidia-k8s-ipam
    repository: ghcr.io/mellanox
    version: v0.1.2
    enableWebhook: false
```

Can be found at: `example/crs/mellanox.com_v1alpha1_nicclusterpolicy_cr-nvidia-ipam.yaml`

#### NICClusterPolicy status
NICClusterPolicy `status` field reflects the current state of the system.
It contains a per sub-state and a global state `status`.

The sub-state `status` indicates if the cluster has transitioned to the desired
state for that sub-state, e.g OFED driver container deployed and loaded on relevant nodes,
RDMA device plugin deployed and running on relevant nodes.

The global state reflects the logical _AND_ of each individual sub-state.

##### Example Status field of a NICClusterPolicy instance
```
status:
  appliedStates:
  - name: state-pod-security-policy
    state: ignore
  - name: state-multus-cni
    state: ready
  - name: state-container-networking-plugins
    state: ignore
  - name: state-ipoib-cni
    state: ignore
  - name: state-whereabouts-cni
    state: ready
  - name: state-OFED
    state: ready
  - name: state-SRIOV-device-plugin
    state: ignore
  - name: state-RDMA-device-plugin
    state: ready
  - name: state-ib-kubernetes
    state: ignore
  - name: state-nv-ipam-cni
    state: ready
  state: ready
```

>__NOTE__: An `ignore` State indicates that the sub-state was not defined in the custom resource
> thus it is ignored.

### MacvlanNetwork CRD
This CRD defines a MacVlan secondary network. It is translated by the Operator to a `NetworkAttachmentDefinition` instance as defined in [k8snetworkplumbingwg/multi-net-spec](https://github.com/k8snetworkplumbingwg/multi-net-spec).

#### MacvlanNetwork spec:
MacvlanNetwork CRD Spec includes the following fields:
- `networkNamespace`: Namespace for NetworkAttachmentDefinition related to this MacvlanNetwork CRD.
- `master`: Name of the host interface to enslave. Defaults to default route interface.
- `mode`: Mode of interface one of ""bridge"", ""private"", ""vepa"", ""passthru"", default ""bridge"".
- `mtu`: MTU of interface to the specified value. 0 for master's MTU.
- `ipam`: IPAM configuration to be used for this network.

##### Example for MacvlanNetwork resource:
In the example below we deploy MacvlanNetwork CRD instance with mode as bridge, MTU 1500, default route interface as master,
with resource ""rdma/rdma_shared_device_a"", that will be used to deploy NetworkAttachmentDefinition for macvlan to default namespace.


With [Whereabouts IPAM CNI](https://github.com/k8snetworkplumbingwg/whereabouts)

```
apiVersion: mellanox.com/v1alpha1
kind: MacvlanNetwork
metadata:
  name: example-macvlannetwork
spec:
  networkNamespace: ""default""
  master: ""ens2f0""
  mode: ""bridge""
  mtu: 1500
  ipam: |
    {
      ""type"": ""whereabouts"",
      ""datastore"": ""kubernetes"",
      ""kubernetes"": {
        ""kubeconfig"": ""/etc/cni/net.d/whereabouts.d/whereabouts.kubeconfig""
      },
      ""range"": ""192.168.2.225/28"",
      ""exclude"": [
       ""192.168.2.229/30"",
       ""192.168.2.236/32""
      ],
      ""log_file"" : ""/var/log/whereabouts.log"",
      ""log_level"" : ""info"",
      ""gateway"": ""192.168.2.1""
    }
```

Can be found at: `example/crs/mellanox.com_v1alpha1_macvlannetwork_cr.yaml`

With [NVIDIA Kubernetes IPAM](https://github.com/Mellanox/nvidia-k8s-ipam)

```
apiVersion: mellanox.com/v1alpha1
kind: MacvlanNetwork
metadata:
  name: example-macvlannetwork
spec:
  networkNamespace: ""default""
  master: ""ens2f0""
  mode: ""bridge""
  mtu: 1500
  ipam: |
    {
      ""type"": ""nv-ipam"",
      ""poolName"": ""my-pool""
    }
```

Can be found at: `example/crs/mellanox.com_v1alpha1_macvlannetwork_cr-nvidia-ipam.yaml`

### HostDeviceNetwork CRD
This CRD defines a HostDevice secondary network. It is translated by the Operator to a `NetworkAttachmentDefinition` instance as defined in [k8snetworkplumbingwg/multi-net-spec](https://github.com/k8snetworkplumbingwg/multi-net-spec).

#### HostDeviceNetwork spec:
HostDeviceNetwork CRD Spec includes the following fields:
- `networkNamespace`: Namespace for NetworkAttachmentDefinition related to this HostDeviceNetwork CRD.
- `resourceName`: Host device resource pool.
- `ipam`: IPAM configuration to be used for this network.

##### Example for HostDeviceNetwork resource:
In the example below we deploy HostDeviceNetwork CRD instance with ""hostdev"" resource pool, that will be used to deploy NetworkAttachmentDefinition for HostDevice network to default namespace.

```
apiVersion: mellanox.com/v1alpha1
kind: HostDeviceNetwork
metadata:
  name: example-hostdevice-network
spec:
  networkNamespace: ""default""
  resourceName: ""hostdev""
  ipam: |
    {
      ""type"": ""whereabouts"",
      ""datastore"": ""kubernetes"",
      ""kubernetes"": {
        ""kubeconfig"": ""/etc/cni/net.d/whereabouts.d/whereabouts.kubeconfig""
      },
      ""range"": ""192.168.3.225/28"",
      ""exclude"": [
       ""192.168.3.229/30"",
       ""192.168.3.236/32""
      ],
      ""log_file"" : ""/var/log/whereabouts.log"",
      ""log_level"" : ""info""
    }
```

Can be found at: `example/crs/mellanox.com_v1alpha1_hostdevicenetwork_cr.yaml`

### IPoIBNetwork CRD
This CRD defines an IPoIBNetwork secondary network. It is translated by the Operator to a `NetworkAttachmentDefinition` instance as defined in [k8snetworkplumbingwg/multi-net-spec](https://github.com/k8snetworkplumbingwg/multi-net-spec).

#### IPoIBNetwork spec:
HostDeviceNetwork CRD Spec includes the following fields:
- `networkNamespace`: Namespace for NetworkAttachmentDefinition related to this HostDeviceNetwork CRD.
- `master`: Name of the host interface to enslave.
- `ipam`: IPAM configuration to be used for this network.

##### Example for IPoIBNetwork resource:
In the example below we deploy IPoIBNetwork CRD instance with ""ibs3f1"" host interface, that will be used to deploy NetworkAttachmentDefinition for IPoIBNetwork network to default namespace.

```
apiVersion: mellanox.com/v1alpha1
kind: IPoIBNetwork
metadata:
  name: example-ipoibnetwork
spec:
  networkNamespace: ""default""
  master: ""ibs3f1""
  ipam: |
    {
      ""type"": ""whereabouts"",
      ""datastore"": ""kubernetes"",
      ""kubernetes"": {
        ""kubeconfig"": ""/etc/cni/net.d/whereabouts.d/whereabouts.kubeconfig""
      },
      ""range"": ""192.168.5.225/28"",
      ""exclude"": [
       ""192.168.6.229/30"",
       ""192.168.6.236/32""
      ],
      ""log_file"" : ""/var/log/whereabouts.log"",
      ""log_level"" : ""info"",
      ""gateway"": ""192.168.6.1""
    }

```

Can be found at: `example/crs/mellanox.com_v1alpha1_ipoibnetwork_cr.yaml`

## System Requirements
* RDMA capable hardware: Mellanox ConnectX-5 NIC or newer.
* NVIDIA GPU and driver supporting GPUDirect e.g Quadro RTX 6000/8000 or Tesla T4 or Tesla V100 or Tesla V100.
(GPU-Direct only)
* Operating Systems: Ubuntu 20.04 LTS

>__NOTE__: As more driver containers are built the operator will be able to support additional platforms.
>__NOTE__: ConnectX-6 Lx is not supported.

## Tested Network Adapters
The following Network Adapters have been tested with NVIDIA Network Operator:
* ConnectX-5
* ConnectX-6 Dx

## Compatibility Notes
* NVIDIA Network Operator is compatible with NVIDIA GPU Operator v1.5.2 and above
* Starting from v465 NVIDIA GPU driver includes a built-in nvidia_peermem module
  which is a replacement for nv_peer_mem module. NVIDIA GPU operator manages nvidia_peermem module loading.

## Deployment Example
Deployment of NVIDIA Network Operator consists of:
* Deploying NVIDIA Network Operator CRDs found under `./config/crd/bases`:
    * mellanox.com_nicclusterpolicies_crd.yaml
    * mellanox.com_macvlan_crds.yaml
    * k8s.cni.cncf.io-networkattachmentdefinitions-crd.yaml
* Deploying network operator resources by running `make deploy`
* Defining and deploying a NICClusterPolicy custom resource.
Example can be found under `./example/crs/mellanox.com_v1alpha1_nicclusterpolicy_cr.yaml`
* Defining and deploying a MacvlanNetwork custom resource.
Example can be found under `./example/crs/mellanox.com_v1alpha1_macvlannetwork_cr.yaml`

A deployment example can be found under `example` folder [here](https://github.com/Mellanox/network-operator/blob/master/example/README.md).

## Docker image
To build a container image for Network Operator use:
```bash
make image
```

To build a multi-arch image and publish to a registry use:
```bash
export REGISTRY=example.com/registry 
export IMAGE_NAME=network-operator 
export VERSION=v1.1.1 
make image-build-multiarch image-push-multiarch
```

## Driver Containers
Driver containers are essentially containers that have or yield kernel modules compatible
with the underlying kernel.
An initialization script loads the modules when the container is run (in privileged mode)
making them available to the kernel.

While this approach may seem odd. It provides a way to deliver drivers to immutable systems.

[Mellanox OFED container](https://github.com/Mellanox/ofed-docker)

Mellanox OFED driver container supports customization of its behaviour via environment variables.
This is regarded as advanced functionallity and generally should not be needed.

check [MOFED Driver Container Environment Variables](docs/mofed-container-env-vars.md)

## Upgrade
Check [Upgrade section in Helm Chart documentation](deployment/network-operator/README.md#upgrade) for details.

## Externally Provided Configurations For Network Operator Sub-Components

In most cases, Network Operator will be deployed together with the related configurations
for the various sub-components it deploys e.g. Nvidia k8s IPAM plugin, RDMA shared device plugin
or SR-IOV Network device plugin.

Specifying configuration either via Helm values when installing NVIDIA
network operator, or by specifying them when directly creating NicClusterPolicy CR.
These configurations eventually trigger the creation of a ConfigMap object in K8s.

> __Note__: It is the responsibility of the user to delete any existing configurations (ConfigMaps) if
> they were already created by the Network Operator as well as deleting his own configuration when they
> are no longer required.",VRAI
memo33/sc4pac,Toolkit,Application System,2025-05-12T18:00:26Z,2025-02-02T13:55:57Z,0,0,0,0,0,2,0,0,2023-10-08T15:07:15Z,2025-03-08T11:22:50Z,688,6,YAML,VRAI,7,FAUX,3,simcity4,3,Metadata channel of package manager Sc4pac,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,6,"sc4pac metadata
===============

Sc4pac is a package manager for SimCity 4 plugins.

This repository contains all the metadata of packages contained in the default channel.

Contribute by adding more packages in [src/yaml](src/yaml/).
This channel and other channels are built using [sc4pac-actions](https://github.com/memo33/sc4pac-actions).

For more information, visit the [website](https://memo33.github.io/sc4pac/) and the [sc4pac-tools](https://github.com/memo33/sc4pac-tools) repository
or the [sc4pac-gui](https://github.com/memo33/sc4pac-gui) repository.",VRAI
meshery/meshery,DevOPs,DevOPs,2025-05-16T03:56:19Z,2025-05-15T14:42:52Z,0,20,0,0,0,0,0,0,2018-11-14T13:41:00Z,2025-04-08T03:27:04Z,1009112,6983,JavaScript,VRAI,2193,FAUX,697,"cloud-native,cncf,control-plane,docker,gitops,golang,gsoc,hacktoberfest,infrastructure-as-code,internal-developer-platform,kubernetes,kubernetes-operator,management-plane,meshery,opa,platform-engineering,reactjs,visualization,wasm,webassembly",697,"Meshery, the cloud native manager",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,1222,"<p style=""text-align:center;"" align=""center""><a href=""https://meshery.io""><picture>
 <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/meshery/meshery/master/.github/assets/images/readme/meshery-logo-light-text-side.svg"">
 <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/meshery/meshery/master/.github/assets/images/readme/meshery-logo-dark-text-side.svg"">
<img src=""https://raw.githubusercontent.com/meshery/meshery/master/.github/assets/images/readme/meshery-logo-dark-text-side.svg""
alt=""Meshery Logo"" width=""70%"" /></picture></a><br /><br /></p>
<p align=""center"">
<a href=""https://hub.docker.com/r/meshery/meshery"" alt=""Docker pulls"">
  <img src=""https://img.shields.io/docker/pulls/layer5/meshery.svg"" /></a>
<a href=""https://github.com/issues?q=is%3Aopen+is%3Aissue+archived%3Afalse+org%3Alayer5io+org%3Ameshery+org%3Aservice-mesh-performance+org%3Aservice-mesh-patterns+org%3A+label%3A%22help+wanted%22+"" alt=""GitHub issues by-label"">
  <img src=""https://img.shields.io/github/issues/layer5io/meshery/help%20wanted.svg?color=informational"" /></a>
<a href=""https://github.com/meshery/meshery/blob/master/LICENSE"" alt=""LICENSE"">
  <img src=""https://img.shields.io/github/license/meshery/meshery?color=brightgreen"" /></a>
<a href=""https://artifacthub.io/packages/helm/meshery/meshery"" alt=""Artifact Hub Meshery"">
  <img src=""https://img.shields.io/endpoint?color=brightgreen&label=Helm%20Chart&style=plastic&url=https%3A%2F%2Fartifacthub.io%2Fbadge%2Frepository%2Fartifact-hub"" /></a>  
<a href=""https://goreportcard.com/report/github.com/meshery/meshery"" alt=""Go Report Card"">
  <img src=""https://goreportcard.com/badge/github.com/meshery/meshery"" /></a>
<a href=""https://github.com/meshery/meshery/actions"" alt=""Build Status"">
  <img src=""https://img.shields.io/github/actions/workflow/status/meshery/meshery/release-drafter.yml"" /></a>
<a href=""https://bestpractices.coreinfrastructure.org/projects/3564"" alt=""CLI Best Practices"">
  <img src=""https://bestpractices.coreinfrastructure.org/projects/3564/badge"" /></a>
<a href=""https://meshery.io/community#discussion-forums"" alt=""Discuss Users"">
  <img src=""https://img.shields.io/discourse/users?label=discuss&logo=discourse&server=https%3A%2F%2Fdiscuss.layer5.io"" /></a>
<a href=""https://slack.meshery.io"" alt=""Join Slack"">
  <img src=""https://img.shields.io/badge/Slack-@layer5.svg?logo=slack"" /></a>
<a href=""https://twitter.com/intent/follow?screen_name=mesheryio"" alt=""Twitter Follow"">
  <img src=""https://img.shields.io/twitter/follow/mesheryio.svg?label=Follow+Meshery&style=social"" /></a>
<a href=""https://github.com/meshery/meshery/releases"" alt=""Meshery Downloads"">
  <img src=""https://img.shields.io/github/downloads/meshery/meshery/total"" /></a>  
<a href=""https://gurubase.io/g/meshery"" alt=""Meshery Guru"">
  <img src=""https://img.shields.io/badge/Gurubase-Ask%20Meshery%20Guru-006BFF"" /></a>
<!-- <a href=""https://app.fossa.com/projects/git%2Bgithub.com%2Fmeshery%2Fmeshery?ref=badge_shield"" alt=""License Scan Report"">
  <img src=""https://app.fossa.com/api/projects/git%2Bgithub.com%2Fmeshery%2Fmeshery.svg?type=shield""/></a>  
  -->
</p>

<h5><p align=""center""><i>If you like Meshery, please <a href=""https://github.com/meshery/meshery/stargazers"">★</a> this repository to show your support! 🤩</i></p></h5>
<p align=""center"" >
MESHERY IS A CLOUD NATIVE COMPUTING FOUNDATION PROJECT
</p>

<div align=""center"" width=""100%"">
<img src="".github/assets/images/readme/cncf-white.svg#gh-dark-mode-only"" width=""30%"" align=""center"" />
<img src="".github/assets/images/readme/cncf-black.svg#gh-light-mode-only"" width=""30%"" align=""center"" />
</div>
<br />
<p align=""center"">
A self-service engineering platform, <a href=""https://meshery.io"">Meshery</a>, is the open source, cloud native manager that enables the design and management of all Kubernetes-based infrastructure and applications (multi-cloud). Among other features,  As an extensible platform, Meshery offers visual and collaborative GitOps, freeing you from the chains of YAML while managing Kubernetes multi-cluster deployments.
</p>
<br />

<div align=""center"" width=""100%"">
 <a href=""https://youtu.be/Do7htKrRzDA""><img src="".github/assets/images/readme/thumbnail.png"" width=""800px"" /></a>
 <p><i>Example extension. See other <a href=""https://meshery.io/extensions"">Meshery Extensions</a>.</i></p>
 <br />Try Meshery in your browser using the <a href=""https://play.meshery.io"">Cloud Native Playground</a> (<a href=""https://www.youtube.com/watch?v=034nVaQUyME&list=PL3A-A6hPO2IO_yzN83wSJJUNQActzCJvO&index=9"">teaser video</a>)
</div>

<br />
<!--
- [Functionality](#functionality)
  - [Meshery Architecture](#meshery-architecture)
  - [Join the Meshery community!](#join-the-meshery-community)
  - [Contributing](#contributing)
    - [Stargazers](#stargazers)
    - [License](#license)
-->
<!-- <p style=""clear:both;"">&nbsp;</p>
<a href=""https://meshery.io""><picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/meshery/meshery/master/.github/assets/images/readme/meshery-logo-light-text-side.svg"">
  <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/meshery/meshery/master/.github/assets/images/readme/meshery-logo-dark-text-side.svg"">
  <img alt=""Meshery - the Cloud Native Manager"" src=""https://raw.githubusercontent.com/meshery/meshery/master/.github/assets/images/readme/meshery-logo-dark-text-side.svg"" align=""left"" width=""45%"">
  </picture></a> 
<a href=""https://meshery.io"">
<h3 style=""margin:auto;""><br /><br />
  <a href=""https://docs.google.com/presentation/d/14kxjwYSJ_FyE3K_6CDEd6oq2kqwn0OSE8RDJ4H-KlKU/edit?usp=sharing""><center><i>Project Overview Presentation</i></center></a>
  <br /><br /><br />
</h3> -->

<p style=""clear:both;"">&nbsp;</p>

# Functionality

## Infrastructure Lifecycle Management

Meshery manages the configuration, deployment, and operation of your Cloud services and Kubernetes clusters while supporting hundreds of different types of cloud native infrastructure integrations. Meshery supports [300+ integrations](https//meshery.io/integrations).

<!--
<a href=""https://www.youtube.com/watch?v=034nVaQUyME""><img alt=""Meshery cloud native management"" src=""https://raw.githubusercontent.com/meshery/meshery/master/.github/assets/images/readme/meshmap.gif""  style=""margin-left:10px; margin-bottom:10px;"" width=""100%"" align=""center"" /></a>
<br /><br />-->
</p>

<a href="".github/assets/images/readme/cloud-native-integrations.png""><img alt=""Meshery Integrations"" src="".github/assets/images/readme/cloud-native-integrations.png""  style=""margin-right:10px;margin-bottom:10px;"" width=""100%"" align=""center""/></a>

Find infrastructure configuration patterns in Meshery's <a href=""https://meshery.io/catalog"">catalog of curated design templates</a> filled with configuration best practices.

## Multiple Kubernetes Clusters and Multiple Clouds

<img src =""https://meshery.io/assets/images/screens/multi-cluster-management.gif"" width=""50%"" alt=""Multi-cluster Kubernetes Manager"" loading=""lazy"" align=""center"" /><br />

Meshery provides a single pane of glass to manage multiple Kubernetes clusters across any infrastructure, including various cloud providers. Meshery enables consistent configuration, operation, and observability across your entire Kubernetes landscape.

<details><summary><h4>Dry-run your deployments</h4></summary>
Meshery leverages Kubernetes' built-in dry-run capabilities to allow you to simulate deployments without actually applying the changes to your cluster. This enables you to:

- Validate configurations: Ensure your deployment specifications (e.g., YAML manifests, Helm charts, Meshery Designs) are syntactically correct and will be accepted by the Kubernetes API server.   
- Identify potential issues: Detect errors in your configurations, such as invalid resource definitions, missing fields, or API version mismatches, before they impact your live environment.
- Preview changes: Understand the objects that Kubernetes would create or modify during a real deployment.
- Integrate with CI/CD: Incorporate dry-run as a step in your continuous integration and continuous delivery pipelines to automate pre-deployment checks and prevent faulty deployments.

By providing this dry-run functionality, Meshery helps you increase the reliability and stability of your Kubernetes deployments by catching potential problems early in the development and deployment process.
<!-- 
Assess your cloud native infrastructure configuration against deployment and operational best practices with Meshery's configuration validator. Manage your workloads with confidence. Check your Kubernetes configuration for anti-patterns and avoid common pitfalls. -->

</details>

### Visually and collaboratively manage your infrastructure

Using a GitOps-centric approach, visually and collaboratively design and manage your infrastructure and microservices. Meshery intelligently infers the manner in which each resource [interrelates](https://docs.meshery.io/concepts/logical/relationships) with each other. Meshery supports a broad variety of built-in relationships between components, which you can use to create your own custom relationships.

<img src="".github/assets/images/readme/edge_mount_relationship_pod_persistent_volume.svg"" width=""50%"" alt=""Multi-cluster Kubernetes Manager"" align=""center"" />

<details><summary><img alt=""OPA Logo"" src="".github/assets/images/readme/opa-logo.svg"" style=""margin-right:10px;"" width=""25px"" align=""left"" /><h4>Context-Aware Policies For Applications</h4></summary>
<img alt=""Meshery and Open Policy Agent Integration"" src="".github/assets/images/readme/meshery-policies-2.png"" style=""margin:auto;text-align:center"" width=""50%"" />
<p>Leverage built-in relationships to enforce configuration best practices consistently from code to Kubernetes. Customize Configure your infrastructure with confidence without needing to know or write Open Policy Agent's Rego query language.</p>
</details>

## Workspaces: Your team's Google Drive for cloud native projects

<img src="".github/assets/images/readme/workspace.gif"" width=""50%"" alt=""Multi-cluster Kubernetes Manager"" loading=""lazy"" />

Workspaces let you organize your work and serve as the central point of collaboration for you and your teams and point of access control to Environments and their resources.

<details><summary><h4>Manage your connections with Environments</h4></summary>

<img src="".github/assets/images/readme/environments.gif"" width=""50%"" alt=""Multi-cluster Kubernetes Manager"" loading=""lazy"" />
<p><a href=""https://docs.meshery.io/concepts/logical/environments"">Environments</a>  make it easier for you to manage, share, and work with a collection of resources as a group, instead of dealing with all your Connections and Credentials on an individual basis.</p>
</details>

<details><summary><h4>See changes to your infra before you merge</h4></summary>

<img src="".github/assets/images/readme/meshery-snapshot.png"" width=""50%"" alt=""Multi-cluster Kubernetes Manager"" loading=""lazy"" align=""center"" />

Get snapshots of your infrastructure directly in your PRs. Preview your deployment, view changes pull request-to-pull request and get infrastructure snapshots within your PRs by connecting Kanvas to your GitHub repositories.
</details>

<!-- <h3>Operate with configuration best practices</h3>
<br /><br />
<p>Assess your configurations against deployment and operational best practices with Meshery's configuration validator.</p>
<br /><br />

<h3>Control all of your infrastructure with mesheryctl</h3>
<br /><br />
<p>Whether managing multiple Meshery deployments, importing designs, discoverying Kubernetes clusters, do so with ease using Meshery CLI in your terminal.</p>
<br /><br /> -->

## Platform Engineering with Meshery's Extension Points

Extend Meshery as your self-service engineering platform by taking advantage of its [vast set of extensibility features](https://docs.meshery.io/extensibility), including gRPC adapters, hot-loadable Reactjs packages and Golang plugins, subscriptions on NATS topics, consumable _and_ extendable API interfaces via REST and GraphQL.The great number of extension points in Meshery make it ideal as the foundation of your internal developer platform.

<details><summary><h4>Access the Cloud Native Patterns for Kubernetes</h4></summary>

<p>Design and manage all of your cloud native infrastructure using the design configurator in Meshery or start from a template using the patterns from the <a href=""https://meshery.io/catalog"">catalog</a>.
</details>

Meshery offers robust capabilities for managing multiple tenants within a shared Kubernetes infrastructure. Meshery provides the tools and integrations necessary to create a secure, isolated, and manageable multi-tenant environments, allowing multiple teams or organizations with granular control over their role-based access controls.

Meshery's ""multi-player"" functionality refers to its collaborative features that enable multiple users to interact with and manage cloud native infrastructure simultaneously. This is primarily facilitated through Kanvas, a Meshery extension visual designer and management interface.

## Performance Management

Meshery offers load generation and performance characterization to help you assess and optimize the performance of your applications and infrastructure.

<img src="".github/assets/images/readme/performance-metrics.gif"" alt=""Multi-cluster Kubernetes Manager"" width=""50%"" />

<p>Create and reuse performance profiles for consistent characterization of the configuration of your infrastructure in context of how it performs.</p>

<details>
<summary><h4> Manage the performance of your infrastructure and its workloads</h4></summary>

<img src = "".github/assets/images/readme/service-mesh-performance-example.gif"">

Baseline and track your cloud native performance from release to release.

- Use performance profiles to track the historical performance of your workloads.
- Track your application performance from version to version.
- Understand behavioral differences between cloud native network functions.
- Compare performance across infrastructure deployments.

</details>

<details>
<summary><h4>Load Generation and Microservice Performance Characteristization</h4></summary>

<picture align=""left"">
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/layer5io/layer5/master/src/assets/images/service-mesh-performance/stacked/smp-light-text.svg""  width=""18%"" align=""left"" style=""margin-left:10px;"" />
  <img alt=""Shows an illustrated light mode meshery logo in light color mode and a dark mode meshery logo dark color mode."" src=""https://raw.githubusercontent.com/layer5io/layer5/master/src/assets/images/service-mesh-performance/stacked/smp-light-text.svg"" width=""18%"" align=""left"" style=""margin-left:10px;"" />
</picture>

- **Multiple Load Generators:** Meshery supports various load generators, including Fortio, Wrk2, and Nighthawk, allowing users to choose the tool that best suits your needs.
- **Configurable Performance Profiles:** Meshery provides a highly configurable set of load profiles with tunable facets, enabling users to generate TCP, gRPC, and HTTP load. You can customize parameters such as duration, concurrent threads, concurrent generators, and load generator type. 
- **Statistical Analysis:** Meshery performs statistical analysis on the results of performance tests, presenting data in the form of histograms with latency buckets. Understand the distribution of response times and identify potential bottlenecks.
- **Comparison of Test Results:** Meshery enables you to compare the difference in request performance (latency and throughput) between independent performance tests. Save your load test configurations as Performance Profiles, making it easy to rerun tests with the same settings and track performance variations over time.
- **Kubernetes Cluster and Workload Metrics:** - Meshery connects to one or more Prometheus servers to gather both cluster and application metrics. Meshery also integrates with Grafana, allowing you to import your existing dashboards and visualize performance data.

<p>In an effort to produce infrastructure agnostic tooling, Meshery uses the <a href=""https://smp-spec.io"">Cloud Native Performance</a> specification as a common format to capture and measure your infrastructure's performance against a universal cloud native performance index. Meshery participates in advancing cloud native infrastructure adoption through the standardization of APIs. Meshery enables you to measure the value provided by Docker, Kubernetes, or other cloud native infrastructure in the context of the overhead incurred.</p>

<!-- 

SCREENSHOT / GIF NEEDED HERE

-->

</details>

<h2><a name=""running""></a>Get Started with Meshery</h2>
<p style=""clear:both;""></p>
<!-- <img alt=""Control Kubernetes and your workloads with mesheryctl"" src="".github/assets/images/readme/mesheryctl.png""  style=""margin-left:10px; margin-bottom:10px;"" width=""50%"" align=""right"" /> -->
<h3>Using `mesheryctl`</h3>
<p>Meshery runs as a set of containers inside or outside of your Kubernetes clusters.</p>
<pre>curl -L https://meshery.io/install | bash -</pre>
<p>Use the <a href=""https://docs.meshery.io/installation/quick-start"">quick start</a> guide.</p>
<details>
  <summary><strong>See all supported platforms</strong></summary>

See the [getting started](https://meshery.io/#getting-started) section to quickly deploy Meshery on any of these supported platforms:

| Platform                                                                                                                                                                                                                             | Supported?  |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :---------: |
| <img src=""https://docs.meshery.io/assets/img/platforms/docker.svg"" width=""20"" height=""20"" vertical-align=""middle"" /> [Docker](https://docs.meshery.io/installation/docker)                                                           |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/docker.svg"" width=""20"" height=""20"" vertical-align=""middle"" /> [Docker - Docker App](https://docs.meshery.io/installation/docker)                           |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/docker.svg"" width=""20"" height=""20"" vertical-align=""middle"" /> [Docker - Docker Extension](https://docs.meshery.io/installation/docker/docker-extensiongit) |      ✔️      |
| <img src=""https://docs.meshery.io/assets/img/platforms/kubernetes.svg"" width=""20"" height=""20"" vertical-align=""middle"" /> [Kubernetes](https://docs.meshery.io/installation/kubernetes)                                               |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/aks.svg"" width=""20"" height=""20"" vertical-align=""middle"" /> [Kubernetes - AKS](https://docs.meshery.io/installation/kubernetes/aks)                         |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/docker.svg"" width=""20"" height=""20"" vertical-align=""middle"" /> [Kubernetes - Docker Desktop](https://docs.meshery.io/installation#mac-or-linux)             |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/eks.png"" width=""20"" height=""20"" vertical-align=""middle"" /> [Kubernetes - EKS](https://docs.meshery.io/installation/kubernetes/eks)                         |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/gke.png"" width=""20"" height=""20"" vertical-align=""middle"" /> [Kubernetes - GKE](https://docs.meshery.io/installation/kubernetes/gke)                         |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/helm.svg"" width=""20"" height=""20"" vertical-align=""middle"" /> [Kubernetes - Helm](https://docs.meshery.io/installation/kubernetes/helm)                      |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/kind.png"" width=""20"" height=""20"" vertical-align=""middle"" /> [Kubernetes - kind](https://docs.meshery.io/installation/kubernetes/kind)                      |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/minikube.png"" width=""20"" height=""20"" vertical-align=""middle"" /> [Kubernetes - Minikube](https://docs.meshery.io/installation/kubernetes/minikube)          |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/openshift.svg"" width=""20"" height=""20"" vertical-align=""middle"" /> [Kubernetes - OpenShift](https://docs.meshery.io/installation/kubernetes)                      |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/kubernetes.svg"" width=""20"" height=""20"" vertical-align=""middle"" /> [Kubernetes - Rancher](https://docs.meshery.io/installation/kubernetes)                      |      ✔️      |
| <img src=""https://docs.meshery.io/assets/img/platforms/linux.svg"" width=""20"" height=""20"" vertical-align=""middle"" /> [Linux](https://docs.meshery.io/installation#mac-or-linux)                                                       |      ✔️      |
| <img src=""https://docs.meshery.io/assets/img/platforms/apple.svg"" width=""20"" height=""20"" vertical-align=""middle"" /> [Mac](https://docs.meshery.io/installation#mac-or-linux)                                                         |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/homebrew.png"" width=""20"" height=""20"" vertical-align=""middle"" /> [Mac - Homebrew](https://docs.meshery.io/installation#mac-or-linux)                        |      ✔️      |
| <img src=""https://docs.meshery.io/assets/img/platforms/wsl2.png"" width=""20"" height=""20"" vertical-align=""middle"" /> [Windows](https://docs.meshery.io/installation#windows)                                                           |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/wsl2.png"" width=""20"" height=""20"" vertical-align=""middle"" /> [Scoop](https://docs.meshery.io/installation#windows)                                                                                                                                                             |      ✔️      |
| &nbsp;&nbsp;&nbsp; <img src=""https://docs.meshery.io/assets/img/platforms/wsl2.png"" width=""20"" height=""20"" vertical-align=""middle"" /> [WSL2](https://docs.meshery.io/installation/platforms/windows#wsl2)                            |      ✔️      |
| <img src=""https://docs.meshery.io/assets/img/platforms/raspberry-pi.png"" width=""20"" height=""20"" vertical-align=""middle"" /> Raspberry Pi                                                                                              | In Progress |

[Meshery documentation](https://docs.meshery.io/installation) offers thorough installation guides for your platform of choice.
 </details>

<p style=""clear:both;"">&nbsp;</p>

<div>&nbsp;</div>

## Join the Meshery community

<a name=""contributing""></a><a name=""community""></a>
Our projects are community-built and welcome collaboration. 👍 Be sure to see the <a href=""https://layer5.io/community/newcomers"">Contributor Journey Map</a> and <a href=""https://meshery.io/community#handbook"">Community Handbook</a> for a tour of resources available to you and the <a href=""https://layer5.io/community/handbook/repository-overview"">Repository Overview</a> for a cursory description of repository by technology and programming language. Jump into community <a href=""https://slack.meshery.io"">Slack</a> or <a href=""https://meshery.io/community#discussion-forums"">discussion forum</a> to participate.

<p style=""clear:both;"">
<a href =""https://meshery.io/community""><img alt=""MeshMates"" src="".github/assets/images/readme/layer5-community-sign.png"" style=""margin-right:36px; margin-bottom:7px;"" width=""140px"" align=""left"" /></a>
<h3>Find your MeshMate</h3>

<p>MeshMates are experienced Layer5 community members, who will help you learn your way around, discover live projects, and expand your community network. Connect with a Meshmate today!</p>

Learn more about the <a href=""https://meshery.io/community#meshmates"">MeshMates</a> program. <br />

</p>
<br /><br />
<div style=""display: flex; justify-content: center; align-items:center;"">
<div>
<a href=""https://meshery.io/community""><img alt=""Layer5 Cloud Native Community"" src=""https://docs.meshery.io/assets/img/readme/community.png"" width=""140px"" style=""margin-right:36px; margin-bottom:7px;"" width=""140px"" align=""left""/></a>
</div>
<div style=""width:60%; padding-left: 16px; padding-right: 16px"">
<p>
✔️ <em><strong>Join</strong></em> any or all of the weekly meetings on <a href=""https://meshery.io/calendar"">community calendar</a>.<br />
✔️ <em><strong>Watch</strong></em> community <a href=""https://www.youtube.com/@mesheryio?sub_confirmation=1"">meeting recordings</a>.<br />
✔️ <em><strong>Fill-in</strong></em> a <a href=""https://layer5.io/newcomers"">member form</a> and gain access to community resources.
<br />
✔️ <em><strong>Discuss</strong></em> in the <a href=""https://meshery.io/community#discussion-forums"">community forum</a>.<br />
✔️ <em><strong>Explore more</strong></em> in the <a href=""https://meshery.io/community#handbook"">community handbook</a>.<br />
</p>
</div><br /><br />
<div>
<a href=""https://slack.meshery.io"">
<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/meshery/meshery/master/.github/assets/images/readme/slack.svg""  width=""110px"" />
  <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/meshery/meshery/master/.github/assets/images/readme/slack.svg"" width=""110px"" />
  <img alt=""Shows an illustrated light mode meshery logo in light color mode and a dark mode meshery logo dark color mode."" src=""https://raw.githubusercontent.com/meshery/meshery/master/.github/assets/images/readme/slack.svg"" width=""110px"" align=""left"" />
</picture>
</a>
</div>
</div>
<br /><br />
<p align=""left"">
&nbsp;&nbsp;&nbsp;&nbsp; <i>Not sure where to start?</i> Grab an open issue with the <a href=""https://github.com/issues?q=is%3Aopen+is%3Aissue+archived%3Afalse+org%3Alayer5io+org%3Ameshery+org%3Alayer5labs+org%3Aservice-mesh-performance+org%3Aservice-mesh-patterns+label%3A%22help+wanted%22+"">help-wanted label</a>.
</p>
<br /><br />

<div>&nbsp;</div>

## Contributing

Please do! We're a warm and welcoming community of open source contributors. Please join. All types of contributions are welcome. Be sure to read the [Contributor Guides](https://docs.meshery.io/project/contributing) for a tour of resources available to you and how to get started.

<!-- <a href=""https://youtu.be/MXQV-i-Hkf8""><img alt=""Deploying Linkerd with Meshery"" src=""https://docs.meshery.io/assets/img/readme/deploying-linkerd-with-meshery.png"" width=""100%"" align=""center"" /></a> -->

<div>&nbsp;</div>

### Stargazers

<p align=""center"">
  <i>If you like Meshery, please <a href=""../../stargazers"">★</a> star this repository to show your support! 🤩</i>
 <br />
<a href=""../../stargazers"">
 <img align=""center"" src=""https://api.star-history.com/svg?repos=meshery/meshery&type=Date"" />
</a></p>

### License

This repository and site are available as open-source under the terms of the [Apache 2.0 License](https://opensource.org/licenses/Apache-2.0).

<!--### Community

See an <a href=""https://layer5.io/community/handbook/repository-overview"">overview of repositories</a> and projects by tech stack in the <a href=""https://meshery.io/community#handbook/"">Community Handbook</a>.

## See Meshery in Action

- [DockerCon 2020](https://docker.events.cube365.net/docker/dockercon/content/Videos/63TCCNpzDC7Xxnm8b) | ([video](https://www.youtube.com/watch?v=5BrbbKZOctw&list=PL3A-A6hPO2IN_HSU0pSfijBboiHggs5mC&index=4&t=0s), [deck](https://calcotestudios.com/talks/decks/slides-dockercon-2020-service-meshing-with-docker-desktop-and-webassembly.html))
- [Deploying Linkerd with Meshery](https://youtu.be/MXQV-i-Hkf8)
- [KubeCon EU 2019](https://kccnceu19.sched.com/event/MPf7/service-meshes-at-what-cost-lee-calcote-layer5-girish-ranganathan-solarwinds?iframe=no&w=100%&sidebar=yes&bg=no) | ([video](https://www.youtube.com/watch?v=LxP-yHrKL4M&list=PLYjO73_1efChX9NuRaU7WocTbgrfvCoPE), [deck](https://calcotestudios.com/talks/decks/slides-kubecon-eu-2019-service-meshes-at-what-cost.html))
- Istio Founders Meetup @ KubeCon EU 2019 | [deck](https://calcotestudios.com/talks/decks/slides-istio-meetup-kubecon-eu-2019-istio-at-scale-large-and-small.html)
- [Cloud Native Rejekts EU 2019](https://cfp.cloud-native.rejekts.io/cloud-native-rejekts-eu-2019/speaker/GZQTEM/) | [deck](https://calcotestudios.com/talks/decks/slides-cloud-native-rejekts-2019-evaluating-service-meshes.html)
- [DockerCon 2019 Open Source Summit](https://dockercon19.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=309149&tclass=popup#.XJxH-TOcbjI.twitter) | [deck](https://calcotestudios.com/talks/decks/slides-dockercon-2019-establishing-an-open-source-office.html), [video](https://www.docker.com/dockercon/2019-videos?watch=open-source-summit-service-mesh)
- [Container World 2019](https://tmt.knect365.com/container-world/speakers/lee-calcote) | [deck](https://calcotestudios.com/talks/decks/slides-container-world-2019-service-meshes-but-at-what-cost.html)
- [Service Mesh Day](https://servicemeshday.com/schedule.html) | [deck](https://docs.google.com/presentation/d/1HwG03okX3DHgGKbma4PL-MO7Xr9zDrjQgd05PRi9i8E/edit?usp=sharing), [video](https://youtu.be/CFj1O_uyhhs)
- [Innotech San Antonio](https://innotechsanantonio2019.sched.com/event/Lmlb/the-enterprise-path-to-service-mesh-architectures?iframe=no&w=100%&sidebar=yes&bg=no) | [deck](https://calcotestudios.com/talks/decks/slides-innotech-san-antonio-2019-the-enterprise-path-to-service-mesh.html)
- [CNCF Networking WG](https://github.com/cncf/wg-networking) | [deck](https://www.slideshare.net/leecalcote/benchmarking-service-meshes-cncf-networking-wg-141938576), [video](https://www.youtube.com/watch?v=2_JwCc-kLMA&list=PLYjO73_1efChX9NuRaU7WocTbgrfvCoPE)
-->",FAUX
mesosphere/kommander-applications,Toolkit,Database,2025-05-15T08:21:22Z,2025-04-30T21:45:11Z,0,0,0,0,1,0,0,2,2021-09-22T12:16:33Z,2025-04-07T14:54:40Z,8475,14,Go,FAUX,22,FAUX,40,,40,,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,70,"[![Coverage Status](https://coveralls.io/repos/github/mesosphere/kommander-applications/badge.svg?branch=main)](https://coveralls.io/github/mesosphere/kommander-applications?branch=main)

# kommander-applications

This repo is dedicated to storing the HelmRelease and other info needed for Kommander's Applications.

### Pre Commit

This repo uses https://pre-commit.com/ to run pre-commit hooks. Please install pre-commit and run `pre-commit install` in the root directory before committing.

### Running Tests

You can run tests with `make go-test`. If your tests do not meet a certain coverage threshold, your build will fail.

### App Image Licenses

This repo contains a list of images, `licenses.d2iq.yaml` used to keep licenses up-to-date. This list is comprised of two sections: `ignore` and `resources`. `ignore` is a list of images that should be ignored when validating the license mappings.

Due to the automation of image version bumping, the original comments were unable to be retained and are listed below by corresponding image:

| Image                                                 | Description                                                                                                                                                                                                                                                                                                             |
|-------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `docker.io/mesosphere/kommander2-kubetools`           | The image is set of tools that is not built from source code. See: https://github.com/mesosphere/kommander (dir: docker)                                                                                                                                                                                                |
| `docker.io/nginxinc/nginx-unprivileged:1.22.0-alpine` | Fossa cannot scan nginx source code (C/C++) Original mapping: <pre>- container_image: docker.io/nginxinc/nginx-unprivileged:1.22.0-alpine<br>  sources:<br>    - url: https://github.com/nginxinc/docker-nginx-unprivileged<br>      ref: 82a186f7a71ca66269dba0a3eef1fb16f9121946<br>      license_path: LICENSE</pre> |
| `docker.io/bitnami/external-dns:0.13.4-debian-11-r2`  | List of bitnami containers that were mapped to build repository source code, but not to the actual bundled software source code                                                                                                                                                                                         |
| `docker.io/mesosphere/capimate:${kommander}`          | Note that this image is within `resources` rather than `ignore`. The `capimate` source code is in `capimate` subdirectory but it shares go.mod with main konvoy2 source code. `directory: capimate`                                                                                                                     |
| `gcr.io/kubecost1/frontend`                           | Partnership                                                                                                                                                                                                                                                                                                             |
| `gcr.io/kubecost1/cost-model`                         | Partnership                                                                                                                                                                                                                                                                                                             |

### Airgapped Build Exclusions

Currently the following applications are excluded from airgapped builds:
- AI-Navigator
- NKP-Pulse

To exclude an application, please ensure the following files have been updated:
- [Exclude Airgapped](.exclude-airgapped)
- [Justfile](justfile)
- [List Images](hack/list-images.sh)

Please ensure PRs are tested with the label `open-kommander-pr` to ensure there aren't any downstream build breaks.",VRAI
metio/kube-custom-resources-rs,DevOPs,Database,2025-05-09T14:23:02Z,2025-04-12T14:02:58Z,0,0,0,0,10,0,0,4,2023-11-01T13:42:17Z,2025-04-06T20:15:20Z,75644,30,Rust,VRAI,7,FAUX,3,"custom-resource,custom-resource-definition,kubernetes",3,Kubernetes Custom Resource Bindings for Rust,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,4,"<!--
SPDX-FileCopyrightText: The kube-custom-resources-rs Authors
SPDX-License-Identifier: 0BSD
 -->

# Kubernetes Custom Resource Bindings for Rust [![Chat](https://img.shields.io/badge/matrix-%23talk.metio:matrix.org-brightgreen.svg?style=social&label=Matrix)](https://matrix.to/#/#talk.metio:matrix.org)

This repository contains [kube-rs](https://kube.rs/) compatible bindings for Kubernetes [custom resources](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/) generated with [kopium](https://github.com/kube-rs/kopium).

Feel free to add your own CRD to the [catalog](https://github.com/metio/kube-custom-resources-rs/blob/main/code-generator/src/catalog.rs)!

## Structure

Originally, this repository contained a single crate for all generated custom resources. However, crates.io has a size limit of 10MiB which we reached and thus this repository was split into multiple smaller crates, one for each custom resource group. Since there are no namespaces on crates.io, we prefix every crate in this repository with `kcr_` (kube-custom-resource) and hope that no name conflict will happen in the future.

## Custom Resources

Each group of a Kubernetes custom resource has a corresponding crate in this repository. The group of a custom resource can be seen in the `apiVersion` field of a resource, e.g.:

```yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodNetworkChaos
metadata:
  ...
```

In the above example, `chaos-mesh.org` is the group and `v1alpha1` is the version. Since Cargo imposes certain rules on how crates can be named, `.` and `-` are mapped to `_`. Therefore, the crate that contains the custom resource from the example above is called `chaos_mesh_org` and can be enabled like this (using the previously mentioned `kcr_` prefix):

```toml
[dependencies]
kcr_chaos_mesh_org = ""<version>""
```

Replace `<version>` with the latest available release of that crate. Each version within a group has a corresponding module in the associated crate, e.g. there is a module called `v1alpha1` in the crate `kcr_chaos_mesh_org`.

## Versioning

This crate uses a calendar based versioning scheme because resources in Kubernetes are versioned themselves.

Updates to all CRDs are fetched [automatically](https://github.com/metio/kube-custom-resources-rs/blob/main/.github/workflows/update-crds.yml) and released every sunday if any changes were detected.

## Available Groups

The following groups are available:

- [about.k8s.io](https://crates.io/crates/kcr_about_k8s_io)
- [acid.zalan.do](https://crates.io/crates/kcr_acid_zalan_do)
- [acme.cert-manager.io](https://crates.io/crates/kcr_acme_cert_manager_io)
- [acmpca.services.k8s.aws](https://crates.io/crates/kcr_acmpca_services_k8s_aws)
- [actions.github.com](https://crates.io/crates/kcr_actions_github_com)
- [actions.summerwind.dev](https://crates.io/crates/kcr_actions_summerwind_dev)
- [addons.cluster.x-k8s.io](https://crates.io/crates/kcr_addons_cluster_x_k8s_io)
- [agent.k8s.elastic.co](https://crates.io/crates/kcr_agent_k8s_elastic_co)
- [akri.sh](https://crates.io/crates/kcr_akri_sh)
- [amd.com](https://crates.io/crates/kcr_amd_com)
- [anywhere.eks.amazonaws.com](https://crates.io/crates/kcr_anywhere_eks_amazonaws_com)
- [apacheweb.arsenal.dev](https://crates.io/crates/kcr_apacheweb_arsenal_dev)
- [api.clever-cloud.com](https://crates.io/crates/kcr_api_clever_cloud_com)
- [api.kubemod.io](https://crates.io/crates/kcr_api_kubemod_io)
- [apicodegen.apimatic.io](https://crates.io/crates/kcr_apicodegen_apimatic_io)
- [apiextensions.crossplane.io](https://crates.io/crates/kcr_apiextensions_crossplane_io)
- [apigatewayv2.services.k8s.aws](https://crates.io/crates/kcr_apigatewayv2_services_k8s_aws)
- [apisix.apache.org](https://crates.io/crates/kcr_apisix_apache_org)
- [apm.k8s.elastic.co](https://crates.io/crates/kcr_apm_k8s_elastic_co)
- [app.k8s.io](https://crates.io/crates/kcr_app_k8s_io)
- [app.kiegroup.org](https://crates.io/crates/kcr_app_kiegroup_org)
- [app.lightbend.com](https://crates.io/crates/kcr_app_lightbend_com)
- [app.redislabs.com](https://crates.io/crates/kcr_app_redislabs_com)
- [app.terraform.io](https://crates.io/crates/kcr_app_terraform_io)
- [application-networking.k8s.aws](https://crates.io/crates/kcr_application_networking_k8s_aws)
- [applicationautoscaling.services.k8s.aws](https://crates.io/crates/kcr_applicationautoscaling_services_k8s_aws)
- [appmesh.k8s.aws](https://crates.io/crates/kcr_appmesh_k8s_aws)
- [appprotect.f5.com](https://crates.io/crates/kcr_appprotect_f5_com)
- [appprotectdos.f5.com](https://crates.io/crates/kcr_appprotectdos_f5_com)
- [apps.3scale.net](https://crates.io/crates/kcr_apps_3scale_net)
- [apps.clusternet.io](https://crates.io/crates/kcr_apps_clusternet_io)
- [apps.emqx.io](https://crates.io/crates/kcr_apps_emqx_io)
- [apps.gitlab.com](https://crates.io/crates/kcr_apps_gitlab_com)
- [apps.kubeblocks.io](https://crates.io/crates/kcr_apps_kubeblocks_io)
- [apps.kubedl.io](https://crates.io/crates/kcr_apps_kubedl_io)
- [apps.kubeedge.io](https://crates.io/crates/kcr_apps_kubeedge_io)
- [apps.m88i.io](https://crates.io/crates/kcr_apps_m88i_io)
- [apps.redhat.com](https://crates.io/crates/kcr_apps_redhat_com)
- [aquasecurity.github.io](https://crates.io/crates/kcr_aquasecurity_github_io)
- [argoproj.io](https://crates.io/crates/kcr_argoproj_io)
- [asdb.aerospike.com](https://crates.io/crates/kcr_asdb_aerospike_com)
- [atlasmap.io](https://crates.io/crates/kcr_atlasmap_io)
- [auth.ops42.org](https://crates.io/crates/kcr_auth_ops42_org)
- [authentication.stackable.tech](https://crates.io/crates/kcr_authentication_stackable_tech)
- [authzed.com](https://crates.io/crates/kcr_authzed_com)
- [automation.kubensync.com](https://crates.io/crates/kcr_automation_kubensync_com)
- [autoscaling.k8s.io](https://crates.io/crates/kcr_autoscaling_k8s_io)
- [autoscaling.karmada.io](https://crates.io/crates/kcr_autoscaling_karmada_io)
- [awx.ansible.com](https://crates.io/crates/kcr_awx_ansible_com)
- [azure.microsoft.com](https://crates.io/crates/kcr_azure_microsoft_com)
- [b3scale.infra.run](https://crates.io/crates/kcr_b3scale_infra_run)
- [b3scale.io](https://crates.io/crates/kcr_b3scale_io)
- [batch.volcano.sh](https://crates.io/crates/kcr_batch_volcano_sh)
- [beat.k8s.elastic.co](https://crates.io/crates/kcr_beat_k8s_elastic_co)
- [beegfs.csi.netapp.com](https://crates.io/crates/kcr_beegfs_csi_netapp_com)
- [binding.operators.coreos.com](https://crates.io/crates/kcr_binding_operators_coreos_com)
- [bitnami.com](https://crates.io/crates/kcr_bitnami_com)
- [bmc.tinkerbell.org](https://crates.io/crates/kcr_bmc_tinkerbell_org)
- [boskos.k8s.io](https://crates.io/crates/kcr_boskos_k8s_io)
- [bpfman.io](https://crates.io/crates/kcr_bpfman_io)
- [bus.volcano.sh](https://crates.io/crates/kcr_bus_volcano_sh)
- [cache.kubedl.io](https://crates.io/crates/kcr_cache_kubedl_io)
- [caching.ibm.com](https://crates.io/crates/kcr_caching_ibm_com)
- [camel.apache.org](https://crates.io/crates/kcr_camel_apache_org)
- [capabilities.3scale.net](https://crates.io/crates/kcr_capabilities_3scale_net)
- [capsule.clastix.io](https://crates.io/crates/kcr_capsule_clastix_io)
- [cassandra.datastax.com](https://crates.io/crates/kcr_cassandra_datastax_com)
- [ceph.rook.io](https://crates.io/crates/kcr_ceph_rook_io)
- [cert-manager.io](https://crates.io/crates/kcr_cert_manager_io)
- [certman.managed.openshift.io](https://crates.io/crates/kcr_certman_managed_openshift_io)
- [chainsaw.kyverno.io](https://crates.io/crates/kcr_chainsaw_kyverno_io)
- [chaos-mesh.org](https://crates.io/crates/kcr_chaos_mesh_org)
- [chaosblade.io](https://crates.io/crates/kcr_chaosblade_io)
- [charts.amd.com](https://crates.io/crates/kcr_charts_amd_com)
- [charts.flagsmith.com](https://crates.io/crates/kcr_charts_flagsmith_com)
- [charts.helm.k8s.io](https://crates.io/crates/kcr_charts_helm_k8s_io)
- [charts.opdev.io](https://crates.io/crates/kcr_charts_opdev_io)
- [charts.operatorhub.io](https://crates.io/crates/kcr_charts_operatorhub_io)
- [che.eclipse.org](https://crates.io/crates/kcr_che_eclipse_org)
- [chisel-operator.io](https://crates.io/crates/kcr_chisel_operator_io)
- [cilium.io](https://crates.io/crates/kcr_cilium_io)
- [claudie.io](https://crates.io/crates/kcr_claudie_io)
- [cloudformation.linki.space](https://crates.io/crates/kcr_cloudformation_linki_space)
- [cloudfront.services.k8s.aws](https://crates.io/crates/kcr_cloudfront_services_k8s_aws)
- [cloudtrail.services.k8s.aws](https://crates.io/crates/kcr_cloudtrail_services_k8s_aws)
- [cloudwatch.aws.amazon.com](https://crates.io/crates/kcr_cloudwatch_aws_amazon_com)
- [cloudwatch.services.k8s.aws](https://crates.io/crates/kcr_cloudwatch_services_k8s_aws)
- [cloudwatchlogs.services.k8s.aws](https://crates.io/crates/kcr_cloudwatchlogs_services_k8s_aws)
- [cluster.clusterpedia.io](https://crates.io/crates/kcr_cluster_clusterpedia_io)
- [cluster.ipfs.io](https://crates.io/crates/kcr_cluster_ipfs_io)
- [cluster.x-k8s.io](https://crates.io/crates/kcr_cluster_x_k8s_io)
- [clusters.clusternet.io](https://crates.io/crates/kcr_clusters_clusternet_io)
- [clustertemplate.openshift.io](https://crates.io/crates/kcr_clustertemplate_openshift_io)
- [confidentialcontainers.org](https://crates.io/crates/kcr_confidentialcontainers_org)
- [config.gatekeeper.sh](https://crates.io/crates/kcr_config_gatekeeper_sh)
- [config.grafana.com](https://crates.io/crates/kcr_config_grafana_com)
- [config.karmada.io](https://crates.io/crates/kcr_config_karmada_io)
- [config.koordinator.sh](https://crates.io/crates/kcr_config_koordinator_sh)
- [config.storageos.com](https://crates.io/crates/kcr_config_storageos_com)
- [control.k8ssandra.io](https://crates.io/crates/kcr_control_k8ssandra_io)
- [core.kubeadmiral.io](https://crates.io/crates/kcr_core_kubeadmiral_io)
- [core.linuxsuren.github.com](https://crates.io/crates/kcr_core_linuxsuren_github_com)
- [core.openfeature.dev](https://crates.io/crates/kcr_core_openfeature_dev)
- [couchbase.com](https://crates.io/crates/kcr_couchbase_com)
- [craftypath.github.io](https://crates.io/crates/kcr_craftypath_github_io)
- [crane.konveyor.io](https://crates.io/crates/kcr_crane_konveyor_io)
- [crd.projectcalico.org](https://crates.io/crates/kcr_crd_projectcalico_org)
- [data.fluid.io](https://crates.io/crates/kcr_data_fluid_io)
- [databases.schemahero.io](https://crates.io/crates/kcr_databases_schemahero_io)
- [databases.spotahome.com](https://crates.io/crates/kcr_databases_spotahome_com)
- [datadoghq.com](https://crates.io/crates/kcr_datadoghq_com)
- [dataprotection.kubeblocks.io](https://crates.io/crates/kcr_dataprotection_kubeblocks_io)
- [designer.kaoto.io](https://crates.io/crates/kcr_designer_kaoto_io)
- [devices.kubeedge.io](https://crates.io/crates/kcr_devices_kubeedge_io)
- [devops.kubesphere.io](https://crates.io/crates/kcr_devops_kubesphere_io)
- [dex.coreos.com](https://crates.io/crates/kcr_dex_coreos_com)
- [dex.gpu-ninja.com](https://crates.io/crates/kcr_dex_gpu_ninja_com)
- [digitalis.io](https://crates.io/crates/kcr_digitalis_io)
- [documentdb.services.k8s.aws](https://crates.io/crates/kcr_documentdb_services_k8s_aws)
- [druid.apache.org](https://crates.io/crates/kcr_druid_apache_org)
- [dynamodb.services.k8s.aws](https://crates.io/crates/kcr_dynamodb_services_k8s_aws)
- [ec2.services.k8s.aws](https://crates.io/crates/kcr_ec2_services_k8s_aws)
- [ecr.services.k8s.aws](https://crates.io/crates/kcr_ecr_services_k8s_aws)
- [efs.services.k8s.aws](https://crates.io/crates/kcr_efs_services_k8s_aws)
- [egressgateway.spidernet.io](https://crates.io/crates/kcr_egressgateway_spidernet_io)
- [eks.services.k8s.aws](https://crates.io/crates/kcr_eks_services_k8s_aws)
- [elasticache.services.k8s.aws](https://crates.io/crates/kcr_elasticache_services_k8s_aws)
- [elasticsearch.k8s.elastic.co](https://crates.io/crates/kcr_elasticsearch_k8s_elastic_co)
- [elbv2.k8s.aws](https://crates.io/crates/kcr_elbv2_k8s_aws)
- [emrcontainers.services.k8s.aws](https://crates.io/crates/kcr_emrcontainers_services_k8s_aws)
- [ensembleoss.io](https://crates.io/crates/kcr_ensembleoss_io)
- [enterprisesearch.k8s.elastic.co](https://crates.io/crates/kcr_enterprisesearch_k8s_elastic_co)
- [everest.percona.com](https://crates.io/crates/kcr_everest_percona_com)
- [execution.furiko.io](https://crates.io/crates/kcr_execution_furiko_io)
- [executor.testkube.io](https://crates.io/crates/kcr_executor_testkube_io)
- [expansion.gatekeeper.sh](https://crates.io/crates/kcr_expansion_gatekeeper_sh)
- [experimental.kubeblocks.io](https://crates.io/crates/kcr_experimental_kubeblocks_io)
- [extensions.istio.io](https://crates.io/crates/kcr_extensions_istio_io)
- [extensions.kubeblocks.io](https://crates.io/crates/kcr_extensions_kubeblocks_io)
- [external-secrets.io](https://crates.io/crates/kcr_external_secrets_io)
- [externaldata.gatekeeper.sh](https://crates.io/crates/kcr_externaldata_gatekeeper_sh)
- [externaldns.k8s.io](https://crates.io/crates/kcr_externaldns_k8s_io)
- [externaldns.nginx.org](https://crates.io/crates/kcr_externaldns_nginx_org)
- [flagger.app](https://crates.io/crates/kcr_flagger_app)
- [flink.apache.org](https://crates.io/crates/kcr_flink_apache_org)
- [flow.volcano.sh](https://crates.io/crates/kcr_flow_volcano_sh)
- [flows.netobserv.io](https://crates.io/crates/kcr_flows_netobserv_io)
- [fluentbit.fluent.io](https://crates.io/crates/kcr_fluentbit_fluent_io)
- [fluentd.fluent.io](https://crates.io/crates/kcr_fluentd_fluent_io)
- [flux-framework.org](https://crates.io/crates/kcr_flux_framework_org)
- [fluxcd.controlplane.io](https://crates.io/crates/kcr_fluxcd_controlplane_io)
- [forklift.konveyor.io](https://crates.io/crates/kcr_forklift_konveyor_io)
- [fossul.io](https://crates.io/crates/kcr_fossul_io)
- [frrk8s.metallb.io](https://crates.io/crates/kcr_frrk8s_metallb_io)
- [gateway.networking.k8s.io](https://crates.io/crates/kcr_gateway_networking_k8s_io)
- [gateway.networking.x-k8s.io](https://crates.io/crates/kcr_gateway_networking_x_k8s_io)
- [gateway.nginx.org](https://crates.io/crates/kcr_gateway_nginx_org)
- [getambassador.io](https://crates.io/crates/kcr_getambassador_io)
- [gitops.hybrid-cloud-patterns.io](https://crates.io/crates/kcr_gitops_hybrid_cloud_patterns_io)
- [grafana.integreatly.org](https://crates.io/crates/kcr_grafana_integreatly_org)
- [groupsnapshot.storage.k8s.io](https://crates.io/crates/kcr_groupsnapshot_storage_k8s_io)
- [hazelcast.com](https://crates.io/crates/kcr_hazelcast_com)
- [helm.sigstore.dev](https://crates.io/crates/kcr_helm_sigstore_dev)
- [helm.toolkit.fluxcd.io](https://crates.io/crates/kcr_helm_toolkit_fluxcd_io)
- [hive.openshift.io](https://crates.io/crates/kcr_hive_openshift_io)
- [hiveinternal.openshift.io](https://crates.io/crates/kcr_hiveinternal_openshift_io)
- [hnc.x-k8s.io](https://crates.io/crates/kcr_hnc_x_k8s_io)
- [hyperfoil.io](https://crates.io/crates/kcr_hyperfoil_io)
- [hyperspike.io](https://crates.io/crates/kcr_hyperspike_io)
- [iam.services.k8s.aws](https://crates.io/crates/kcr_iam_services_k8s_aws)
- [ibmcloud.ibm.com](https://crates.io/crates/kcr_ibmcloud_ibm_com)
- [image.toolkit.fluxcd.io](https://crates.io/crates/kcr_image_toolkit_fluxcd_io)
- [imaging-ingestion.alvearie.org](https://crates.io/crates/kcr_imaging_ingestion_alvearie_org)
- [inference.kubedl.io](https://crates.io/crates/kcr_inference_kubedl_io)
- [infinispan.org](https://crates.io/crates/kcr_infinispan_org)
- [infra.contrib.fluxcd.io](https://crates.io/crates/kcr_infra_contrib_fluxcd_io)
- [infrastructure.cluster.x-k8s.io](https://crates.io/crates/kcr_infrastructure_cluster_x_k8s_io)
- [install.istio.io](https://crates.io/crates/kcr_install_istio_io)
- [installation.mattermost.com](https://crates.io/crates/kcr_installation_mattermost_com)
- [instana.io](https://crates.io/crates/kcr_instana_io)
- [integration.rock8s.com](https://crates.io/crates/kcr_integration_rock8s_com)
- [iot.eclipse.org](https://crates.io/crates/kcr_iot_eclipse_org)
- [ipam.cluster.x-k8s.io](https://crates.io/crates/kcr_ipam_cluster_x_k8s_io)
- [ipam.metal3.io](https://crates.io/crates/kcr_ipam_metal3_io)
- [isindir.github.com](https://crates.io/crates/kcr_isindir_github_com)
- [jaegertracing.io](https://crates.io/crates/kcr_jaegertracing_io)
- [jenkins.io](https://crates.io/crates/kcr_jenkins_io)
- [jetstream.nats.io](https://crates.io/crates/kcr_jetstream_nats_io)
- [jobset.x-k8s.io](https://crates.io/crates/kcr_jobset_x_k8s_io)
- [jobsmanager.raczylo.com](https://crates.io/crates/kcr_jobsmanager_raczylo_com)
- [k6.io](https://crates.io/crates/kcr_k6_io)
- [k8gb.absa.oss](https://crates.io/crates/kcr_k8gb_absa_oss)
- [k8s.keycloak.org](https://crates.io/crates/kcr_k8s_keycloak_org)
- [k8s.mariadb.com](https://crates.io/crates/kcr_k8s_mariadb_com)
- [k8s.nginx.org](https://crates.io/crates/kcr_k8s_nginx_org)
- [k8s.otterize.com](https://crates.io/crates/kcr_k8s_otterize_com)
- [k8up.io](https://crates.io/crates/kcr_k8up_io)
- [kafka.banzaicloud.io](https://crates.io/crates/kcr_kafka_banzaicloud_io)
- [kafka.services.k8s.aws](https://crates.io/crates/kcr_kafka_services_k8s_aws)
- [kafka.strimzi.io](https://crates.io/crates/kcr_kafka_strimzi_io)
- [kamaji.clastix.io](https://crates.io/crates/kcr_kamaji_clastix_io)
- [karpenter.k8s.aws](https://crates.io/crates/kcr_karpenter_k8s_aws)
- [karpenter.sh](https://crates.io/crates/kcr_karpenter_sh)
- [keda.sh](https://crates.io/crates/kcr_keda_sh)
- [keycloak.k8s.reddec.net](https://crates.io/crates/kcr_keycloak_k8s_reddec_net)
- [keycloak.org](https://crates.io/crates/kcr_keycloak_org)
- [keyspaces.services.k8s.aws](https://crates.io/crates/kcr_keyspaces_services_k8s_aws)
- [kibana.k8s.elastic.co](https://crates.io/crates/kcr_kibana_k8s_elastic_co)
- [kinesis.services.k8s.aws](https://crates.io/crates/kcr_kinesis_services_k8s_aws)
- [kmm.sigs.x-k8s.io](https://crates.io/crates/kcr_kmm_sigs_x_k8s_io)
- [kms.services.k8s.aws](https://crates.io/crates/kcr_kms_services_k8s_aws)
- [kom.kaiso.github.io](https://crates.io/crates/kcr_kom_kaiso_github_io)
- [kuadrant.io](https://crates.io/crates/kcr_kuadrant_io)
- [kube-green.com](https://crates.io/crates/kcr_kube_green_com)
- [kubean.io](https://crates.io/crates/kcr_kubean_io)
- [kubecost.com](https://crates.io/crates/kcr_kubecost_com)
- [kubevious.io](https://crates.io/crates/kcr_kubevious_io)
- [kueue.x-k8s.io](https://crates.io/crates/kcr_kueue_x_k8s_io)
- [kuma.io](https://crates.io/crates/kcr_kuma_io)
- [kustomize.toolkit.fluxcd.io](https://crates.io/crates/kcr_kustomize_toolkit_fluxcd_io)
- [kyverno.io](https://crates.io/crates/kcr_kyverno_io)
- [lambda.services.k8s.aws](https://crates.io/crates/kcr_lambda_services_k8s_aws)
- [leaksignal.com](https://crates.io/crates/kcr_leaksignal_com)
- [lerentis.uploadfilter24.eu](https://crates.io/crates/kcr_lerentis_uploadfilter24_eu)
- [limitador.kuadrant.io](https://crates.io/crates/kcr_limitador_kuadrant_io)
- [listeners.stackable.tech](https://crates.io/crates/kcr_listeners_stackable_tech)
- [litmuschaos.io](https://crates.io/crates/kcr_litmuschaos_io)
- [logging-extensions.banzaicloud.io](https://crates.io/crates/kcr_logging_extensions_banzaicloud_io)
- [logging.banzaicloud.io](https://crates.io/crates/kcr_logging_banzaicloud_io)
- [logstash.k8s.elastic.co](https://crates.io/crates/kcr_logstash_k8s_elastic_co)
- [loki.grafana.com](https://crates.io/crates/kcr_loki_grafana_com)
- [longhorn.io](https://crates.io/crates/kcr_longhorn_io)
- [m4e.krestomat.io](https://crates.io/crates/kcr_m4e_krestomat_io)
- [machine-deletion-remediation.medik8s.io](https://crates.io/crates/kcr_machine_deletion_remediation_medik8s_io)
- [maps.k8s.elastic.co](https://crates.io/crates/kcr_maps_k8s_elastic_co)
- [mariadb.mmontes.io](https://crates.io/crates/kcr_mariadb_mmontes_io)
- [mariadb.persistentsys](https://crates.io/crates/kcr_mariadb_persistentsys)
- [marin3r.3scale.net](https://crates.io/crates/kcr_marin3r_3scale_net)
- [mattermost.com](https://crates.io/crates/kcr_mattermost_com)
- [memorydb.services.k8s.aws](https://crates.io/crates/kcr_memorydb_services_k8s_aws)
- [metacontroller.k8s.io](https://crates.io/crates/kcr_metacontroller_k8s_io)
- [metal3.io](https://crates.io/crates/kcr_metal3_io)
- [metallb.io](https://crates.io/crates/kcr_metallb_io)
- [microcks.github.io](https://crates.io/crates/kcr_microcks_github_io)
- [minio.min.io](https://crates.io/crates/kcr_minio_min_io)
- [mirrors.kts.studio](https://crates.io/crates/kcr_mirrors_kts_studio)
- [model.kubedl.io](https://crates.io/crates/kcr_model_kubedl_io)
- [monitoring.coreos.com](https://crates.io/crates/kcr_monitoring_coreos_com)
- [monitoring.giantswarm.io](https://crates.io/crates/kcr_monitoring_giantswarm_io)
- [monocle.monocle.change-metrics.io](https://crates.io/crates/kcr_monocle_monocle_change_metrics_io)
- [mq.services.k8s.aws](https://crates.io/crates/kcr_mq_services_k8s_aws)
- [multicluster.crd.antrea.io](https://crates.io/crates/kcr_multicluster_crd_antrea_io)
- [multicluster.x-k8s.io](https://crates.io/crates/kcr_multicluster_x_k8s_io)
- [mutations.gatekeeper.sh](https://crates.io/crates/kcr_mutations_gatekeeper_sh)
- [nativestor.alauda.io](https://crates.io/crates/kcr_nativestor_alauda_io)
- [netchecks.io](https://crates.io/crates/kcr_netchecks_io)
- [networkfirewall.services.k8s.aws](https://crates.io/crates/kcr_networkfirewall_services_k8s_aws)
- [networking.gke.io](https://crates.io/crates/kcr_networking_gke_io)
- [networking.istio.io](https://crates.io/crates/kcr_networking_istio_io)
- [networking.k8s.aws](https://crates.io/crates/kcr_networking_k8s_aws)
- [networking.karmada.io](https://crates.io/crates/kcr_networking_karmada_io)
- [nfd.k8s-sigs.io](https://crates.io/crates/kcr_nfd_k8s_sigs_io)
- [nfd.kubernetes.io](https://crates.io/crates/kcr_nfd_kubernetes_io)
- [nodeinfo.volcano.sh](https://crates.io/crates/kcr_nodeinfo_volcano_sh)
- [notebook.kubedl.io](https://crates.io/crates/kcr_notebook_kubedl_io)
- [notification.toolkit.fluxcd.io](https://crates.io/crates/kcr_notification_toolkit_fluxcd_io)
- [objectbucket.io](https://crates.io/crates/kcr_objectbucket_io)
- [objectstorage.k8s.io](https://crates.io/crates/kcr_objectstorage_k8s_io)
- [ocmagent.managed.openshift.io](https://crates.io/crates/kcr_ocmagent_managed_openshift_io)
- [onepassword.com](https://crates.io/crates/kcr_onepassword_com)
- [opensearch.opster.io](https://crates.io/crates/kcr_opensearch_opster_io)
- [opensearchservice.services.k8s.aws](https://crates.io/crates/kcr_opensearchservice_services_k8s_aws)
- [opentelemetry.io](https://crates.io/crates/kcr_opentelemetry_io)
- [operations.kubeblocks.io](https://crates.io/crates/kcr_operations_kubeblocks_io)
- [operations.kubeedge.io](https://crates.io/crates/kcr_operations_kubeedge_io)
- [operator.aquasec.com](https://crates.io/crates/kcr_operator_aquasec_com)
- [operator.authorino.kuadrant.io](https://crates.io/crates/kcr_operator_authorino_kuadrant_io)
- [operator.cluster.x-k8s.io](https://crates.io/crates/kcr_operator_cluster_x_k8s_io)
- [operator.cryostat.io](https://crates.io/crates/kcr_operator_cryostat_io)
- [operator.marin3r.3scale.net](https://crates.io/crates/kcr_operator_marin3r_3scale_net)
- [operator.open-cluster-management.io](https://crates.io/crates/kcr_operator_open_cluster_management_io)
- [operator.shipwright.io](https://crates.io/crates/kcr_operator_shipwright_io)
- [operator.tekton.dev](https://crates.io/crates/kcr_operator_tekton_dev)
- [operator.tigera.io](https://crates.io/crates/kcr_operator_tigera_io)
- [operator.victoriametrics.com](https://crates.io/crates/kcr_operator_victoriametrics_com)
- [oracle.db.anthosapis.com](https://crates.io/crates/kcr_oracle_db_anthosapis_com)
- [org.eclipse.che](https://crates.io/crates/kcr_org_eclipse_che)
- [organizations.services.k8s.aws](https://crates.io/crates/kcr_organizations_services_k8s_aws)
- [parameters.kubeblocks.io](https://crates.io/crates/kcr_parameters_kubeblocks_io)
- [perses.dev](https://crates.io/crates/kcr_perses_dev)
- [pgv2.percona.com](https://crates.io/crates/kcr_pgv2_percona_com)
- [pipes.services.k8s.aws](https://crates.io/crates/kcr_pipes_services_k8s_aws)
- [pkg.crossplane.io](https://crates.io/crates/kcr_pkg_crossplane_io)
- [policies.kyverno.io](https://crates.io/crates/kcr_policies_kyverno_io)
- [policy.clusterpedia.io](https://crates.io/crates/kcr_policy_clusterpedia_io)
- [policy.karmada.io](https://crates.io/crates/kcr_policy_karmada_io)
- [policy.kubeedge.io](https://crates.io/crates/kcr_policy_kubeedge_io)
- [policy.networking.k8s.io](https://crates.io/crates/kcr_policy_networking_k8s_io)
- [postgres-operator.crunchydata.com](https://crates.io/crates/kcr_postgres_operator_crunchydata_com)
- [postgresql.cnpg.io](https://crates.io/crates/kcr_postgresql_cnpg_io)
- [projectcontour.io](https://crates.io/crates/kcr_projectcontour_io)
- [prometheusservice.services.k8s.aws](https://crates.io/crates/kcr_prometheusservice_services_k8s_aws)
- [ps.percona.com](https://crates.io/crates/kcr_ps_percona_com)
- [psmdb.percona.com](https://crates.io/crates/kcr_psmdb_percona_com)
- [ptp.openshift.io](https://crates.io/crates/kcr_ptp_openshift_io)
- [pxc.percona.com](https://crates.io/crates/kcr_pxc_percona_com)
- [quay.redhat.com](https://crates.io/crates/kcr_quay_redhat_com)
- [quota.codeflare.dev](https://crates.io/crates/kcr_quota_codeflare_dev)
- [ray.io](https://crates.io/crates/kcr_ray_io)
- [rbacmanager.reactiveops.io](https://crates.io/crates/kcr_rbacmanager_reactiveops_io)
- [rc.app.stacks](https://crates.io/crates/kcr_rc_app_stacks)
- [rds.services.k8s.aws](https://crates.io/crates/kcr_rds_services_k8s_aws)
- [redhatcop.redhat.io](https://crates.io/crates/kcr_redhatcop_redhat_io)
- [registry.apicur.io](https://crates.io/crates/kcr_registry_apicur_io)
- [registry.devfile.io](https://crates.io/crates/kcr_registry_devfile_io)
- [reliablesyncs.kubeedge.io](https://crates.io/crates/kcr_reliablesyncs_kubeedge_io)
- [remediation.medik8s.io](https://crates.io/crates/kcr_remediation_medik8s_io)
- [repo-manager.pulpproject.org](https://crates.io/crates/kcr_repo_manager_pulpproject_org)
- [reports.kyverno.io](https://crates.io/crates/kcr_reports_kyverno_io)
- [reports.x-k8s.io](https://crates.io/crates/kcr_reports_x_k8s_io)
- [resources.teleport.dev](https://crates.io/crates/kcr_resources_teleport_dev)
- [rocketmq.apache.org](https://crates.io/crates/kcr_rocketmq_apache_org)
- [route53.services.k8s.aws](https://crates.io/crates/kcr_route53_services_k8s_aws)
- [route53resolver.services.k8s.aws](https://crates.io/crates/kcr_route53resolver_services_k8s_aws)
- [rules.kubeedge.io](https://crates.io/crates/kcr_rules_kubeedge_io)
- [runtime.cluster.x-k8s.io](https://crates.io/crates/kcr_runtime_cluster_x_k8s_io)
- [s3.services.k8s.aws](https://crates.io/crates/kcr_s3_services_k8s_aws)
- [s3.snappcloud.io](https://crates.io/crates/kcr_s3_snappcloud_io)
- [sagemaker.services.k8s.aws](https://crates.io/crates/kcr_sagemaker_services_k8s_aws)
- [scheduling.koordinator.sh](https://crates.io/crates/kcr_scheduling_koordinator_sh)
- [scheduling.sigs.k8s.io](https://crates.io/crates/kcr_scheduling_sigs_k8s_io)
- [scheduling.volcano.sh](https://crates.io/crates/kcr_scheduling_volcano_sh)
- [schemas.schemahero.io](https://crates.io/crates/kcr_schemas_schemahero_io)
- [scylla.scylladb.com](https://crates.io/crates/kcr_scylla_scylladb_com)
- [secretgenerator.mittwald.de](https://crates.io/crates/kcr_secretgenerator_mittwald_de)
- [secrets-store.csi.x-k8s.io](https://crates.io/crates/kcr_secrets_store_csi_x_k8s_io)
- [secrets.crossplane.io](https://crates.io/crates/kcr_secrets_crossplane_io)
- [secrets.doppler.com](https://crates.io/crates/kcr_secrets_doppler_com)
- [secrets.hashicorp.com](https://crates.io/crates/kcr_secrets_hashicorp_com)
- [secrets.stackable.tech](https://crates.io/crates/kcr_secrets_stackable_tech)
- [secretsmanager.services.k8s.aws](https://crates.io/crates/kcr_secretsmanager_services_k8s_aws)
- [secscan.quay.redhat.com](https://crates.io/crates/kcr_secscan_quay_redhat_com)
- [security-profiles-operator.x-k8s.io](https://crates.io/crates/kcr_security_profiles_operator_x_k8s_io)
- [security.istio.io](https://crates.io/crates/kcr_security_istio_io)
- [self-node-remediation.medik8s.io](https://crates.io/crates/kcr_self_node_remediation_medik8s_io)
- [sematext.com](https://crates.io/crates/kcr_sematext_com)
- [servicebinding.io](https://crates.io/crates/kcr_servicebinding_io)
- [servicemesh.cisco.com](https://crates.io/crates/kcr_servicemesh_cisco_com)
- [services.k8s.aws](https://crates.io/crates/kcr_services_k8s_aws)
- [serving.kubedl.io](https://crates.io/crates/kcr_serving_kubedl_io)
- [sfn.services.k8s.aws](https://crates.io/crates/kcr_sfn_services_k8s_aws)
- [site.superedge.io](https://crates.io/crates/kcr_site_superedge_io)
- [slinky.slurm.net](https://crates.io/crates/kcr_slinky_slurm_net)
- [slo.koordinator.sh](https://crates.io/crates/kcr_slo_koordinator_sh)
- [sloth.slok.dev](https://crates.io/crates/kcr_sloth_slok_dev)
- [snapscheduler.backube](https://crates.io/crates/kcr_snapscheduler_backube)
- [snapshot.storage.k8s.io](https://crates.io/crates/kcr_snapshot_storage_k8s_io)
- [sns.services.k8s.aws](https://crates.io/crates/kcr_sns_services_k8s_aws)
- [sonataflow.org](https://crates.io/crates/kcr_sonataflow_org)
- [source.toolkit.fluxcd.io](https://crates.io/crates/kcr_source_toolkit_fluxcd_io)
- [sparkoperator.k8s.io](https://crates.io/crates/kcr_sparkoperator_k8s_io)
- [spv.no](https://crates.io/crates/kcr_spv_no)
- [sqs.services.k8s.aws](https://crates.io/crates/kcr_sqs_services_k8s_aws)
- [sriovnetwork.openshift.io](https://crates.io/crates/kcr_sriovnetwork_openshift_io)
- [status.gatekeeper.sh](https://crates.io/crates/kcr_status_gatekeeper_sh)
- [storage.kubeblocks.io](https://crates.io/crates/kcr_storage_kubeblocks_io)
- [storageos.com](https://crates.io/crates/kcr_storageos_com)
- [sts.min.io](https://crates.io/crates/kcr_sts_min_io)
- [stunner.l7mp.io](https://crates.io/crates/kcr_stunner_l7mp_io)
- [submariner.io](https://crates.io/crates/kcr_submariner_io)
- [superset.stackable.tech](https://crates.io/crates/kcr_superset_stackable_tech)
- [telemetry.istio.io](https://crates.io/crates/kcr_telemetry_istio_io)
- [templates.gatekeeper.sh](https://crates.io/crates/kcr_templates_gatekeeper_sh)
- [tempo.grafana.com](https://crates.io/crates/kcr_tempo_grafana_com)
- [temporal.io](https://crates.io/crates/kcr_temporal_io)
- [tests.testkube.io](https://crates.io/crates/kcr_tests_testkube_io)
- [tf.tungsten.io](https://crates.io/crates/kcr_tf_tungsten_io)
- [theketch.io](https://crates.io/crates/kcr_theketch_io)
- [tinkerbell.org](https://crates.io/crates/kcr_tinkerbell_org)
- [topology.node.k8s.io](https://crates.io/crates/kcr_topology_node_k8s_io)
- [topolvm.cybozu.com](https://crates.io/crates/kcr_topolvm_cybozu_com)
- [trace.kubeblocks.io](https://crates.io/crates/kcr_trace_kubeblocks_io)
- [traefik.io](https://crates.io/crates/kcr_traefik_io)
- [training.kubedl.io](https://crates.io/crates/kcr_training_kubedl_io)
- [trident.netapp.io](https://crates.io/crates/kcr_trident_netapp_io)
- [trino.stackable.tech](https://crates.io/crates/kcr_trino_stackable_tech)
- [troubleshoot.sh](https://crates.io/crates/kcr_troubleshoot_sh)
- [trust.cert-manager.io](https://crates.io/crates/kcr_trust_cert_manager_io)
- [upgrade.cattle.io](https://crates.io/crates/kcr_upgrade_cattle_io)
- [upgrade.managed.openshift.io](https://crates.io/crates/kcr_upgrade_managed_openshift_io)
- [velero.io](https://crates.io/crates/kcr_velero_io)
- [virt.virtink.smartx.com](https://crates.io/crates/kcr_virt_virtink_smartx_com)
- [volsync.backube](https://crates.io/crates/kcr_volsync_backube)
- [vpcresources.k8s.aws](https://crates.io/crates/kcr_vpcresources_k8s_aws)
- [wgpolicyk8s.io](https://crates.io/crates/kcr_wgpolicyk8s_io)
- [wildfly.org](https://crates.io/crates/kcr_wildfly_org)
- [work.karmada.io](https://crates.io/crates/kcr_work_karmada_io)
- [workload.codeflare.dev](https://crates.io/crates/kcr_workload_codeflare_dev)
- [workloads.kubeblocks.io](https://crates.io/crates/kcr_workloads_kubeblocks_io)
- [workspace.maistra.io](https://crates.io/crates/kcr_workspace_maistra_io)
- [zonecontrol.k8s.aws](https://crates.io/crates/kcr_zonecontrol_k8s_aws)
- [zookeeper.pravega.io](https://crates.io/crates/kcr_zookeeper_pravega_io)
- [zookeeper.stackable.tech](https://crates.io/crates/kcr_zookeeper_stackable_tech)",VRAI
MicrosoftEdge/WebView2Samples,Documentations,Documentations,2025-05-12T10:44:41Z,2024-07-23T17:18:47Z,1,0,0,0,0,0,0,0,2019-05-03T21:33:28Z,2025-04-04T03:32:01Z,24631,920,C++,VRAI,493,FAUX,65,,65,Microsoft Edge WebView2 samples,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,33,"# WebView2 Samples

Welcome to the WebView2Samples repo.  This repo contains several types of samples for [WebView2](https://learn.microsoft.com/microsoft-edge/webview2/):

*  Getting Started tutorial projects - Completed Visual Studio projects that result from following the steps in the [Getting Started tutorials](https://learn.microsoft.com/microsoft-edge/webview2/get-started/get-started).  These are like Hello World basic apps.

*  Sample apps - WebView2 sample apps for various frameworks and platforms, as Visual Studio projects.  These samples have menus and demonstrate various APIs.  For more information, see [Sample apps](https://learn.microsoft.com/microsoft-edge/webview2/code-samples-links).

*  Deployment samples - Samples that demonstrate deploying the WebView2 Runtime.  For more information, see [Deployment samples](https://learn.microsoft.com/microsoft-edge/webview2/samples/deployment-samples).


## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.


## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft Trademark and Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.",FAUX
microsoft/all-things-azure,Documentations,Documentations,2025-02-28T01:10:39Z,2024-10-16T17:59:19Z,0,0,0,0,1,0,0,0,2024-10-16T17:59:18Z,2025-04-04T02:00:28Z,19505,13,Python,VRAI,8,FAUX,1,,1,,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,7,"# 🌐 All Things Azure

[![GitHub Stars](https://img.shields.io/github/stars/microsoft/all-things-azure?style=social)](https://github.com/microsoft/all-things-azure/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/microsoft/all-things-azure?style=social)](https://github.com/microsoft/all-things-azure/network/members)
[![GitHub Contributors](https://img.shields.io/github/contributors/microsoft/all-things-azure)](https://github.com/microsoft/all-things-azure/graphs/contributors)
[![License: MIT](https://img.shields.io/github/license/microsoft/all-things-azure)](./LICENSE)

## Introduction

**All Things Azure** is a Microsoft Developer Blog series focusing on developer how-tos, use cases, and solutions on Microsoft Azure. This repository serves as the companion to the blog posts on *All Things Azure*, providing sample code, demos, and resources that complement the articles. Many blog posts in *All Things Azure* link to this repo for hands-on examples, enabling readers to explore the concepts discussed in the articles in a practical way.

## Repository Purpose

- **💡 Blog Companion** – The content here directly **complements the [All Things Azure blog](https://devblogs.microsoft.com/all-things-azure/)**. Each folder or file in this repository corresponds to one or more blog posts, offering code and configurations as an extension of the written tutorials.
- **🛠️ Sample Code & Demos** – You’ll find working sample projects, scripts, and demonstrations for various Azure scenarios (AI, app development, DevOps, cloud architecture, and more). These examples are intended to help you jump-start your own projects by showing how to implement solutions described on the blog.
- **📁 Resources & Guides** – In some cases, the repository includes detailed guides or configuration files referenced in the blog. For instance, if a blog post outlines a solution, the accompanying content here might include deployment templates, code notebooks, or configuration YAML/JSON to replicate that solution.

## 🔗 Related Blog Posts and Samples

Below are a few featured blog posts from *All Things Azure* and the corresponding resources available in this repository. We highly recommend reading the blog posts for full context, then using this repo to dive into the implementation:

- **Agentic AI Explained: A Philosophical Framework for Understanding AI Agents** – *(All Things Azure blog post by David Barkol)*. This article explores how to create AI agents with personas inspired by philosophers.  
  📌 *Code Sample*: See the [`agentic-philosophers/`](./agentic-philosophers) folder for the sample implementation. It contains code (using Semantic Kernel) to bring Socrates, Plato, and Aristotle agent personas to life. *(Read the blog post here for background: [Agentic AI Explained](https://devblogs.microsoft.com/all-things-azure/agentic-philosophers/).)*

- **Multi-Cluster Layer 4 Load Balancing with Azure Fleet Manager** – *(All Things Azure blog post by Diogo Casati)*. This guide shows how to set up cross-cluster layer-4 load balancing for Azure Kubernetes Service (AKS) using Fleet Manager, improving high availability and traffic distribution across multiple clusters.  
  📌 *Guide*: Refer to the markdown guide [`multi-cluster-layer-4-load-balancing-with-fleet-manager.md`](./multi-cluster-layer-4-load-balancing-with-fleet-manager.md) in this repo. It provides step-by-step configuration details and code snippets for implementing the multi-cluster load balancer as described in the blog. *(Read the full article on the blog for conceptual background and context.)*

- **Project Stormbreaker – Running HPC Simulations on AKS** – *(Upcoming All Things Azure post by the App Dev GBB Team)*. This example demonstrates deploying and running heavy simulation models (e.g., ADCIRC/SWAN for coastal simulations) on Azure Kubernetes Service using Terraform and containerized workloads.  
  📌 *Resources*: Check out the [`project-stormbreaker/`](./project-stormbreaker) directory for Terraform scripts, Kubernetes manifests, and documentation to reproduce the simulation environment. These resources correspond to the blog’s scenario of running complex **HPC workloads on Azure** at scale, enabling you to follow along with the deployment steps.

- **Visualize ROI of GitHub Copilot Usage** – *(All Things Azure blog post by Xuefeng Yin)*. This guide explains how to visualize the return on investment (ROI) of using GitHub Copilot in your projects.  
  📌 *Submodule*: The [`visualize-roi-of-ghcp-usage`](https://github.com/satomic/copilot-usage-advanced-dashboard) directory is a submodule pointing to the original repository. It contains tools and dashboards to help you measure and visualize the impact of GitHub Copilot on your development workflow. *(Read the full article on the blog for detailed instructions and insights.)*

> **Note:** More samples will be added over time as new posts are published on the *All Things Azure* blog. Be sure to watch ★ this repository for updates. We aim to keep the code up-to-date with Azure’s evolving services and best practices.

## 🚀 Getting Started 

1. **Browse the Blog** – Visit the [All Things Azure blog](https://devblogs.microsoft.com/all-things-azure/) and find a post that interests you. Each post provides the background, problem scenario, and a walkthrough of the solution.
2. **Clone the Repo** – Grab the code by cloning this repository: `git clone https://github.com/microsoft/all-things-azure.git`. You can also download it as a ZIP if you prefer.
3. **Find the Relevant Folder** – Locate the folder or file in the repo that matches the blog post. (The names are usually similar to the blog title or topic, as listed above.) Inside, you’ll find the sample code, scripts, or configuration files referenced in the article.
4. **Read Instructions** – Many sample folders include their own README or comments to help you run the code. Make sure to read any provided instructions. The blog post itself often serves as a guide, so keep it open for reference.
5. **Run and Explore** – Follow the steps to deploy or execute the sample in your Azure environment. You may need an Azure subscription and certain Azure services enabled (the blog post will mention prerequisites). Feel free to experiment with the code and adapt it to your needs.
6. **Provide Feedback** – If something doesn’t work as expected or you have ideas for improvement, let us know! You can open an issue in this repo or comment on the blog post. Collaboration is welcome.

## 🤝 Contributing

We welcome contributions to enhance the samples or add new ones! If you have a fix or a new example to share, please open a pull request. Before contributing, review the following:

- **Contributor License Agreement (CLA)** – For any significant contributions, you'll be prompted to sign a CLA, per the Microsoft Open Source guidelines. This is an electronic form to ensure you agree to the terms. (You only need to do this once for all Microsoft repos.) More details here: [Microsoft CLA](https://cla.opensource.microsoft.com).
- **Code of Conduct** – Please read and respect our [Code of Conduct](./CODE_OF_CONDUCT.md). We adhere to the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct), and we expect all contributors to do the same. Let's keep our community respectful and inclusive.
- **Project Support** – See [SUPPORT.md](./SUPPORT.md) for how to get help or report issues. You can use GitHub Issues to report bugs or suggest enhancements.

When submitting a PR, make sure to detail the purpose of the change and which blog post (if any) it relates to. This helps the maintainers review and integrate it more easily. 🎉

## 📫 Support & Feedback

If you need help with a sample or have questions:

- **Blog Comments** – The fastest way might be to ask on the blog post itself. Authors and the community might respond with guidance.
- **GitHub Issues** – Feel free to open an [issue](https://github.com/microsoft/all-things-azure/issues) in this repository. We’ll do our best to respond in a timely manner.
- **Email/Contact** – If the blog or repo points to a contact (e.g., via a Microsoft alias or contact form), you can reach out that way for specific queries. (Check the blog’s footer or the SUPPORT.md for contact info.)

We appreciate feedback! It helps us improve both the blog content and the sample code here.

## 🔒 License

All code in this repository is released under the [MIT License](./LICENSE). This means you are free to use, modify, and distribute the samples in your own projects. We kindly ask that you include a reference back to this repo or the All Things Azure blog when using significant portions of the code, so others can find the original source. 

## 🙏 Acknowledgments

This repository is maintained by the contributors of the **All Things Azure** blog – a team of Microsoft Cloud Advocates, Azure Global Black Belt engineers, and other Azure experts. The goal is to share real-world Azure solutions with the developer community in an open and collaborative way. 

Special thanks to all the blog post authors and code contributors who have made this content possible. Each sample includes attribution to its original author(s) where applicable. We’re excited to see the community build upon these examples and create even more amazing Azure solutions.

Happy Azure coding! ☁️🚀 Read the latest from **All Things Azure** on the [official blog](https://devblogs.microsoft.com/all-things-azure/) and feel free to join the conversation there.",FAUX
microsoft/azuredatastudio,Toolkit,Application System,2025-05-15T22:28:10Z,2025-03-18T22:35:23Z,0,0,0,0,0,0,9,0,2017-11-02T01:00:13Z,2025-04-07T18:04:15Z,755518,7665,TypeScript,VRAI,932,FAUX,2548,"azure,azure-data-studio,electron,microsoft,postgresql,sql,sql-server,typescript",2548,"Azure Data Studio is a data management and development tool with connectivity to popular cloud and on-premises databases. Azure Data Studio supports Windows, macOS, and Linux, with immediate capability to connect to Azure SQL and SQL Server. Browse the extension library for more database support options including MySQL, PostgreSQL, and MongoDB.",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,199,"# Azure Data Studio

> [!IMPORTANT]
> Azure Data Studio will be retired on **February 28, 2026**.  [Read more](https://aka.ms/ads-retirement)

----

[![Join the chat at https://gitter.im/Microsoft/sqlopsstudio](https://badges.gitter.im/Microsoft/sqlopsstudio.svg)](https://gitter.im/Microsoft/sqlopsstudio?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Build Status](https://dev.azure.com/ms/azuredatastudio/_apis/build/status/AzureDataStudio-Localization-CI?branchName=main)](https://dev.azure.com/ms/azuredatastudio/_build/latest?definitionId=453&branchName=main)
[![Twitter Follow](https://img.shields.io/twitter/follow/azuredatastudio?style=social)](https://twitter.com/azuredatastudio)

Azure Data Studio is a data management and development tool with connectivity to popular cloud and on-premises databases. Azure Data Studio supports Windows, macOS, and Linux, with immediate capability to connect to Azure SQL and SQL Server.  Browse the [extension library](https://github.com/microsoft/azuredatastudio/wiki/List-of-Extensions) for additional database support options including MySQL, PostgreSQL, and MongoDB.

## **Download the latest Azure Data Studio release**

|Platform |Type             |Download |
| --------|-----------------|----------------------- |
|Windows  |User Installer   |[64 bit][win-user]&emsp;[ARM][win-user-arm64] |
|         |System Installer |[64 bit][win-system]&emsp;[ARM][win-system-arm64] |
|         |.zip             |[64 bit][win-zip]&emsp;[ARM][win-zip-arm64] |
|Linux    |.tar.gz          |[64 bit][linux-zip] |
|         |.deb             |[64 bit][linux-deb] |
|         |.rpm             |[64 bit][linux-rpm] |
|Mac      |.zip             |[Universal][osx-universal]&emsp;[Intel Chip][osx-zip]&emsp;[Apple Silicon][osx-arm64] |

[win-user]: https://azuredatastudio-update.azurewebsites.net/latest/win32-x64-user/stable
[win-system]: https://azuredatastudio-update.azurewebsites.net/latest/win32-x64/stable
[win-zip]: https://azuredatastudio-update.azurewebsites.net/latest/win32-x64-archive/stable
[win-user-arm64]: https://azuredatastudio-update.azurewebsites.net/latest/win32-arm64-user/stable
[win-system-arm64]: https://azuredatastudio-update.azurewebsites.net/latest/win32-arm64/stable
[win-zip-arm64]: https://azuredatastudio-update.azurewebsites.net/latest/win32-arm64-archive/stable
[linux-zip]: https://azuredatastudio-update.azurewebsites.net/latest/linux-x64/stable
[linux-deb]: https://azuredatastudio-update.azurewebsites.net/latest/linux-deb-x64/stable
[linux-rpm]: https://azuredatastudio-update.azurewebsites.net/latest/linux-rpm-x64/stable
[osx-universal]: https://azuredatastudio-update.azurewebsites.net/latest/darwin-universal/stable
[osx-zip]: https://azuredatastudio-update.azurewebsites.net/latest/darwin/stable
[osx-arm64]: https://azuredatastudio-update.azurewebsites.net/latest/darwin-arm64/stable

Go to our [download page](https://aka.ms/getazuredatastudio) for more specific instructions.

Please visit our [download page](https://aka.ms/getazuredatastudio) for more specific installation instructions.
Check out the [change log](https://github.com/Microsoft/azuredatastudio/blob/main/CHANGELOG.md) or [release notes](https://learn.microsoft.com/sql/azure-data-studio/release-notes-azure-data-studio) for additional details of what's in the each release.
The [Azure Data Studio documentation](https://learn.microsoft.com/sql/azure-data-studio) includes complete details on getting started, connection quickstarts, and feature tutorials.

## **Feature Highlights**

- Cross-Platform DB management for Windows, macOS and Linux with simple XCopy deployment
- SQL Server Connection Management with Connection Dialog, Server Groups, Azure Integration and Registered Servers
- Object Explorer supporting schema browsing and contextual command execution
- T-SQL Query Editor with advanced coding features such as autosuggestions, error diagnostics, tooltips, formatting and peek definition
- Query Results Viewer with advanced data grid supporting large result sets, export to JSON\CSV\Excel, query plan and charting
- Management Dashboard supporting customizable widgets with drill-through actionable insights
- Visual Data Editor that enables direct row insertion, update and deletion into tables
- Backup and Restore dialogs that enables advanced customization and remote filesystem browsing, configured tasks can be executed or scripted
- Task History window to view current task execution status, completion results with error messages and task T-SQL scripting
- Scripting support to generate CREATE, SELECT, ALTER and DROP statements for database objects
- Workspaces with full Git integration and Find In Files support to managing T-SQL script libraries
- Modern light-weight shell with theming, user settings, full-screen support, integrated terminal and numerous other features

Here are some of these features in action.

<img src='https://github.com/Microsoft/azuredatastudio/blob/main/docs/overview_screen.jpg' width='800px'>

## Contributing
If you are interested in fixing issues and contributing directly to the code base,
please see the document [How to Contribute](https://github.com/Microsoft/azuredatastudio/wiki/How-to-Contribute), which covers the following:

* [How to build and run from source](https://github.com/Microsoft/azuredatastudio/wiki/How-to-Contribute#Build-and-Run-From-Source)
* [The development workflow, including debugging and running tests](https://github.com/Microsoft/azuredatastudio/wiki/How-to-Contribute#development-workflow)
* [Submitting pull requests](https://github.com/Microsoft/azuredatastudio/wiki/How-to-Contribute#pull-requests)

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Localization
Azure Data Studio is localized into 10 languages: French, Italian, German, Spanish, Simplified Chinese, Traditional Chinese, Japanese, Korean, Russian, and Portuguese (Brazil). The language packs are available in the Extension Manager marketplace. Simply, search for the specific language using the extension marketplace and install. Once you install the selected language, Azure Data Studio will prompt you to restart with the new language.

## Telemetry

Azure Data Studio collects telemetry data, which is used to help understand how to improve the product. For example, this usage data helps to debug issues, such as slow start-up times, and to prioritize new features. While we appreciate the insights this data provides, we also know that not everyone wants to send usage data and you can disable telemetry as described in the [disable telemetry reporting](https://aka.ms/ads-disable-telemetry) documentation.

## Privacy Statement
The [Microsoft Privacy Statement](https://go.microsoft.com/fwlink/?LinkID=824704) describes the privacy statement of this software.

## Contributions and ""Thank You""
We would like to thank all our users who raised issues, and in particular the following users who helped contribute fixes:

* eulercamposbarros for `Prevent connections from moving on click (#7528)`
* AlexFsmn for `Fixed issue where task icons got hidden if text was too long`
* jamesrod817 for `Tempdb (#7022)`
* dzsquared for `fix(snippets): ads parenthesis to sqlcreateindex snippet #7020`
* devmattrick for `Update row count as updates are received #6642`
* mottykohn for `In Message panel onclick scroll to line #6417`
* Stevoni for `Corrected Keyboard Shortcut Execution Issue #5480`
* yamatoya for `fix the format #4899`
* GeoffYoung for `Fix sqlDropColumn description #4422`
* AlexFsmn for `Added context menu for DBs in explorer view to backup & restore db. #2277`
* sadedil for `Missing feature request: Save as XML #3729`
* gbritton1 for `Removed reference to object explorer #3463`
* Tarig0  for `Add Routine_Type to CreateStoredProc fixes #3257 (#3286)`
* oltruong  for `typo fix #3025'`
* Thomas-S-B for `Removed unnecessary IErrorDetectionStrategy #749`
* Thomas-S-B for `Simplified code #750`
* rdaniels6813  for `Add query plan theme support #3031`
* Ruturaj123 for `Fixed some typos and grammatical errors #3027`
* PromoFaux for `Use emoji shortcodes in CONTRIBUTING.md instead of � #3009`
* ckaczor for `Fix: DATETIMEOFFSET data types should be ISO formatted #714`
* hi-im-T0dd for `Fixed sync issue with my forked master so this commit is correct #2948`
* hi-im-T0dd for `Fixed when right clicking and selecting Manage-correct name displays #2794`
* philoushka  for `center the icon #2760`
* anthonypants for `Typo #2775`
* kstolte for `Fix Invalid Configuration in Launch.json #2789`
* kstolte for `Fixing a reference to SQL Ops Studio #2788`
* AlexFsmn `Feature: Ability to add connection name #2332`
* AlexFsmn `Disabled connection name input when connecting to a server. #2566`
* SebastianPfliegel `Added more saveAsCsv options #2099`
* ianychoi `Fixes a typo: Mimunum -> Minimum #1994`
* AlexFsmn `Fixed bug where proper file extension wasn't appended to the filename. #2151`
* AlexFsmn `Added functionality for adding any file to import wizard #2329`
* AlexFsmn `Fixed background issue when copying a chart to clipboard #2215`
* AlexFsmn `Fixed problem where vertical charts didn't display labels correctly. #2263`
* AlexFsmn `Fixed Initial values for charts to match visuals #2266`
* AlexFsmn `Renamed chart option labels #2264`
* AlexFsmn `Added feature for the opening file after exporting to CSV/XLS/JSON & query files #2216`
* AlexFsmm `Get Connection String should copy to clipboard #2175`
* lanceklinger `Fix for double-clicking column handle in results table #1504`
* westerncj for `Removed duplicate contribution from README.md (#753)`
* ntovas for `Fix for duplicate extensions shown in ""Save File"" dialog. (#779)`
* SebastianPfliegel for `Add cursor snippet (#475)`
* mikaoelitiana for the fix: `revert README and CONTRIBUTING after last VSCode merge (#574)`
* alextercete for `Reinstate menu item to install from VSIX (#682)`
* alextercete for `Fix ""No extension gallery service configured"" error (#427)`
* mwiedemeyer for `Fix #58: Default sort order for DB size widget (#111)`
* AlexTroshkin for `Show disconnect in context menu only when connectionProfile connected (#150)`
* AlexTroshkin for `Fix #138: Invalid syntax color highlighting (identity not highlighting) (#140))`
* stebet for `Fix #153: Fixing sql snippets that failed on a DB with a case-sensitive collation. (#152)`
* SebastianPfliegel `Remove sqlExtensionHelp (#312)`
* olljanat for `Implemented npm version check (#314)`
* Adam Machanic for helping with the `whoisactive` extension

And of course, we'd like to thank the authors of all upstream dependencies.  Please see a full list in the [ThirdPartyNotices.txt](https://raw.githubusercontent.com/Microsoft/azuredatastudio/main/ThirdPartyNotices.txt)

## License

Copyright (c) Microsoft Corporation. All rights reserved.

Licensed under the [MIT License](LICENSE.txt).",VRAI
microsoft/dsl-copilot,Toolkit,ML Model,2024-07-17T15:01:50Z,2024-06-14T20:42:54Z,0,0,0,0,0,1,0,0,2024-03-18T19:53:28Z,2025-04-04T02:01:29Z,7322,51,C#,VRAI,9,FAUX,11,,11,,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,5,"# Summary
Domain-Specific Languages (DSLs) are specialized programming languages designed for specific problem domains or application contexts. Companies use DSLs because they offer precision and productivity. Unlike general-purpose languages (such as Python, Java, or C++), DSLs are tailored to address particular tasks within a specific industry, field, or domain. DSLs also give companies the ability to customize software solutions to their specific needs. By creating DSLs tailored to their business processes, companies can adapt existing tools or frameworks without extensive modifications. Salesforce’s Apex language, designed for customizing their CRM platform, is an example of such adaptability

OpenAI has trained models on a vast amounts of existing text data, including coding languages and documentation. By leveraging this extensive training, GPT-based models can generate contextually relevant responses for code generation and code completion. Companies and developers utilize these models to enhance software development, automate code writing, and improve programming-related tasks.

OpenAI has the capability to address code based questions related to languages that the model has been exposed to during its training. However, since most Domain-Specific Languages (DSLs) are proprietary and not publicly available, Large Language Models (LLMs) typically require additional context to effectively answer questions or generate code in these languages. The following is a series of vetted steps that have been proven to enhance an LLM’s proficiency in interpreting a custom DSL language.

## Architecture

### Resource Architecture Design
![alt text](./assets/dsl-design.png ""Resource Design"")
1. Virtual Network - Single virtual network for all the resource.
2. Storage Account - Utilized to drop documents and trigger an Azure function to chunk the document and store in AI Search. 
3. Function - Triggered by a document in the storage account and chunks data to store in AI Search. The Function also utilizes OpenAI to create embeddings on the document content.
4. App Service - Hosts the chat application for DSL based questions 
5. AI Search - Used as a vector database and for semantic searching across indexes of chunked data.

### Semantic Kernel Flow
![alt text](./assets/semantic-flow.png ""Semantic Kernel Flow"")
1. Code Generation Agent - Initial prompt to generate code in the DSL based language. This agent utilizes the grammar, local examples, and indexed example plugins. The grammar plugin is optional and should be able to be removed once a fine tuned model is created. 
2. Code Compilation and Retry - Before responding to the user call the compiler to validate the code output. If the code is valid move to the next step if not then send OpenAI the code along with the compiler messages and retry compilation with the response.
3. Linting - Lint the code to ensure it is formatted based on standards. 
4. Feedback - Prompt the user for feedback and if the code is the correct response allow the user to accept it and store the response in AI Search for fine tuning later.

### Fine Tuning Pipeline
![alt text](./assets/fine-tuning-pipeline.png ""Fine Tuning Pipeline"")
To limit the number of tokens required in the UI and to allow for the removal of the grammar file, an additional set of code has been created. This code forms a pipeline that accepts a set of prompts and iterates through them to generate a JSONL file. This data will support a custom fine-tuned model with example prompts and responses for the code DSL. Once you have created a fine-tuned model, you should be able to remove the grammar file from the UI pipeline, which will dramatically decrease the number of tokens.

1. The code generation agent utilizes the grammar, local example, and indexed code example plugins to help support the code generation process for fine tuning.
2. The validation plugin will run the custom parser against the code and report back any errors that need to be fixed.

These steps are run several times to help support valid code generation. 
#### Example Prompts
```yml
- ""Create a Classroom program with a main action that initializes a value variable with 10.""
- ""Create a Classroom program with a main action that initializes a note variable with 'Hello World'.""
- ""Create a Classroom program with a main action that prints the note 'Welcome to class!' using Notes.take.""
- ""Create a Classroom program with a main action that initializes a value variable with 5 and another value variable with 10.""
- ""Create a Classroom program with a main action that initializes a note variable with 'This is a note' and prints it using Notes.take.""
- ""Create a Classroom program with a main action that initializes a value variable with 20 and prints 'Class started!' using Notes.take.""
```


# Grammar File
Code grammar files serve as essential components used by code editors to tokenize and highlight source code. These files break down code into smaller units known as tokens and associate them with specific scopes for syntax highlighting. Editors like Visual Studio Code rely on TextMate grammars, which define rules for tokenization using regular expressions. Additionally, grammar files include a repository of language elements (such as functions and comments) and patterns that correspond to these elements. By visually distinguishing keywords, strings, and other code components, code grammar files significantly enhance readability. Importantly, each programming language has its own dedicated grammar file, defining language-specific rules and features.

During our testing, we discovered that using a grammar file for Domain-Specific Languages (DSLs) in the system prompt enabled us to anchor the Language Model (LLM) within the specific language context. This allowed the LLM to respond effectively to questions related to code writing. The grammar file provides a structural outline of the language, and although the LLM’s responses were formatted in DSL syntax, the LLM still hallucinated responses based on assumptions about the language layout.

## ANTLR
For our testing we utilized ANTLR (ANother Tool for Language Recognition) which is a parser generator used for processing structured text. ANTLR provides language processing primitives such as lexers, grammars, and parsers, along with the runtime to process text using them. ANTLR helps create parsers that transform a piece of text into an organized structure called a parse tree or Abstract Syntax Tree (AST). The AST represents the logical content of code, assembled by combining various elements. To obtain a parse tree, you define a lexer and parser grammar, invoke ANTLR to generate the lexer and parser in your target language (e.g., Java, Python, C#), and then use these generated components to recognize and construct the parse tree. ANTLR enables you to work with various languages, data formats, and other text-based structures, making it a valuable tool for language processing and development

# Prompt
When working with large language models like GPT-4, designing a prompt is a critical part of the process. A prompt serves as the input provided to the model to help produce relevant output. By utilizing prompt engineering, you can design and optimize prompts to guide an LLM in creating specific, high-quality outputs. Whether you’re asking questions to language models, generating code, or creating images, prompt engineering helps bridge the gap between queries and meaningful AI-generated responses, ultimately improving the quality of the outputs.

## System Prompt
The system prompt is used to ground the Language Model (LLM) around the DSL (Domain-Specific Language). During our testing, we discovered that providing a grammar file and a few well-documented examples assists the LLM in generating an initial pass at code generation, but still causes the LLM to hallucinate. We are currently in the process of testing code documentation and the RAG (Rating, Annotation, and Guidance) pattern for incorporating that documentation into the initial prompt. Our goal is to determine if this approach will reduce the likelihood of the LLM experiencing hallucinations.

# Code Validation/Compilation
Code compilation and validation is an area where we can enhance the quality of LLM-generated output. By taking the generated code and sending it through a compiler, we can validate the code produced. Any errors from the compiler can then be sent back through the LLM for correction. Additionally, retaining these errors in the history helps ground the LLM, preventing similar mistakes as more questions are asked in the same chat.

# Linting
- TODO - Some DSLs have linting rules that assist with syntax errors, style issues, and programming errors. Linters offer feedback on these issues and help maintain a consistent coding style. In the semantic flow, it would be ideal to incorporate a linting step to ensure that formatting is adhered to when suggesting code samples.

# Feedback loop and documented examples
- Another important area for adding context involves providing well-documented examples of code, explaining what each piece of code accomplishes, and discussing the underlying reasoning behind the code structure. This approach will help establish context for the Language Model (LLM) as it begins to generate code. Our assumption is that a substantial number of commented examples will be necessary for this process to be effective.  
To provide these examples to the prompt there are currently two ways in the code base. The first opption is to provide a set of static examples in a yaml file that is located in the example folder under the language name. The second option is to utilize the code indexing function and provide a yaml file in the code indexing container that will be chunked by prompt and added to AI Search. The indexed prompts will be searched and only included if they are a close match to the question asked, but the examples will be included on every prompt. Both currently require a strict type to load data and there is an example below.
```yaml
language: ""csharp""
prompts:
- prompt: ""Create a person POCO""
  additionalDetails: ""The greeting method should take a string parameter and write a greeting to the console.""
  response: |
    class Person
    {
        public string Name { get; set; }
    }
- prompt: ""Create a person with a greeting method""
  additionalDetails: ""The greeting method should take a string parameter and write a greeting to the console.""
  response: |
    class Person
    {
        public string Name { get; set; }
        public void Greeting(string name)
        {
            System.Console.WriteLine(""Hello, {0}. I'm {1}! Nice to meet you!"", name, Name);
        }
    }
```
 
- TODO - One area where we believe we can enhance context with higher quality is by enabling users to rate responses and storing them in a database. These ratings can then be fed back into the Language Model’s (LLM) context, improving its performance over time.",VRAI
microsoft/hcsshim,Toolkit,Application System,2025-04-25T19:41:46Z,2024-12-11T17:56:57Z,0,14,0,0,0,0,0,0,2015-06-29T16:44:26Z,2025-04-04T02:02:16Z,41578,599,Go,VRAI,265,FAUX,191,,191,Windows - Host Compute Service Shim,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,125,"# hcsshim

[![Build status](https://github.com/microsoft/hcsshim/actions/workflows/ci.yml/badge.svg?branch=master)](https://github.com/microsoft/hcsshim/actions?query=branch%3Amaster)

This package contains the Golang interface for using the Windows [Host Compute Service](https://techcommunity.microsoft.com/t5/containers/introducing-the-host-compute-service-hcs/ba-p/382332) (HCS) to launch and manage [Windows Containers](https://docs.microsoft.com/en-us/virtualization/windowscontainers/about/). It also contains other helpers and functions for managing Windows Containers such as the Golang interface for the Host Network Service (HNS), as well as code for the [guest agent](./internal/guest/README.md) (commonly referred to as the GCS or Guest Compute Service in the codebase) used to support running Linux Hyper-V containers.

It is primarily used in the [Moby](https://github.com/moby/moby) and [Containerd](https://github.com/containerd/containerd) projects, but it can be freely used by other projects as well.

## Building

While this repository can be used as a library of sorts to call the HCS apis, there are a couple binaries built out of the repository as well. The main ones being the Linux guest agent, and an implementation of the [runtime v2 containerd shim api](https://github.com/containerd/containerd/blob/master/runtime/v2/README.md).

### Linux Hyper-V Container Guest Agent

To build the Linux guest agent itself all that's needed is to set your GOOS to ""Linux"" and build out of ./cmd/gcs.

```powershell
C:\> $env:GOOS=""linux""
C:\> go build .\cmd\gcs\
```

or on a Linux machine

```sh
> go build ./cmd/gcs
```

If you want it to be packaged inside of a rootfs to boot with alongside all of the other tools then you'll need to provide a rootfs that it can be packaged inside of. An easy way is to export the rootfs of a container.

```sh
docker pull busybox
docker run --name base_image_container busybox
docker export base_image_container | gzip > base.tar.gz
BASE=./base.tar.gz
make all
```

If the build is successful, in the `./out` folder you should see:

```sh
> ls ./out/
delta.tar.gz  initrd.img  rootfs.tar.gz
```

### Containerd Shim

For info on the [Runtime V2 API](https://github.com/containerd/containerd/blob/main/core/runtime/v2/README.md).

Contrary to the typical Linux architecture of shim -> runc, the runhcs shim is used both to launch and manage the lifetime of containers.

```powershell
C:\> $env:GOOS=""windows""
C:\> go build .\cmd\containerd-shim-runhcs-v1
```

Then place the binary in the same directory that Containerd is located at in your environment.
A default Containerd configuration file can be generated by running:

```powershell
.\containerd.exe config default | Out-File ""C:\Program Files\containerd\config.toml"" -Encoding ascii
```

This config file will already have the shim set as the default runtime for cri interactions.

To trial using the shim out with ctr.exe:

```powershell
C:\> ctr.exe run --runtime io.containerd.runhcs.v1 --rm mcr.microsoft.com/windows/nanoserver:2004 windows-test cmd /c ""echo Hello World!""
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit [Microsoft CLA](https://cla.microsoft.com).

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

We require that contributors sign their commits
to certify they either authored the work themselves or otherwise have permission to use it in this project.

We also require that contributors sign their commits using  using [`git commit --signoff`][git-commit-s]
to certify they either authored the work themselves or otherwise have permission to use it in this project.
A range of commits can be signed off using [`git rebase --signoff`][git-rebase-s].

Please see  [the developer certificate](https://developercertificate.org) for more info,
as well as to make sure that you can attest to the rules listed.
Our CI uses the [DCO Github app](https://github.com/apps/dco) to ensure that all commits in a given PR are signed-off.

### Linting

Code must pass a linting stage, which uses [`golangci-lint`][lint].
Since `./test` is a separate Go module, the linter is run from both the root and the
`test` directories. Additionally, the linter is run with `GOOS` set to both `windows` and
`linux`.

The linting settings are stored in [`.golangci.yaml`](./.golangci.yaml), and can be run
automatically with VSCode by adding the following to your workspace or folder settings:

```json
    ""go.lintTool"": ""golangci-lint"",
    ""go.lintOnSave"": ""package"",
```

Additional editor [integrations options are also available][lint-ide].

Alternatively, `golangci-lint` can be [installed][lint-install] and run locally:

```shell
# use . or specify a path to only lint a package
# to show all lint errors, use flags ""--max-issues-per-linter=0 --max-same-issues=0""
> golangci-lint run
```

To run across the entire repo for both `GOOS=windows` and `linux`:

```powershell
> foreach ( $goos in ('windows', 'linux') ) {
    foreach ( $repo in ('.', 'test') ) {
        pwsh -Command ""cd $repo && go env -w GOOS=$goos && golangci-lint.exe run --verbose""
    }
}
```

### Go Generate

The pipeline checks that auto-generated code, via `go generate`, are up to date.
Similar to the [linting stage](#linting), `go generate` is run in both the root and test Go modules.

This can be done via:

```shell
> go generate ./...
> cd test && go generate ./...
```

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Dependencies

This project requires Golang 1.18 or newer to build.

For system requirements to run this project, see the Microsoft docs on [Windows Container requirements](https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/system-requirements).

## Reporting Security Issues

Security issues and bugs should be reported privately, via email, to the Microsoft Security
Response Center (MSRC) at [secure@microsoft.com](mailto:secure@microsoft.com). You should
receive a response within 24 hours. If for some reason you do not, please follow up via
email to ensure we received your original message. Further information, including the
[MSRC PGP](https://technet.microsoft.com/en-us/security/dn606155) key, can be found in
the [Security TechCenter](https://technet.microsoft.com/en-us/security/default).

For additional details, see [Report a Computer Security Vulnerability](https://technet.microsoft.com/en-us/security/ff852094.aspx) on Technet

---------------
Copyright (c) 2018 Microsoft Corp.  All rights reserved.

[lint]: https://golangci-lint.run/
[lint-ide]: https://golangci-lint.run/usage/integrations/#editor-integration
[lint-install]: https://golangci-lint.run/usage/install/#local-installation

[git-commit-s]: https://git-scm.com/docs/git-commit#Documentation/git-commit.txt--s
[git-rebase-s]: https://git-scm.com/docs/git-rebase#Documentation/git-rebase.txt---signoff",VRAI
microsoft/OpenXR-Unity-MixedReality-Samples,Application System,Documentations,2024-12-05T19:43:16Z,2022-12-16T01:25:43Z,6,0,0,0,0,0,0,0,2021-03-24T23:51:43Z,2025-04-07T08:18:35Z,79710,255,ShaderLab,VRAI,72,FAUX,3,,3,Sample Unity projects to demo how to use OpenXR plugin in Unity for HoloLens 2 and Mixed Reality headsets,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,9,[Error reading file],VRAI
microsoft/rego-cpp,Toolkit,Toolkit,2025-02-01T14:27:17Z,2023-09-15T12:27:01Z,0,29,0,0,0,0,0,0,2023-04-28T10:40:36Z,2025-04-04T02:04:25Z,3352,35,C++,VRAI,11,FAUX,13,"c,cpp,opa,policy,policy-engine,python,rust",13,A C++ interpreter for the OPA policy language Rego,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,7,"# `rego-cpp`

This project is an effort to create a C++ interpreter for the OPA policy language,
[Rego](https://www.openpolicyagent.org/docs/latest/policy-language/). Our goal is
to build both a standalone executable and a library such that programmers who are working
in C++ can interpret Rego programs natively. To achieve this we are building our
interpreter on top of the experimental term rewriter
[Trieste](https://github.com/microsoft/trieste).

> **Warning**
> While this project has progressed to the point that we support full Rego language
> (see [Language Support](#language-support) below) we do not support all built-ins.
> That said, we have verified compliance with the OPA Rego test suite. Even so, it
> should still be considered experimental software and used with discretion.

## Getting Started

Start by installing [CMake](https://cmake.org/) in the way appropriate for your
environment.

### Linux

Create a build directory and initialize the cmake project:

    mkdir build
    cd build
    cmake .. --preset release-clang

You can then build and run the tests using:

    ninja install
    ctest

### Windows

Create a build directory and initialize the cmake project:

    mkdir build
    cd build
    cmake .. --preset release

You can then build and run the tests using:

    cmake --build . --config Release --target INSTALL
    ctest -C Release

### Using the `rego` CLI

The interpreter tool will be located at `build/dist/bin/rego`. Here are
some example commands using the provided example files and run from the suggested
`dist` install directory:

    ./bin/rego -d examples/scalars.rego -q data.scalars.greeting
    {""expressions"":[""Hello""]}

    ./bin/rego -d examples/objects.rego -q data.objects.sites[1].name
    {""expressions"":[""smoke1""]}

    ./bin/rego -d examples/data0.json examples/data1.json examples/objects.rego -i examples/input0.json  -q ""[data.one, input.b, data.objects.sites[1]]""
    {""expressions"":[[{""bar"":""Foo"", ""baz"":5, ""be"":true, ""bop"":23.4}, ""20"", {""name"":""smoke1""}]]}

    ./bin/rego -q ""5 + (2 - 4 * 0.25) * -3 + 7.4""
    {""bindings"":{""x"":5, ""y"":9.4}}

    ./bin/rego -d examples/bodies.rego -i examples/input1.json -q data.bodies.e
    {""expressions"":[{""one"":15, ""two"":15}]}

You can run the test driver from the same directory:

    ./bin/rego_test tests/regocpp.yaml

### Using the `rego` Library

See the [examples](examples/README.md) directory for examples of how to use the
library from different langauages.

## Language Support

We support v0.68.0 of Rego as defined by OPA, with the following grammar:

```ebnf
module          = package { import } policy
package         = ""package"" ref
import          = ""import"" ref [ ""as"" var ]
policy          = { rule }
rule            = [ ""default"" ] rule-head { rule-body }
rule-head       = ( ref | var ) ( rule-head-set | rule-head-obj | rule-head-func | rule-head-comp )
rule-head-comp  = [ assign-operator term ] [ ""if"" ]
rule-head-obj   = ""["" term ""]"" [ assign-operator term ] [ ""if"" ]
rule-head-func  = ""("" rule-args "")"" [ assign-operator term ] [ ""if"" ]
rule-head-set   = ""contains"" term [ ""if"" ] | ""["" term ""]""
rule-args       = term { "","" term }
rule-body       = [ ""else"" [ assign-operator term ] [ ""if"" ] ] ( ""{"" query ""}"" ) | literal
query           = literal { ( "";"" | ( [CR] LF ) ) literal }
literal         = ( some-decl | expr | ""not"" expr ) { with-modifier }
with-modifier   = ""with"" term ""as"" term
some-decl       = ""some"" term { "","" term } { ""in"" expr }
expr            = term | expr-call | expr-infix | expr-every | expr-parens | unary-expr
expr-call       = var [ ""."" var ] ""("" [ expr { "","" expr } ] "")""
expr-infix      = expr infix-operator expr
expr-every      = ""every"" var { "","" var } ""in"" ( term | expr-call | expr-infix ) ""{"" query ""}""
expr-parens     = ""("" expr "")""
unary-expr      = ""-"" expr
membership      = term [ "","" term ] ""in"" term
term            = ref | var | scalar | array | object | set | membership | array-compr | object-compr | set-compr
array-compr     = ""["" term ""|"" query ""]""
set-compr       = ""{"" term ""|"" query ""}""
object-compr    = ""{"" object-item ""|"" query ""}""
infix-operator  = assign-operator | bool-operator | arith-operator | bin-operator
bool-operator   = ""=="" | ""!="" | ""<"" | "">"" | "">="" | ""<=""
arith-operator  = ""+"" | ""-"" | ""*"" | ""/"" | ""%""
bin-operator    = ""&"" | ""|""
assign-operator = "":="" | ""=""
ref             = ( var | array | object | set | array-compr | object-compr | set-compr | expr-call ) { ref-arg }
ref-arg         = ref-arg-dot | ref-arg-brack
ref-arg-brack   = ""["" ( scalar | var | array | object | set | ""_"" ) ""]""
ref-arg-dot     = ""."" var
var             = ( ALPHA | ""_"" ) { ALPHA | DIGIT | ""_"" }
scalar          = string | NUMBER | TRUE | FALSE | NULL
string          = STRING | raw-string
raw-string      = ""`"" { CHAR-""`"" } ""`""
array           = ""["" term { "","" term } ""]""
object          = ""{"" object-item { "","" object-item } ""}""
object-item     = ( scalar | ref | var ) "":"" term
set             = empty-set | non-empty-set
non-empty-set   = ""{"" term { "","" term } ""}""
empty-set       = ""set("" "")""
```

> [!NOTE]
> This grammar corresponds to Rego with `rego.v1` enabled (See [OPA v1.0](https://www.openpolicyagent.org/docs/latest/opa-1) for more info).

Definitions:
```
[]     optional (zero or one instances)
{}     repetition (zero or more instances)
|      alternation (one of the instances)
()     grouping (order of expansion)
STRING JSON string
NUMBER JSON number
TRUE   JSON true
FALSE  JSON false
NULL   JSON null
CHAR   Unicode character
ALPHA  ASCII characters A-Z and a-z
DIGIT  ASCII characters 0-9
CR     Carriage Return
LF     Line Feed
```

### Builtins

At the moment support the following builtins are available:

- `aggregates`
- `arrays`
- `bits`
- `casts`
- `encoding`
- `graphs`
- `numbers`
- `objects`
- `regex`
- `semver`
- `sets`
- `strings`
- `time`
- `types`
- `units`
- `uuid`
- miscellaneous
    * `opa.runtime`
    * `print`

### Compatibility with the OPA Rego Go implementation

Our goal is to achieve and maintain full compatibility with the reference Go
implementation. We have developed a test driver which runs the same tests
and validates that we produce the same outputs. At this stage we pass all
the non-builtin specific test suites, which we clone from the
[OPA repository](https://github.com/open-policy-agent/opa/tree/main/test/cases/testdata).
To build with the OPA tests available for testing, use one of the following presets:
- `release-clang-opa`
- `release-opa`

At present, we are **NOT** passing the following test suites in full:
- `crypto*`
- `glob*`
- `graphql`
- `invalidkeyerror`
- `json*` (except `jsonbuiltins`)
- `jwt*`
- `net*`
- `planner-ir`
- `providers-aws`

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.",FAUX
microsoft/regorus,Toolkit,Application System,2025-05-14T15:41:06Z,2025-03-03T23:58:31Z,0,26,0,0,0,0,0,0,2023-02-09T18:46:41Z,2025-04-07T20:18:06Z,1802,188,Open Policy Agent,VRAI,37,FAUX,24,"c,confidential-computing,cpp,csharp,golang,interpreter,java,javascript,no-std,opa,policy-as-code,python,rego,rust,wasm",24,"Regorus - A fast, lightweight Rego (OPA policy language) interpreter written in Rust.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,14,"# Regorus

**Regorus** is

  - *Rego*-*Rus(t)*  - A fast, light-weight [Rego](https://www.openpolicyagent.org/docs/latest/policy-language/)
   interpreter written in Rust.
  - *Rigorous* - A rigorous enforcer of well-defined Rego semantics.

Regorus is also
  - *cross-platform* - Written in platform-agnostic Rust.
  - *no_std compatible* - Regorus can be used in `no_std` environments too. Most of the builtins are supported.
  - *current* - We strive to keep Regorus up to date with latest OPA release. Regorus defaults to `v1` of the Rego language.
  - *compliant* - Regorus is mostly compliant with the latest [OPA release v1.2.0](https://github.com/open-policy-agent/opa/releases/tag/v1.2.0). See [OPA Conformance](#opa-conformance) for details. Note that while we behaviorally produce the same results, we don't yet support all the builtins.
  - *extensible* - Extend the Rego language by implementing custom stateful builtins in Rust.
    See [add_extension](https://github.com/microsoft/regorus/blob/fc68bf9c8bea36427dae9401a7d1f6ada771f7ab/src/engine.rs#L352).
    Support for extensibility using other languages coming soon.
  - *polyglot* - In addition to Rust, Regorus can be used from *C*, *C++*, *C#*, *Golang*, *Java*, *Javascript*, *Python*, and *Ruby*.
    This is made possible by the excellent FFI tools available in the Rust ecosystem. See [bindings](#bindings) for information on how to use Regorus from different languages.

    To try out a *Javascript(WASM)* compiled version of Regorus from your browser, visit [Regorus Playground](https://anakrish.github.io/regorus-playground/).



Regorus is available as a library that can be easily integrated into your Rust projects.
Here is an example of evaluating a simple Rego policy:

```rust
fn main() -> anyhow::Result<()> {
    // Create an engine for evaluating Rego policies.
    let mut engine = regorus::Engine::new();

    let policy = String::from(
        r#""
       package example

       allow if {
          ## All actions are allowed for admins.
          input.principal == ""admin""
       } else if {
          ## Check if action is allowed for given user.
          input.action in data.allowed_actions[input.principal]
       }
	""#,
    );

    // Add policy to the engine.
    engine.add_policy(String::from(""policy.rego""), policy)?;

    // Add data to engine.
    engine.add_data(regorus::Value::from_json_str(
        r#""{
     ""allowed_actions"": {
        ""user1"" : [""read"", ""write""],
        ""user2"" : [""read""]
     }}""#,
    )?)?;

    // Set input and evaluate whether user1 can write.
    engine.set_input(regorus::Value::from_json_str(
        r#""{
      ""principal"": ""user1"",
      ""action"": ""write""
    }""#,
    )?);

    let r = engine.eval_rule(String::from(""data.example.allow""))?;
    assert_eq!(r, regorus::Value::from(true));

    // Set input and evaluate whether user2 can write.
    engine.set_input(regorus::Value::from_json_str(
        r#""{
      ""principal"": ""user2"",
      ""action"": ""write""
    }""#,
    )?);

    let r = engine.eval_rule(String::from(""data.example.allow""))?;
    assert_eq!(r, regorus::Value::Undefined);

    Ok(())
}
```

Regorus is designed with [Confidential Computing](https://confidentialcomputing.io/about/) in mind. In Confidential Computing environments,
it is important to be able to control exactly what is being run. Regorus allows enabling and disabling various components using cargo
features. By default all features are enabled.

The default build of regorus example program is 6.3M:
```bash
$ cargo build -r --example regorus; strip target/release/examples/regorus; ls -lh target/release/examples/regorus
-rwxr-xr-x  1 anand  staff   6.3M May 11 22:03 target/release/examples/regorus*
```


When all default features are disabled, the binary size drops down to 1.9M.
```bash
$ cargo build -r --example regorus --no-default-features; strip target/release/examples/regorus; ls -lh target/release/examples/regorus
-rwxr-xr-x  1 anand  staff   1.9M May 11 22:04 target/release/examples/regorus*
```

Regorus passes the [OPA v1.2.0 test-suite](https://www.openpolicyagent.org/docs/latest/ir/#test-suite) barring a few
builtins. See [OPA Conformance](#opa-conformance) below.

## Bindings

Regorus can be used from a variety of languages:

- *C*: C binding is generated using [cbindgen](https://github.com/mozilla/cbindgen).
  [corrosion-rs](https://github.com/corrosion-rs/corrosion) can be used to seamlessly use Regorous
  in your CMake based projects. See [bindings/c](https://github.com/microsoft/regorus/tree/main/bindings/c).
- *C freestanding*: [bindings/c_no_std](https://github.com/microsoft/regorus/tree/main/bindings/c_no_std) shows how to use Regorus from C environments without a libc.
- *C++*: C++ binding is generated using [cbindgen](https://github.com/mozilla/cbindgen).
  [corrosion-rs](https://github.com/corrosion-rs/corrosion) can be used to seamlessly use Regorous
  in your CMake based projects. See [bindings/cpp](https://github.com/microsoft/regorus/tree/main/bindings/cpp).
- *C#*: C# binding is generated using [csbindgen](https://github.com/Cysharp/csbindgen). See [bindings/csharp](https://github.com/microsoft/regorus/tree/main/bindings/csharp) for an example of how to build and use Regorus in your C# projects.
- *Golang*: The C bindings are exposed to Golang via [CGo](https://pkg.go.dev/cmd/cgo). See [bindings/go](https://github.com/microsoft/regorus/tree/main/bindings/go) for an example of how to build and use Regorus in your Go projects.
- *Python*: Python bindings are generated using [pyo3](https://github.com/PyO3/pyo3). Wheels are created using [maturin](https://github.com/PyO3/maturin). See [bindings/python](https://github.com/microsoft/regorus/tree/main/bindings/python).
- *Java*: Java bindings are developed using [jni-rs](https://github.com/jni-rs/jni-rs).
  See [bindings/java](https://github.com/microsoft/regorus/tree/main/bindings/java).
- *Javascript*: Regorus is compiled to WASM using [wasmpack](https://github.com/rustwasm/wasm-pack).
  See [bindings/wasm](https://github.com/microsoft/regorus/tree/main/bindings/wasm) for an example of using Regorus from nodejs.
  To try out a *Javascript(WASM)* compiled version of Regorus from your browser, visit [Regorus Playground](https://anakrish.github.io/regorus-playground/).
- *Ruby*: Ruby bindings are developed using [magnus](https://github.com/matsadler/magnus).
  See [bindings/ruby](https://github.com/microsoft/regorus/tree/main/bindings/ruby).

To avoid operational overhead, we currently don't publish these bindings to various repositories.
It is straight-forward to build these bindings yourself.


## Getting Started

[examples/regorus](https://github.com/microsoft/regorus/blob/main/examples/regorus.rs) is an example program that
shows how to integrate Regorus into your project and evaluate Rego policies.

To build and install it, do

```bash
$ cargo install --example regorus --path .
```

Check that the regorus example program is working

```bash
$ regorus
Usage: regorus <COMMAND>

Commands:
  eval   Evaluate a Rego Query
  lex    Tokenize a Rego policy
  parse  Parse a Rego policy
  help   Print this message or the help of the given subcommand(s)

Options:
  -h, --help     Print help
  -V, --version  Print version
```


First, let's evaluate a simple Rego expression `1*2+3`

```bash
$ regorus eval ""1*2+3""
```

This produces the following output

```json
{
  ""result"": [
    {
      ""expressions"": [
        {
           ""value"": 5,
           ""text"": ""1*2+3"",
           ""location"": {
              ""row"": 1,
              ""col"": 1
            }
        }
      ]
    }
  ]
}
```

Next, evaluate a sample [policy](https://github.com/microsoft/regorus/blob/main/examples/server/allowed_server.rego) and [input](https://github.com/microsoft/regorus/blob/main/examples/server/input.json)
(borrowed from [Rego tutorial](https://www.openpolicyagent.org/docs/latest/#2-try-opa-eval)):

```bash
$ regorus eval -d examples/server/allowed_server.rego -i examples/server/input.json data.example
```

Finally, evaluate real-world [policies](tests/aci/) used in Azure Container Instances (ACI)

```bash
$ regorus eval -b tests/aci -d tests/aci/data.json -i tests/aci/input.json data.policy.mount_overlay=x
```

## Policy coverage

Regorus allows determining which lines of a policy have been executed using the `coverage` feature (enabled by default).

We can try it out using the `regorus` example program by passing in the `--coverage` flag.

```shell
$ regorus eval -d examples/server/allowed_server.rego -i examples/server/input.json data.example --coverage
```

It produces the following coverage report which shows that all lines are executed except the line that sets `allow` to true.

![coverage.png](https://github.com/microsoft/regorus/blob/main/docs/coverage.png?raw=true)

See [Engine::get_coverage_report](https://docs.rs/regorus/latest/regorus/struct.Engine.html#method.get_coverage_report) for details.
Policy coverage information is useful for debugging your policy as well as to write tests for your policy so that all
lines of the policy are exercised by the tests.

## ACI Policies

Regorus successfully passes the ACI policy test-suite. It is fast and can run each of the tests in a few milliseconds.

```bash
$ cargo test -r --test aci
    Finished release [optimized + debuginfo] target(s) in 0.05s
    Running tests/aci/main.rs (target/release/deps/aci-2cd8d21a893a2450)
aci/mount_device                                  passed    3.863292ms
aci/mount_overlay                                 passed    3.6905ms
aci/scratch_mount                                 passed    3.643041ms
aci/create_container                              passed    5.046333ms
aci/shutdown_container                            passed    3.632ms
aci/scratch_unmount                               passed    3.631333ms
aci/unmount_overlay                               passed    3.609916ms
aci/unmount_device                                passed    3.626875ms
aci/load_fragment                                 passed    4.045167ms
```

Run the ACI policies in the `tests/aci` directory, using data `tests/aci/data.json` and input `tests/aci/input.json`:

```bash
$ regorus eval -b tests/aci -d tests/aci/data.json -i tests/aci/input.json data.policy.mount_overlay=x
```

Verify that [OPA](https://github.com/open-policy-agent/opa/releases) produces the same output

```bash
$ diff <(regorus eval -b tests/aci -d tests/aci/data.json -i tests/aci/input.json data.framework.mount_overlay=x) \
       <(opa eval -b tests/aci -d tests/aci/data.json -i tests/aci/input.json data.framework.mount_overlay=x)
```


## Performance

To check how fast Regorus runs on your system, first install a tool like [hyperfine](https://github.com/sharkdp/hyperfine).

```bash
$ cargo install hyperfine
```

Then benchmark evaluation of the ACI policies,

```bash
$ hyperfine ""regorus eval -b tests/aci -d tests/aci/data.json -i   tests/aci/input.json data.framework.mount_overlay=x""
Benchmark 1: regorus eval -b tests/aci -d tests/aci/data.json -i tests/aci/input.json data.framework.mount_overlay=x
  Time (mean ± σ):       4.6 ms ±   0.2 ms    [User: 4.1 ms, System: 0.4 ms]
  Range (min … max):     4.4 ms …   6.0 ms    422 runs
```

Compare it with OPA

```bash
$ hyperfine ""opa eval -b tests/aci -d tests/aci/data.json -i tests/aci/input.json data.framework.mount_overlay=x""
Benchmark 1: opa eval -b tests/aci -d tests/aci/data.json -i tests/aci/input.json data.framework.mount_overlay=x
  Time (mean ± σ):      45.2 ms ±   0.6 ms    [User: 68.8 ms, System: 5.1 ms]
  Range (min … max):    43.8 ms …  46.7 ms    62 runs

```
## OPA Conformance

Regorus has been verified to be compliant with [OPA v1.2.0](https://github.com/open-policy-agent/opa/releases/tag/v1.2.0)
using a [test driver](https://github.com/microsoft/regorus/blob/main/tests/opa.rs) that loads and runs the OPA testsuite using Regorus, and verifies that expected outputs are produced.

The test driver can be invoked by running:

```bash
$ cargo test -r --test opa --features opa-testutil,serde_json/arbitrary_precision
```

Currently, Regorus passes all the non-builtin specific tests.
See [passing tests suites](https://github.com/microsoft/regorus/blob/main/tests/opa.passing).

The following test suites don't pass fully due to missing builtins:
- `globsmatch`
- `graphql`
- `invalidkeyerror`
- `jsonpatch`
- `jwtbuiltins`
- `jwtdecodeverify`
- `jwtencodesign`
- `jwtencodesignheadererrors`
- `jwtencodesignpayloaderrors`
- `jwtencodesignraw`
- `jwtverifyhs256`
- `jwtverifyhs384`
- `jwtverifyhs512`
- `jwtverifyrsa`
- `netcidrcontains`
- `netcidrcontainsmatches`
- `netcidrexpand`
- `netcidrintersects`
- `netcidrisvalid`
- `netcidrmerge`
- `netcidroverlap`
- `netlookupipaddr`
- `providers-aws`
- `regometadatachain`
- `regometadatarule`
- `regoparsemodule`
- `rendertemplate`

They are captured in the following [github issues](https://github.com/microsoft/regorus/issues?q=is%3Aopen+is%3Aissue+label%3Alib).

Cryptographic builtins are not supported by design. Users that need cryptographic builtins are encouraged to use [extensions](https://docs.rs/regorus/latest/regorus/struct.Engine.html#method.add_extension).

### Grammar

The grammar used by Regorus to parse Rego policies is described in [grammar.md](https://github.com/microsoft/regorus/blob/main/docs/grammar.md)
in both [W3C EBNF](https://www.w3.org/Notation.html) and [RailRoad Diagram](https://en.wikipedia.org/wiki/Syntax_diagram) formats.


## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.",VRAI
ministryofjustice/cloud-platform-environments,Toolkit,Application System,2025-05-15T17:46:13Z,2025-05-15T13:32:20Z,0,24,0,0,0,0,0,0,2018-05-29T09:51:12Z,2025-04-08T09:15:20Z,101085,72,HCL,VRAI,36,FAUX,126,"cloud-platform,go,kubernetes-manifests,terraform-configurations",126,Environment configuration for the Cloud Platform,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,1057,"# cloud-platform-environments

## Intro

This repository is where kubernetes namespaces are managed, across all the clusters. Kubernetes namespaces and resources are defined in the `namespaces` directory in this repository under the corresponding `cluster` name.

### Functionality

The pipeline will for each defined `cluster`:

1. Create a namespace as defined in the namespaces/`cluster` directory. If the namespace already exists on the cluster it will be ignored.
2. Delete any namespaces that exist in the cluster but are not defined in the repository.
3. Create any kubernetes resource that is defined under namespaces/`cluster`/`namespace`

### Namespaces

The `namespaces/` directory contains sub directories named after the existing cluster names, and inside, sub directories named after each of the desired namespaces you want to create for each cluster. Placed inside are the kubernetes resource files you want to create in the kubernetes format. Those will be created automatically after a push is made to the Repositories master branch by the AWS code pipeline.

### AWS resources

In a similar fashion as namespaces, you can create AWS resources in your desired namespace. The file structure for that is namespaces/`cluster`/`namespace`/terraform/ and Terraform files should be placed in that route for the pipeline to be triggered and create those AWS resources. Different terraform modules exist, for example: [ECR credentials](https://github.com/ministryofjustice/cloud-platform-terraform-ecr-credentials), [S3 bucket](https://github.com/ministryofjustice/cloud-platform-terraform-s3-bucket), and should be used to create these resources as follows:

### Changes within namespaces

Changes within namespaces directory are managed by the `build-environments` concourse job configured [here](https://github.com/ministryofjustice/cloud-platform-concourse/tree/master/pipelines/cloud-platform-live-0/main/build-environments.yaml).
GitHub triggers the build process using [webhook](https://github.com/ministryofjustice/cloud-platform-environments/settings/hooks/32085881). Build itself runs script `whichNamespace.sh` checking for last commit changes, and if it detects any within namespace folder it executes `namespace.py` with appropriate cluster(s) parameter.

#### Example terraform file

```
module ""my_S3_bucket"" {
  source = ""github.com/ministryofjustice/cloud-platform-terraform-s3-bucket?ref=1.0""

  team_name = ""my-team""
  bucket_id = ""my-bucket""
}

resource ""kubernetes_secret"" ""my_S3_bucket_creeds"" {
  metadata {
    name = ""my-S3-bucket-creeds""
  }

  data {
    access_key_id     = ""${module.my_s3_bucket.access_key_id}""
    Secret_access_key = ""${module.my_s3_bucket.secret_access_key}""
    Bucket_name       = ""${module.my_s3_bucket.bucket_name}""
  }
}
```

#### concourse-ci/status Check

There are occasions where Terraform in the code/pipeline plan will return something like the following
which appears to remove policy config. However examination of the plan will reveal that it has been re-added by terraform

```
      ~ policy = jsonencode(
            {
              - Statement = [
                  - {
                      - Action   = ""dynamodb:*""
                      - Effect   = ""Allow""
                      - Resource = ""arn:aws:dynamodb:eu-west-2:1234567898:table/cp-abcdefghij123""
                      - Sid      = """"
                    },
                ]
              - Version   = ""2012-10-17""
            }
        ) -> (known after apply)
        user   = ""cp-dynamo-""cp-abcdefghij123
```",VRAI
ministryofjustice/modernisation-platform,Application System,Documentations,2025-05-15T14:59:24Z,2025-05-14T15:30:20Z,0,14,0,0,0,0,0,0,2020-08-05T13:32:28Z,2025-04-08T08:27:49Z,26090,696,HCL,VRAI,291,FAUX,112,"architecture-decisions,aws,civil-service,documentation,modernisation-platform",112,A place for the core work of the Modernisation Platform • This repository is defined and managed in Terraform,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,134,"# Ministry of Justice Modernisation Platform

[![Standards Icon]][Standards Link] [![Format Code Icon]][Format Code Link] [![Scorecards Icon]][Scorecards Link] [![SCA Icon]][SCA Link] [![Terraform SCA Icon]][Terraform SCA Link]

## About this repository

This is the Ministry of Justice [Modernisation Platform team](https://github.com/orgs/ministryofjustice/teams/modernisation-platform)'s repository for core work on the Modernisation Platform. The Modernisation Platform team is a [platform engineering product team](https://www.thoughtworks.com/radar/techniques/platform-engineering-product-teams) which provides a hosting platform for Ministry of Justice applications which cannot be hosted on the [Cloud Platform](https://user-guide.cloud-platform.service.justice.gov.uk/#cloud-platform-user-guide).

For more information on the Modernisation Platform please see the [user guidance](https://user-guide.modernisation-platform.service.justice.gov.uk).

## Contents

This repository currently holds the Modernisation Platform's:

- [Architecture Decision Record (ADR)](architecture-decision-record)
- [Environment definitions](environments)
- [Infrastructure as code](terraform)
- [Source code for user-guide.modernisation-platform.service.justice.gov.uk](source)

## Other useful repositories

### Core repositories

| Name                                                                                                                                      | Description                                                                         |
| ----------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| [Modernisation Platform](https://github.com/ministryofjustice/modernisation-platform) (this one)                                          | Our repository for core work, including our ADR and infrastructure as code          |
| [Modernisation Platform Environments](https://github.com/ministryofjustice/modernisation-platform-environments)                           | The repository for user application infrastructure as code and deployment workflows |
| [modernisation-platform-ami-builds](https://github.com/ministryofjustice/modernisation-platform-ami-builds)                               | Repository for creating pipelines to build AMIs for use on the platform             |
| [modernisation-platform-configuration-management](https://github.com/ministryofjustice/modernisation-platform-configuration-management)   | Repository for configuration management code used on the platform                   |
| [modernisation-platform-terraform-module-template](https://github.com/ministryofjustice/modernisation-platform-terraform-module-template) | Template repository used for creating other Terraform module repositories           |

### Terraform modules - for member account use

Modernisation Platform users can use these modules in their infrastructure. They are designed to comply with best practices and to work with the platform, to make creating infrastructure quicker, easier and more secure.

| Name                                                                                                                                                  | Description                                                                                                                                                                        |
| ----------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [modernisation-platform-terraform-bastion-linux](https://github.com/ministryofjustice/modernisation-platform-terraform-bastion-linux)                 | Module for creating Linux bastion servers in member AWS accounts                                                                                                                   |
| [modernisation-platform-terraform-ecs-cluster](https://github.com/ministryofjustice/modernisation-platform-terraform-ecs-cluster)                     | Module for creating ECS cluster                                                                                                                                                    |
| [modernisation-platform-terraform-s3-bucket](https://github.com/ministryofjustice/modernisation-platform-terraform-s3-bucket)                         | Module for creating S3 buckets with sensible defaults e.g. replication, encryption                                                                                                 |
| [modernisation-platform-terraform-aws-vm-import](https://github.com/ministryofjustice/modernisation-platform-terraform-aws-vm-import)                 | Module that creates s3 and roles needed to import virtual machine (VM) images from your virtualization environment to Amazon EC2 as Amazon Machine Images (AMI)                    |
| [modernisation-platform-terraform-pagerduty-integration](https://github.com/ministryofjustice/modernisation-platform-terraform-pagerduty-integration) | Module associating an SNS topic with a PagerDuty service                                                                                                                           |
| [modernisation-platform-terraform-loadbalancer](https://github.com/ministryofjustice/modernisation-platform-terraform-loadbalancer)                   | Module that creates application load balancer in AWS with logging enabled, s3 to store logs and Athena DB to query logs                                                            |
| [modernisation-platform-terraform-ssm-patching](https://github.com/ministryofjustice/modernisation-platform-terraform-ssm-patching)                   | Module that automates the patching of ec2 instances via ssm. It creates an s3 bucket for log storage, as well as maintenance windows, tasks, resource groups, and patch baselines. |
| [modernisation-platform-terraform-ec2-instance](https://github.com/ministryofjustice/modernisation-platform-terraform-ec2-instance)                   | Module for creating an EC2 instance                                                                                                                                                |
| [modernisation-platform-terraform-ec2-autoscaling-group](https://github.com/ministryofjustice/modernisation-platform-terraform-ec2-autoscaling-group) | Module for creating an EC2 autoscaling group                                                                                                                                       |
| [modernisation-platform-terraform-lambda-function](https://github.com/ministryofjustice/modernisation-platform-terraform-lambda-function)             | Module for creating a Lambda Function                                                                                                                                              |

### Terraform modules - used by the core platform

These modules are used by the Modernisation Platform's core infrastructure

| Name                                                                                                                                                | Description                                                                                    |
| --------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |
| [modernisation-platform-terraform-baselines](https://github.com/ministryofjustice/modernisation-platform-terraform-baselines)                       | Module for enabling and configuring common baseline services such as SecurityHub               |
| [modernisation-platform-terraform-cross-account-access](https://github.com/ministryofjustice/modernisation-platform-terraform-cross-account-access) | Module for creating an IAM role that can be assumed from another account                       |
| [modernisation-platform-terraform-environments](https://github.com/ministryofjustice/modernisation-platform-terraform-environments)                 | Module for creating organizational units and accounts within AWS Organizations from JSON files |
| [modernisation-platform-terraform-iam-superadmins](https://github.com/ministryofjustice/modernisation-platform-terraform-iam-superadmins)           | Module for creating defined IAM users as superadmins                                           |
| [modernisation-platform-terraform-member-vpc](https://github.com/ministryofjustice/modernisation-platform-terraform-member-vpc)                     | Module for member VPC accounts                                                                 |
| [modernisation-platform-github-oidc-provider](https://github.com/ministryofjustice/modernisation-platform-github-oidc-provider)                     | Module for creating OIDC providers to use in GitHub Actions                                    |

### Tools

| Name                                                                                                                        | Description                                                                                                                                                                                           |
| --------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [modernisation-platform-instance-scheduler](https://github.com/ministryofjustice/modernisation-platform-instance-scheduler) | A Go lambda function for stopping and starting instance, rds resources and autoscaling groups. The lambda is used by the core platform and can be reused outside of the platform with minimal changes |
| [modernisation-platform-cp-network-test](https://github.com/ministryofjustice/modernisation-platform-cp-network-test)       | Container bundled with utilities for network testing                                                                                                                                                  |

[Standards Link]: https://github-community.service.justice.gov.uk/repository-standards/modernisation-platform ""Repo standards badge.""
[Standards Icon]: https://github-community.service.justice.gov.uk/repository-standards/api/modernisation-platform/badge
[Format Code Icon]: https://img.shields.io/github/actions/workflow/status/ministryofjustice/modernisation-platform/format-code.yml?labelColor=231f20&style=for-the-badge&label=Formate%20Code
[Format Code Link]: https://github.com/ministryofjustice/modernisation-platform/actions/workflows/format-code.yml
[Scorecards Icon]: https://img.shields.io/github/actions/workflow/status/ministryofjustice/modernisation-platform/scorecards.yml?branch=main&labelColor=231f20&style=for-the-badge&label=Scorecards
[Scorecards Link]: https://github.com/ministryofjustice/modernisation-platform/actions/workflows/scorecards.yml
[SCA Icon]: https://img.shields.io/github/actions/workflow/status/ministryofjustice/modernisation-platform/code-scanning.yml?branch=main&labelColor=231f20&style=for-the-badge&label=Secure%20Code%20Analysis
[SCA Link]: https://github.com/ministryofjustice/modernisation-platform/actions/workflows/code-scanning.yml
[Terraform SCA Icon]: https://img.shields.io/github/actions/workflow/status/ministryofjustice/modernisation-platform/code-scanning.yml?branch=main&labelColor=231f20&style=for-the-badge&label=Terraform%20Static%20Code%20Analysis
[Terraform SCA Link]: https://github.com/ministryofjustice/modernisation-platform/actions/workflows/terraform-static-analysis.yml",VRAI
mongodb/mongodb-enterprise-kubernetes,DevOPs,DevOPs,2025-05-12T08:08:04Z,2023-12-14T09:58:43Z,0,0,0,0,0,0,0,7,2018-05-18T13:45:28Z,2025-04-07T08:54:28Z,4023,325,Dockerfile,FAUX,130,FAUX,9,"cloud-manager,kubernetes,kubernetes-operator,mongodb,mongodb-database,operator,ops-manager",9,MongoDB Enterprise Kubernetes Operator,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,38,"# MongoDB Enterprise Kubernetes Operator #

Welcome to the MongoDB Enterprise Kubernetes Operator. The Operator enables easy deploy of the following applications into Kubernetes clusters:
* MongoDB - Replica Sets, Sharded Clusters and Standalones - with authentication, TLS and many more options.
* Ops Manager - our enterprise management, monitoring and backup platform for MongoDB. The Operator can install and manage Ops Manager in Kubernetes for you. Ops Manager can manage MongoDB instances both inside and outside Kubernetes.

The Operator requires access to one of our database management tools - Ops Manager or Cloud Manager - to deploy MongoDB instances. You may run Ops Manager either inside or outside Kubernetes, or may use Cloud Manager (cloud.mongodb.com) instead.

This is an Enterprise product, available under the Enterprise Advanced license.
We also have a [Community Operator](https://github.com/mongodb/mongodb-kubernetes-operator).

## Support, Feature Requests and Community ##

The Enterprise Operator is supported by the [MongoDB Support Team](https://support.mongodb.com/). If you need help, please file a support ticket.
If you have a feature request, you can make one on our [Feedback Site](https://feedback.mongodb.com/forums/924355-ops-tools)

You can discuss this integration in our new [Community Forum](https://developer.mongodb.com/community/forums/) - please use the tag [kubernetes-operator](https://developer.mongodb.com/community/forums/tag/kubernetes-operator)

## Videos ##

Here are some talks from MongoDB Live 2020 about the Operator:
* [Kubernetes, MongoDB, and Your MongoDB Data Platform](https://www.youtube.com/watch?v=o1fUPIOdKeU)
* [Run it in Kubernetes! Community and Enterprise MongoDB in Containers](https://www.youtube.com/watch?v=2Xszdg-4T6A)

## Documentation ##

[Install Kubernetes Operator](https://docs.opsmanager.mongodb.com/current/tutorial/install-k8s-operator)

[Deploy MongoDB](https://docs.mongodb.com/kubernetes-operator/stable/mdb-resources/)

[Deploy Ops Manager](https://docs.mongodb.com/kubernetes-operator/stable/om-resources/)

[MongoDB Resource Specification](https://docs.opsmanager.mongodb.com/current/reference/k8s-operator-specification)

[Ops Manager Resource Specification](https://docs.mongodb.com/kubernetes-operator/stable/reference/k8s-operator-om-specification/)

[Troubleshooting Kubernetes Operator](https://docs.opsmanager.mongodb.com/current/reference/troubleshooting/k8s/)

[Known Issues for Kubernetes Operator](https://docs.mongodb.com/kubernetes-operator/stable/reference/known-issues/)

## Requirements ##

Please refer to the [Installation Instructions](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/plan-k8s-operator-install/)
to see which Kubernetes and Openshift versions the Operator is compatible with

To work with MongoDB resource this Operator requires [Ops Manager](https://docs.opsmanager.mongodb.com/current/) (Ops Manager can
be installed into the same Kubernetes cluster by the Operator or installed outside of the cluster manually)
or [Cloud Manager](https://cloud.mongodb.com/user#/cloud/login).
> If this is your first time trying the Operator, Cloud Manager is easier to get started. Log in, and create 'Cloud Manager' Organizations and Projects to use with the Operator.


## Installation

### Create Kubernetes Namespace

The Mongodb Enterprise Operator is installed, into the `mongodb` namespace by default, but this namespace is not created automatically. To create this namespace you should execute:

    kubectl create namespace mongodb

To use a different namespace, update the yaml files' `metadata.namespace` attribute to point to your preferred namespace.  If using `helm` you need to override the `namespace` attribute with `--set namespace=<..>` during helm installation.

### Installation using yaml files

#### Create CustomResourceDefinitions

`CustomResourceDefinition`s (or `CRDs`) are Kubernetes Objects which can be used to instruct the Operators to perform operations on your Kubernetes cluster. Our CRDs control MongoDB and Ops Manager deployments. They should be installed before installing the Operator.
CRDs are defined cluster-wide, so to install them, you must have Cluster-level access. However, once the CRDs are installed, MongoDB instances can be deployed with namespace-level access only.

    kubectl apply -f https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/crds.yaml

#### Operator Installation

> In order to install the Operator in OpenShift, please follow [these](openshift-install.md) instructions instead.

To install the Operator using yaml files, you may apply the config directly from github;

    kubectl apply -f https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/mongodb-enterprise.yaml

or can clone this repo, make any edits you need, and apply it from disk:

    kubectl apply -f mongodb-enterprise.yaml

### Installation using the Helm Chart

MongoDB's official Helm Charts are hosted at https://github.com/mongodb/helm-charts

## MongoDB Resource ##

*This section describes how to deploy MongoDB instances. This requires a working Ops or Cloud Manager installation. See below for instructions on how to configure Ops Manager.*

### Adding Ops Manager Credentials ###

For the Operator to work, you will need the following information:

* Base URL - the URL of an Ops Manager instance (for Cloud Manager use `https://cloud.mongodb.com`)
* (optional) Project Name - the name of an Ops Manager Project for MongoDB instances to be deployed into. This project will be created by the Operator if it doesn't exist. We recommend that you allow the Operator to create and manage the projects it uses. By default, the Operator will use the name of the MongoDB resource as the project name.
* (optional) Organization ID - the ID of the Organization which the Project belongs to. By default, the Operator will create an Organization with the same name as the Project.
* API Credentials. This can be any pair of:
  * Public and Private Programmatic API keys. They correspond to `user` and `publicApiKey` fields in the Secret storing
credentials. More information about the way to create them using Ops Manager UI can be found
[here](https://docs.opsmanager.mongodb.com/current/tutorial/configure-public-api-access/#programmatic-api-keys)
  * Username and Public API key. More information about the way to create them using Ops Manager UI can be found
 [here](https://docs.opsmanager.mongodb.com/current/tutorial/configure-public-api-access/#personal-api-keys-deprecated)

Note: When creating API credentials, you must allow the Pod IP range of your Kubernetes cluster to use the credentials - otherwise, API requests from the Operator to Ops Manager will be rejected.
You can get the Pod IP range of your kubernetes cluster by executing the command: ```kubectl cluster-info dump | grep -m 1 cluster-cidr```

This is documented in greater detail in our [installation guide](https://docs.opsmanager.mongodb.com/current/tutorial/install-k8s-operator)


### Projects ###

A `Project` object is a Kubernetes `ConfigMap` that points to an Ops Manager installation and a `Project`. This `ConfigMap` has the following structure:

```
$ cat my-project.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-project
  namespace: mongodb
data:
  projectName: myProjectName # this is an optional parameter
  orgId: 5b890e0feacf0b76ff3e7183 # this is an optional parameter
  baseUrl: https://my-ops-manager-or-cloud-manager-url
```
> `projectName` is optional, and the value of `metadata.name` will be used if it is not defined.
> `orgId` is required.

Apply this file to create the new `Project`:

    kubectl apply -f my-project.yaml

### Credentials ###

For a user to be able to create or update objects in this Ops Manager Project they need either a Public API Key or a
Programmatic API Key. These will be held by Kubernetes as a `Secret` object. You can create this Secret with the following command:

``` bash
$ kubectl -n mongodb create secret generic my-credentials --from-literal=""user=my-public-api-key"" --from-literal=""publicApiKey=my-private-api-key""
```

### Creating a MongoDB Resource ###

A MongoDB resource in Kubernetes is a MongoDB. We are going to create a replica set to test that everything is working as expected. There is a MongoDB replica set yaml file in `samples/mongodb/minimal/replica-set.yaml`.

If you have a Project with the name `my-project` and Credentials stored in a secret called `my-credentials`, then after applying this file everything should be running and a new Replica Set with 3 members should soon appear in Ops Manager UI.

    kubectl apply -f samples/mongodb/minimal/replica-set.yaml -n mongodb

## MongoDBOpsManager Resource ##

This section describes how to create the Ops Manager Custom Resource in Kubernetes. Note, that this requires all
the CRDs and the Operator application to be installed as described above.

### Create Admin Credentials Secret ###

Before creating the Ops Manager resource you need to prepare the information about the admin user which will be
created automatically in Ops Manager. You can use the following command to do it:

```bash
$ kubectl create secret generic ops-manager-admin-secret  --from-literal=Username=""user.name@example.com"" --from-literal=Password=""Passw0rd.""  --from-literal=FirstName=""User"" --from-literal=LastName=""Name"" -n <namespace>
```

Note, that the secret is needed only during the initialization of the Ops Manager object - you can remove it or
change the password using Ops Manager UI after the Ops Manager object is created.

### Create MongoDBOpsManager Resource ###

Use the file `samples/ops-manager/ops-manager.yaml`. Edit the fields and create the object in Kubernetes:

```bash
$ kubectl apply -f samples/ops-manager/ops-manager.yaml -n <namespace>
```

Note, that it can take up to 8 minutes to initialize the Application Database and start Ops Manager.

## Accessing the Ops Manager UI using your web browser

In order to access the Ops Manager UI from outside the Kubernetes cluster, you must enable `spec.externalConnectivity` in the Ops Manager resource definition. The easiest approach is by configuring the LoadBalancer service type.

You will be able to fetch the URL to connect to Ops Manager UI from the `Service` object created by the Operator.

## Removing the Operator, Databases and Ops Manager from your Kubernetes cluster ##

As the Operator manages MongoDB and Ops Manager resources, if you want to remove them from your Kubernetes cluster, database instances and Ops Manager must be removed before removing the Operator. Removing the Operator first, or deleting the namespace will cause delays or stall the removal process of MongoDB objects, requiring manual intervention.

Here is the correct order to completely remove the Operator and the services managed by it:

* Remove all database clusters managed by the Operator
* Remove Ops Manager
* Remove the Operator
* Remove the CRDs

## Contributing

For PRs to be accepted, all contributors must sign our [CLA](https://www.mongodb.com/legal/contributor-agreement).

Reviewers, please ensure that the CLA has been signed by referring to [the contributors tool](https://contributors.corp.mongodb.com/) (internal link).",FAUX
mongodb/terraform-provider-mongodbatlas,Toolkit,DevOPs,2025-05-15T08:22:03Z,2025-04-24T07:32:47Z,0,0,0,0,0,0,0,0,2019-08-15T15:55:57Z,2025-04-08T15:18:23Z,41172,255,Go,VRAI,189,FAUX,21,"atlas,mongodb-atlas-provider,mongodbatlas,terraform,terraform-provider",21,"Terraform MongoDB Atlas Provider: Deploy, update, and manage MongoDB Atlas infrastructure as code through HashiCorp Terraform",FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,100,,VRAI
MoreEventsMod/More_Events_Mod_Beta,Application System,Documentations,2024-05-07T15:08:42Z,2023-11-27T16:05:40Z,0,0,0,0,0,5,0,0,2016-05-31T20:24:16Z,2024-05-07T15:09:03Z,453211,19,Python,VRAI,35,FAUX,107,,107,Testing Branch for the Next More Events Mod version,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,53,"# More_Events_Mod_Beta
Testing Branch for the next More Events Mod version

Make sure your error.log file is clear before sending a pull request to reduce bug testing required before release.",VRAI
nakamasato/kubernetes-training,Documentations,Documentations,2025-05-14T18:48:45Z,2024-12-03T19:32:50Z,0,7,0,0,0,0,0,4,2020-05-03T01:47:38Z,2025-04-02T21:56:40Z,17335,61,Go,VRAI,6,FAUX,16,"kubernetes,managed-by-terraform",16,Kubernetes training from basics to advanced,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,4,"package main

import (
	""fmt""
	""log""
	""os""
	""strings""

	""github.com/nakamasato/kubernetes-training/doc""
	yaml ""gopkg.in/yaml.v3""
)

func main() {

	f, err := os.Open(""readme.yml"")
	if err != nil {
		log.Fatal(err)
	}
	defer f.Close()

	d := yaml.NewDecoder(f)

	var m map[string][]Version

	if err := d.Decode(&m); err != nil {
		log.Fatal(err)
	}

	generateReadme(m[""versions""], ""README.md"")

	updateVersionInReadme(m[""versions""], ""contents/README.md"")
}

type Version struct {
	Name     string `yaml:""name""`
	Version  string `yaml:""version""`
	RepoUrl  string `yaml:""repoUrl""`
	Category string `yaml:""category""`
	ToDo     bool   `yaml:""todo""`
	Dir string `yaml:""dir""`
}

func (v Version) getReleaseUrl() string {
	if v.Version == """" {
		return v.RepoUrl
	} else if v.Version == ""latest"" {
		return v.RepoUrl + ""/releases""
	} else {
		return v.RepoUrl + ""/releases/tag/"" + v.Version
	}
}

func (v Version) getToDoString() string {
	if v.ToDo {
		return ""(ToDo)""
	} else {
		return """"
	}
}

func generateReadme(versions []Version, filename string) {
	book := doc.NewMarkDown()
	book.WriteTitle(""Kubernetes Training"", doc.LevelTitle).
		WriteLines(2)

	book.WriteList(fmt.Sprintf(""Read on Website: %s"", book.GetLink(""https://www.nakamasato.com/kubernetes-training"", ""https://www.nakamasato.com/kubernetes-training"")))
	book.WriteList(
		fmt.Sprintf(
			""Read on GitHub: %s (All files were moved to `contents` in %s"",
			book.GetLink(""contents"", ""contents""),
			book.GetLink(""#105"", ""https://github.com/nakamasato/kubernetes-training/pull/105""),
		),
	)
	book.WriteList(""README.md is generated by `readme.go`"")

	book.WriteTitle(""Versions"", doc.LevelSection)
	for _, version := range versions {
		fmt.Printf(""name: %s, version: %s\n"", version.Name, version.Version)
		book.WriteList(
			fmt.Sprintf(
				""%s: %s%s"",
				version.Name,
				book.GetLink(version.Version, version.getReleaseUrl()),
				version.getToDoString(),
			),
		)
	}

	book.WriteTitle(""Cloud Native Trail Map"", doc.LevelSection)
	book.WriteList(book.GetLink(""https://github.com/cncf/trailmap"", ""https://github.com/cncf/trailmap""))
	book.WriteList(book.GetLink(""https://www.cncf.io/blog/2018/03/08/introducing-the-cloud-native-landscape-2-0-interactive-edition/"", ""https://www.cncf.io/blog/2018/03/08/introducing-the-cloud-native-landscape-2-0-interactive-edition/""))

	book.WriteWordLine(fmt.Sprintf(""!%s"", book.GetLink("""", ""https://github.com/cncf/trailmap/blob/master/CNCF_TrailMap_latest.png?raw=true"")))

	err := book.Export(filename)
	if err != nil {
		log.Fatal(err)
	}
}

func updateVersionInReadme(versions []Version, filename string) {
	input, err := os.ReadFile(filename)
	if err != nil {
		log.Fatalln(err)
	}

	lines := strings.Split(string(input), ""\n"")

	for _, version := range versions {
		for i, line := range lines {
			if strings.Contains(line, ""Contents"") { // not update Contents
				break
			}
			if strings.Contains(line, fmt.Sprintf(""1. %s:"", version.Name)) {
				lines[i] = fmt.Sprintf(
					""1. %s: [%s](%s)%s"",
					version.Name,
					version.Version,
					version.getReleaseUrl(),
					version.getToDoString(),
				)
				break
			}
		}
	}
	output := strings.Join(lines, ""\n"")
	err = os.WriteFile(filename, []byte(output), 0644)
	if err != nil {
		log.Fatalln(err)
	}
}",FAUX
Near-One/eth-connector,Toolkit,Application System,2024-09-06T12:01:14Z,2021-07-08T20:15:19Z,0,0,0,0,0,2,0,0,2021-02-15T14:21:04Z,2024-09-06T12:01:18Z,1167,40,JavaScript,VRAI,6,FAUX,18,,18,A temporary repo for ETH connector to be used by EVM,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,12,"# ETH connector for Rainbow bridge

## Definitions
`bridgedETH` - NEP-141 fungible-token representation of ETH inside Near.

`nETH` - native ETH inside Near EVM.

## Deployments

| Version        | Description            | Status      | Ethereum Connector Address                 | NEAR Connector Account |
|----------------|------------------------|-------------|--------------------------------------------|------------------------|
| develop.aurora | NEAR testnet - Ropsten | [Working](https://explorer.testnet.near.org/accounts/develop.aurora)   | 0x4a8FfD609122b80E1da0d95e51a31667804eA890 |          develop.aurora        |
|     aurora     | NEAR testnet - Ropsten | [Working](https://explorer.testnet.near.org/accounts/aurora)   | 0x9006a6D7d08A388Eeea0112cc1b6b6B15a4289AF |              aurora            |

# Step-by-step testing guide

## Pre-requisites

### Installation
1. Make sure you have installed npm.

2. Install NEAR CLI: `$ npm install -g near-cli`.

3. Install Yarn: `$ npm install --global yarn`.

4. Clone this repo, and run `$ cd eth-custodian` and `$ yarn install`.

5. Create an account in Metamask in Ropsten testnet.

6. Get some Ropsten ETH. For example using this faucet: https://faucet.ropsten.be/.

7. Create an account in NEAR TestNet: https://wallet.testnet.near.org/.

8. Make sure that you're working with the NEAR TestNet: `$ export NODE_ENV=testnet`.

9. Log in to the NEAR Wallet from the CLI: `$ near login`. The browser should pop up and a NEAR Wallet should ask for a permission for adding a full access key.

### Configuration
1. Go to _eth-custodian_ directory: `$ cd eth-custodian`.

2. (Optional) Update `scripts/json/ethereum-config.json` with the actual data on the addresses.

3. Create `.env` file inside `eth-custodian` directory: `$ touch .env`.

4. Add to the file your RPC endpoint (with or without API key):
`$ echo ""WEB3_RPC_ENDPOINT=YOUR_WEB3_RPC_ENDPOINT_HERE"" >> .env` <br/>
(Optional) RPC access can be easily gained from [Alchemy](https://www.alchemyapi.io/).

5. Add to the file Ropsten Private key:
`$ echo ""ROPSTEN_PRIVATE_KEY=YOUR_ROPSTEN_PRIVATE_KEY_HERE"" >> .env`

6. Add path to the Near credentials (e.g. this usually will be at `/home/<YOUR_USER_NAME>/.near-credentials` on Linux <br/>
and `$HOME/.near-credentials` on MacOS: <br/>
`$ echo ""NEAR_KEY_STORE_PATH=PATH_TO_YOUR_NEAR_CREDENTIALS_HERE"" >> .env`

7. Compile Ethereum contracts with: <br/>
`$ make compile`

## Utilities
To get the balance of bridgedETH (NEP-141):
`$ make near-ft-balance-of NEAR_ACCOUNT=<YOUR_NEAR_ACCOUNT_HERE>`

To get the balance of nETH (native ETH in Aurora-EVM):
`$ make near-ft-balance-of-eth NEAR_ACCOUNT=<YOUR_NEAR_ACCOUNT_HERE> ETH_ADDRESS=<ETH_ADDRESS_OF_ACCOUNT_IN_EVM_HERE>`

## Ethereum -> Near transfer (ETH -> nETH (NEP-141))
1. Go to _eth-custodian_ directory: `$ cd eth-custodian`.

2. **Transfer ETH to EthCustodian**.
Send `depositToNear` transaction to EthCustodian contract.  This will transfer `AMOUNT` (wei) from your account
and deposit `AMOUNT` (wei) having `FEE` (wei) to `NEAR_RECIPIENT` account on Near. <br/>
Run: `$ make eth-deposit-to-near NEAR_RECIPIENT=<ACCOUNT_ID_OF_RECIPIENT_HERE> AMOUNT=<DEPOSIT_AMOUNT_HERE> FEE=<DEPOSIT_FEE_HERE>`.

3. **Wait sufficiently long**
You need to wait for 3 confirmations for Ropsten blockchain. This is needed to achieve finality of Ropsten block, including locking transaction.
The status of syncing of the bridge can be observed [here](http://35.235.76.186:8002/metrics).
First metric (`near_bridge_eth2near_client_block_number`) should become more than the height of a block with transaction from the step 2 at least by 3,
for a successful finalization of the transfer.

4. **Finalize deposit to Near**
Call deposit in Near blockchain to finalize the deposit transaction with the given `TX_HASH`. You can find `TX_HASH` in the output of the previous step.
You will need to provide your `NEAR_ACCOUNT` AccountId which will be used to relay the ETH proof to the Near blockchain to mint appropriate amount of
bridgedETH for the `NEAR_RECIPIENT` (this parameter is optional here and only serves for verbose purposes to show the balance of the recipient before and after) <br/>
Run: `$ make near-finalize-deposit-from-eth TX_HASH=<DEPOSIT_TX_HASH_HERE> NEAR_ACCOUNT=<YOUR_NEAR_ACCOUNT_HERE> NEAR_RECIPIENT=<RECIPIENT_HERE>`

## Near -> Ethereum transfer (nETH -> ETH)
1. Go to _eth-custodian_ directory: `$ cd eth-custodian`.

2. **Begin withdraw**
Send a `withdraw` transaction to the bridgedETH contract to withdraw `AMOUNT` bridgedETH (wei) from the `NEAR_ACCOUNT` and
transfer the appropriate amount of ETH (wei) to `ETH_RECIPIENT` (Specify without '0x' prefix).
During the execution, the contract will issue an execution outcome, which would be used during finalization step to contruct the proof for the EthCustodian in Ethereum. <br/>
Run: `$ make near-withdraw-to-eth NEAR_ACCOUNT=<YOUR_NEAR_ACCOUNT_HERE> ETH_RECIPIENT=<ETH_ADDRESS_OF_RECIPIENT_HERE> AMOUNT=<WITHDRAW_AMOUNT_HERE> FEE=<WITHDRAW_FEE_HERE>`

3. **Wait sufficiently long**
This approximately takes 10 minutes for the Ropsten bridge deployment.
This is needed to relay NEAR block with the height higher than the block with transaction from previous step to Ethereum, plus wait a challenge period.
The status of syncing of the bridge can be observed [here](http://35.235.76.186:8001/metrics).
First metric `near_bridge_near2eth_client_height` should become higher than the block height displayed in console during the previous step.
4. **Finalize withdraw to Eth**
Call withdraw in Near blockchain to finalize the deposit transaction with the given `RECEIPT_ID`. You can find `RECEIPT_ID` in the output of the previous step.
Send a `withdraw` transaction to the EthCustodian contract. After bridge syncing we are able to prove the fact of withdrawal transaction on NEAR to the EthCustodian contract. <br/>
Run: `$ make eth-finalize-withdraw-from-near RECEIPT_ID=<RECEIPT_ID_FROM_STEP_2_HERE> NEAR_ACCOUNT=<YOUR_NEAR_ACCOUNT_HERE>`

## Ethereum -> Near transfer (ETH -> ETH (native ETH in Aurora-EVM))
1. Go to _eth-custodian_ directory: `$ cd eth-custodian`.

2. **Transfer ETH to EthCustodian**.
Send `depositToEVM` transaction to EthCustodian contract.  This will transfer `AMOUNT` (wei) from your account
and deposit `AMOUNT` (wei) having `FEE` (wei) to `NEAR_RECIPIENT` account on Near. <br/>
Run: `$ make eth-deposit-to-evm ETH_RECIPIENT=<ETH_ADDRESS_OF_RECIPIENT_IN_EVM_HERE> AMOUNT=<DEPOSIT_AMOUNT_HERE> FEE=<DEPOSIT_FEE_HERE>`.

3. **Wait sufficiently long**
You need to wait for 3 confirmations for Ropsten blockchain. This is needed to achieve finality of Ropsten block, including locking transaction.
The status of syncing of the bridge can be observed [here](http://35.235.76.186:8002/metrics).
First metric (`near_bridge_eth2near_client_block_number`) should become more than the height of a block with transaction from the step 2 at least by 3,
for a successful finalization of the transfer.

4. **Finalize deposit to Near**
Call deposit in Near blockchain to finalize the deposit transaction with the given `TX_HASH`. You can find `TX_HASH` in the output of the previous step.
You will need to provide your `NEAR_ACCOUNT` AccountId which will be used to relay the ETH proof to the Near blockchain to mint appropriate amount of
bridgedETH for the `NEAR_RECIPIENT` (this parameter is optional here and only serves for verbose purposes to show the balance of the recipient before and after) <br/>
Run: `$ make near-finalize-deposit-from-eth-to-evm TX_HASH=<DEPOSIT_TX_HASH_HERE> NEAR_ACCOUNT=<YOUR_NEAR_ACCOUNT_HERE> ETH_RECIPIENT=<ETH_RECIPIENT_HERE>`

## Near -> Ethereum transfer (ETH -> ETH)
WIP

## Advanced

### Contract deployment

To deploy the contract, you need at least _proverAddress_ and _nearEvmAccount_ addresses to be configured in
`ethereum-config.json` prior to the deployment.

After that call: <br />
`$ make eth-deploy-contracts`

As a result of the function call you will get the address of the freshly deployed `EthCustodian` that you can put in
your `ethereum-config.json` file in the `ethConnectorAddress` field.

After `ethConnectorAddress` is set, you can run

`$ make eth-deploy-proxy`

to deploy the proxy contract and make it the admin of `EthCustodian`.

### Other scripts

For more advanced usage, please examine the `hardhat.config.js` file which contains a lot of scripts that are performed
in this step-by-step guide via more simplified `make` commands. You can see the list of available tasks by running:
<br/>
`$ yarn hardhat`

To show help and required arguments on how to use the specific task from the task list, use the following command structure:
`$ yarn hardhat <TASK_NAME> --help` <br/>

e.g.:

`$ yarn hardhat eth-deposit-to-evm --help`",VRAI
negz/crossplane-scale,Application System,Documentations,2024-04-23T01:10:56Z,2021-10-14T17:23:21Z,0,0,0,0,1,0,0,0,2021-09-13T03:53:06Z,2024-04-23T01:11:16Z,2490,6,Shell,VRAI,5,FAUX,0,,0,@crossplane scale testing,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,3,"# crossplane-scale

This repository contains configurations and scripts used to test the scalability
of Crossplane. It's mostly for my own future reference.

## Controller Scalability

These tests use two Crossplane configurations:

* `xclusters.test.crossplane.io` deploys the control plane that will be tested.
* `xbuckets.test.crossplane.io` is deployed many times to the control plane.

The idea is to create a Kubernetes cluster broadly representative of somewhere
Crossplane might be deployed in production, then to create many instances of a
Crossplane Composite Resource (XR) that again might be broadly representative of
a real, production XR. The `xbuckets` configuration defines an XR that attempts
to exercise many of Crossplane's features. Specifically it:

* Offers a claim.
* Exposes a connection secret.
* Composes a realistic number of managed resources.
* Patches both to and from the composite resource.

The test uses GCP and GCS buckets, which _appear_ to be a good candidate for
scale testing. Buckets can be spun up and down fairly quickly, and do not appear
to be subject to any quota limits - i.e. there is no documented maximum number
of allowed buckets per project. Instead bucket creation and deletion is [rate
limited][storage-quotas] to approximately one CRUD operation every two seconds.
GCS does [bill per operation][operations-pricing], so running scale tests is not
free. The cost appears to be minimal though, at $0.05 per 10,000 creates (aka
'inserts'), and $0.004 per 10,000 observes (aka 'gets'). IAM service accounts
are also used in the scale testing composition. There are no documented costs
for number of service accounts, but they are limited to 100 per project. It's
possible to request a quota increase via the GCP console, and we've had the
quota used for our testing project increased to 1,200.

Roughly:

1. Create a GKE cluster by applying
  a. `package/xclusters.test.crossplane.io/composition-gcp.yaml`
  b. `package/xclusters.test.crossplane.io/definition.yaml`
  c. `cluster-gcp.yaml`.
2. Deploy Crossplane to said GKE cluster (e.g. `up uxp install --set metrics.enabled=true`).
3. Apply https://github.com/prometheus-operator/kube-prometheus/tree/release-0.9/manifests
4. Apply https://raw.githubusercontent.com/caicloud/event_exporter/master/deploy/deploy.yml
5. Apply `prom-pod-monitors.yaml`.

## CRD Scalability

These tests (optionally) use the same configurations as above to spin up a
Kubernetes cluster to test. The rest is down to filling the API Server up with
CRDs. You can do this one of two ways:

```console
# Install Crossplane and use the package manager

# Install the up CLI. See https://github.com/upbound/up.
curl -sL https://cli.upbound.io | sh

# Install Crossplane. We need 1.9.0-up.3 or above.
# See https://github.com/upbound/universal-crossplane
up uxp install 1.9.0-up.3 --set metrics.enabled=true

# You'll want kubectl v1.25.0 or above to avoid slowness due to too many CRDs
# per https://blog.upbound.io/scaling-kubernetes-to-thousands-of-crds/
kubectl apply -f full-coverage-providers.yaml
```

or...

```console
# Directly apply ~1,800 Crossplane CRDs.
kubectl apply -f etoomanycrds/latest-crds 
```

[storage-quotas]: https://cloud.google.com/storage/quotas
[operations-pricing]: https://cloud.google.com/storage/pricing#operations-pricing",VRAI
netscaler/cloud-native-getting-started,DevOPs,Documentations,2024-03-19T07:43:04Z,2023-03-13T07:28:47Z,0,0,0,0,0,0,0,1,2019-01-02T14:25:16Z,2025-01-28T16:17:35Z,117771,49,HCL,FAUX,23,FAUX,7,,7,Learn how to use Citrix ADC in Kubernetes for 2 tier microservices architecture for Cloud Native applications,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,15,"# Citrix Cloud Native Networking (CNN) hands-on guides
**Citrix cloud-native solutions** leverage the advanced traffic management, observability, and comprehensive security features of Citrix ADCs to ensure enterprise grade reliability and security. Lets get started with CNN GitHub!

###### You’ll learn how to:
* Deploy [Citrix Ingress Controller](https://github.com/citrix/citrix-k8s-ingress-controller) for Citrix Cloud Native stack
* Deploy Citrix ADC containerized proxy - CPX
* Deploy Citrix Cloud native stack in different K8s platforms (On-prem, OpenShift, Rancher, EKS, AKS, GKE, PKS)
* Deploy Citrix Cloud native stack for 
  * Unified Ingress topology
  * Two tier topology
  * Service Mesh lite topology
  * ISTIO: Service Mesh topology

## Getting Started
Here are very cool hands-on guides for you to understand Citrix Cloud Native portfolio
* Citrix Cloud Native Beginners Guides
  * [Deploy a Citrix ADC CPX proxy in docker](/beginners-guide/cpx-in-docker.md)
  * [Citrix Ingress Controller (CIC) deployment modes in K8s cluster](/beginners-guide/cic-in-k8s.md)
  * [Deploy Citrix ADC CPX in Minikube](/beginners-guide/cpx-in-minikube.md)
  * [Deploy Ingress proxy - CPX on NodePort](/beginners-guide/North-South-cpx-ingress-proxy.md)
  * [Deploy Citrix ADC CPX as East-West proxy without sideacar proxy](/beginners-guide/East-West-cpx-ingress-proxy.md)
  * [Deployment modes for Citrix ADC CPX](/beginners-guide/CPX-deployment-modes.md)
  * [Update Citrix Ingress Controller logging using ConfigMap](/beginners-guide/configmap-for-loglevels.md)
  * [Deploy Citrix Observability exporter in K8s for sending metrics to Prometheus](/beginners-guide/tier1-prometheus-coe.md)

* Citrix Cloud Native Advanced Guides
  * AWS
    * [NetScaler VPX as Ingress Proxy for AWS Kubernetes Service](/aws/unified-ingress)
    * [NetScaler VPX as Ingress proxy and CPX as k8s proxy for AWS Kubernetes Service](/aws/dual-tier)
  * Azure
    * [Citrix ADC VPX & Ingress Controller as External LoadBalancer/Ingress for Azure Kubernetes Service](/azure/unified-ingress/README.md)
    * [Citrix ADC CPX & Ingress Controller for Azure Kubernetes Service](/azure/marketplace-cpx/README.md)
  * GCP (Google Cloud Platform)
    * [Two-Tier deployment with Citrix ADC VPX, Citrix Ingress Controller, Citrix ADC CPX and Application Delivery Management(ADM) on Google Cloud](/gcp/two-tier-vpc-cpx-adm/README.md)
    * [Citrix ADC with Google Anthos: Autoscaling Lab](/gcp/anthos/scaleup/README.md)
    * [Citrix ADC with Google Anthos: WAF with Policy Controller Lab](/gcp/anthos/waf/README.md)
    * [Citrix ADC with Google Anthos: Dual-tier API Gateway with ACM Lab](/gcp/anthos/apigw/README.md)
    * [Citrix ADC with GKE: ADC CPX for Service Mesh Lite](/gcp/sml/README.md)
  * On-Prem (using VMs on Xenserver)
    * [Unified Ingress topology: Tier 1 ADC - MPX/BLX/VPX to load balance microservice applications (North-South traffic)](/on-prem/Unified-Ingress/README.md)
    * [2-Tier Ingress topology: Tier 1 ADC - MPX/BLX/VPX & Tier 2 ADC - CPX to load balance microservice applications (North-South traffic)](/on-prem/2-Tier-deployment/README.md)
    * [Service mesh Lite topology: Tier 1 ADC - MPX/BLX/VPX & Tier 2 ADC - CPX to load balance microservice applications (North-South as well as East-West traffic)](/on-prem/README.md)
    * [Citrix Observability Exporter to troubleshoot microservices using Grafana, Kibana monitoring tools](/on-prem/ServiceMeshLite/coe/README.md)
    * [API gateway use cases: Tier 1 ADC - MPX/BLX/VPX or Tier 2 ADC - CPX to provide Rate limit, Basic Auth, Content routing, IP filtering use cases](/on-prem/ServiceMeshLite/API-gateway/README.md)
    * [Configure WAF policies on Tier 1 ADC VPX in Unified Ingress deployment](/on-prem/Unified-Ingress/README.md#section-e-configure-waf-policies-on-vpx-using-waf-crds)

  * OpenShift (Red Hat Enterprise Linux VMs on xenserver)
    * [Service mesh lite using Ingress rules](/openshift/README.md)
    * [Unified Ingress using OpenShift routes and route sharding](/openshift/openshift-routes/README.md)

* Cloud Native stack for Sock Shop application
    * [Deploy Socks Shop microservice application using Citrix ADC](/on-prem/ServiceMeshLite/sock-shop/README.md)

## Contact Us

Looking to get started or take the next step in your app modernization? Our team is now offering free consultations! Send an email to appmodernization@citrix.com to schedule your session, and a specialist will promptly reply with options to connect.

![CN-emailID.png](/VPX/images/CN-emailID.png)",VRAI
netscaler/netscaler-k8s-ingress-controller,Toolkit,Application System,2025-05-07T15:10:30Z,2024-04-24T03:27:33Z,0,0,0,0,0,0,0,7,2018-09-21T06:40:36Z,2025-03-12T05:48:00Z,64719,314,HCL,VRAI,91,FAUX,33,"api-gateway,citrix,ingress,ingress-controller,k8s,kubernetes,loadbalancer,loadbalancing,netscaler,service-mesh,service-proxy",33,NetScaler Ingress Controller for Kubernetes:,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,42,"[![Docker Repository on Quay](https://quay.io/repository/netscaler/netscaler-k8s-ingress-controller/status)](https://quay.io/repository/netscaler/netscaler-k8s-ingress-controller)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](./license/LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/netscaler/netscaler-k8s-ingress-controller.svg)](https://github.com/netscaler/netscaler-k8s-ingress-controller/stargazers)
[![HitCount](https://hits.dwyl.com/netscaler/netscaler-k8s-ingress-controller.svg)](http://hits.dwyl.com/netscaler/netscaler-k8s-ingress-controller)
[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/netscaler)](https://artifacthub.io/packages/search?repo=netscaler)
---

# NetScaler Ingress Controller

## Description

This repository contains the NetScaler ingress controller built around [Kubernetes Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/).
### Participate:

   You can reach out for any questions via our email: **netscaler-appmodernization@cloud.com**.

   To enhance our understanding of your Kubernetes/microservices application deployment architecture and provide you with the latest NetScaler capabilities, we kindly request you to fill out [Requirement Gathering Questionnaire](https://docs.google.com/forms/d/e/1FAIpQLSd9ueKkfgk-oy8TR1G5cp5HexFwU03kkwx_CvDyOFVFweuXOw/viewform). Your valuable insights will enable us to better serve your specific needs. Thank you for taking the time to contribute to our understanding and improvement efforts.

## What is an ingress controller?

An Ingress Controller is a [controller](https://kubernetes.io/docs/concepts/architecture/controller/) that watches the Kubernetes API server for updates to the Ingress resource and reconfigures the Ingress load balancer accordingly.

## What is the NetScaler ingress controller?

NetScaler provides an ingress controller for NetScaler MPX (hardware), NetScaler VPX (virtualized), and NetScaler CPX (containerized) for [bare metal](https://github.com/netscaler/netscaler-k8s-ingress-controller/tree/master/deployment/baremetal) and [cloud](https://github.com/netscaler/netscaler-k8s-ingress-controller/tree/master/deployment) deployments. It is built around Kubernetes [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) and automatically configures NetScaler based on the Ingress resource configuration.

The NetScaler ingress controller can be deployed either by directly using [yamls](https://github.com/netscaler/netscaler-k8s-ingress-controller/tree/master/deployment/baremetal) or by [helm charts](https://github.com/netscaler/netscaler-k8s-ingress-controller/tree/master/charts).

## Features

Features supported by the NetScaler ingress controller can be found [here](https://github.com/netscaler/netscaler-k8s-ingress-controller/tree/master/deployment).
The NetScaler API Gateway features can be found [here](https://github.com/netscaler/netscaler-k8s-ingress-controller/blob/master/docs/deploy/citrix-api-gateway.md).

## Supported platforms and deployments

Click [here](docs/support-matrix.md) for detailed list of supported platforms and deployments.

## Documentation

For detailed documentation, see the [Netscaler ingress controller Live Documentation](https://docs.netscaler.com/en-us/netscaler-k8s-ingress-controller/).

## Deployment solutions

You can deploy the NetScaler ingress controller in many platforms. For detailed information, see [Deployment Architecture](https://github.com/netscaler/netscaler-k8s-ingress-controller/tree/master/deployment).

## Examples

Deploy the Guestbook application and use the [NetScaler CPX](https://docs.netscaler.com/en-us/cpx/current-release/about.html) to provide the Ingress:

-  [Quick Deploy using YAML](./example)
-  [Quick Deploy using Helm](https://github.com/netscaler/netscaler-helm-charts/tree/master/examples/netscaler-cpx-with-ingress-controller)
-  [Quick Deploy using Kops](./docs/deploy/deploy-cic-kops.md)
-  [Deployment in Google Cloud](https://github.com/netscaler/netscaler-k8s-ingress-controller/blob/master/deployment/gcp)
-  [Deployment in Azure Cloud](https://github.com/netscaler/netscaler-k8s-ingress-controller/tree/master/deployment/azure)

## Release notes

Click [here](https://github.com/netscaler/netscaler-k8s-ingress-controller/releases) for the release notes of the latest NetScaler ingress controller release.

## Questions and support

For questions and support the following channels are available:

   - [NetScaler Discussion Forum](https://community.citrix.com/forums/forum/8-microservices/)

To enhance our understanding of your Kubernetes/microservices application deployment architecture and provide you with the latest NetScaler capabilities, we kindly request you to fill out [Requirement Gathering Questionnaire.](https://docs.google.com/forms/d/e/1FAIpQLSd9ueKkfgk-oy8TR1G5cp5HexFwU03kkwx_CvDyOFVFweuXOw/viewform). Your valuable insights will enable us to better serve your specific needs. Thank you for taking the time to contribute to our understanding and improvement efforts.

For more information about NetScaler cloud native solutions, you can reach out to the NetScaler product team at: **netscaler-appmodernization@cloud.com**

![ ](./docs/media/contact-team.png)

### NetScaler `kubectl` plugin
NetScaler offers a convenient [kubectl plugin](https://github.com/netscaler/modern-apps-toolkit/blob/main/netscaler-plugin/README.md) designed to inspect NetScaler Ingress controller deployments and conduct troubleshooting operations. This feature is available from CIC version 1.32.7 onwards. Utilize the various subcommands provided by this plugin to streamline your troubleshooting process and gain valuable insights into your Ingress controller setup. We encourage you to explore the capabilities of this plugin for a more effective and efficient management of your deployments.

## Issues

Describe issues in detail, collect logs, and use the [discussion forum](https://community.citrix.com/forums/forum/8-microservices/) to report issues.

Use the following command to collect logs:

```
Get Logs: kubectl logs netscaler-k8s-ingress-controller > log_file
```

To facilitate a more efficient troubleshooting process and identify the issue, we recommend utilizing the [CIC Diagnostic tool](https://github.com/netscaler/modern-apps-toolkit/tree/main/cic_diagnostics_tool). This tool will gather deployment logs and other essential details that are crucial for our analysis.

You can report any issues using the following forum:
`https://community.citrix.com/forums/forum/8-microservices/`

For information on how to troubleshoot some of the common issues that you may encounter while using NetScaler CPX, see the
[NetScaler CPX documentation](https://docs.netscaler.com/en-us/cpx/current-release/cpx-troubleshooting.html).

## Code of Conduct

This project adheres to the [Kubernetes Community Code of Conduct](https://github.com/kubernetes/community/blob/master/code-of-conduct.md). By participating in this project you agree to abide by its terms.

## License

[Apache License 2.0](./license/LICENSE)",VRAI
networkservicemesh/sdk,Application System,Documentations,2025-05-06T12:34:57Z,2024-10-17T23:52:26Z,0,9,0,0,0,0,0,0,2020-01-14T18:21:48Z,2025-04-01T06:55:34Z,4040,34,Go,VRAI,36,FAUX,143,networkservicemesh,143,,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,41,"# Network Service Mesh sdk

This repo is for platform independent sdk code

For platform specific sdk code, please see the sdk-${platform} repos:

- [sdk-vppagent](https://github.com/networkservicemesh/sdk-vppagent)
- [sdk-kernel](https://github.com/networkservicemesh/sdk-kernel)
- [sdk-sriov](https://github.com/networkservicemesh/sdk-sriov)",VRAI
Neurone/stellaris-italian-translation,Application System,Application System,2024-09-07T17:19:44Z,2024-09-07T17:03:17Z,0,0,0,0,0,41,0,0,2019-05-02T20:33:59Z,2024-09-07T17:19:48Z,20794,9,JavaScript,VRAI,7,FAUX,2,"italian,italiano,mod,paradox,stellaris,traduzione,translation",2,Mod per giocare Stellaris in italiano,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,7,"# Traduzione italiana di Stellaris

Questo mod, **compatibile** con la **modalità Ironman** e gli **achievement di Steam**, sostituisce la lingua inglese con la lingua italiana.

Basato sulla versione: **Cepheus v3.4.3**

Di seguito trovate l'attuale stato di traduzione del gioco base e dei DLC, ordinati per data di uscita.

| Uscita | DLC | Stato Traduzione |
|--|--|--|
|2024.09.24|Cosmic Storms|**Da tradurre**|
|2020.05.07|The Machine Age|**Da tradurre**|
|2023.11.16|Pacchetto Narrativo - Astral Planes|**Da tradurre**|
|2023.05.09|Galactic Paragons|**Da tradurre**|
|2023.03.14|Pacchetto Narrativo - First Contact|**Da tradurre**|
|2022.09.20|Pacchetto Specie - Toxoids|**Da tradurre**|
|2022.05.12|Overlord|**Da tradurre**|
|2021.11.22|Pacchetto Specie - Aquatics|Completa|
|2021.04.15|Nemesis|**Parziale**|
|2020.10.29|Pacchetto Specie - Necroids|Completa|
|2020.03.17|Federations|**Parziale**|
|2019.10.24|Pacchetto Specie - Lithoids|Completa|
|2019.06.04|Pacchetto Narrativo - Ancient Relics|Completa|
|2018.12.06|MegaCorp|Completa, **in revisione**|
|2018.05.22|Pacchetto Narrativo - Distant Stars|Completa|
|2018.02.22|Apocalypse|Completa|
|2017.12.07|Pacchetto Specie - Humanoids|Completa|
|2017.09.21|Pacchetto Narrativo - Synthetic Dawn|Completa|
|2017.04.06|Utopia|Completa|
|2016.10.20|Pacchetto Narrativo - Leviathans|Completa|
|2016.08.04|Pacchetto Specie - Plantoids|Completa|
|2016.05.09|Stellaris Base|Completa, **in revisione**|

Per facilitare il confronto con il resto del mondo, **non sono tradotti volutamente i nomi degli achievement ma solo la loro descrizione**.

Grazie a tutta la comunità per i contributi alle traduzioni, e in particolare a [Nefando](https://github.com/Nefando), [AndryRock](https://github.com/AR-9217), [Daddie2](https://github.com/Daddie2) e [anthx80](https://github.com/anthx80).

Ricordo infine che ho creato questo mod a maggio 2019 partendo dai testi diligentemente tradotti dal team Battle of Paradox Italia, ed in particolare dal buon [Puxxup](url=https://steamcommunity.com/profiles/76561198030059915), perché volevo creare una versione compatibile con Ironman e achievement di Steam. Tanta acqua è passata sotto i ponti, nuovi DLC sono usciti, molte traduzioni sono state revisionate e aggiunte, e il mod ha preso quindi da tempo una strada tutta sua, ma è sempre bene ricordare le proprie origini, ed è per questo che troverete citati nei credits in-game anche il team BoPI, Puxxup e diversi altri traduttori.

## Contribuire alla traduzione

**Lo strumento principale per contribuire al progetto è [Transifex](https://www.transifex.com/internet-of-peers/stellaris-italian-translation)**.

Potete utilizzare comunque anche Github e proporre direttamente pull request su questo repository: utilizzate le informazioni riportate di seguito riportate per maggiori dettagli su come fare. 

Paradox aggiorna spesso i propri prodotti, non solo aggiungendo nuovi DLC e nuovi dialoghi, ma anche correggendo dove e quando serve quelli vecchi. Questo significa che è difficile avere una traduzione sempre aggiornata al 100% all'ultima release, quindi ogni aiuto con le traduzioni è ben accetto :)

Per come è strutturato il gioco, e poiché questo mod vuole mantenere gli achievement attivi, non è possibile aggiungere una lingua al gioco: l'unica alternativa per avere l'italiano è modificare una delle lingue disponibili. Per comodità quindi questo mod sovrascrive direttamente la lingua di default, ossia l'inglese. La fonte principale di frasi da tradurre di trova all'interno della cartella [src/mod/localisation/english](src/mod/localisation/english).

### Stato della traduzione, per file

In precedenza qui tenevo l'elenco dei file in lavorazione, ma sono diventati troppi (161 alla versione 3.43). Per verificare lo stato corrente di ogni file (completato, in revisione, da tradurre, ecc.) è possibile verificare direttamente le traduzioni dal [progetto Transifex](https://www.transifex.com/internet-of-peers/stellaris-italian-translation/content/)

### Cosa posso modificare su Github?

Tutto quello che trovate qui su GitHub all'interno del percorso [src/mod](/src/mod) è già traducibile senza rischi di disattivare la modalità Ironman e gli achievement.

Se dovesse servire invece aggiungere un file non ancora presente su questo repository, prima di modificarlo bisogna assicurarsi che non sia incluso nella lista di file che vengono controllati all'avvio del gioco.

Il file che indica a Stellaris la lista dei file _intoccabili_ si chiama `checksum_manifest.txt` e si trova all'interno della cartella principale di gioco. Attualmente il contenuto del file è il seguente:

```txt
directory 
name = common
sub_directories = yes
file_extension = .txt

directory 
name = common
sub_directories = yes
file_extension = .lua

directory
name = common
sub_directories = yes
file_extension = .csv

directory
name = events
sub_directories = yes
file_extension = .txt

directory
name = map
sub_directories = yes
file_extension = .lua

directory
name = map
sub_directories = yes
file_extension = .txt

directory
name = localisation_synced
sub_directories = yes
file_extension = .yml
```

Prendendo ad esempio la prima sezione, significa che all'avvio del gioco verranno controllati tutti i file `.txt` all'interno della cartella `common` e tutte le sue sotto cartelle. Se anche solo un file di questo tipo viene modificato, il controllo fallisce e il gioco impedisce l'attivazione della modalità Ironman e degli achievement. E lo stesso vale per tutte le altre sezioni indicate nel file di checksum.

### Chiavi e valori

Ogni file contiene delle stringhe formate da una chiave, uguale per ogni linguaggio, e dalla sua traduzione corrispondente. Es. all'interno del file [ai_crisis_l_english.yml](src/mod/localisation/english/ai_crisis_l_english.yml) si trova la linea:

```yml
crisis.2010.name:0 ""Il Segnale Fantasma""
```

In questo caso `crisis.2010.name` è la chiave, `0` è la versione (in caso di aggiornamenti alla chiave la versione viene incrementata), mentre `""Il Segnale Fantasma""` è la traduzione. Da notare i doppi apici `""` che delimitano il valore vero e proprio della traduzione, in questo caso già tradotto.

### Commenti

Le linee che iniziano con il carattere speciale `#` sono dei commenti o delle frasi non più utilizzate. **Non sono da tradurre** perché non verranno mai visualizzate all'interno del gioco.

### Una riga, una traduzione

Non andate a capo durante la traduzione di un testo. Se serve andare a capo, si utilizza il doppio carattere speciale `\n\n` Lo vedrete utilizzare anche nella frase originale inglese, ma potrete usare il carattere per andare a capo in italiano dove preferite, non necessariamente nella stessa posizione della frase originale.

### Caratteri speciali

Alcune frasi contengono caratteri speciali quali `§H` oppure `§!`. **Non devono essere tradotti o rimossi**, il gioco li utilizza per dare enfasi alle frasi del gioco. Immaginate come se fossero dei delimitatori per le parole da mettere in grassetto, con `§H` che significa inizio grassetto e `§!` fine grassetto

### Variabili

Alcune frasi contengono delle variabili che possono essere racchiuse tra parentesi quadre - ad esempio `[Root.Capital.GetName]` - o dal carattere del dollaro - ad esempio `$ancrel.8010.intro$`. Queste parole **non vanno tradotte**: il gioco le sostituirà automaticamente in tempo reale con il valore corretto. **NOTA:** Alcune variabili in italiano vi creeranno problemi, in particolare tutte quelle che prevedono un articolo davanti, perché in italiano non sappiamo esattamente quale usare in anticipo. Prendete ad esempio la seguente frase:

```yml
mirror_trade_reply:0 ""I was about to suggest that myself, for the benefit of all [Root.Owner.Species.GetNamePlural]!""
```

La traduzione corretta sarebbe:

```yml
mirror_trade_reply:0 ""Stavo per suggerirlo io stesso, a beneficio di tutti/e i/gli/le [Root.Owner.Species.GetNamePlural]!""
```

Come potete notare, in inglese `the`, `of all`, ecc. funzionano praticamente sempre bene per tutto - _...of all Humanoids_, _...of all Reptilians_, ecc. - ma in italiano la specie potrebbe essere ad esempio ""i Rettiliani"", ""gli Umanoidi"", ecc. E questo solo per citare le specie esistenti e definite all'interno del file [name_lists_l_english.yml](src/mod/localisation/english/name_lists_l_english.yml). Ma in questo gioco anche l'utente può definire le sue specie personalizzate, quindi non lo sapremo mai con certezza a priori. Il gioco possiede variabili per il singolare e il plurale, ma non ad esempio per la differenza tra gli articoli da utilizzare.

Una possibile soluzione è quella di prenderci qualche libertà durante la traduzione e scrivere ad esempio:

```yml
mirror_trade_reply:0 ""Stavo per suggerirlo io stesso, a beneficio di tutto il popolo [Root.Owner.Species.GetAdj]!""
```

In questo caso non solo ho aggiunto la parola `popolo`, ma ho usato la variabile utilizzata per definire l'aggettivo caratteristico della specie. All'interno del gioco quindi questa frase verrebbe tradotta con `popolo Rettiliano` o `popolo Umanoide`.

In generale, se vi trovate in una impasse sulla possibile traduzione, provate a vedere come è stato fatto in altri punti e seguite la stessa strada. Altrimenti aprite una segnalazione sul repo e discutiamo delle possibili traduzioni.

### Codifica dei caratteri

**La codifica dei caratteri (charset) deve essere UTF-8 con BOM**. Per sicurezza utilizzate direttamente un editor di testo che supporti più charset come ad esempio [Notepad++](https://notepad-plus-plus.org/downloads/) e il charset corretto verrà riconosciuto senza problemi.

Da notare inoltre che i file di testo possono utilizzare dei caratteri speciali all'inizio per determinare l'ordine di scrittura dei byte (il [BOM](https://it.wikipedia.org/wiki/Byte_Order_Mark)). Usando il metodo del _copia e incolla_ questo dettaglio è trascurabile, e solitamente anche se si modificano file esistenti l'editor non tocca il BOM se è presente. Se avete comunque un editor che vi permette di scegliere se applicare o meno il BOM, es. Notepad++ o Visual Studio Code, optate sempre per mantenerlo.

![Notepad++ con codifica ""UTF-8-BOM""](meta/10.png)

![Visual Studio Code con codifica ""UTF-8 with BOM""](meta/11.png)

### Ok, capito, ma come modifico i file?

Per contribuire alle traduzioni, potete clonare questo repository, aggiornare direttamente i file e poi procedere con una pull request per effettuare l'unione del vostro repository con quello principale.

Se non siete pratici di `git`, il mio suggerimento è di diventarlo :) È veramente semplice da utilizzare e vi permette di lavorare in locale sul vostro PC.

Un'alternativa, è lavorare comunque sul vostro PC ma effettuare la proposta di modifica direttamente online. Una volta scaricato e tradotto il file che volete aggiornare:

1. Tornate su questo repository, assicuratevi di essere sul ramo **dev**
![Ramo dev](meta/01.png)

1. Navigate fino al file che volete aggiornare e premete l'icona in alto a destra per la modifica, la matita che dice ""Edit this file""
![Modifica file](meta/02.png)

1. GitHub vi avverte che non avete permessi di scrittura sul repo, che verrà creato un repository uguale a questo sulla vostra utenza e che verrà creato un nuovo ramo con le vostre modifiche. A quel punto, potrete proporre una pull request sul repository principale. Fate una modifica puntuale o, in caso di modifiche multiple, direttamente copia e incolla di tutto il contenuto del vostro file aggiornato
![Modifica file](meta/03.png)

1. premete `Preview changes` per verificare che le modifiche siano quelle che vi aspettate. Se è tutto ok, inserite un messaggio che descrive brevemente la vostra modifica e premete **Propose file change**
![Anteprima modifiche](meta/04.png)

1. A questo punto avete aggiornato il vostro repository, e potete procedere con la proposta di unione. Premete il pulsante **Create pull request**, modificate il testo del commit - se per qualche motivo volete cambiare il testo rispetto a quanto avete inserito precedentemente - e poi premete nuovamente **Create pull request** per confermate
![Verifica differenze](meta/05.png) ![Crea pull request](meta/06.png)

1. La vostra pull request è stata inoltrata correttamente, risulta in stato aperto (_Open_) e non dovrebbe dare errori di unione (_This branch has no conflicts with the base branch_)
![Stato pull request](meta/07.png)

1. Attendete che la PR sia accettata e unita

1. _Opzionale_. Una volta accetta la vostra PR sarà chiusa (_Closed_) e il vostro ramo con la modifica unito (_Merged_) a quello principale. Potete quindi decidere di cancellare il ramo che è rimasto nel vostro repository, visto che ha assolto al proprio compito di proporre la modifica che è stata accettata
![Ramo unito](meta/08.png) ![Cancella ramo](meta/09.png)

## Installazione mod da release

Trovate le release già pronte del mod nell'apposita sezione [releases](https://github.com/Neurone/stellaris-italian-translation/releases). Scaricate la release che vi interessa e decomprimetela all'interno della cartella:

```txt
(Windows)   : %USERPROFILE%\Documents\Paradox Interactive\Stellaris\mod
(GNU/Linux) : ~/.local/share/Paradox Interactive/Stellaris/mod
(Mac)       : ~/Documents/Paradox Interactive/Stellaris/mod
```

Nel caso non fosse presente la cartella `mod` potete crearla voi normalmente.

## Creazione mod da codice sorgente

Clonare il repository:

```bash
git clone https://github.com/Neurone/stellaris-italian-translation.git
```

Installare le dipendenze:

```bash
cd stellaris-italian-translation
npm install
```

### Versione release

Creare il mod:

```bash
npm run release
```

L'output sarà simile al seguente:

```bash
> stellaris-italian-translation@2.1.0 release
> npx grunt

Running ""clean:init"" (clean) task
>> 83 paths cleaned.

Running ""copy:release"" (copy) task
Created 5 directories, copied 73 files

Running ""compress:build"" (compress) task
>> Compressed 78 files.

Running ""copy:complete"" (copy) task
Copied 1 file

Running ""compress:release"" (compress) task
>> Compressed 2 files.

Running ""clean:complete"" (clean) task
>> 2 paths cleaned.

Done.
```

Verrà creato il file `stellaris-italian-translation-x.x.x.zip` all'interno della cartella `build\dist`, dove `x.x.x` sarà la versione corrente del mod (in questo esempio `2.1.0`).

Per installare il mod, scompattare il file all'interno della cartella:

```txt
(Windows)   : %USERPROFILE%\Documents\Paradox Interactive\Stellaris\mod
(GNU/Linux) : ~/.local/share/Paradox Interactive/Stellaris/mod
(Mac)       : ~/Documents/Paradox Interactive/Stellaris/mod
```

Nel caso non fosse presente la cartella `mod` potete crearla voi normalmente. La struttura finale dei file sarà la seguente:

```txt
[...]/Paradox Interactive/Stellaris/mod/stellaris-italian-translation.zip (zip con tutte le risorse del mod)
[...]/Paradox Interactive/Stellaris/mod/stellaris-italian-translation.mod (descrittore del mod)
```

### Versione sviluppo o per Workshop Steam

La versione sviluppo del mod è utile per fare modifiche, test o per effettuare il caricamento sul Workshop Steam. **La versione release non permette di fare il caricamento sul Workshop Steam**.

Creare il mod in versione sviluppo:

```bash
npm run dev
```

L'output sarà simile al seguente:

```bash
> stellaris-italian-translation@2.1.0 dev
> npx grunt dev

Running ""clean:init"" (clean) task
>> 83 paths cleaned.

Running ""copy:dev"" (copy) task
Created 5 directories, copied 75 files

Running ""copy:testing"" (copy) task
Created 6 directories, copied 75 files

Done.
```

Verrà creata la cartella `build\dist` e all'interno troverete un file ed una cartella:

```txt
stellaris-italian-translation\*
stellaris-italian-translation.mod
```

Lo script copierà automaticamente tutti file necessari per il test nella cartella dei mod, in particolare:

```txt
(Windows)   : %USERPROFILE%\Documents\Paradox Interactive\Stellaris\mod
(GNU/Linux) : ~/.local/share/Paradox Interactive/Stellaris/mod
(Mac)       : ~/Documents/Paradox Interactive/Stellaris/mod
```

La struttura finale dei file sarà la seguente:

```txt
[...]/Paradox Interactive/Stellaris/mod/stellaris-italian-translation/... (cartella contenente tutte le risorse del mod)
[...]/Paradox Interactive/Stellaris/mod/stellaris-italian-translation.mod (descrittore del mod)
```

## Configurazione di integrazione per Transifex

```yml
filters:
  - filter_type: dir
    file_format: YAML_GENERIC
    source_file_extension: yml
    source_language: en
    source_file_dir: src/transifex-en/
    translation_files_expression: 'src/transifex-<lang>/'
```",FAUX
Neutrollized/free-tier-gke,Toolkit,Application System,2025-05-15T20:12:45Z,2024-10-19T16:40:56Z,0,0,0,0,6,0,0,2,2020-08-16T17:52:53Z,2025-03-23T10:48:07Z,317,298,HCL,VRAI,36,FAUX,0,,0,Run your very own GKE cluster at a steep discount!,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,2,"# Free-tier GKE Cluster
[GKE Cluster](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster)

[GKE Container Node Pool](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_node_pool)

It's not 100% free, but with my 1 node setup, you can pay as low as ~$9USD/mth for a fully managed Kubernetes cluster.  This works by taking advantage of Google [always free](https://cloud.google.com/free/docs/gcp-free-tier) tier which waives the management fee of one **zonal** GKE cluster, so you only have to pay for your nodes.  Combine this with using ~~[preemptible VMs](https://cloud.google.com/compute/docs/instances/preemptible)~~ [Spot VMs](https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms) as your nodes and you'll have some spectacular savings.

This is great if you're looking for a small k8s cluster that more closely resembles what you might see in the real world (not that [Minikube](https://kubernetes.io/docs/tasks/tools/install-minikube/) or [MicroK8s](https://microk8s.io/) isn't good as a learning tool -- it's just not the same).  Here, you can also scale in/out your cluster easily if you want test some features or add-ons (like service meshes!).

## GKE vs EKS vs AKS
I'm going to use a single node (2CPUs/4GB memory) Kubernetes cluster as the basis for comparison between the 3 major cloud providers (*prices subject to change -- please check cloud provider website for latest numbers*).  The math is shown below, but it doesn't take an extreme couponer to figure out which is the best deal.

#### GKE
- 1 free zonal GKE cluster
- e2-medium @ $27USD/mth (or $14USD/mth for ~~[preemptible](https://cloud.google.com/compute/docs/instances/preemptible)~~ [Spot VM pricing](https://cloud.google.com/spot-vms/pricing?hl=en))

#### EKS
- $0.10/hr per EKS cluster @ 730hrs/mth (or $73USD/mth)
- t3.medium @ $29USD/mth ([Spot](https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&cards.sort-order=asc) instances available at up to 90% savings)

#### AKS
- [free cluster management](https://azure.microsoft.com/en-ca/pricing/details/kubernetes-service/)
- B2S @ $34USD/mth ([Spot](https://azure.microsoft.com/en-us/pricing/spot/) instances available at up to 90% savings). This only applies to non-default node pools as the default node pool is also the [System Node Pool](https://learn.microsoft.com/en-us/azure/aks/use-system-pools?tabs=azure-cli)

Azure's AKS combined with Spot instances are actually incredibly competitive in pricing vs ~~preemptibles~~ spots, but in my mind, ~~preemptibles~~ spots have the edge due to ease of use -- no price bidding and a generably more reliable/predictable uptime (in my use don't think I've had any node get terminated before 22hrs).


## IMPORTANT
The key to getting the savings here is to limit the amount of nodes in your cluster (until you need it).  The 3 key settings to ensure this is `location`, `node_locations` and `node_count` (or `initial_node_count`).  

`location` specifies where to place the cluster (masters).  By specifying a zone, you have a free, zonal cluster.  If you replaced it with a region instead, it becomes a regional cluster -- ideal for a production cluster, but not part of the free tier offering.

Leaving `node_locations` blank will default your node to be in the same zone as your GKE cluster's zone.  Any zone you specify will be **in addition** to the the cluster's zone (i.e. `node_locations = [""northamerica-northeast1-a"",]`), meaning your nodes will span more than one zone.  This is referred to as a multi-zone cluster.

`node_count` specifies how many nodes **per zone** rather than the total node count in your cluster.  Therefore, if you set 3 zones in `node_locations` with a `node_count` of 2, you're going to have 6 nodes in total.

### Enable Required APIs
You can do this via console or...
```
gcloud services enable --async \
  container.googleapis.com
```

### Additional Deployment Notes
- You will need to set an [environment variable](https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/provider_reference#full-reference) to provide credentials to Terraform in order to deploy these blueprints (typically one of `GOOGLE_CREDENTIALS`, `GOOGLE_APPLICATION_CREDENTIALS` or `GOOGLE_OAUTH_ACCESS_TOKEN`) ...or you can `gcloud auth application-default login`
- While `e2-micro` is a viable option for `machine_type`, in practice it's not very useful as all the overhead that comes with GKE such as Stackdriver agent, `kube-dns`, `kube-proxy`, etc. consumes most of available memory.  I recommend starting with at least an `e2-medium` (2CPUs/4GB memory)
- Leaving [`release_channel`](https://cloud.google.com/kubernetes-engine/docs/concepts/release-channels) as `UNSPECIFIED` means that you will perform upgrades manually, where as if you subscribed to a channel, you will the get the regular updates that gets released to that channel
- Depending on your workload/application that you're running, you definitely could run most (or all) of it on a ~~preemptible~~ spot node pool in GCP, but if you're going to run production, please provision a **regional** cluster rather than cheap out for the free zonal one
- If you deployed a private cluster, some of your k8s deployments may fail due to your pods [not having outbound access to the public Internet](https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#docker_hub)...having said that, some of the more common images like the nginx one that I used in my examples folder may still work because you're [pulling from a Docker Hub cache](https://cloud.google.com/container-registry/docs/pulling-cached-images).  Ideally, you should be pulling images from your private GCR in this case
- If `confidential_nodes_enabled` is set to true, the `machine_type` needs to be from the [N2D family](https://cloud.google.com/kubernetes-engine/docs/how-to/confidential-gke-nodes) where the smallest node size starts at `n2d-standard-2` (2CPUs/8GB memory) and it must also NOT be a ~~preemptible~~ spot node (which effectively nullifies one of the cost-saving components of this free-tier GKE)
- [Using GKE with Terraform](https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/using_gke_with_terraform) guide from the Google provider docs


## eBPF, Cilium and GKE Dataplane V2
I've been learning a lot about [eBPF](https://ebpf.io/) and experimenting with [Cilium](https://cilium.io/) in particular.  New in [v0.4.0](https://github.com/Neutrollized/free-tier-gke/blob/master/CHANGELOG.md#040---2021-09-09), you will have the option of enabling [GKE Dataplane V2](https://cloud.google.com/blog/products/containers-kubernetes/bringing-ebpf-and-cilium-to-google-kubernetes-engine) which leverages the power of eBPF and Cilium to provide enhanced security and observability in your GKE cluster.  

When Dataplane V2 is enabled, one of the things you may notice is the absence of **kube-proxy** in the cluster.  That's becuase it has been replaced by Cilium CNI!  It replaces iptables as component that controls connections between pods (and between nodes). Iptables is an old-school (albeit, extensive and powerful) program that allows the configuration of (mainly static) IP packet filter rules in a Linux kernel firewall and was never meant for something as dynamic as a Kubernetes environment.  The sheer number of iptables rules in very large clusters makes scaling difficult and hence a kube-proxy replacement such as Cilium would be very welcomed in such a scenario.

If you wish to install open-sourced Cilium, you will need to set `dataplane_v2_enabled = false` and set a node taint (see [terraform.tfvars.sample](./terraform.tfvars.sample) for details) and if you wish to use DPV2, then make sure you don't set the taint!

If you would like to learn more about Cilium and how to get started, I wrote a short Medium article about it [here](https://medium.com/@glen.yu/getting-started-with-ebpf-and-cilium-on-gke-6553c5d7e02a).

### Hubble
Hubble is an observability platform built on top of Cilium and as of [v0.14.0](https://github.com/Neutrollized/free-tier-gke/blob/master/CHANGELOG.md#0140---2023-08-27) it can be enabled as part of [GKE Dataplane V2 observability tools](https://cloud.google.com/kubernetes-engine/docs/how-to/configure-dpv2-observability#configure-gke-dpv2-observability-tools).  Please see the [Hubble README](./examples/hubble/README.md) for more details.


## Private GKE Cluster and Nodes
As of [v0.8.0](https://github.com/Neutrollized/free-tier-gke/blob/master/CHANGELOG.md#080---2022-07-15), you will have the option of provisioning a private GKE nodes.  Doing so will also provision a [Cloud NAT](https://cloud.google.com/nat/docs/overview) router in order for your nodes to get internet -- but this, of course will incur extra costs.

If you decide to go the full private GKE cluster route (private GKE endpoint/control-plane AND private GKE nodes) then it will provision an additional /29 subnet that will house a VM running [tinyproxy](https://tinyproxy.github.io/) that will act as a forwarding proxy to the private GKE endpoint. 

See this [Medium article](https://medium.com/google-cloud/accessing-gke-private-clusters-through-iap-14fedad694f8) if you want to see how the network traffic flows in this setup.

### IMPORTANT
To use the IAP tunnel, your user needs to have the IAP-secured Tunnel User (**roles/iap.tunnelResourceAccessor**) -- even if you're the Owner of the project, you will need to add this role!!

You will need to create an IAP tunnel from your local machine/laptop to the IAP proxy VM (command will be in the Terraform output) and you will also have to `export HTTPS_PROXY=localhost:8888` (just remember to unset the env var when you're done).  Alternatively you can set an alias which prepends the env var (e.g. `alias k='HTTPS_PROXY=localhost:8888 kubectl '`).

## DNS-based Control Plane Endpoint
New security feature announced in Nov '24, you can read more about it [here](https://cloud.google.com/blog/products/containers-kubernetes/new-dns-based-endpoint-for-the-gke-control-plane). In [v0.24.0](https://github.com/Neutrollized/free-tier-gke/blob/master/CHANGELOG.md#0240---2025-03-06), I've added support for this new feature and below is a table of the various combinations of settings and the resulting endpoint access type:

| `enable_dns_endpoint` | `enable_private_endpoint` | `enable_private_nodes` | Result                                                  |
|:---------------------:|:-------------------------:|:----------------------:|:--------------------------------------------------------|
| `true`                | `true`                    | Either                 | DNS-based endpoint                                      |
| `false`               | `false`                   | Either                 | Public IP-based endpoint with Master Authorized Network |
| `false`               | `true`                    | `true`                 | Private IP-based endpoint with IAP Proxy VM             |

## Test Framework
Starting in [v0.15.0](https://github.com/Neutrollized/free-tier-gke/blob/master/CHANGELOG.md#0150---2023-10-16), I will be including some tests that utilize the native testing framework that was added in Terraform v1.16.0.  To run the tests:

```
terraform test
```   

- sample output:
```console
tests/gke.tftest.hcl... in progress
  run ""setup_tests""... pass
  run ""create_zonal_gke""... pass
tests/gke.tftest.hcl... tearing down
tests/gke.tftest.hcl... pass

Success! 2 passed, 0 failed.
```",VRAI
NewbMiao/opa-koans,Toolkit,Application System,2024-12-16T22:13:49Z,2024-11-30T02:03:52Z,0,17,0,0,0,0,0,0,2020-03-11T07:20:49Z,2025-02-04T03:31:21Z,2452,53,Shell,VRAI,9,FAUX,11,"docker-compose,golang,grafana,nodejs,opa,openpolicyagent,prometheus,rbac,rego,rust,vscode-opa,wasm",11,koans for OPA,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,5,"# opa-koans

![opa-koans](https://github.com/NewbMiao/opa-koans/workflows/opa-koans/badge.svg?branch=master)

OPA入门系列

OPA（OpenPolicyAgent）, 云原生时代的通用规则引擎，重新定义策略引擎，灵活而强大的声明式语言全面支持通用策略定义。

而且，2019年4月2号`OPA`正式进入了`CNCF`，作为孵化级托管项目，详见[声明](https://www.cncf.io/blog/2019/04/02/toc-votes-to-move-opa-into-cncf-incubator/)

## 什么是OPA

> see in [OPA philosophy docs](https://www.openpolicyagent.org/docs/latest/philosophy/#what-is-opa)

关键词：

- 轻量级的通用策略引擎
- 可与服务共存
- 集成方式可以是sidecar、主机级守护进程或库引入

![opa](/misc/opa-service.png)

文字太直白，看看OPA作者怎么说：

- [OPA: The Cloud Native Policy Engine - Torin Sandall, Styra](https://www.bilibili.com/video/BV1AE411V7Hs/)
- [Deep Dive- Open Policy Agent - Torin Sandall & Tim Hinrichs, Styra（2019)](https://www.bilibili.com/video/BV19E411A7BH/)

## 优点

- 强大的声明式策略
  - 上下文感知
  - 表达性强
  - 快速
  - 可移植

- 输入和输出支持任意格式

配合强大的声明式策略语言`Rego`，描述任意规则都不是问题

- 全面支持规则和系统解耦

![如图](/misc/decouple.png)

- 集承方式多
  - Daemon式服务
  - Go类库引入
- 决策快
  - [rule indexing](https://blog.openpolicyagent.org/optimizing-opa-rule-indexing-59f03f17caf3)
  
    ![决策树索引](/misc/rule-indexing.png)

  - [partial evaluation](https://blog.openpolicyagent.org/partial-evaluation-162750eaf422)
  
    ![将动态计算尽可能转为编译时确定的静态规则](/misc/partial-evaluation.png)

- 应用广泛

除了继承做auth外，还可以应用到`k8s`,`terraform`,`docker`,`kafka`,`sql`,`linux`上做规则决策

- 工具齐全
  - 有命令行，有交互式运行环境
  - 支持测试，性能分析（底层实现Go）
  - 有强大的交互式编辑器扩展[vscode-opa](https://marketplace.visualstudio.com/items?itemName=tsandall.opa)
  - 有[playground](https://play.openpolicyagent.org/)分享代码

## 安装

为了性能，推荐使用最新 [OPA latest release](https://github.com/open-policy-agent/opa/releases/latest)

```shell
# mac
curl -L -o opa https://openpolicyagent.org/downloads/latest/opa_darwin_amd64
chmod +x opa
mv opa /usr/local/bin/opa

# or use brew（not latest version）
brew install opa
```

## 入门

### [一个RBAC例子](https://newbmiao.github.io/2020/03/13/opa-quick-start.html)

几行代码实现一个简单的RBAC认证服务: [example_rbac](https://github.com/NewbMiao/opa-koans/tree/master/quick-start)

```sh
cd quick-start
opa eval -i input.json -d data.json -d example.rego ""data.example_rbac""
```

### [如何优雅的开发](https://newbmiao.github.io/2020/03/14/how-to-use-opa-cli-elegantly.html)

### 语法进阶

- [函数和虚拟文档要分清](https://newbmiao.github.io/2020/03/18/opa-func-and-virtual-doc.html)
- [简洁的推导式](https://newbmiao.github.io/2020/03/20/opa-comprehensions.html)
- [测试、性能分析和基准测试](https://newbmiao.github.io/2020/04/05/opa-test-profile-and-benchmark.html)
- [分布式利器Bundle](https://newbmiao.github.io/2020/04/16/opa-bundle.html)
  - [Bundle demo doc](https://newbmiao.github.io/opa-koans/bundle/)

## 实战

[可扩展的Entitlements api demo](/demos/): 可扩展的rules data + entitlemnts policy


文档一点点完善中。。。",VRAI
nirmata/kyverno-charts,Application System,Documentations,2025-05-08T07:48:03Z,2025-01-20T05:03:20Z,0,0,0,0,32,0,0,0,2021-11-06T22:32:10Z,2025-04-03T14:01:15Z,7666,5,Mustache,VRAI,14,FAUX,24,,24,Repository for Nirmata supported Helm charts for Kyverno operator and adapters,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,26,"## Nirmata Kyverno For Enterprise (N4K)

This page contains the list of Helm charts for Nirmata Kyverno Enterprise, the commercial distribution of Kyverno supported by Nirmata. 

### Installing the Enterprise Kyverno Chart 
The [installation](https://github.com/nirmata/kyverno-charts/blob/main/charts/nirmata/README.md) guide lists the instructions to install the Kyverno helm chart as well as the parameters that can be configured during installation.

**Note:** You need the token to download the images. Please contact <support@nirmata.com> to get the token.

In case you have any other questions, you can contact us at <info@nirmata.com>

### Support Contact
<support@nirmata.com>",VRAI
nirmata/kyverno-policies,Documentations,Database,2025-05-13T07:43:23Z,2025-02-21T13:47:51Z,0,0,0,0,284,0,0,0,2021-11-29T18:15:20Z,2025-03-29T18:21:25Z,1387,16,HCL,VRAI,16,FAUX,24,"kubernetes,policy-as-code,security",24,Curated Policy Sets from Nirmata,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,26,"<img referrerpolicy=""no-referrer-when-downgrade"" src=""https://static.scarf.sh/a.png?x-pxid=5b0e6ae5-31f7-455c-8938-de55b61d7607"" />

# Kyverno Policies

This repository contains a set of Kyverno policies curated and managed by the Nirmata team for use as policy groups within [Nirmata Enterprise for Kyverno](https://nirmata.com/kyverno-enterprise/), and [Nirmata Policy Manager](https://nirmata.com/nirmata-cloud-native-policy-manager/).

For more information, visit our website at [https://nirmata.com/](https://nirmata.com/).

Sign-up for a free trial today at [https://try.nirmata.io/](https://try.nirmata.io/)


## Installing Policies

**Clone Repository:**

Clone the kyverno-policies repository.

```console
git clone https://github.com/nirmata/kyverno-policies.git
```

**Install Policies:**

To install Pod Security Standard policies, refer to [pod-security/README.md](pod-security/README.md)

To install Kubernetes Best Practices policies, refer to [best-practices/README.md](best-practices/README.md)

To install PCI-DSS Best Practices policies, refer to [pci-dss/README.md](pci-dss/README.md)

To install Multitenancy and EKS Best Practices

```console
cd kyverno-policies
kubectl apply -f multitenancy
```

Once policies are installed, you can check if they are ready using the command:

```console
kubectl get cpol
```",VRAI
nscuro/dtapac,Toolkit,Application System,2024-10-08T10:13:21Z,2024-09-07T13:37:42Z,0,7,0,0,0,0,0,0,2022-04-15T15:56:50Z,2025-02-09T01:08:43Z,3393,33,Go,VRAI,5,FAUX,12,"dependency-track,dtrack,go,golang,opa,open-policy-agent,owasp,policy-as-code",12,Audit Dependency-Track findings and policy violations via policy as code,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,4,"# dtapac

[![Build Status](https://github.com/nscuro/dtapac/actions/workflows/ci.yml/badge.svg)](https://github.com/nscuro/dtapac/actions/workflows/ci.yml)
[![Latest GitHub release](https://img.shields.io/github/v/release/nscuro/dtapac?sort=semver)](https://github.com/nscuro/dtapac/releases/latest)
[![License](https://img.shields.io/badge/license-Apache%202.0-brightgreen.svg)](LICENSE)

*Audit Dependency-Track findings and policy violations via policy as code*

> Consider this project to be a proof-of-concept. It is not very sophisticated, but it gets the job done.
> Try it in a test environment first. **Do not skip this step, do not run it in production without prior testing**!

## Introduction

[Dependency-Track](https://dependencytrack.org/) offers a fairly sophisticated auditing workflow for vulnerabilities 
and policy violations. However, this workflow is scoped to individual findings or policy violations right now.

I often found myself wanting a mechanism that lets me make more generalized audit decisions that would affect
multiple (and sometimes even *all*) projects in my portfolio. While the most common use case for something like this
is definitely suppressing false positives, there are times when other audit actions are desirable as well.
A [suppression file](https://jeremylong.github.io/DependencyCheck/general/suppression.html) as seen in projects like 
[Dependency-Check](https://jeremylong.github.io/DependencyCheck/) simply won't cut it.

Using scripts to mass-apply analyses works, but then you're stuck with re-running that script everytime the thing
you analysed pops up in another project or project version. Not cool. Individual scripts also don't scale well,
sharing with team members is tedious.

I've written a fair share of tools that provide the desired functionality based on some kind of configuration file, 
but I quickly came to the realization that configuration files are too limiting for my needs. 
Not only are they not a good fit for dynamic decisions, they are also a pain to test.

It turns out that you can have your cake and eat it too using *policy as code*. The most popular implementation of PaC 
probably being [Open Policy Agent](https://www.openpolicyagent.org/) (OPA). 

Think of *dtapac* as a bridge between Dependency-Track and OPA.

![Overview](./.github/images/overview.png)

> Note that *dtapac* as it stands now is **not** intended for performing all auditing through it.
> It's not a complete replacement for manual auditing. Use it for decisions that are likely to affect larger
> parts of your portfolio. Check the [example policies](./examples/policies) to get an idea of what *dtapac*
> can be used for.

## How it works

### Ad-hoc auditing through notifications

The main way that *dtapac* uses to integrate with Dependency-Track is by consuming 
[notifications](https://docs.dependencytrack.org/integrations/notifications/) (aka alerts). 
When receiving a `NEW_VULNERABILITY` or `POLICY_VIOLATION` notification, *dtapac* will immediately 
query OPA for an analysis decision. 

```mermaid
sequenceDiagram
    autonumber
    actor Client
    Client->>Dependency-Track: Upload BOM
    Dependency-Track->>Dependency-Track: Analyze BOM
    alt New Vulnerability identified
        Dependency-Track->>dtapac: NEW_VULNERABILITY notification
    else Policy Violation identified
        Dependency-Track->>dtapac: POLICY_VIOLATION notification
    end
    dtapac->>OPA: Query analysis decision
    OPA->>OPA: Evaluate policy
    OPA->>dtapac: Analysis decision
    dtapac->>Dependency-Track: Query existing analysis
    Dependency-Track->>dtapac: Analysis
    dtapac->>dtapac: Compare analyses
    opt Analysis has changed
        dtapac->>Dependency-Track: Update analysis
    end
```

*dtapac* will only submit the resulting analysis if it differs from what's already recorded in Dependency-Track. 
This ensures that the audit trail won't be cluttered with redundant information, even if *dtapac* receives multiple 
notifications for the same finding or policy violation.

Note that this also means that if you make changes to an analysis that *dtapac* applied for you in Dependency-Track,
*dtapac* will override it during its next execution. This is by design.

### Portfolio auditing on policy change

If configured, *dtapac* can listen for [status updates](https://www.openpolicyagent.org/docs/latest/management-status/) 
from OPA. *dtapac* will keep track of the revision of the policy bundle, and trigger a portfolio-wide analysis when it 
changes.

```mermaid
sequenceDiagram
    autonumber
    loop Periodically
        OPA->>Nginx: Pull Bundle
        Nginx->>OPA: archive
    end
    loop Periodically
        OPA->>dtapac: Status
        dtapac->>dtapac: Compare reported bundle revision with last-known
        opt Bundle revision changed
            dtapac->>Dependency-Track: Fetch all projects
            Dependency-Track->>dtapac: Projects
            loop For each project
                dtapac->>Dependency-Track: Fetch all findings & policy violations
                Dependency-Track->>dtapac: Findings & policy violations
                loop For each finding & policy violation
                    dtapac->>OPA: Query analysis decision
                    OPA->>OPA: Evaluate policy
                    OPA->>dtapac: Analysis decision
                    dtapac->>Dependency-Track: Query existing analysis
                    Dependency-Track->>dtapac: Analysis
                    dtapac->>dtapac: Compare analyses
                    opt Analysis has changed
                        dtapac->>Dependency-Track: Update analysis
                    end
                end
            end
        end
    end
```

This makes it possible to have new policies applied to the entire portfolio shortly after publishing them, without the
need to restart any service or edit files on any server.

### Shortcomings

Some limitations of *dtapac* that you should be aware of before using it:

* **No retries**. If an analysis decision could not be submitted to Dependency-Track for any reason, it won't be retried.
* **No persistence**. If you stop *dtapac* while it's still processing something, that something is gone.
* **No access control**. *dtapac* trusts that whatever is inside the notifications it receives is valid. Notifications can be forged. Expose *dtapac* to an internal network or use a service mesh.
  * [Webhook authentication](https://github.com/DependencyTrack/dependency-track/issues/1555) is a planned feature in Dependency-Track.

## Usage

```
USAGE
  dtapac [FLAGS...]

Audit Dependency-Track findings and policy violations via policy as code.

FLAGS
  -config ...                 Path to config file
  -dry-run=false              Only log analyses but don't apply them
  -dtrack-apikey ...          Dependency-Track API key
  -dtrack-url ...             Dependency-Track API server URL
  -finding-policy-path ...    Policy path for finding analysis
  -host 0.0.0.0               Host to listen on
  -log-json=false             Output log in JSON format
  -log-level info             Log level
  -opa-url ...                Open Policy Agent URL
  -port 8080                  Port to listen on
  -violation-policy-path ...  Policy path for violation analysis
  -watch-bundle ...           OPA bundle to watch
```

All options can alternatively be provided via configuration file and environment variables.

## Writing Policies

The basic idea is that a policy receives a finding or policy violation as input and returns an analysis.

![Policy Overview](./.github/images/policy_overview.png)

OPA's policy language is powerful yet concise and is a perfect fit for our use case.

Please take a moment to read a little about [OPA](https://www.openpolicyagent.org/docs/latest/) and
[Rego](https://www.openpolicyagent.org/docs/latest/policy-language/). I can also recommend
the [Rego style guide](https://github.com/StyraInc/rego-style-guide) for a little more hands-on advice.

### Structure

Policies written in Rego are designed to return either one or multiple results. If a policy is written in a way
that only allows for a single result, OPA will fail when multiple rules match the given input. If a policy allows
for multiple results, OPA (per default) makes no guarantees regarding the order in which results are returned.

This behavior is problematic for *dtapac* for the following reasons:

* For any given finding or violation, there should always be *exactly one* analysis, or none at all
  * Multiple analyses for the same finding make no sense
* It is inevitable that multiple policy rules match
  * If one rule matches on vulnerability V, and another on component C, both will match an alert about C being affected by V
* If multiple rules match, which one should be applied?
  * Auditing should be deterministic. If the output of results has no guaranteed order, what's the correct result?

Luckily, OPA provides a way to indicate that the first matching rule should take precedence over others: [`else`](https://www.openpolicyagent.org/docs/latest/faq/#statement-order).
Policies for *dtapac* thus must be more or less a single if-else-statement (refer to the [example policies](./examples/policies) 
to see how that looks like).

You as policy author have to ensure that you define your rules in an order that fits your requirements.
For example, ordering them by applicability from broad to specific:

![Policy Structure](./.github/images/policy_structure.png)

### Guidelines

Policies for *dtapac* must adhere to the following guidelines:

1. Result MUST be named `analysis`
2. Result MUST be an object
3. There MUST be exactly one result named `analysis`
    * In case of conflicting rules, use [`else`](https://www.openpolicyagent.org/docs/latest/faq/#statement-order)
    * [Incremental definitions](https://www.openpolicyagent.org/docs/latest/policy-language/#incremental-definitions) are NOT supported
4. If no rule is matched, an empty object MUST be returned
    * Use `default analysis = {}` for this
5. Policies for findings and violations MUST be in separate packages
    * For example, use `package dtapac.finding` for findings and `package dtapac.violation` for violations

Have a look at the example policies at [`./examples/policies`](./examples/policies) if you need inspiration.

### Inputs

#### Finding

For findings, the `input` document is structured as follows:

```json
{
  ""component"": {},
  ""project"": {},
  ""vulnerability"": {}
}
```

The available properties of those fields are documented here:

* [`component`](https://pkg.go.dev/github.com/DependencyTrack/client-go#Component)
* [`project`](https://pkg.go.dev/github.com/DependencyTrack/client-go#Project)
* [`vulnerability`](https://pkg.go.dev/github.com/DependencyTrack/client-go#Vulnerability)

Obviously not all properties are always available.

##### Example

```json
{
  ""component"": {
    ""group"": ""com.h2database"",
    ""isInternal"": false,
    ""md5"": ""18c05829a03b92c0880f22a3c4d1d11d"",
    ""name"": ""h2"",
    ""purl"": ""pkg:maven/com.h2database/h2@1.4.200?type=jar"",
    ""sha1"": ""f7533fe7cb8e99c87a43d325a77b4b678ad9031a"",
    ""sha256"": ""3ad9ac4b6aae9cd9d3ac1c447465e1ed06019b851b893dd6a8d76ddb6d85bca6"",
    ""sha512"": ""d1ed996ff57ac22ab10cfcd1831633de20be80982f127f8ab4fdd59bef37457c0882c67ae825d8070c4d9599de93e80ff3860ae9ab66f1102f3b9e8eddb4d883"",
    ""uuid"": ""f1f6fd0a-6dbb-4aab-a1f4-9b0c21754ee8"",
    ""version"": ""1.4.200""
  },
  ""project"": {
    ""name"": ""acme-app"",
    ""tags"": [
      {
        ""name"": ""env/production""
      }
    ],
    ""uuid"": ""8f8203ab-42e0-4d86-a452-a219f5c68daf"",
    ""version"": ""1.2.3""
  },
  ""vulnerability"": {
    ""cvssV2BaseScore"": 10,
    ""cvssV3BaseScore"": 9.8,
    ""description"": ""H2 Console before 2.1.210 allows remote attackers to execute arbitrary code via a jdbc:h2:mem JDBC URL containing the IGNORE_UNKNOWN_SETTINGS=TRUE;FORBID_CREATION=FALSE;INIT=RUNSCRIPT substring, a different vulnerability than CVE-2021-42392."",
    ""source"": ""NVD"",
    ""uuid"": ""21d0e27a-c05f-40b8-a986-0e1c19fb288e"",
    ""vulnId"": ""CVE-2022-23221""
  }
}
```

#### Violation

For policy violations, the `input` document is structured as follows:

```json
{
  ""component"": {},
  ""project"": {},
  ""policyViolation"": {}
}
```

The available properties of those fields are documented here:

* [`component`](https://pkg.go.dev/github.com/DependencyTrack/client-go#Component)
* [`project`](https://pkg.go.dev/github.com/DependencyTrack/client-go#Project)
* [`policyViolation`](https://pkg.go.dev/github.com/DependencyTrack/client-go#PolicyViolation)

##### Example

```json
{
  ""component"": {
    ""group"": ""com.h2database"",
    ""isInternal"": false,
    ""md5"": ""18c05829a03b92c0880f22a3c4d1d11d"",
    ""name"": ""h2"",
    ""purl"": ""pkg:maven/com.h2database/h2@1.4.200?type=jar"",
    ""sha1"": ""f7533fe7cb8e99c87a43d325a77b4b678ad9031a"",
    ""sha256"": ""3ad9ac4b6aae9cd9d3ac1c447465e1ed06019b851b893dd6a8d76ddb6d85bca6"",
    ""sha512"": ""d1ed996ff57ac22ab10cfcd1831633de20be80982f127f8ab4fdd59bef37457c0882c67ae825d8070c4d9599de93e80ff3860ae9ab66f1102f3b9e8eddb4d883"",
    ""uuid"": ""f1f6fd0a-6dbb-4aab-a1f4-9b0c21754ee8"",
    ""version"": ""1.4.200""
  },
  ""policyViolation"": {
    ""uuid"": ""9e3330f7-40f6-4121-a5f2-13fc67c4e36d"",
    ""type"": ""OPERATIONAL"",
    ""policyCondition"": {
      ""uuid"": ""6159e278-26f1-490c-921b-e6d3adf0ee4b"",
      ""operator"": ""MATCHES"",
      ""subject"": ""COORDINATES"",
      ""value"": ""{\""group\"":\""*\"",\""name\"":\""h2\"",\""version\"":\""*\""}"",
      ""policy"": {
        ""uuid"": ""8fc2b2fd-2535-4e45-8d73-ffc1cce0ff13"",
        ""name"": ""ACME Policy"",
        ""violationState"": ""FAIL""
      }
    }
  },
  ""project"": {
    ""name"": ""acme-app"",
    ""tags"": [
      {
        ""name"": ""env/production""
      }
    ],
    ""uuid"": ""8f8203ab-42e0-4d86-a452-a219f5c68daf"",
    ""version"": ""1.2.3""
  }
}
```

### Results

#### Finding

A finding analysis has the same fields as in the Dependency-Track UI:

```json
{
  ""state"": """",
  ""justification"": """",
  ""response"": """",
  ""details"": """",
  ""comment"": """",
  ""suppress"": false
}
```

You can set all fields, or none. No field is strictly required, but it's good practice to at least provide
a `state`, and a `comment` or `justification`. 

##### Example

```json
{
  ""state"": ""EXPLOITABLE"",
  ""details"": ""Exploitable because I say so.""
}
```

#### Violation

A violation analysis has the same fields as in the Dependency-Track UI:

```json
{
   ""state"": """",
   ""comment"": """",
   ""suppress"": false
}
```

##### Example

```json
{
   ""state"": ""APPROVED"",
   ""comment"": ""Bill paid me to approve all his violations."",
   ""suppress"": true
}
```

## Policy Management

It is generally a good idea to keep your policies in their own Git repository.
Treat it just like any other code in your SDLC:

* Write tests
* Create pull requests
* Perform code reviews
* Have a CI pipeline

In your policy CI pipeline, you should:

* [Check](https://www.openpolicyagent.org/docs/latest/cli/#opa-check) your policies using [strict mode](https://www.openpolicyagent.org/docs/latest/strict/) and [schemas](https://www.openpolicyagent.org/docs/latest/schemas/)
    * You can use the input schemas in [`./examples/schemas`](./examples/policies)
    * If you want to write your own schemas, be aware of the [limitations](https://www.openpolicyagent.org/docs/latest/schemas/#limitations)
* [Test](https://www.openpolicyagent.org/docs/latest/policy-testing/) your policies
* Package your policies into a [bundle](https://www.openpolicyagent.org/docs/latest/management-bundles/#bundle-file-format)
    * Always set a `revision` (using the Git commit hash makes sense)
* (Optional) Push the bundle to a server [compatible](https://www.openpolicyagent.org/docs/latest/management-bundles/#implementations) with OPA's bundle API

Check out the [Policy CI](./.github/workflows/policy-ci.yml) workflow if you need some inspiration.

## Deployment

A quick walk through for how to deploy *dtapac* with OPA and NGINX as bundle server.
We're going to use Docker Compose with [`examples/deployment/with-bundleserver/docker-compose.yml`](./examples/deployment/with-bundleserver/docker-compose.yml) 
here. Adapt to your existing Dependency-Track deployment as necessary.

### Preparation

Pull images for Dependency-Track, OPA and NGINX, and build the *dtapac* image:

```shell
docker-compose -f ./examples/deployment/with-bundleserver/docker-compose.yml pull
docker-compose -f ./examples/deployment/with-bundleserver/docker-compose.yml build --pull
```

Launch Dependency-Track:

```shell
docker-compose -f ./examples/deployment/with-bundleserver/docker-compose.yml up -d dtrack
```

Navigate to `http://localhost:8080` and perform the usual setup.

### Get an API key

For *dtapac* to be able to use the Dependency-Track API, it needs an API key
with the following permissions:

| Permission                  | Reason                              |
|:----------------------------|:------------------------------------|
| `VIEW_PORTFOLIO`            | Fetch project + component info      |
| `VIEW_VULNERABILITY`        | Fetch findings + vulnerability info |
| `VULNERABILITY_ANALYSIS`    | Apply analyses to findings          |
| `VULNERABILITY_MANAGEMENT`  | Fetch vulnerability info            |
| `VIEW_POLICY_VIOLATION`     | Fetch policy violations             |
| `POLICY_VIOLATION_ANALYSIS` | Apply analyses to policy violations |

It's recommended to create a dedicated team for *dtapac*, like so:

![Team Permissions](./.github/images/deploy_team.png)

### Setup dtapac

Provide the API key to *dtapac* via `DTRACK_APIKEY` environment variable:

```yaml
# docker-compose.yml

services:
  # ...
  dtapac:
    # ...
    environment:
      # ...
      DTRACK_APIKEY: ""apiKeyFromAbove""
```

Launch *dtapac*:

```shell
docker-compose -f ./examples/deployment/with-bundleserver/docker-compose.yml up -d dtapac
```

### Setup OPA

Create a policy bundle from the [example policies](./examples/policies):

```shell
make build-example-bundle
```

The bundle will be created in [`examples/bundles`](./examples/bundles) as `dtapac.tar.gz`.
The `bundles` directory is mounted into the NGINX container, so that it can be served to OPA.

Launch OPA and NGINX:

```shell
docker-compose -f ./examples/deployment/with-bundleserver/docker-compose.yml up -d opa
```

Verify that OPA successfully fetched the bundle by inspecting its logs:

```shell
docker-compose -f ./examples/deployment/with-bundleserver/docker-compose.yml logs opa
```

You should see a log entry that says something along the lines of:

```json
{""level"":""info"",""msg"":""Bundle loaded and activated successfully. Etag updated to \""628fc22c-31b\""."",""name"":""dtapac"",""plugin"":""bundle"",""time"":""2022-06-22T20:48:56Z""}
```

Starting OPA should've also triggered a portfolio analysis in *dtapac*. Verify by inspecting its logs:

```shell
docker-compose -f ./examples/deployment/with-bundleserver/docker-compose.yml logs -f dtapac
```

You should see something along the lines of:

```
with-bundleserver-dtapac-1  | 8:54PM INF bundle update detected bundle=dtapac revision=1f96e28d4d3f81e3e89889cafff81a06a074c644 svc=bundleWatcher
with-bundleserver-dtapac-1  | 8:54PM INF starting portfolio analysis svc=portfolioAnalyzer
with-bundleserver-dtapac-1  | 8:54PM DBG fetching projects svc=portfolioAnalyzer
...
```

### Set up a webhook

Create a new alert with scope `Portfolio` and publisher `Outbound Webhook`:

![Create Alert](./.github/images/deploy_create-alert.png)

Point the destination to *dtapac*'s `/api/v1/dtrack/notification` endpoint and enable the
`NEW_VULNERABILITY` and `POLICY_VIOLATION` groups:

![Configure Alert](./.github/images/deploy_alert.png)

Goes without saying that you should use a domain or hostname that is reach- and resolvable by
your Dependency-Track instance.

### Testing

The example policy for findings contains a rule that will suppress all [h2](https://h2database.com/html/main.html) 
vulnerabilities for projects with name `Flux Capacitor` or `Mr. Robot`. So let's test that, shall we?

Dependency-Track v4.4.2 ships with a vulnerable h2 version.

1. Download the BOM from [here](https://github.com/DependencyTrack/dependency-track/releases/download/4.4.2/bom.json)
2. In Dependency-Track, create a new project named `Flux Capacitor`, version doesn't matter
3. Upload the BOM you just downloaded
4. In a terminal, follow the logs of *dtapac*
5. Wait for a moment until Dependency-Track finishes its BOM analysis

*dtapac*'s logs should indicate that analyses for h2 related vulnerabilities have been applied, while others
are not covered by the policy:

```
with-bundleserver-dtapac-1  | 9:12PM DBG auditing finding finding={""component"":""8dad0438-00b7-4250-8409-d8e1008e37bc"",""project"":""21790356-27e4-4ffb-837f-a25afdfdf0ff"",""vulnerability"":""e10e283b-8ade-4e86-8697-3687e3af8b92""} svc=auditor
with-bundleserver-dtapac-1  | 9:12PM DBG auditing finding finding={""component"":""8dad0438-00b7-4250-8409-d8e1008e37bc"",""project"":""21790356-27e4-4ffb-837f-a25afdfdf0ff"",""vulnerability"":""9b93e587-e438-4cc1-aa16-f70618e6f839""} svc=auditor
with-bundleserver-dtapac-1  | 9:12PM DBG received finding analysis analysis={""comment"":"""",""details"":""h2 is only used in unit tests."",""justification"":""CODE_NOT_REACHABLE"",""response"":"""",""state"":""NOT_AFFECTED"",""suppress"":true} svc=auditor
with-bundleserver-dtapac-1  | 9:12PM DBG received finding analysis analysis={""comment"":"""",""details"":""h2 is only used in unit tests."",""justification"":""CODE_NOT_REACHABLE"",""response"":"""",""state"":""NOT_AFFECTED"",""suppress"":true} svc=auditor
with-bundleserver-dtapac-1  | 9:12PM INF applying analysis component=8dad0438-00b7-4250-8409-d8e1008e37bc project=21790356-27e4-4ffb-837f-a25afdfdf0ff svc=applier vulnerability=e10e283b-8ade-4e86-8697-3687e3af8b92
with-bundleserver-dtapac-1  | 9:12PM DBG auditing finding finding={""component"":""64623e22-0bad-4dff-8105-dca956745b38"",""project"":""21790356-27e4-4ffb-837f-a25afdfdf0ff"",""vulnerability"":""c61e52b1-30ca-4f5c-9ec1-b7fd116d4b2c""} svc=auditor
with-bundleserver-dtapac-1  | 9:12PM DBG finding is not covered by policy finding={""component"":""64623e22-0bad-4dff-8105-dca956745b38"",""project"":""21790356-27e4-4ffb-837f-a25afdfdf0ff"",""vulnerability"":""c61e52b1-30ca-4f5c-9ec1-b7fd116d4b2c""} svc=auditor
with-bundleserver-dtapac-1  | 9:12PM INF applying analysis component=8dad0438-00b7-4250-8409-d8e1008e37bc project=21790356-27e4-4ffb-837f-a25afdfdf0ff svc=applier vulnerability=9b93e587-e438-4cc1-aa16-f70618e6f839
```

Verify by inspecting the project's findings in the Dependency-Track UI (✅ the *Show suppressed findings* box):

![Applied Analyses in Dependency-Track](./.github/images/test_analyses.png)",VRAI
NVIDIA/cloud-native-stack,DevOPs,Toolkit,2025-05-06T17:25:44Z,2025-02-18T16:24:18Z,0,0,0,0,1,0,0,0,2020-03-20T16:39:34Z,2025-04-07T02:48:56Z,5079,167,Markdown,VRAI,62,FAUX,5,,5,Run cloud native workloads on NVIDIA GPUs,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,11,"# NVIDIA Cloud Native Stack 


## Introduction

NVIDIA Cloud Native Stack (CNS) is a collection of software to run cloud native workloads on NVIDIA GPUs. NVIDIA Cloud Native Stack is based on Ubuntu/RHEL, Kubernetes, Helm and the NVIDIA GPU and Network Operator.

Interested in deploying NVIDIA Cloud Native Stack? This repository has [install guides](https://github.com/NVIDIA/cloud-native-stack/tree/master/install-guides) for manual installations and [ansible playbooks](https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks) for automated installations.

Interested in a pre-provisioned NVIDIA Cloud Native Stack environment? [NVIDIA LaunchPad](https://www.nvidia.com/en-us/data-center/launchpad/) provides pre-provisioned environments so that you can quickly get started.

## Objective

- CNS comes as a reference architecture that list all components that have been tested successfully together. the CNS reference architecture can be used as specification for production deployments.

- CNS also comes as installation guides and playbook that can be used to instantiate a quick K8s environment with NVIDIA operators. The CNS installation guides and playbook are intended only for test and PoC environments.

  Note: The K8s layer that CNS install guide or playbook deploys is basic (no HA for instance) and as such cannot be used for production. However all NVIDIA componants in CNS are fully operable in production environment. 

## Life Cycle

When NVIDIA Cloud Native Stack batch is released, the previous batch enters maintenance support and only receives patch release updates. All prior batches enter end-of-life (EOL) and are no longer supported and do not receive patch updates.

> Note: Upgrades are only supported from previous batch to latest batch.


|  Batch  | Status              |
| :-----: | :--------------:|
| [25.4.0](https://github.com/NVIDIA/cloud-native-stack/releases/tag/v25.4.0)                   | Generally Available | 
| [24.11.2](https://github.com/NVIDIA/cloud-native-stack/releases/tag/v24.11.2)                   | Maintenance         |
| [24.5.0](https://github.com/NVIDIA/cloud-native-stack/releases/tag/v24.5.0) and lower                   | EOL                 |

`NOTE:` CNS Version 15.0 only is Now supports Ubuntu 24.04

For more information, Refer [Cloud Native Stack Releases](https://github.com/NVIDIA/cloud-native-stack/releases)

## Component Matrix

#### Cloud Native Stack Batch 25.4.0 (Release Date: 10 April 2025)

| CNS Version               | 15.0    | 14.1 | 13.3 |
| :-----:                   | :-----: | :------: | :------: |
| Platforms                 | <ul><li>NVIDIA Certified Server (x86 & arm64)</li><li>DGX Server</li></ul> | <ul><li>NVIDIA Certified Server (x86 & arm64)</li><li>DGX Server</li></ul> | <ul><li>NVIDIA Certified Server (x86 & arm64)</li><li>DGX Server</li></ul> |
| Supported OS              |  <ul><li>Ubuntu 24.04 LTS</li></ul> |  <ul><li>Ubuntu 22.04 LTS</li></ul> |  <ul><li>Ubuntu 22.04 LTS</li></ul> |
| Containerd                | 2.0.3  | 2.0.3  | 1.7.27 |
| NVIDIA Container Toolkit  | 1.17.5 | 1.17.5 | 1.17.5 |
| CRI-O                     | 1.32.1 | 1.31.5 | 1.30.10 |
| Kubernetes                | 1.32.2 | 1.31.6 | 1.30.10 |
| CNI (Calico)              | 3.29.2 | 3.29.2 |  3.29.2 |
| NVIDIA GPU Operator       | 25.3.0 | 25.3.0 | 25.3.0 |
| NVIDIA Network Operator   | 25.1.0 | 25.1.0 | 25.1.0 |
| NVIDIA Data Center Driver | 570.124.06 | 570.124.06 | 570.124.06 |
| Helm                      | 3.17.2 | 3.17.2 | 3.17.2 |

> Note: To Previous Cloud Native Stack release information can be found [here](https://github.com/NVIDIA/cloud-native-stack/tree/24.5.0?tab=readme-ov-file#nvidia-cloud-native-stack-component-matrix)

`NOTE:` Cloud Native Stack versions are available with the master branch but it's recommend to use the specific branch.

# Software

- Kubernetes
  - [GPU Operator](https://github.com/NVIDIA/gpu-operator)
  - [Network Operator](https://github.com/Mellanox/network-operator)  
  - [NVIDIA NIM Operator](https://docs.nvidia.com/nim-operator/latest/index.html)
  - [FeatureGates](https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks#enable-feature-gates-to-cloud-native-stack)
- [MicroK8s on CNS](https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks#enable-microk8s)
- [Installation on CSP's](https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks#installation-on-csps)
- [Storage on CNS](https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks#storage-on-cns)
- [Monitoring on CNS](https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks#monitoring-on-cns)
- [LoadBalancer on CNS](https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks#load-balancer-on-cns)
- [Kserve](https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks#enable-kserve-on-cns)
- [LeaderWorkerSet(lws)](https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks#enable-leaderworkerset)

| CNS Version               | 15.0    | 14.1 | 13.3 |
| :-----:                   | :-----: | :------: | :------: |
| MicroK8s                  | 1.32    | 1.31     | 1.30 |
| KServe                    | <br /> **0.15** <br /> <br /> <ul><li>Istio: 1.23.2</li><li>Knative: 1.15.7</li><li>CertManager: 1.16.1</li></ul> | <br /> **0.15** <br /> <br /> <ul><li>Istio: 1.23.2</li><li>Knative: 1.15.7</li><li>CertManager: 1.16.1</li></ul>  | <br /> **0.15** <br /> <br /> <ul><li>Istio: 1.23.2</li><li>Knative: 1.15.7</li><li>CertManager: 1.16.1</li></ul> | 
| LeaderWorkerSet           | 0.5.1 | 0.5.1 | 0.5.1|
| LoadBalancer              | MetalLB: 0.14.9 | MetalLB: 0.14.9 | MetalLB: 0.14.9 |
| Storage                   | NFS: 4.0.18 <br /> Local Path: 0.0.31 | NFS: 4.0.18 <br /> Local Path: 0.0.31 | NFS: 4.0.18 <br /> Local Path: 0.0.31 | 
| Monitoring                | Prometheus: 70.3.0 <br /> Prometheus Adapter: 4.13.0 <br /> Elastic: 8.17.4 | Prometheus: 70.3.0 <br /> Prometheus Adapter: 4.13.0 <br /> Elastic: 8.17.4  | Prometheus: 70.3.0 <br /> Prometheus Adapter: 4.13.0 <br /> Elastic: 8.17.4  |

# Getting Started

#### Prerequisites

Please make sure to meet the following prerequisites to Install the Cloud Native Stack

- system has direct internet access
- system should have an Operating system either Ubuntu 22.04/24.04 and above
- system has adequate internet bandWidth
- DNS server is working fine on the System
- system can access Google repo(for k8s installation)
- system has only 1 network interface configured with internet access. The IP is static and doesn't change
- UEFI secure boot is disabled
- Root file system should has at least 40GB capacity
- system has 2CPU and 4GB Memory
- At least one NVIDIA GPU attached to the system

#### Installation 

Run the below commands to clone the NVIDIA Cloud Native Stack.

```
git clone https://github.com/NVIDIA/cloud-native-stack.git
cd cloud-native-stack/playbooks
```

Update the hosts file in playbooks directory with master and worker nodes(if you have) IP's with username and password like below

```
nano hosts

[master]
<master-IP> ansible_ssh_user=nvidia ansible_ssh_pass=nvidipass ansible_sudo_pass=nvidiapass ansible_ssh_common_args='-o StrictHostKeyChecking=no'
[nodes]
<worker-IP> ansible_ssh_user=nvidia ansible_ssh_pass=nvidiapass ansible_sudo_pass=nvidiapass ansible_ssh_common_args='-o StrictHostKeyChecking=no'
```

Install the NVIDIA Cloud Native Stack stack by running the below command. ""Skipping"" in the ansible output refers to the Kubernetes cluster is up and running.

```
bash setup.sh install
```
For more Information about customize the values, please refer [Installation](https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks#installation)

# Topologies

- Cloud Native Stack allows to deploy:
    - 1 node with both control plane and worker functionalities
    - 1 control plane node and any number of worker nodes

`NOTE:` (Cloud Native Stack does not allow the deployment of several control plane nodes)

# Troubleshooting

[Troubleshoot CNS installation issues](https://github.com/NVIDIA/cloud-native-stack/blob/master/troubleshooting/README.md)

# Getting help or Providing feedback

Please open an [issue](https://github.com/NVIDIA/cloud-native-stack/issues) on the GitHub project for any questions. Your feedback is appreciated.

# Useful Links
- [NVIDIA LaunchPad](https://www.nvidia.com/en-us/data-center/launchpad/)
- [NVIDIA LaunchPad Labs](https://docs.nvidia.com/launchpad/index.html)
- [Cloud Native Stack on LaunchPad](https://docs.nvidia.com/LaunchPad/developer-labs/overview.html)
- [NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html)
- [NVIDIA Network Operator](https://docs.nvidia.com/networking/display/COKAN10/Network+Operator)
- [NVIDIA Certified Systems](https://www.nvidia.com/en-us/data-center/products/certified-systems/)
- [NVIDIA GPU Cloud (NGC)](https://catalog.ngc.nvidia.com/)",VRAI
NVIDIA/gpu-operator,Toolkit,Toolkit,2025-05-16T00:30:25Z,2025-04-28T20:31:31Z,0,0,0,0,78,0,0,0,2019-02-26T22:56:06Z,2025-04-08T11:32:10Z,96048,2078,Go,VRAI,337,FAUX,362,"cuda,gpu,kubernetes,nvidia",362,"NVIDIA GPU Operator creates, configures, and manages GPUs in Kubernetes",FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,60,"[![license](https://img.shields.io/github/license/NVIDIA/gpu-operator?style=flat-square)](https://raw.githubusercontent.com/NVIDIA/gpu-operator/master/LICENSE)
[![pipeline status](https://gitlab.com/nvidia/kubernetes/gpu-operator/badges/master/pipeline.svg)](https://gitlab.com/nvidia/kubernetes/gpu-operator/-/pipelines)
[![coverage report](https://gitlab.com/nvidia/kubernetes/gpu-operator/badges/master/coverage.svg)](https://gitlab.com/nvidia/kubernetes/gpu-operator/-/pipelines)

# NVIDIA GPU Operator

![nvidia-gpu-operator](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/egx/nvidia-egx-platform-gold-image-full-2c50-d@2x.jpg)

Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which  are difficult and prone to errors.
The NVIDIA GPU Operator uses the [operator framework](https://cloud.redhat.com/blog/introducing-the-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision GPU. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling, [DCGM](https://developer.nvidia.com/dcgm) based monitoring and others.

## Audience and Use-Cases
The GPU Operator allows administrators of Kubernetes clusters to manage GPU nodes just like CPU nodes in the cluster. Instead of provisioning a special OS image for GPU nodes, administrators can rely on a standard OS image for both CPU and GPU nodes and then rely on the GPU Operator to provision the required software components for GPUs.

Note that the GPU Operator is specifically useful for scenarios where the Kubernetes cluster needs to scale quickly - for example provisioning additional GPU nodes on the cloud or on-prem and managing the lifecycle of the underlying software components. Since the GPU Operator runs everything as containers including NVIDIA drivers, the administrators can easily swap various components - simply by starting or stopping containers.

## Product Documentation
For information on platform support and getting started, visit the official documentation [repository](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html).

## Webinar
[How to easily use GPUs on Kubernetes](https://info.nvidia.com/how-to-use-gpus-on-kubernetes-webinar.html)

## Contributions
[Read the document on contributions](https://github.com/NVIDIA/gpu-operator/blob/master/CONTRIBUTING.md). You can contribute by opening a [pull request](https://help.github.com/en/articles/about-pull-requests).

## Support and Getting Help
Please open [an issue on the GitHub project](https://github.com/NVIDIA/gpu-operator/issues/new) for any questions. Your feedback is appreciated.",FAUX
okteto/okteto,Toolkit,Application System,2025-05-14T09:21:47Z,2025-03-06T16:08:07Z,1,0,0,0,0,0,0,0,2018-08-07T08:57:27Z,2025-04-08T11:42:01Z,25577,3332,Go,VRAI,311,FAUX,35,"cloud-native,cloud-native-developers,debug,developer-tools,development,docker,hacktoberfest,helm,kubernetes,okteto",35,Develop your applications directly in your Kubernetes Cluster,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,73,"# Okteto: A Tool to Develop Applications on Kubernetes

[![CircleCI](https://circleci.com/gh/okteto/okteto.svg?style=svg)](https://circleci.com/gh/okteto/okteto)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3055/badge)](https://bestpractices.coreinfrastructure.org/projects/3055)
[![GitHub release](https://img.shields.io/github/release/okteto/okteto.svg?style=flat-square)](https://github.com/okteto/okteto/releases)
[![Apache License 2.0](https://img.shields.io/github/license/okteto/okteto.svg?style=flat-square)](https://github.com/okteto/okteto/blob/master/LICENSE)
![Total Downloads](https://img.shields.io/github/downloads/okteto/okteto/total?logo=github&logoColor=white)
[![Chat in Slack](https://img.shields.io/badge/slack-@kubernetes/okteto-red.svg?logo=slack)](https://kubernetes.slack.com/messages/CM1QMQGS0/)

## Overview

Kubernetes has made it very easy to deploy applications to the cloud at a higher scale than ever, but development practices have not evolved at the same speed as application deployment patterns.

Today, most developers try to either run parts of the infrastructure locally or just test their integrations directly in the cluster via CI jobs, or the _docker build/redeploy_ cycle. It works, but this workflow is painful and incredibly slow.

`okteto` accelerates the development workflow of Kubernetes applications. You write your code locally and `okteto` detects the changes and instantly updates your Kubernetes applications.

## How it works

Okteto enables development inside a container, providing a seamless IDE and tool integration as if you were working locally but with the resources of a remote cluster. When you run `okteto up` your Kubernetes deployment is replaced by a Development Container that contains your development tools (e.g. maven and jdk, or npm, python, go compiler, debuggers, etc). This development container can use any [docker image](https://okteto.com/docs/development/images/). The development container inherits the same secrets, configmaps, volumes or any other configuration value of the original Kubernetes deployment.

<img align=""left"" src=""images/how-does-it-work.png"">

The end result is a remote cluster that is seen by your IDE and tools as a local filesystem/environment. You keep writing code on your local IDE and as soon as you save a file, the change goes to the development container, and your application instantly updates (taking advantage of any hot-reload mechanism you already have). This whole process happens in an instant. No docker images need to be created and no Kubernetes manifests need to be applied to the cluster.

## Why Okteto

`okteto` has several advantages when compared to more traditional development approaches:

- **Fast inner loop development**: build and run your application using your favorite tools directly from your development container. Native builds are always faster than the _docker build/redeploy_ cycle.
- **Realistic development environment**: your development container reuses the same variables, secrets, sidecars, volumes as your original Kubernetes deployment. Realistic environments eliminate integration issues.
- **Replicability**: development containers eliminate the need to install your dependencies locally, everything is pre-configured in your development image.
- **Unlimited resources**: get access to the hardware and network of your cluster when developing your application.
- **Deployment independent**: `okteto` decouples deployment from development. You can deploy your application with kubectl, Helm, a serverless framework, or even a CI pipeline and use `okteto up` to develop it. This is especially useful for cloud-native applications where deployment pipelines are not trivial.
- **Works anywhere**: `okteto` works with any Kubernetes cluster, local or remote. `okteto` is also available for macOS, Linux, and Windows.

## Getting started

All you need to get started is to [install the Okteto CLI](https://www.okteto.com/docs/get-started/install-okteto-cli/) and have access to a Kubernetes cluster. You can follow our [guide for setting up a Kubernetes cluster on AWS here](https://www.okteto.com/docs/get-started/install/amazon-eks/#deploy-a-kubernetes-cluster).

### Okteto Open Source CLI Features

The Okteto Open Source CLI requires access to a Kubernetes cluster. The Okteto Open Source CLI aims to enable you to develop your cloud-native applications using Development Containers in your Kubernetes clusters. If you are interested in helping your entire team share a Kubernetes cluster for development, we recommend you check out our commercial product, the [Okteto Platform](https://okteto.com)

The Okteto Open Source CLI supports the following commands:

- `okteto context`
- `okteto up`
- `okteto down`

> [!NOTE]
> ⚠️ Notice: The open-source version of Okteto only supports the [dev section](https://www.okteto.com/docs/reference/okteto-manifest/#dev-object-optional) of the Okteto manifest. For additional features and full functionality, consider exploring the [Okteto Platform](https://www.okteto.com/get-demo/).

We have getting started guides for the Open Source mode for the following languages:

- [ASP.NET](samples/aspnetcore/README.md)
- [Golang](samples/golang/README.md)
- [Java Gradle](samples/java-gradle/README.md)
- [Java Maven](samples/java-maven/README.md)
- [Node.js](samples/node.js/README.md)
- [PHP](samples/php/README.md)
- [Python](samples/python/README.md)
- [Ruby](samples/ruby/README.md)

### Okteto Platform CLI Features

The Okteto Platform CLI requires the installation of the [Okteto Helm Chart](https://www.okteto.com/docs/get-started/overview/) in your Kubernetes cluster. In this mode, all the Okteto CLI commands are available (`build`, `deploy`, `up`, `down`, `destroy`, etc). The Okteto Platform comes with additional features like:

- User authentication and access control to Kubernetes using your Identity provider
- Build service for remote container image creation
- Integration with GitHub, Gitlab, Bitbucket, etc.
- Preview environments for every pull request
- Dynamic scaling of environments based on usage
- Secrets manager for your development environments
- Okteto Insights to provide observability on your development environments

And much more! Please take a look at the [Okteto Platform docs](https://www.okteto.com/docs) to learn more.

## Features Comparison

| Feature                    | Okteto Open Source CLI                   | Okteto Platform CLI                           |
| -------------------------- | ---------------------------------------- | --------------------------------------------- |
| **Development Containers** | Available                                | Available                                     |
| **Build Service**          | Not Available                            | Available                                     |
| **User Management**        | Not Available                            | Available                                     |
| **Access Control**         | Not Available                            | Available                                     |
| **Automated Scaling**      | Not Available                            | Available                                     |
| **Secrets Management**     | Not Available                            | Available                                     |
| **Observability Tools**    | Not Available                            | Available                                     |
| **Support**                | Community Support                        | Premium Support available                     |
| **Documentation**          | [Open Source Samples](samples/README.md) | [Platform Docs](https://www.okteto.com/docs/) |

## Useful links

- [Getting started](https://www.okteto.com/docs/get-started/install-okteto-cli/)
- [CLI reference](https://okteto.com/docs/reference/okteto-cli)
- [Okteto manifest reference](https://okteto.com/docs/reference/okteto-manifest/)
- [Okteto Open Source Samples](samples/README.md)

## Releases

Okteto is monthly released into three channels: stable, beta, and dev. When Okteto is installed, the stable channel is used by default. If you need to access features that are not yet available, you can install the Okteto CLI from the beta or dev channels. More information can be found in the [release documentation](docs/RELEASE.md).

## Support and Community

Got questions? Have feedback? Join the conversation in our [Community Forum](https://community.okteto.com/).

Follow [@OktetoHQ](https://twitter.com/oktetohq) on Twitter for important announcements.

## ✨ Contributions

We ❤️ contributions, big or small. [See our guide](contributing.md) on how to get started.

You can also join us in the [#okteto](https://kubernetes.slack.com/messages/CM1QMQGS0/) Slack channel and chat with us! If you don't already have a Kubernetes Slack account, [sign up here](https://slack.k8s.io/).

### Thanks to all our contributors!

<a href=""https://github.com/okteto/okteto/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=okteto/okteto"" />
</a>
<!--  https://contrib.rocks -->",VRAI
ollionorg/gcp-landing-zone,Toolkit,Documentations,2024-04-12T09:04:01Z,2023-12-06T09:02:26Z,0,215,0,0,0,0,0,136,2022-08-26T05:56:02Z,2025-04-01T15:09:53Z,4995,33,HCL,VRAI,14,FAUX,7,"cis-gcp-benchmark,cloud-compliance,cloud-security,compliance,foundation,gcp,gcp-enterprice-foundations-blueprint,gcp-landing-zone,gcp-landing-zones,google-cloud-landing-zone,google-cloud-landing-zone-platform,nist800-53,opa,pci-dss,regula,security,security-foundation",7,"Leverage Ollion's GCP Landing Zone to deploy a secure, compliant foundation with ease. The repository contains an implementation of a secure and compliant landing zone pattern that will help expedite cloud migration for an enterprise in a heavily regulated industry.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,12,"![OLLION](https://github.com/ollionorg/gcp-landing-zone/assets/88661454/464b63a2-b85b-4a82-a376-f11b72b639bf)

# Google Cloud Foundation Landing Zone

[![Documentation](https://img.shields.io/badge/Wiki-User_Guide-red?logo=read-the-docs)](https://github.com/ollionorg/gcp-landing-zone/wiki/User-Guide)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Discord](https://discordapp.com/api/guilds/1212635303412506694/widget.png?style=shield)](https://discord.gg/PCyyu2Xq)
[![Reports](https://img.shields.io/badge/Compliance_report-100%25-green.svg)](https://github.com/ollionorg/gcp-landing-zone/wiki/User-Guide#how-compliant-your-landing-zone-to-well-known-best-practices)


This repository contains the Terraform code necessary to set up a Landing Zone using the Google Cloud Platform (GCP). It is designed to follow best practices outlined in the Google Cloud Architecture's Security Foundations. By leveraging this code, users can create a secure, scalable, and efficient cloud environment that aligns with Google's recommended practices.


**[Landing Zone User Guide](https://github.com/ollionorg/gcp-landing-zone/wiki/User-Guide)**

## Repository Directory Structure

To maintain organization and facilitate navigation, this repository is structured as follows:

```
repo:
|- 0-bootstrap
   |- <terraform_code>
|- 1-org
   |- <terraform_code>
|- 2-env
   |- <terraform_code>
|- 3-networks
   |- <terraform_code>
|- 4-projects
   |- <terraform_code>
|- 5-app-infra
   |- <terraform_code>
|- 6-anthos
   |- <application_manifests>   
|- kcc
   |- <terraform_code>
   |- <resource_manifests>
|- modules
   |- <terraform_module_folder_1>
   |- <terraform_module_folder_2>
```

## Software Prerequisites 

To run the commands described in this document, you need to have the following installed:

- **Google Cloud SDK:** Version 319.0.0 or later. [Install Guide](https://cloud.google.com/sdk/install)
- **Terraform:** Version 1.0.9. [Download](https://www.terraform.io/downloads.html)
- **Git:** Version v2.38.1


## Prerequisites

Before you begin deploying the Landing Zone, ensure you have the following prerequisites in place:

> **Note:** Consistency in Terraform version is crucial to avoid state lock errors. Follow the steps below to set up your environment correctly:

1. **Google Cloud Organization:** Set up as described [here](https://cloud.google.com/resource-manager/docs/creating-managing-organization).
2. **Billing Account:** Create and manage as per instructions [here](https://cloud.google.com/billing/docs/how-to/manage-billing-account). Remember to:
   - Note the billing ID.
   - Increase the quota for associating projects to 50.
3. **Authentication:** Configure by creating Cloud Identity or Google Workspace groups for admins.
4. **Permissions:** Create a `group_org_admins` group and assign necessary roles including 
     - `roles/resourcemanager.projectCreator`
     - `roles/resourcemanager.folderCreator `
     - `roles/resourcemanager.organizationAdmin`
     - `roles/billing.admin`.
5. **Service Accounts:** Ensure GitHub Actions and Cloud Build service accounts are added to the `group_org_admins`.

For a detailed setup, refer to the [organization bootstrap module documentation](https://github.com/terraform-google-modules/terraform-google-bootstrap).

## Deployment Process

### Step 0 - Forking Github repo

Follow the steps to fork or clone the landing zone GITHUB repo on your local machine:

1. Create bot machine user and create a personal access token (PAT) on GitHub for user .
   Grants access to PAT as following permission `read:org` and `read:discussions`.   
   `Goto GitHub profile` > `Settings` > `Developer Settings` > `Personal Access Tokens` > `Generate New Token`. Note the new token value.
2. Create GitHub Token Secret in GitHub as  `GH_TOKEN`.
3. Clone the repo:
      ```
      git clone https://github.com/ollionorg/gcp-landing-zone.git
      ```

### Step 1 - Collect configuration information

> The following steps should be executed on your local system to initialize and run the bootstrap phase for the GCP landing zone.

1. Ensure the tf variable file inside the file [0-bootstrap/terraform.tfvars](https://github.com/ollionorg/gcp-landing-zone/blob/main/0-bootstrap/terraform.tfvars) are properly set and commit the changes to the repository. For setting the parameters, please refer to the [input documentation](https://github.com/ollionorg/gcp-landing-zone/blob/main/0-bootstrap/README.md#inputs).
2. Add correct billing ID in the `0-bootstrap/terraform.tfvars` file for the initial deployment of the 0-bootstrap stage.
3. Optional if required, Create Slack WebHook Secret in GitHub `GCF_SLACK_WEBHOOK`.
5. Setup `GH_TOKEN` as `$GITHUB_PAT` environment variable and execute the [wrapper script](https://github.com/ollionorg/gcp-landing-zone/blob/main/prerequisites/scripts/wrapper.sh) using below commands (Ensure .terraform directories created locally from previous runs are deleted):
   ```
      $ cd ./prerequisites/scripts

         $ read -s token
         <enter_github_token_obtained_in_step_9>

         $ export GITHUB_PAT=$token

         $ chmod +x wrapper.sh ; ./wrapper.sh
   ```
6. After execution of wrapper script, add cloudbuild service account from cicd project as a principal in the billing account with `billing.administrator` and `billing.user`
7. Wrapper script will create following branch protection rules for branch `${bu_name}-main`.
   ```
   Require a pull request before merging.
   Required two number of approvals before merging
   Dismiss stale pull request approvals when new commits are pushed
   Require review from Code Owners
   Require status checks to pass before merging
   Require branches to be up to date before merging
   Require conversation resolution before merging.
   Require signed commits.
   Requires administrator
   Allow auto-merge and allow auto-deletion of branches
   ```
8. Commit and push changes from `.github/workflows`, `build`, `0-bootstrap`, `README.md` to `${bu_name}-init` and ensure everything gets successfully applied. 
9. Start deployment by raising PRs for subsequent stages by pushing the backend.tf changes made by wrapper script for each stage in ${bu_name}-init.

      
## Troubleshooting

Describe the problem clearly and send an email to lzhelp@ollion.com.",VRAI
ONSdigital/sdg-data,Application System,Database,2024-03-08T15:32:08Z,2023-09-07T14:22:33Z,0,0,0,0,0,2,0,0,2018-05-11T14:48:42Z,2025-03-02T18:25:50Z,426445,18,Python,VRAI,61,FAUX,3,,3,Data repository for SDGs,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,95,"# SDG Data Repository   




[![Build Status](https://travis-ci.org/ONSdigital/sdg-data.svg?branch=develop)](https://travis-ci.org/ONSdigital/sdg-data)
 [![LICENSE.](https://img.shields.io/badge/license-OGL--3-brightgreen.svg?style=flat)](http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/)

This repository holds the UK data for SDG reporting. The data is served via a static http server.

# Servers

The develop branch serves from the `gh-pages` branch on this repository. This is for staging.

The master branch serves from the main deployment org and is for prod.

# Routes:

Loosely speaking with have: `/<datatype>/<id>.<format>` and support csv and json file formats. You can also look at the file structure at https://github.com/ONSdigital/sdg-data/tree/gh-pages and it shows how it's all laid out.

## Versions

We don't currently support API versioning.

## Data

The main data set, the raw data lives in `/data/` in the repo.

```
/data/<id>.<format>

/data/1-2-1.csv
/data/1-2-1.json
```
e.g. https://ONSdigital.github.io/sdg-data/data/1-2-1.json

### Edges

```
/edges/1-2-1.csv

/edges/1-2-1.json
```

### Combined data and edges

```
/comb/1-2-1.csv
/comb/1-2-1.json
```

## Metadata

```
/meta/<id>.json
```

## Statistics

Statistics generated from the data and metadata. Currently just the reporting status statistics.

```
/stats/reporting.json
```

## Build time routes

At build time you'll need everything. Rather than making you download each indicator separately we have an ID of `all` which you can use.

### Headline data

Headlines json comes formatted as records, instead of in list format.

```
/headline/<id>.<format>
/headline/all.json
```


### Metadata

The following is all metadata for all indicators in one blob for the build.

```
/meta/all.json
```

It's a JSON object with `{<id>: <meta>}` pairs.

Scripts:

* `check_data.py`: Runs data and metadata checks and will prevent deployment if fails.
* `build_data.py`: Builds main data, headline, and edges output in csv, and json.

Packages:

This uses the `sdg` package from the `sdg-build` repository during the build.

## License

Data (`data/` and `meta/`) is under [open government license v3](http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/).

Code (`scripts/` and top level) is MIT © Office for National Statistics",VRAI
open-cluster-management-io/governance-policy-framework-addon,Toolkit,Toolkit,2025-04-03T15:40:03Z,2024-04-29T18:07:02Z,0,0,0,0,0,0,0,14,2022-09-07T08:03:39Z,2025-04-03T16:27:03Z,8042,8,Go,VRAI,21,FAUX,1,,1,The governance-policy-framework OCM addon that handles syncing from and to the Hub.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,21,"[comment]: # "" Copyright Contributors to the Open Cluster Management project ""

# Governance Policy Framework Addon [![KinD tests](https://github.com/open-cluster-management-io/governance-policy-framework-addon/actions/workflows/kind.yml/badge.svg?branch=main&event=push)](https://github.com/open-cluster-management-io/governance-policy-framework-addon/actions/workflows/kind.yml)[![License](https://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)

## Description

### Secret Sync Controller

The secret sync controller runs on managed clusters and syncs the `policy-encryption-key` `Secret` from the Hub to the
managed cluster. This controller requires access to get, create, update, and delete `Secret` objects in the managed
cluster namespace. Since the managed cluster namespace is not known at build time, the configuration in
`deploy/operator.yaml` grants this access cluster wide. In a production environment, limit this to just the managed
cluster namespace.

### Spec Sync Controller

The spec sync controller runs on managed clusters, updating local `Policy` specs to match `Policies` in the cluster's
namespace on the hub cluster.

The controller watches for changes to Policies in the cluster's namespace on the hub cluster to trigger a reconcile.
Every reconcile creates/updates/deletes replicated policies on the managed cluster to match the spec from the hub
cluster.

### Status Sync Controller

The status sync controller runs on managed clusters, updating `Policy` statuses on both the hub and (local) managed
clusters, based on events and changes in the managed cluster.

This controller watches for the following changes to trigger a reconcile:

1. policy changes in the watched cluster namespace on the managed cluster
2. events on policies in the watched cluster namespace on the managed cluster

Every reconcile does the following things:

1. Creates/updates the policy status on the hub and managed cluster in cluster namespace

### Template Sync Controller

The template sync controller runs on managed clusters and updates objects defined in the templates of `Policies` in the
cluster namespace.

This controller watches for changes on `Policies` in the cluster namespace on the managed cluster to trigger a
reconcile. On each reconcile, it creates/updates/deletes objects defined in the `spec.policy-templates` of those
`Policies`.

## Getting started

For documentation and installation guidance, see the
[Open Cluster Management documentation](https://open-cluster-management.io/getting-started/integration/policy-framework/).

Go to the
[Contributing guide](https://github.com/open-cluster-management-io/community/blob/main/sig-policy/contribution-guidelines.md)
to learn how to get involved.

Check the [Security guide](SECURITY.md) if you need to report a security issue.

### Build and deploy locally

You will need [kind](https://kind.sigs.k8s.io/docs/user/quick-start/) installed.

```bash
make kind-bootstrap-cluster-dev
make build-images
make kind-deploy-controller-dev
```

### Running tests

```
make test-dependencies
make test

make e2e-dependencies
make e2e-test
```

### Clean up

```
make kind-delete-cluster
```

### deploy/operator.yaml

The `deploy/operator.yaml` file is generated via Kustomize. The `deploy/rbac` directory of Kustomize files is managed by
the operator-sdk and Kubebuilder using [markers](https://book.kubebuilder.io/reference/markers.html). After updating the
markers or any of the Kustomize files, you may regenerate `deploy/operator.yaml` by running
`make generate-operator-yaml`.

## References

- The `governance-policy-framework-addon` is part of the `open-cluster-management` community. For more information,
  visit: [open-cluster-management.io](https://open-cluster-management.io).

<!---
Date: 2022-11-28
-->",FAUX
open-cluster-management-io/policy-collection,Documentations,Documentations,2025-05-07T13:46:01Z,2024-06-11T09:48:33Z,0,8,0,0,91,0,0,21,2020-06-17T20:49:04Z,2025-03-28T14:47:22Z,7678,217,Shell,VRAI,261,FAUX,29,,29,A collection of policy examples for Open Cluster Management,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,68,"# Policy Collection

A collection of policy examples for Open Cluster Management.

> **Important**: The `PlacementRule` resource has been deprecated so policy users must begin moving to
the Placement API instead. See the
[Transitioning from `PlacementRule`(deprecated) to `Placement`](#transitioning-from-placementruledeprecated-to-placement)
that provides details below to learn how to begin using Placement. Policies will no longer include
placement details as part of contributions since placement resources can be shared to avoid
duplication and to allow users to choose different ways of including placement with gitops.

## Repository structure

This repository hosts policies for Open Cluster Management. You can deploy these policies using
[Open Cluster Management](https://open-cluster-management.io/) which includes a policy framework
that is available as an addon. Policies are organized in two ways:

1. By support expectations which are detailed below.
2. By [NIST Special Publication 800-53](https://nvd.nist.gov/800-53).

The following folders are used to separate policies by the support expectations:

- [stable](stable) -- Policies in the `stable` folder are contributions that are being supported by
  the Open Cluster Management Policy SIG.
- [3rd-party](3rd-party) -- Policies in the `3rd-party` folder are contributions that are being
  supported, but not by the Open Cluster Management Policy SIG. See the details of the policy to
  understand the support being provided.
- [community](community) -- Policies in the `community` folder are contributed from the open source
  community. Contributions should start in the community.

In addition to policy contributions, there is the option to contribute groups of policies as a set.
This is known as `PolicySets` and these contributions are made in directories organized as
`PolicyGenerator` projects. The folder containing these contributions is located here:
[`PolicySet` projects](https://github.com/open-cluster-management-io/policy-collection/tree/main/policygenerator/policy-sets).

## Using GitOps to deploy policies to a cluster

Fork this repository and use the forked version as the target to run the sync. This avoids
unintended changes to your cluster. To get the latest policies from the `policy-collection`
repository, pull the latest changes from `policy-collection` and then create a pull request to merge
these changes into your forked repository. Any further changes to your repository will be applied to
your cluster automatically.

Make sure you have [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) installed and
that you are logged into your hub cluster in terminal.

Run `kubectl create ns policies` to create a ""policies"" ns on hub. If you prefer to call the
namespace something else, you can run `kubectl create ns <custom ns>` instead.

From within this directory in terminal, run `cd deploy` to access the deployment directory, then run
`bash ./deploy.sh -u <url> -p <path> -n <namespace>`. (Details on all of the parameters for this
command can be viewed in its [README](deploy/README.md).) This script assumes you have enabled
Application lifecycle management as an addon in your Open Cluster Management installation. See
[Application lifecycle management](https://open-cluster-management.io/getting-started/integration/app-lifecycle/)
for details on installing the Application addon. **Note**: If you are using ArgoCD for gitops, a
similar script [argoDeploy.sh](deploy/argoDeploy.sh) is provided that does not require the
Application Lifecycle addon.

### Subscription Administrator

In new versions of Open Cluster Management you must be a subscription administrator in order to
deploy policies using a subscription. In these cases the subscription is still successfully created,
but policy resources are not distributed as expected. You can view the status of the subscription to
see the subscription errors. If the subscription administrator role is required, a message similar
to the following one appears for any resource that is not created:

```
        demo-stable-policies-chan-Policy-policy-cert-ocp4:
          lastUpdateTime: ""2021-10-15T20:37:59Z""
          phase: Failed
          reason: 'not deployed by a subscription admin. the resource apiVersion: policy.open-cluster-management.io/v1 kind: Policy is not deployed'
```

To become a subscription administrator, you must add an entry for your user to the
`ClusterRoleBinding` named `open-cluster-management:subscription-admin`. A new entry may look like
the following:

```
subjects:
  - kind: User
    apiGroup: rbac.authorization.k8s.io
    name: my-username
```

After updating the `ClusterRoleBinding`, you need to delete the subscription and deploy the
subscription again.

### Policy Generator

GitOps through Open Cluster Management is able to handle Kustomize files, so you can also use the
[Policy Generator](https://github.com/stolostron/policy-generator-plugin) Kustomize plugin to
generate policies from Kubernetes manifests in your repository. The Policy Generator handles
Kubernetes manifests as well as policy engine manifests from policy engines like
[Gatekeeper](https://open-policy-agent.github.io/gatekeeper/) and [Kyverno](https://kyverno.io/).

For additional information about the Policy Generator:

- [Policy Generator product documentation](https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html/governance/governance#policy-generator)
- [Policy Generator source repository documentation](https://github.com/stolostron/policy-generator-plugin/blob/main/README.md)
- [Policy Generator reference YAML](https://github.com/stolostron/policy-generator-plugin/blob/main/docs/policygenerator-reference.yaml)
- [Policy Generator examples](policygenerator)

## Distributing Policies to Managed Clusters

Distributing a `Policy` to a managed cluster requires four parts, all of which must be in the same
namespace:

- [Policy](https://open-cluster-management.io/docs/concepts/policy/) is a grouping mechanism for Policy
  Templates and is the smallest deployable unit on the hub cluster. Embedded Policy Templates are
  distributed to applicable managed clusters and acted upon by the appropriate `policy controller`.

- [PlacementBinding](https://open-cluster-management.io/docs/concepts/policy/#placementbinding) binds a
  Placement to a `Policy` or `PolicySet`.

- [Placement](https://open-cluster-management.io/docs/concepts/placement/): Dynamically selects a set of
  `ManagedClusters` in one or multiple `ManagedClusterSet`s.

- [ManagedClusterSetBinding](https://open-cluster-management.io/docs/concepts/managedclusterset/): Binds
  a `ManagedClusterSet` to a namespace, making this group of managed clusters available for
  selection by `Placement`.

When the `Policy` is bound to a `Placement` using a `PlacementBinding`, the `Policy` is distributed
to the managed clusters and the `Policy` status will report on each cluster returned by the bound
`Placement`.

### Using `Placement` with `Policies`

See the [Placement documentation](https://open-cluster-management.io/docs/concepts/placement/) for
additional details on selecting managed clusters using `Placement`.

- **NOTE:** `PlacementRule` is **deprecated**. See the
  [migration section](#transitioning-from-placementruledeprecated-to-placement) for detail on
  migrating to `Placement`.

To use `Placement` to distribute `Policies`, bind the `Policy` to the `Placement` using a
`PlacementBinding`. All of the objects must be in the same namespace. View the following sample
`Placement` and `PlacementBinding` bound to a `Policy` named `policy-example` in the namespace
`example-ns`. The `Placement` selects managed clusters that have the label `environment: dev`:

```yaml
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement-policy-example
  namespace: example-ns
spec:
  predicates:
    - requiredClusterSelector:
        labelSelector:
          matchExpressions:
            - { key: environment, operator: In, values: [""dev""] }
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: binding-policy-example
  namespace: example-ns
placementRef:
  name: placement-policy-example
  kind: Placement
  apiGroup: cluster.open-cluster-management.io
subjects:
  - name: policy-example
    kind: Policy
    apiGroup: policy.open-cluster-management.io
```

### Transitioning from `PlacementRule`(deprecated) to `Placement`

`PlacementRule` had unrestricted access to selecting managed clusters. However, `Placement` requires
binding managed clusters to the `Policy` namespace in order for `Policies` to be distributed to
those managed clusters, bringing an additional layer of control to system administrators. View the
following steps on migrating from `PlacementRule` to `Placement`:

1. The desired managed clusters must be contained in a `ManagedClusterSet`. See the
   [ManagedClusterSet documentation](https://open-cluster-management.io/docs/concepts/managedclusterset/)
   to read more, including the default sets that are available that include all managed clusters
   that would replicate `PlacementRule` behavior.

2. Create a `ManagedClusterSetBinding` to bind the `ManagedClusterSet` to the namespace where you
   are authoring policies. See the
   [ManagedClusterSet documentation](https://open-cluster-management.io/docs/concepts/managedclusterset/).

3. Create the `Placement` manifest to replace the `PlacementRule`. To do this, take the selector
   `spec.clusterSelector` from the `PlacementRule` and put it into
   `spec.predicates.requiredClusterSelector.labelSelector`. For example, to select managed clusters
   with the label `environment: dev`:
   ```yaml
   apiVersion: cluster.open-cluster-management.io/v1beta1
   kind: Placement
   metadata:
     name: placement-policy-example
     namespace: example-ns // Ensure this namespace matches the ManagedClusterSetBinding
   spec:
     predicates:
     - requiredClusterSelector:
         labelSelector:
           matchExpressions: // From PlacementRule
           - {key: environment, operator: In, values: [""dev""]}
   ```

See the [Placement documentation](https://open-cluster-management.io/docs/concepts/placement/) for
additional details on selecting managed clusters using `Placement`.

4. Identify any `PlacementBinding` resources that reference a `PlacementRule`. Update the
   `PlacementBinding` to reference the new `Placement`:

   - Change the `placementRef.kind` to `Placement`
   - Update the `placementRef.apiGroup` to `cluster.open-cluster-management.io` to reflect the
     `Placement`'s API version

   View the following sample `PlacementBinding` that references a `Placement`:

   ```yaml
   apiVersion: policy.open-cluster-management.io/v1
   kind: PlacementBinding
   metadata:
     name: binding-policy-example
   placementRef:
     apiGroup: cluster.open-cluster-management.io // Set to cluster.open-cluster-management.io
     kind: Placement                              // Set to Placement
     name: placement-policy-example
   subjects:
     - name: policy-example
       kind: Policy
       apiGroup: policy.open-cluster-management.io
   ```

## Community, discussion, contribution, and support

Check the [Contributing policies](CONTRIBUTING.md) document for guidelines on how to contribute to
the repository.

**Blogs**: Read our blogs that are in the [blogs folder](blogs/README.md).

**Resources**: View the following resources for more information on the components and mechanisms
are implemented in the product governance framework.

- [Open Cluster Management Quick Start](https://https://open-cluster-management.io/getting-started/quick-start/)",FAUX
open-cluster-management-io/policy-generator-plugin,Toolkit,Toolkit,2025-04-21T15:59:41Z,2024-07-16T13:49:53Z,0,0,0,0,1,0,0,1,2021-09-07T00:24:55Z,2025-03-27T16:16:14Z,485,29,Go,VRAI,30,FAUX,1,,1,A Kustomize generator plugin to generate Open Cluster Management policies,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,18,"# Policy Generator

## Overview

The Policy Generator constructs Open Cluster Management policies from Kubernetes YAML files provided through a
PolicyGenerator Custom Resource. The Policy Generator is a binary compiled for use as a
[kustomize](https://kustomize.io/) exec plugin.

- [Installing the Policy Generator](#installing-the-policy-generator)
  - [Prerequisite](#prerequisite)
  - [Install the binary](#install-the-binary)
    - [Download a released version](#download-a-released-version)
    - [Using `go install` (available for `v1.11.0` and higher)](#using-go-install-available-for-v1110-and-higher)
    - [Build from source](#build-from-source)
- [Using the Policy Generator](#using-the-policy-generator)
  - [As a Kustomize plugin](#as-a-kustomize-plugin)
  - [As a standalone binary](#as-a-standalone-binary)
- [Additional Policy Generator references](#additional-policy-generator-references)

For more about Open Cluster Management and its Policy Framework:

- [Open Cluster Management website](https://open-cluster-management.io/)
- [Governance Policy Framework](https://open-cluster-management.io/getting-started/integration/policy-framework/)
- [Policy Collection repository](https://github.com/open-cluster-management-io/policy-collection)

## Install the Policy Generator

### Prerequisite

Create the plugin directory (optional if using the generator without Kustomize):

```bash
mkdir -p ${HOME}/.config/kustomize/plugin/policy.open-cluster-management.io/v1/policygenerator
```

**NOTE:** The default directory for Kustomize plugins is `${HOME}/.config/kustomize/plugin/`, which is used directly in
this readme. You can change this by exporting `KUSTOMIZE_PLUGIN_HOME` to a different path and updating the root of the
paths used in this document.

### Install the binary

#### Download a released version

1. Download the precompiled plugin binary from the
   [release](https://github.com/open-cluster-management-io/policy-generator-plugin/releases) of your choice.

2. Make the binary executable and move the binary to the plugin directory:

   - Linux:

     ```bash
     chmod +x linux-amd64-PolicyGenerator
     mv linux-amd64-PolicyGenerator ${HOME}/.config/kustomize/plugin/policy.open-cluster-management.io/v1/policygenerator/PolicyGenerator
     ```

   - MacOS:
     ```bash
     chmod +x darwin-amd64-PolicyGenerator
     mv darwin-amd64-PolicyGenerator ${HOME}/.config/kustomize/plugin/policy.open-cluster-management.io/v1/policygenerator/PolicyGenerator
     ```

#### Use `go install` (available for `v1.11.0` and higher)

Set the `GOBIN` to the plugin directory and specify the desired version (this command uses `latest`):

```bash
GOBIN=${HOME}/.config/kustomize/plugin/policy.open-cluster-management.io/v1/policygenerator \
go install open-cluster-management.io/policy-generator-plugin/cmd/PolicyGenerator@latest
```

#### Build from source

```bash
make build
```

**NOTE:**

- This defaults to placing the binary in the Kustomize default plugin directory `${HOME}/.config/kustomize/plugin/`. You
  can change this by exporting `KUSTOMIZE_PLUGIN_HOME` to a different path.
- Alternatively, you can run `make build-binary` to place the binary at the root of the repository and either use it
  directly from there or move it to the plugin directory to use with Kustomize.

## Using the Policy Generator

### As a Kustomize plugin

1. Create a `kustomization.yaml` file that points to `PolicyGenerator` manifest(s), with any additional desired patches
   or customizations (see [`examples/policyGenerator.yaml`](./examples/policyGenerator.yaml) for an example):

   ```yaml
   generators:
     - path/to/generator/file.yaml
   ```

   - To read more about the `PolicyGenerator` YAML structure, see the
     [Policy Generator reference YAML](./docs/policygenerator-reference.yaml)

2. To use the plugin to generate policies, run the Kustomize build command from any directory with a
   `kustomization.yaml` file pointing to `PolicyGenerator` manifests:
   ```bash
   kustomize build --enable-alpha-plugins
   ```

  **NOTE:** To enable Helm processing when passing a Kustomize directory into the generator, set
  the environment variable `POLICY_GEN_ENABLE_HELM` to `""true""`. If the Helm directory is outside of the Kustomize path,
  you may set the environment variable `POLICY_GEN_DISABLE_LOAD_RESTRICTORS` to `""true""`.

### As a standalone binary

In order to bypass Kustomize and run the generator binary directly, change to the directory of PolicyGenerator
manifest(s) and run the binary with the manifest(s) as the input arguments:

```bash
path/to/PolicyGenerator <path/to/file/1> ... <path/to/file/n>
```

For example:

```bash
make build-binary # This places the binary at the root of the repo, so this is optional if it was done previously
cd examples
../PolicyGenerator policyGenerator.yaml
```

**NOTE:** 
- To print the trace in the case of an error, you can add the `--debug` flag to the arguments.
- To enable Helm processing when passing a Kustomize directory into the generator, set
  the environment variable `POLICY_GEN_ENABLE_HELM` to `""true""`. If the Helm directory is outside of the Kustomize path,
  you may set the environment variable `POLICY_GEN_DISABLE_LOAD_RESTRICTORS` to `""true""`.

## Additional Policy Generator references

- [Policy Generator reference YAML](./docs/policygenerator-reference.yaml)
- [Policy Generator technical documentation](./docs/policygenerator.md)",FAUX
open-policy-agent/cert-controller,Application System,Toolkit,2025-05-15T23:45:07Z,2025-02-25T19:10:55Z,0,0,0,0,0,0,0,0,2020-07-06T13:36:37Z,2025-04-07T18:22:49Z,274,99,Go,VRAI,42,FAUX,28,"certificate-generation,kubernetes",28,,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,24,"# Certificate Controller

## Purpose

The purpose of the Certificate Controller library is to provide
an easy way for controller authors to bootstrap webhooks while
making it possible for users to use more customizable projects like
[cert-manager](https://cert-manager.io/docs/) should they desire
to do so. Its purpose is not to be a fully-featured certificate
solution, but a simple solution that allows webhook authors to avoid
having a hard dependency on the existence of any third-party
certificate generation solution.

This library was originally written as part of Gatekeeper to replace
the certificate generation functionality provided by controller-runtime
that was removed as part of a migration to Kubebuilder 2.0. It was then
spun off as a separate repo so that other projects, such as
[Hierarchical Namespace Controller](https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc)
could avoid maintaining duplicate code.

## Design

All behavior is governed by a `CertRotator` object that has two
main control loops:

   1. A tick-based control loop that periodically examines the certificates stored
      in the secret and makes sure they are still valid, regenerating them if not
   2. A watch-based control loop that watches relevant webhook resources and the
      certificate secret. Whenever any of these resources change, the controller
      runs a reconcile to make sure all objects agree on the correct cert, as defined
      by the secret.

The secret is where all certificates are stored and is considered the source of truth.
All resources will be reconciled to match the secret.

## Usage

The following code snippet is taken from the Gatekeeper project:

```
	// Make sure certs are generated and valid if cert rotation is enabled.
	setupFinished := make(chan struct{})
	if !*disableCertRotation && operations.IsAssigned(operations.Webhook) {
		setupLog.Info(""setting up cert rotation"")
		if err := rotator.AddRotator(mgr, &rotator.CertRotator{
			SecretKey: types.NamespacedName{
				Namespace: util.GetNamespace(),
				Name:      secretName,
			},
			CertDir:        *certDir,
			CAName:         caName,
			CAOrganization: caOrganization,
			DNSName:        dnsName,
			ExtraDNSNames:  extraDnsNames,
			IsReady:        setupFinished,
			VWHName:        vwhName,
		}); err != nil {
			setupLog.Error(err, ""unable to set up cert rotation"")
			os.Exit(1)
		}
	} else {
		close(setupFinished)
	}
```

The basic pattern is to call `AddRotator`, which adds `CertRotator`
to the controller-runtime manager, where it behaves like a standard controller.

The channel passed to `IsReady` is closed when the certificate has been
fully bootstrapped into local storage. This can be used to delay the
registration of webhooks until a certificate is available to be loaded. This
prevents any crashing of the webhook pod during startup.

Users who set the `RestartOnSecretRefresh` field on the `CertRotator` struct will have the Pod
restart when the cert refreshes or is initialized. This may improve mean
time to availability of a bootstrapping webhook.

## Questions?

If you have questions about the project, please file a GitHub issue.",VRAI
open-policy-agent/conftest,Toolkit,Toolkit,2025-05-10T18:46:06Z,2025-03-01T21:51:36Z,0,74,0,0,0,0,0,0,2019-03-28T17:12:29Z,2025-04-08T09:01:04Z,4535,2942,Go,VRAI,309,FAUX,46,"kubernetes,open-policy-agent,openpolicyagent,rego,testing",46,Write tests against structured configuration data using the Open Policy Agent Rego query language,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,111,"# Conftest

[![Go Report Card](https://goreportcard.com/badge/open-policy-agent/opa)](https://goreportcard.com/report/open-policy-agent/conftest) [![Netlify](https://api.netlify.com/api/v1/badges/2d928746-3380-4123-b0eb-1fd74ba390db/deploy-status)](https://app.netlify.com/sites/vibrant-villani-65041c/deploys)

Conftest helps you write tests against structured configuration data. Using Conftest you can
write tests for your Kubernetes configuration, Tekton pipeline definitions, Terraform code,
Serverless configs or any other config files.

Conftest uses the Rego language from [Open Policy Agent](https://www.openpolicyagent.org/) for writing
the assertions. You can read more about Rego in [How do I write policies](https://www.openpolicyagent.org/docs/how-do-i-write-policies.html)
in the Open Policy Agent documentation.

Here's a quick example. Save the following as `policy/deployment.rego`:

```rego
package main

deny[msg] {
  input.kind == ""Deployment""
  not input.spec.template.spec.securityContext.runAsNonRoot

  msg := ""Containers must not run as root""
}

deny[msg] {
  input.kind == ""Deployment""
  not input.spec.selector.matchLabels.app

  msg := ""Containers must provide app label for pod selectors""
}
```

Assuming you have a Kubernetes deployment in `deployment.yaml` you can run Conftest like so:

```console
$ conftest test deployment.yaml
FAIL - deployment.yaml - Containers must not run as root
FAIL - deployment.yaml - Containers must provide app label for pod selectors

2 tests, 0 passed, 0 warnings, 2 failures, 0 exceptions
```

Conftest isn't specific to Kubernetes. It will happily let you write tests for any configuration files in a variety of different formats. See the [documentation](https://www.conftest.dev/) for [installation instructions](https://www.conftest.dev/install/) and
more details about the features.

## Want to contribute to Conftest?

* See [DEVELOPMENT.md](DEVELOPMENT.md) to build and test Conftest itself.
* See [CONTRIBUTING.md](CONTRIBUTING.md) to get started.

For discussions and questions join us on the [Open Policy Agent Slack](https://slack.openpolicyagent.org/)
in the `#opa-conftest` channel.",FAUX
open-policy-agent/contrib,Documentations,Documentations,2024-11-29T19:36:40Z,2023-08-22T19:02:56Z,0,48,0,0,0,0,0,0,2017-04-25T21:22:53Z,2025-04-06T15:33:34Z,19226,333,Go,VRAI,153,FAUX,9,"contrib,hack,integrations,opa,open-policy-agent,proof-of-concept",9,"Integrations, examples, and proof-of-concepts that are not part of OPA proper.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,56,"# Open Policy Agent - Contributions

This repository holds integrations, examples, and proof-of-concepts that work with the Open Policy Agent (OPA) project.

## Examples and Integrations

- [Kafka Authorization](./kafka_authorizer)
- [HTTP API Authorization (Spring Security)](./spring_authz)
- [HTTP API Authorization (Linkerd)](./linkerd_authz)
- [HTTP API Authorization (Python)](./api_authz)
- [HTTP API Authorization (Dart)](./dart_authz)
- [HTTP API Authorization (Kong)](./kong_api_authz)
- [SSH and sudo Authorization (PAM)](./pam_opa)
- [Puppet Authorization](./puppet_example)
- [Container Image Policy (Kubernetes and CoreOS Clair)](./image_enforcer)
- [Data Filtering (SQL)](./data_filter_example)
- [Data Filtering (Elasticsearch)](./data_filter_elasticsearch)
- [Data Filtering (MongoDB)](./data_filter_mongodb)
- [Data Filtering (Azure)](./data_filter_azure)
- [Cloud Foundry Policies](./cloud_foundry)
- [Decision Logger Plugin](./decision_logger_plugin_example)
- [IPTables (Linux)](./opa-iptables)
- [IPTables (Envoy)](./envoy_iptables)
- [JUnit Test Format Conversion](./junit)
- [Kubernetes Authorization](./k8s_authorization)
- [Kubernetes Node Selector](./k8s_node_selector)
- [Kubernetes API Client](./k8s_api_client)
- [Grafana Dashboard](./grafana-dashboard)
- [OpenAPI Specification for OPA](./open_api)
- [SonarCloud Test Coverage Conversion](./sonarcloud)

For a comprehensive list of integrations, see the OPA [ecosystem](https://www.openpolicyagent.org/docs/latest/ecosystem/) page.

## Contributing

If you have built an integration, example, or proof-of-concept on top of OPA that you would like to release to the community, feel free to submit a Pull Request against this repository. Please create a new top-level directory containing:

- A README.md explaining what your integration does
- A Makefile to build your integration

## Building and Releasing

Most integrations include a top-level Makefile with two targets:

* `build` - compiles/lints/tests the integration
* `push` - builds the integration and publishes artifacts

Many of the integrations produce one or more Docker images. These Docker images can be pushed to the hub.docker.com/u/openpolicyagent repository (assuming you are authorized.)

The Makefile in this directory contains `build` and `push` targets to build and push all integrations.",FAUX
open-policy-agent/frameworks,Toolkit,Toolkit,2025-04-29T23:12:06Z,2025-01-15T14:53:56Z,0,2,0,0,0,0,0,4,2019-03-05T00:28:33Z,2025-04-07T20:04:09Z,78481,120,Go,VRAI,53,FAUX,21,,21,,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,35,"# Open Policy Agent Frameworks

Open Policy Agent is a general-purpose policy system designed to policy-enable other projects and services.  The OPA Frameworks repository defines opinionated APIs for policy that are less flexible than the OPA API but are well-suited to particular classes of use cases. For example, Role Based Acces Control (RBAC), Attribute Based Access Control, Access Control Lists (ACLs), and IAM can all be implemented on top of the OPA API and its policy language, and could each be defined as an OPA Framework.  One analogy from the web development world that seems to help people is that Frameworks are to OPA as Rails is to Ruby.",FAUX
open-policy-agent/gatekeeper,Toolkit,Toolkit,2025-05-15T21:22:43Z,2025-04-14T17:38:25Z,0,10,0,0,0,0,0,57,2018-10-26T21:05:57Z,2025-04-07T20:59:25Z,176885,3838,Go,VRAI,787,FAUX,148,"admission,cncf,gatekeeper,hacktoberfest,kubernetes,mutation,opa,policy,policy-engine,security,validation",148,🐊 Gatekeeper - Policy Controller for Kubernetes,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,236,"# Gatekeeper

![Static Badge](https://img.shields.io/badge/OPA%20Version-v0.60.0-blue)

## How is Gatekeeper different from OPA?

Compared to using [OPA with its sidecar kube-mgmt](https://www.openpolicyagent.org/docs/kubernetes-admission-control.html) (aka Gatekeeper v1.0), Gatekeeper introduces the following functionality:

   * An extensible, parameterized [policy library](https://open-policy-agent.github.io/gatekeeper-library/website/)
   * Native Kubernetes CRDs for instantiating the policy library (aka ""constraints"")
   * Native Kubernetes CRDs for extending the policy library (aka ""constraint templates"")
   * Native Kubernetes CRDs for [mutation](https://open-policy-agent.github.io/gatekeeper/website/docs/mutation/) support
   * Audit functionality
   * External data support

## Getting started

Check out the [installation instructions](https://open-policy-agent.github.io/gatekeeper/website/docs/install) to deploy Gatekeeper components to your Kubernetes cluster.

## Documentation

Please see the [Gatekeeper website](https://open-policy-agent.github.io/gatekeeper/website/docs/howto) for more in-depth information.

## Policy Library

See the [Gatekeeper policy library](https://open-policy-agent.github.io/gatekeeper-library/website/) for a collection of constraint templates and sample constraints that you can use with Gatekeeper.

## Community & Contributing

Please refer to [Gatekeeper's contribution guide](https://open-policy-agent.github.io/gatekeeper/website/docs/help) to find out how you can help.

## Code of conduct

This project is governed by the [CNCF Code of conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md).

## Security

For details on how to report vulnerabilities and security release process, please refer to [Gatekeeper Security](https://open-policy-agent.github.io/gatekeeper/website/docs/security) for more information.",FAUX
open-policy-agent/gatekeeper-library,Toolkit,Toolkit,2025-05-12T19:14:45Z,2024-12-30T19:00:03Z,0,134,0,0,0,0,0,187,2019-06-11T19:34:50Z,2025-04-08T06:50:12Z,9665,672,Open Policy Agent,VRAI,336,FAUX,21,"cncf,gatekeeper,hacktoberfest,kubernetes,opa,policy,policy-library",21,📚 The OPA Gatekeeper policy library,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,74,"# OPA Gatekeeper Library
[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/gatekeeper-policies)](https://artifacthub.io/packages/search?repo=gatekeeper-policies)

A community-owned library of policies for the [OPA Gatekeeper project](https://open-policy-agent.github.io/gatekeeper/website/docs/).

## Validation and Mutation
The library consists of two main components: `Validation` and `Mutation`.
- Validation: Gatekeeper can validate resources in the cluster against Gatekeeper validation policies, such as these defined in the library. The policies are defined as `ConstraintTemplates` and `Constraints`. `ConstraintTemplates` can be applied directly to a cluster and then `Constraints` can be applied to customize policy to fit your specific needs.
- Mutation: Gatekeeper can mutate resources in the cluster against the Gatekeeper mutation policies, such as these defined in the library. Mutation policies are only examples, they should be customized to meet your needs before being applied.

## Usage

### kustomize

You can use [kustomize](https://kubectl.docs.kubernetes.io/installation/kustomize/) to install some or all of the templates alongside your own constraints.

First, create a `kustomization.yaml` file:

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- github.com/open-policy-agent/gatekeeper-library/library
# You can optionally install a subset by specifying a subfolder, or specify a commit SHA
# - github.com/open-policy-agent/gatekeeper-library/library/pod-security-policy?ref=0c82f402fb3594097a90d15215ae223267f5b955
- constraints.yaml
```

Then define your constraints in a file called `constraints.yaml` in the same directory. Example constraints can be found in the ""samples"" folders.

You can install everything with `kustomize build . | kubectl apply -f -`.

More information can be found in the [kustomization documentation](https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/).

### kubectl

Instead of using kustomize, you can directly apply the `template.yaml` and `constraint.yaml` provided in each directory under `library/`

For example

```bash
cd library/general/httpsonly/
kubectl apply -f template.yaml
kubectl apply -f samples/ingress-https-only/constraint.yaml
kubectl apply -f library/general/httpsonly/sync.yaml # optional: when GK is running with OPA cache
```

## Testing

The `suite.yaml` files define test cases for each ConstraintTemplate in the library.
Changes to gatekeeper-library ConstraintTemplates may be tested with the gator CLI:

```bash
gatekeeper-library$ gator verify ./...
```

The gator CLI may be downloaded from the Gatekeeper
[releases page](https://github.com/open-policy-agent/gatekeeper/releases).

## How to contribute to the library

### New policy

If you have a policy you would like to contribute, please submit a pull request.
Each new policy should contain:

* A constraint template named `src/<policy-name>/constraint.tmpl` with a `description` annotation and the parameter structure, if any, defined in `spec.crd.spec.validation.openAPIV3Schema`. The template is rendered using [gomplate](https://docs.gomplate.ca/).
* One or more sample constraints, each with an example of an allowed (`example_allowed.yaml`) and disallowed (`example_disallowed.yaml`) resource under `library/<policy-name>/samples/<policy-name>`
* `kustomization.yaml` and `suite.yaml` under `library/<policy-name>`
* The rego source, as `src.rego` and unit tests as `src_test.rego` in the corresponding subdirectory under `src/<policy-name>`
* [Versioning](https://docs.google.com/document/d/1IYiypA-mRcdfSVfmoeyuaeG8XtA1u4GkcqH3kEkv2uw/edit) has been introduced for Gatekeeper Library policies. Please make sure to add or bump the version of the policy as per the guidelines in the `src/<policy-name>/constraint.tmpl` annotation.
  * Major version bump required: Whenever there is a breaking change in the policy e.g.  updating template Kind, updating existing parameter schema, adding the `requires-sync-data` annotation to sync new data, or any other breaking changes
  * Minor version bump required: Whenever there is a backward compatible change in the policy e.g. adding a parameter, updating Rego logic
  * Patch version bump required: Whenever there is a simple backward compatible change in the policy, e.g. Simple Rego fix, updating policy metadata
  * Note: Sample constraints, mutations, and expansion templates are provided as examples, and severable changes do not require a version bump.

### Development

* policy code and tests are maintained in `src/<policy-name>/src.rego` and `src/<policy-name>/src_test.rego`
* `make generate` will generate `library/<policy-name>/template.yaml` from `src/<policy-name>/src.rego` using [gomplate](https://docs.gomplate.ca/).
* `make generate-website-docs` will generate the markdown files required for the website.
* `make generate-artifacthub-artifacts` will generate or update the artifact hub packages and associated `artifacthub-pkg.yml` file under `/artifacthub` directory.
* `make generate-all` will generate all artifacts above.
* `make validate` will run validation checks on the library repo. Currently it validates directory structure of `website/docs` directory.
* `make unit-test` will run all unit tests in the scripts directory.
* run all tests with `./test.sh`
* run single test with `opa test src/<folder>/src.rego src/<folder>/src_test.rego --verbose`
* print results with `trace(sprintf(""%v"", [thing]))`",FAUX
open-policy-agent/npm-opa-wasm,Toolkit,Toolkit,2024-11-25T12:02:33Z,2024-05-23T08:00:42Z,0,12,0,0,0,0,0,0,2019-05-14T18:33:20Z,2025-03-02T18:29:23Z,1786,145,JavaScript,VRAI,45,FAUX,15,"authorization,browser,declarative,deno,nodejs,opa,open-policy-agent,policy,wasm,webassembly",15,Open Policy Agent WebAssembly NPM module (opa-wasm),FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,23,"**Work in Progress -- Contributions welcome!!**

# Open Policy Agent WebAssemby NPM Module

This is the source for the
[@open-policy-agent/opa-wasm](https://www.npmjs.com/package/@open-policy-agent/opa-wasm)
NPM module which is a small SDK for using WebAssembly (wasm) compiled
[Open Policy Agent](https://www.openpolicyagent.org/) Rego policies.

# Getting Started

## Install the module

```
npm install @open-policy-agent/opa-wasm
```

## Usage

There are only a couple of steps required to start evaluating the policy.

### Import the module

```javascript
const { loadPolicy } = require(""@open-policy-agent/opa-wasm"");
```

### Load the policy

```javascript
loadPolicy(policyWasm);
```

The `loadPolicy` function returns a Promise with the loaded policy. Typically
this means loading it in an `async` function like:

```javascript
const policy = await loadPolicy(policyWasm);
```

Or something like:

```javascript
loadPolicy(policyWasm).then((policy) => {
  // evaluate or save the policy
}, (error) => {
  console.error(""Failed to load policy: "" + error);
});
```

The `policyWasm` needs to be either the raw byte array of the compiled policy
Wasm file, or a WebAssembly module.

For example:

```javascript
const fs = require(""fs"");

const policyWasm = fs.readFileSync(""policy.wasm"");
```

Alternatively the bytes can be pulled in remotely from a `fetch` or in some
cases (like CloudFlare Workers) the Wasm binary can be loaded directly into the
javascript context through external APIs.

### Evaluate the Policy

The loaded policy object returned from `loadPolicy()` has a couple of important
APIs for policy evaluation:

`setData(data)` -- Provide an external `data` document for policy evaluation.

- `data` MUST be a serializable object or `ArrayBuffer`, which assumed to be a
  well-formed stringified JSON

`evaluate(input)` -- Evaluates the policy using any loaded data and the supplied
`input` document.

- `input` parameter MAY be an `object`, primitive literal or `ArrayBuffer`,
  which assumed to be a well-formed stringified JSON

> `ArrayBuffer` supported in the APIs above as a performance optimisation
> feature, given that either network or file system provided contents can easily
> be represented as `ArrayBuffer` in a very performant way.

Example:

```javascript
input = '{""path"": ""/"", ""role"": ""admin""}';

loadPolicy(policyWasm).then((policy) => {
  resultSet = policy.evaluate(input);
  if (resultSet == null) {
    console.error(""evaluation error"");
  } else if (resultSet.length == 0) {
    console.log(""undefined"");
  } else {
    console.log(""allowed = "" + resultSet[0].result);
  }
}).catch((error) => {
  console.error(""Failed to load policy: "", error);
});
```

> For any `opa build` created WASM binaries the result set, when defined, will
> contain a `result` key with the value of the compiled entrypoint. See
> [https://www.openpolicyagent.org/docs/latest/wasm/](https://www.openpolicyagent.org/docs/latest/wasm/)
> for more details.

### Writing the policy

See
[https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/](https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/)

### Compiling the policy

Either use the
[Compile REST API](https://www.openpolicyagent.org/docs/latest/rest-api/#compile-api)
or `opa build` CLI tool.

For example, with OPA v0.20.5+:

```bash
opa build -t wasm -e example/allow example.rego
```

Which is compiling the `example.rego` policy file with the result set to
`data.example.allow`. The result will be an OPA bundle with the `policy.wasm`
binary included. See [./examples](./examples) for a more comprehensive example.

See `opa build --help` for more details.

## Development

### Lint and Format checks

This project is using Deno's
[lint](https://deno.land/manual@v1.14.0/tools/linter) and
[formatter](https://deno.land/manual@v1.14.0/tools/formatter) tools in CI. With
`deno`
[installed locally](https://deno.land/manual@v1.14.0/getting_started/installation),
the same checks can be invoked using `npm`:

- `npm run lint`
- `npm run fmt` -- this will fix the formatting
- `npm run fmt:check` -- this happens in CI

All of these operate on git-tracked files, so make sure you've committed the
code you'd like to see checked. Alternatively, you can invoke
`deno lint my_new_file.js` directly, too.

### Build

The published package provides four different entrypoints for consumption:

1. A CommonJS module for consumption with older versions of Node or those using
   `require()`:
   ```js
   const { loadPolicy } = require(""@open-policy-agent/opa-wasm"");
   ```
1. An ESM module for consumption with newer versions of Node:
   ```js
   import { loadPolicy } from ""@open-policy-agent/opa-wasm"";
   ```
1. An ESM module for consumption in modern browsers (this will contain all
   dependencies already bundled and can be used standalone).
   ```html
   <script type=""module"">
   import opa from 'https://unpkg.com/@open-policy-agent/opa-wasm@latest/dist/opa-wasm-browser.esm.js';
   opa.loadPolicy(...);
   </script>
   ```
1. A script for consumption in all browsers (this will export an `opa` global
   variable).
   ```js
   <script src=""https://unpkg.com/@open-policy-agent/opa-wasm@latest/dist/opa-wasm-browser.js""></script>
   <script>
   opa.loadPolicy(...);
   </script>
   ```

The browser builds are generated in the `./build.sh` script and use
[`esbuild`][esbuild]. All exports are defined in the `exports` field in the
package.json file. More detials on how these work are described in the
[Conditional Exports][conditional-exports] documentation.

For TypeScript projects we also generate an opa.d.ts declaration file that will
give correct typings and is also defined under the `types` field in the
package.json.

[esbuild]: https://esbuild.github.io/
[conditional-exports]: https://nodejs.org/api/packages.html#conditional-exports",FAUX
open-policy-agent/opa,Toolkit,Toolkit,2025-05-15T15:09:54Z,2025-04-25T16:04:18Z,0,98,0,0,0,0,0,0,2015-12-28T22:08:25Z,2025-04-08T09:06:54Z,1090131,10113,Go,VRAI,1397,FAUX,391,"authorization,cloud-native,compliance,declarative,doge,json,lolcat,opa,open-policy-agent,policy",391,"Open Policy Agent (OPA) is an open source, general-purpose policy engine.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,507,"# ![logo](./logo/logo-144x144.png) Open Policy Agent

[![Build Status](https://github.com/open-policy-agent/opa/workflows/Post%20Merge/badge.svg?branch=main)](https://github.com/open-policy-agent/opa/actions) [![Go Report Card](https://goreportcard.com/badge/open-policy-agent/opa)](https://goreportcard.com/report/open-policy-agent/opa) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1768/badge)](https://bestpractices.coreinfrastructure.org/projects/1768) [![Netlify Status](https://api.netlify.com/api/v1/badges/4a0a092a-8741-4826-a28f-826d4a576cab/deploy-status)](https://app.netlify.com/sites/openpolicyagent/deploys)

Open Policy Agent (OPA) is an open source, general-purpose policy engine that enables unified, context-aware policy enforcement across the entire stack.

OPA is proud to be a graduated project in the [Cloud Native Computing Foundation](https://cncf.io) (CNCF) landscape. For details read the CNCF [announcement](https://www.cncf.io/announcements/2021/02/04/cloud-native-computing-foundation-announces-open-policy-agent-graduation/).

## Get started with OPA

- Write your first Rego policy with the [Rego Playground](https://play.openpolicyagent.org) or use it to share your work with others for feedback and support. Have a look at the [Access Control examples](https://play.openpolicyagent.org/?example-group=access-control) if you're not sure where to start.
- Install the [VS Code extension](https://marketplace.visualstudio.com/items?itemName=tsandall.opa) to get started locally with live diagnostics, debugging and formatting. See [Editor and IDE Support](https://www.openpolicyagent.org/docs/edge/editor-and-ide-support/) for other supported editors.
- Go to the [OPA Documentation](https://www.openpolicyagent.org/docs/latest/) to
  learn about the Rego language as well as how to deploy and integrate OPA.
- Check out the learning resources in the [Learning Rego](https://www.openpolicyagent.org/ecosystem/learning-rego/) section of the ecosystem directory.
- Follow the [Running OPA](https://www.openpolicyagent.org/docs/latest/#running-opa) instructions to get started with the OPA CLI locally.
- See [Docker Hub](https://hub.docker.com/r/openpolicyagent/opa/tags/) for container images and the [GitHub releases](https://github.com/open-policy-agent/opa/releases) for binaries.
- Check out the [OPA Roadmap](https://docs.google.com/presentation/d/16QV6gvLDOV3I0_guPC3_19g6jHkEg3X9xqMYgtoCKrs/edit?usp=sharing) to see a high-level snapshot of OPA features in-progress and planned.

## Want to talk about OPA or get support?

- Join the [OPA Slack](https://inviter.co/opa) to talk to other OPA users and maintainers. See `#help` for support.
- Check out the [Community Discussions](https://github.com/orgs/open-policy-agent/discussions) to ask questions.
- See the [Support](https://www.openpolicyagent.org/support/) page for commercial support options.

## Interested to learn what others are doing with OPA?

- Browse community projects on the [OPA Ecosystem Directory](http://openpolicyagent.org/ecosystem/) - don't forget to [list your own](https://github.com/open-policy-agent/opa/tree/main/docs#opa-ecosystem)!
- Check out the [ADOPTERS.md](./ADOPTERS.md) file for a list of production adopters. Does your organization use OPA in production? Support the OPA project by submitting a PR to add your organization to the list with a short description of your OPA use cases!

## Want to integrate OPA?

- See the high-level [Go SDK](https://www.openpolicyagent.org/docs/latest/integration/#integrating-with-the-go-sdk) or the low-level Go API
  [![GoDoc](https://godoc.org/github.com/open-policy-agent/opa?status.svg)](https://godoc.org/github.com/open-policy-agent/opa/rego)
  to integrate OPA with services written in Go.
- See the [REST API](https://www.openpolicyagent.org/docs/rest-api.html)
  reference to integrate OPA with services written in other languages.
- See the [integration docs](https://www.openpolicyagent.org/docs/latest/integration/) for more options.

## Want to contribute to OPA?

- Read the [Contributing Guide](https://www.openpolicyagent.org/docs/latest/contributing/) to learn how to make your first contribution.
- Use [#contributors](https://openpolicyagent.slack.com/archives/C02L1TLPN59) in Slack to talk to other contributors and OPA maintainers.
- File a [GitHub Issue](https://github.com/open-policy-agent/opa/issues) to request features or report bugs.

## How does OPA work?

OPA gives you a high-level declarative language to author and enforce policies
across your stack.

With OPA, you define _rules_ that govern how your system should behave. These
rules exist to answer questions like:

- Can user X call operation Y on resource Z?
- What clusters should workload W be deployed to?
- What tags must be set on resource R before it's created?

You integrate services with OPA so that these kinds of policy decisions do not
have to be _hardcoded_ in your service. Services integrate with OPA by
executing _queries_ when policy decisions are needed.

When you query OPA for a policy decision, OPA evaluates the rules and data
(which you give it) to produce an answer. The policy decision is sent back as
the result of the query.

For example, in a simple API authorization use case:

- You write rules that allow (or deny) access to your service APIs.
- Your service queries OPA when it receives API requests.
- OPA returns allow (or deny) decisions to your service.
- Your service _enforces_ the decisions by accepting or rejecting requests accordingly.

For concrete examples of how to integrate OPA with systems like [Kubernetes](https://www.openpolicyagent.org/docs/kubernetes-admission-control.html), [Terraform](https://www.openpolicyagent.org/docs/terraform.html), [Docker](https://www.openpolicyagent.org/docs/docker-authorization.html), [SSH](https://www.openpolicyagent.org/docs/ssh-and-sudo-authorization.html), and more, see [openpolicyagent.org](https://www.openpolicyagent.org).

## Presentations

- Open Policy Agent (OPA) Intro & Deep Dive @ Kubecon NA 2023: [video](https://www.youtube.com/watch?v=wJkjsvVpj_Q)
- Open Policy Agent (OPA) Intro & Deep Dive @ Kubecon EU 2023: [video](https://www.youtube.com/watch?v=6RNp3m_THw4)
- Running Policy in Hard to Reach Places with WASM & OPA @ CN Wasm Day EU 2023: [video](https://www.youtube.com/watch?v=BdeBhukLwt4)
- OPA maintainers talk @ Kubecon NA 2022: [video](https://www.youtube.com/watch?v=RMiovzGGCfI)
- Open Policy Agent (OPA) Intro & Deep Dive @ Kubecon EU 2022: [video](https://www.youtube.com/watch?v=MhyQxIp1H58)
- Open Policy Agent Intro @ KubeCon EU 2021: [Video](https://www.youtube.com/watch?v=2CgeiWkliaw)
- Using Open Policy Agent to Meet Evolving Policy Requirements @ KubeCon NA 2020: [video](https://www.youtube.com/watch?v=zVuM7F_BTyc)
- Applying Policy Throughout The Application Lifecycle with Open Policy Agent @ CloudNativeCon 2019: [video](https://www.youtube.com/watch?v=cXfsaE6RKfc)
- Open Policy Agent Introduction @ CloudNativeCon EU 2018: [video](https://youtu.be/XEHeexPpgrA), [slides](https://www.slideshare.net/TorinSandall/opa-the-cloud-native-policy-engine)
- Rego Deep Dive @ CloudNativeCon EU 2018: [video](https://youtu.be/4mBJSIhs2xQ), [slides](https://www.slideshare.net/TorinSandall/rego-deep-dive)
- How Netflix Is Solving Authorization Across Their Cloud @ CloudNativeCon US 2017: [video](https://www.youtube.com/watch?v=R6tUNpRpdnY), [slides](https://www.slideshare.net/TorinSandall/how-netflix-is-solving-authorization-across-their-cloud).
- Policy-based Resource Placement in Kubernetes Federation @ LinuxCon Beijing 2017: [slides](https://www.slideshare.net/TorinSandall/policybased-resource-placement-across-hybrid-cloud), [screencast](https://www.youtube.com/watch?v=hRz13baBhfg&feature=youtu.be)
- Enforcing Bespoke Policies In Kubernetes @ KubeCon US 2017: [video](https://www.youtube.com/watch?v=llDI8VvkUj8), [slides](https://www.slideshare.net/TorinSandall/enforcing-bespoke-policies-in-kubernetes)
- Istio's Mixer: Policy Enforcement with Custom Adapters @ CloudNativeCon US 2017: [video](https://www.youtube.com/watch?v=czZLXUqzd24), [slides](https://www.slideshare.net/TorinSandall/istios-mixer-policy-enforcement-with-custom-adapters-cloud-nativecon-17)

## Security

A third party security audit was performed by Cure53, you can see the full report [here](SECURITY_AUDIT.pdf).

Please report vulnerabilities by email to [open-policy-agent-security](mailto:open-policy-agent-security@googlegroups.com).
We will send a confirmation message to acknowledge that we have received the
report and then we will send additional messages to follow up once the issue
has been investigated.",FAUX
open-policy-agent/opa-docker-authz,Toolkit,Toolkit,2025-05-08T06:59:38Z,2022-09-21T08:15:58Z,0,3,0,0,0,0,0,0,2016-06-08T16:43:57Z,2025-03-18T08:55:18Z,234597,87,Go,VRAI,26,FAUX,13,"authorization,docker,opa",13,A policy-enabled authorization plugin for Docker.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,17,"# opa-docker-authz

This project is used to show how OPA can help policy-enable an existing service.

In this example, we policy-enable the authorization functionality available in the Docker Engine, which is implemented using a plugin architecture. Plugins were introduced in the Docker Engine in 1.10, as a v1 implementation, and further extended in 1.13, as a v2 implementation. Plugins that adhere to the former are often termed [legacy plugins](https://docs.docker.com/engine/extend/legacy_plugins/), whilst the latter are termed [managed plugins](https://docs.docker.com/engine/extend/).

`opa-docker-authz` is an [authorization plugin](https://docs.docker.com/engine/extend/plugins_authorization/) for the Docker Engine, and can be run as a legacy plugin, or as a managed plugin. The managed plugin is the recommended configuration.

## Usage

See the [detailed example](http://www.openpolicyagent.org/docs/docker-authorization.html) to setup a running example of this plugin.

### Build

A makefile is provided for creating different artifacts, each of which requires Docker:

- `make build` - builds the `opa-docker-authz` binary
- `make image` - builds a Docker image for use as a legacy plugin
- `make plugin` - builds a managed plugin

### Install

To make use of the `opa-docker-authz` plugin, [TLS must be enabled](https://docs.docker.com/engine/security/https/), in order for the Docker daemon to authenticate the client user. The client's X.509 certificate subject common name, should be [configured](https://docs.docker.com/engine/extend/plugins_authorization/#default-user-authorization-mechanism) with the user who is the subject of the authorization request.

**Managed Plugin**

The managed plugin is a special pre-built Docker image, and as such, has no prior knowledge of the user's intended policy. OPA policy is defined using the [Rego language](https://www.openpolicyagent.org/docs/language-reference.html), which for the purposes of the `opa-docker-authz` plugin, is either contained within a file (using the `-policy-file` argument) or fetched from bundles through an OPA [configuration](https://www.openpolicyagent.org/docs/latest/configuration/) file (using the `-config-file` argument). Since the latter option allows not just remote bundles, but any of the OPA management features such as decision logging, it is the recommended choice. The plugin needs to be made aware of either the location of the policy file, or the config file, during its installation.

In order to provide user-defined OPA policy or config, the plugin is configured with a bind mount; `/etc/docker` is mounted at `/opa` inside the plugin's container, which is its working directory. If you define your config in a file located at the path `/etc/docker/config/opa-conf.yaml`, for example, it will be available to the plugin at `/opa/config/opa-conf.yaml`.

If the plugin is installed without a reference to a Rego policy file, or a config file, all authorization requests sent to the plugin by the Docker daemon, fail open, and are authorized by the plugin.

The following steps detail how to install the managed plugin.

Download the `opa-docker-authz` plugin from the Docker Hub (depending on how your Docker environment is configured, you may need to execute the following commands using the `sudo` utility), and specify the location of the policy file, or config file, using the `opa-args` key, and an appropriate value:

```
$ docker plugin install --alias opa-docker-authz openpolicyagent/opa-docker-authz-v2:0.8 opa-args=""-config-file /opa/config/opa-conf.yaml""
Plugin ""openpolicyagent/opa-docker-authz-v2:<VERSION>"" is requesting the following privileges:
 - mount: [/etc/docker]
Do you grant the above permissions? [y/N] y
...
Installed plugin openpolicyagent/opa-docker-authz-v2:<VERSION>
```

Check the plugin is installed and enabled:

```
$ docker plugin ls
ID                  NAME                      ENABLED
cab1329e2a5a        opa-docker-authz:latest   true
```

With the plugin installed and enabled, the Docker daemon needs to be configured to make use of the plugin. There are a couple of ways of doing this, but perhaps the easiest is to add a configuration option to the daemon's configuration file (usually `/etc/docker/daemon.json`):

```json
{
    ""authorization-plugins"": [""openpolicyagent/opa-docker-authz-v2:0.8""]
}
```

To update the Docker daemon's configuration, send a `HUP` signal to its process:

```
$ sudo kill -HUP $(pidof dockerd)
```

The Docker daemon will now send authorization requests for all Docker client API calls, to the `opa-docker-authz` plugin, for evaluation.

**Legacy Plugin**

If you prefer to use the legacy plugin, it needs to be started as a container, before applying the same configuration to the Docker daemon, as detailed above:

```
$ docker container run -d --restart=always --name opa-docker-authz \
    -v /run/docker/plugins:/run/docker/plugins \
    -v $HOME/opa/policies:/opa \
    openpolicyagent/opa-docker-authz:0.6 -policy-file /opa/authz.rego
```

### Logs

If using the plugin with the `-config-file` option, full decision logging capabilities - including configuring remote endpoints - is at your disposal.

If using a policy file, the activity describing the interaction between the Docker daemon and the authorization plugin, and the authorization decisions made by OPA, can be found in the daemon's logs. Their [location](https://docs.docker.com/config/daemon/#read-the-logs) is dependent on the host operating system configuration.

Logs are generated in a json format similar to [decision logs](https://www.openpolicyagent.org/docs/latest/management/#decision-logs):

```
{
  ""config_hash"": ""a2e84e38eafd14a816194357860b253becbc739e601cf4307078413a0a578a89"",
  ""decision_id"": ""8d4c6d08-b56e-4625-b66c-3e6c00d7a6e7"",
  ""input"": {
    ""AuthMethod"": """",
    ""BindMounts"": [],
    ""Body"": null,
    ""Headers"": {
      ""Content-Length"": ""0"",
      ""Content-Type"": ""text/plain"",
      ""User-Agent"": ""Docker-Client/19.03.11 (linux)""
    },
    ""Method"": ""POST"",
    ""Path"": ""/v1.40/images/create?fromImage=registry.company.com%3A8885%2Fbash\\u0026tag=latest"",
    ""PathArr"": [
      """",
      ""v1.40"",
      ""images"",
      ""create""
    ],
    ""PathPlain"": ""/v1.40/images/create"",
    ""Query"": {
      ""fromImage"": [
        ""registry.company.com:8885/bash""
      ],
      ""tag"": [
        ""latest""
      ]
    },
    ""User"": """"
  },
  ""labels"": {
    ""app"": ""opa-docker-authz"",
    ""id"": ""396f1138-ea63-4be0-9ce0-3184cb20b1dd"",
    ""opa_version"": ""v0.18.0"",
    ""plugin_version"": ""0.8""
  },
  ""result"": true,
  ""timestamp"": ""2020-06-16T16:44:54.328705305Z""
}
```

### Input Processing

The Rego `input` document is largely identical to the JSON data structure given to opa-docker-authz by Docker, with the following additions
to enrich the document with additional information and assist policy authoring:
 - PathPlain - the Path portion of the RequestURI (exposed as 'Path'), i.e. without the query string 
 - PathArr - PathPlain split into an array of path elements by '/'
 - BindMounts - an array of bind mount objects, as specified via either 'Binds' or 'Mounts' (see below)
 
#### BindMounts

The BindMounts array is populated with information about the source, readonly status and resolved symlink path of each bind.  The each object in the array
has the schema

```
{
  ""Source"": ""<source path>"",
  ""ReadOnly"": true|false,
  ""Resolved"": ""<resolved source path>""
}
```

where 'Resolved' is either the empty string ("""") or the full host path that corresponds to `Source` after resolving any symbolic links. 
This allows for effective policy checking of bind mount sources, including where the true source path is obfuscated with symlinks. This
mitigates against a known trivial bypass of policy that check for binds, for example

```
cd /home/user
ln -sf / root
docker run --rm -it -v/home/user/root:/mnt image
# /mnt is now / in the hostfs
docker run --rm -it -v/home/user/root/var:/mnt image
# /mnt is now /var on the host
```

In each of the above examples, the 'Resolved' path allows for the situation to be detected by policy (it will resolve to ""/"" and ""/var"", respectively).

**Note**: in order for the bind mount resolution to work, the opa-docker-authz plugin must have read access to all parts of the filesystem for which
these checks are required by the policy.  The easiest way to achieve this is to run the plugin as a legacy plugin as `root`.  If using a managed plugin,
the `config.json` would need to rebuilt with a custom bind configuration that exposes the relevant parts of the hostfs to the plugin as read only binds. 

### Uninstall

Uninstalling the `opa-docker-authz` plugin is the reverse of installing. First, remove the configuration applied to the Docker daemon, not forgetting to send a `HUP` signal to the daemon's process.

If you're using the legacy plugin, use the `docker container rm -f opa-docker-authz` command to remove the plugin. Otherwise, use the `docker plugin rm -f opa-docker-authz` command to remove the managed plugin.",FAUX
open-policy-agent/opa-envoy-plugin,Toolkit,Toolkit,2025-05-02T18:57:39Z,2025-01-22T18:16:24Z,0,1,0,0,0,0,0,0,2018-03-08T19:55:10Z,2025-04-07T10:09:30Z,913144,336,Go,VRAI,116,FAUX,19,"authorization,cloud-native,compliance,envoy,istio-proxy,opa,opa-envoy,policy",19,A plugin to enforce OPA policies with Envoy,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,54,"# opa-envoy-plugin

[![Build Status](https://github.com/open-policy-agent/opa-envoy-plugin/actions/workflows/checks.yaml/badge.svg?branch=main)](https://github.com/open-policy-agent/opa-envoy-plugin/actions) [![Go Report Card](https://goreportcard.com/badge/github.com/open-policy-agent/opa-envoy-plugin)](https://goreportcard.com/report/github.com/open-policy-agent/opa-envoy-plugin)

This repository contains an extended version of OPA (**OPA-Envoy**) that allows you to enforce OPA policies with Envoy.

## Issue Management

Use [OPA GitHub Issues](https://github.com/open-policy-agent/opa/issues) to request features or file bugs.

## Examples with Envoy-based service meshes

The OPA-Envoy plugin can be deployed with Envoy-based service meshes such as:

* [Istio](./examples/istio)
* [Gloo Edge](./examples/gloo-edge)

## Overview

OPA-Envoy extends OPA with a gRPC server that implements the [Envoy External
Authorization
API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/security/ext_authz_filter.html).
You can use this version of OPA to enforce fine-grained, context-aware access
control policies with Envoy _without_ modifying your microservice.

More information about the OPA-Envoy plugin including performance benchmarks, debugging tips, detailed usage examples
can be found [here](https://www.openpolicyagent.org/docs/edge/envoy-introduction/).

## Quick Start

This section assumes you are testing with Envoy v1.10.0 or later.

1. Start Minikube.

    ```bash
    minikube start
    ```

1. Install OPA-Envoy.

    ```bash
    kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/opa-envoy-plugin/main/quick_start.yaml
    ```

    The `quick_start.yaml` manifest defines the following resources:

    * A ConfigMap containing an Envoy configuration with an External Authorization Filter to direct authorization checks to the OPA-Envoy sidecar.
    See `kubectl get configmap proxy-config` for details.

    * OPA configuration file, and an OPA policy into ConfigMaps in the namespace where the app will be deployed, e.g., `default`.

    * A Deployment consisting an example Go application with OPA-Envoy and Envoy sidecars. The sample app provides information
    about employees in a company and exposes APIs to `get` and `create` employees. More information about the app
    can be found [here](https://github.com/ashutosh-narkar/go-test-server). The deployment also includes an init container that
    installs iptables rules to redirect all container traffic through the Envoy proxy sidecar. More information can be
    found [here](https://github.com/open-policy-agent/contrib/tree/main/envoy_iptables).

1. Make the application accessible outside the cluster.

    ```bash
    kubectl expose deployment example-app --type=NodePort --name=example-app-service --port=8080
    ```

1. Set the `SERVICE_URL` environment variable to the service’s IP/port.

    **minikube**:

    ```bash
    export SERVICE_PORT=$(kubectl get service example-app-service -o jsonpath='{.spec.ports[?(@.port==8080)].nodePort}')
    export SERVICE_HOST=$(minikube ip)
    export SERVICE_URL=$SERVICE_HOST:$SERVICE_PORT
    echo $SERVICE_URL
    ```

    **minikube (example)**:

    ```bash
    192.168.99.100:31380
    ```

1. Exercise the sample OPA policy.

    For convenience, we’ll want to store Alice’s and Bob’s tokens in environment variables.

    ```bash
    export ALICE_TOKEN=""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiZ3Vlc3QiLCJzdWIiOiJZV3hwWTJVPSIsIm5iZiI6MTUxNDg1MTEzOSwiZXhwIjoxOTQxMDgxNTM5fQ.rN_hxMsoQzCjg6lav6mfzDlovKM9azaAjuwhjq3n9r8""
    export BOB_TOKEN=""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJzdWIiOiJZbTlpIiwibmJmIjoxNTE0ODUxMTM5LCJleHAiOjE5NDEwODE1Mzl9.ek3jmNLPclafELVLTfyjtQNj0QKIEGrbhKqpwXmQ8EQ""
    ```

    Check that `Alice` can get employees **but cannot** create one.

    ```bash
    curl -i -H ""Authorization: Bearer ""$ALICE_TOKEN"""" http://$SERVICE_URL/people
    curl -i -H ""Authorization: Bearer ""$ALICE_TOKEN"""" -d '{""firstname"":""Charlie"", ""lastname"":""OPA""}' -H ""Content-Type: application/json"" -X POST http://$SERVICE_URL/people
    ```

   Check that `Bob` can get employees and also create one.

   ```bash
    curl -i -H ""Authorization: Bearer ""$BOB_TOKEN"""" http://$SERVICE_URL/people
    curl -i -H ""Authorization: Bearer ""$BOB_TOKEN"""" -d '{""firstname"":""Charlie"", ""lastname"":""Opa""}' -H ""Content-Type: application/json"" -X POST http://$SERVICE_URL/people
    ```

   Check that `Bob` **cannot** create an employee with the same firstname as himself.

   ```bash
    curl -i  -H ""Authorization: Bearer ""$BOB_TOKEN"""" -d '{""firstname"":""Bob"", ""lastname"":""Rego""}' -H ""Content-Type: application/json"" -X POST http://$SERVICE_URL/people
    ```

## Configuration

To deploy OPA-Envoy include the following container in your Kubernetes Deployments:

```yaml
containers:
- image: openpolicyagent/opa:latest-envoy
  imagePullPolicy: IfNotPresent
  name: opa-envoy
  volumeMounts:
  - mountPath: /config
    name: opa-envoy-config
  args:
  - run
  - --server
  - --addr=localhost:8181
  - --diagnostic-addr=0.0.0.0:8282
  - --config-file=/config/config.yaml
  livenessProbe:
    httpGet:
      path: /health?plugins
      port: 8282
  readinessProbe:
    httpGet:
      path: /health?plugins
      port: 8282
```

The OPA-Envoy configuration file should be volume mounted into the container. Add the following volume to your Kubernetes Deployments:

```yaml
volumes:
- name: opa-envoy-config
  configMap:
    name: opa-envoy-config
```

### Example Bundle Configuration

In the [Quick Start](#quick-start) section an OPA policy is loaded via a volume-mounted ConfigMap. For production
deployments, we recommend serving policy [Bundles](http://www.openpolicyagent.org/docs/bundles.html) from a remote HTTP server.

Using the configuration shown below, OPA will download a sample bundle from [https://www.openpolicyagent.org](https://www.openpolicyagent.org).
The sample bundle contains the exact same policy that was loaded into OPA via the volume-mounted ConfigMap.

**config.yaml**:

```yaml
services:
  - name: controller
    url: https://www.openpolicyagent.org
bundles:
  envoy/authz:
    service: controller
plugins:
  envoy_ext_authz_grpc:
    addr: :9191 # default `:9191`
    path: envoy/authz/allow # default: `envoy/authz/allow`
    dry-run: false # default: false
    enable-reflection: false # default: false
    grpc-max-recv-msg-size: 40194304 # default: 1024 * 1024 * 4
    grpc-max-send-msg-size: 2147483647 # default: max Int
    skip-request-body-parse: false # default: false
    enable-performance-metrics: false # default: false. Adds `grpc_request_duration_seconds` prometheus histogram metric 
```

You can download the bundle and inspect it yourself:

```bash
mkdir example && cd example
curl -s -L https://www.openpolicyagent.org/bundles/envoy/authz | tar xzv
```

In this way OPA can periodically download bundles of policy from an external server and hence loading the policy via a
volume-mounted ConfigMap would not be required. The `readinessProbe` to `GET /health?bundles` ensures that the `opa-envoy`
container becomes ready after the bundles are activated.

## Dependencies

Dependencies are managed with [Modules](https://github.com/golang/go/wiki/Modules).
If you need to add or update dependencies, modify the `go.mod` file or
use `go get`. More information is available [here](https://github.com/golang/go/wiki/Modules#how-to-upgrade-and-downgrade-dependencies).
Finally commit all changes to the repository.

## Maintainers

Please see the [MAINTAINERS.md](./MAINTAINERS.md) file for maintainer details.",FAUX
open-policy-agent/opa-idea-plugin,Toolkit,Toolkit,2025-04-18T10:56:43Z,2022-12-12T15:13:18Z,0,56,0,0,0,0,0,0,2020-03-11T23:04:33Z,2025-03-18T16:03:54Z,1234,57,Kotlin,VRAI,22,FAUX,36,,36,Open Policy Agent plugin for IntelliJ ,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,14,"# Opa IntelliJ plugin
A plugin for [IntelliJ](https://www.jetbrains.com/idea/) that provides support for [Open Policy Agent](https://www.openpolicyagent.org/)

Main features are:
* highlighting
* `opa eval` run configuration
* `opa test` run configuration

# Compatibility

The plugin is compatible with all IntelliJ-based IDEs starting from the version 2020.3, with the following differences in the sets of the available features:


|                        | [IntelliJ IDEA] Community and Ultimate (commercial) |Other IDE
|------------------------|---|---|
| Rego project creation  | + | - |
| Other features         | + | + |


Plugin has been tested against OPA `0.28.0`, but should work with more recent versions.


# Installation 
OPA binary must be in the path.
Installation instructions for OPA can be found [here](https://www.openpolicyagent.org/docs/latest/#running-opa).

## from Jetbrains repository
Go to `Settings / Preferences / Plugins` menu. Then, search `opa` in the `Marketplace` tab and install the plugin.

![Step 3](docs/user/img/3_install_opa_plugin.png)
## from source
You can build the project from source and then install it. Build instructions are available [here](docs/devel/setup_development_env.md).

# Documentation 
User documentation is available [here](docs/user/README.md).

# Contributing
Interested in contributing? Please, start by reading this [document](CONTRIBUTING.md).

# Acknowledgment
This project is widely inspired by [rust IntelliJ plugin](https://github.com/intellij-rust/intellij-rust). So, thank you
[JetBrains](https://www.jetbrains.org/) team.",FAUX
open-sdg/open-sdg,Application System,Apl,2025-05-05T18:33:59Z,2024-12-06T15:50:25Z,0,0,0,0,0,5,0,0,2018-11-30T19:14:43Z,2025-03-21T03:35:16Z,20577,66,JavaScript,VRAI,83,FAUX,51,,51,A platform for collecting and disseminating data for the Sustainable Development Goal global indicators.,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,23,"# Open SDG

This is a platform for collecting and disseminating data for the Sustainable Development Goal global indicators.

## Documentation

Complete documentation can be found [here](https://open-sdg.readthedocs.io/en/latest/).

## Development

To see the platform while developing (requires Ruby and Python):

```
make serve
```

To run the tests (also requires Node.js):

```
make test
```

To run particular tests:

```
# Test for broken links, images, and other HTML issues.
make test.html
# Test for broken functionality.
make test.features
# Test for accessibility problems.
make test.accessibility
```

To clean up (remove temporary files and stop the web server) after tests:

```
make clean
```",VRAI
open-sdg/open-sdg-data-starter,Application System,Documentations,2025-01-17T13:03:35Z,2022-06-23T19:28:24Z,0,0,0,0,0,1,0,0,2018-10-20T04:10:26Z,2025-01-17T13:03:39Z,47264,9,Python,VRAI,69,FAUX,9,,9,A starting point for the data repository of an Open SDG platform implementation.,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,2,"# Open SDG - Data starter

This is a starter repository to help in implementing the [Open SDG](https://github.com/open-sdg/open-sdg) platform. [See here for documentation](https://open-sdg.readthedocs.io).",VRAI
open-sdg/sdg-translations,Toolkit,Application System,2025-04-04T13:14:30Z,2025-03-21T02:15:10Z,0,0,0,0,0,27,0,0,2018-06-24T15:02:02Z,2025-04-04T13:14:35Z,39056,10,Python,VRAI,40,FAUX,4,,4,Translations of language for use with the Sustainable Development Goals.,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,42,"# SDG Translations

<a href=""https://hosted.weblate.org/engage/sdg-translations/"">
<img src=""https://hosted.weblate.org/widgets/sdg-translations/-/open-graph.png"" alt=""Translation status"" />
</a>

A project to compile and translate text related to the United Nation's
  Sustainable Development Goals, and then provide those translations as JSON.

These translations are intended primarily for the open-source national reporting platform [Open SDG](https://github.com/open-sdg/open-sdg), but can certainly be used for other NRPs or SDG-related projects.

More information [here](https://open-sdg.org/sdg-translations/).",VRAI
open-telemetry/semantic-conventions,Toolkit,Documentations,2025-05-15T21:02:49Z,2025-05-02T14:35:00Z,0,10,0,0,0,0,0,0,2023-05-09T17:15:53Z,2025-04-08T01:30:17Z,6582,347,Open Policy Agent,VRAI,216,FAUX,549,,549,"Defines standards for generating consistent, accessible telemetry across a variety of domains",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,229,"# <img src=""https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png"" alt=""OpenTelemetry Icon"" width=""45"" height=""""> OpenTelemetry Semantic Conventions

[![Checks](https://github.com/open-telemetry/semantic-conventions/workflows/Checks/badge.svg)](https://github.com/open-telemetry/semantic-conventions/actions?query=workflow%3A%22Checks%22+branch%3Amain)
[![GitHub tag (latest SemVer)](https://img.shields.io/github/tag/open-telemetry/semantic-conventions.svg?logo=opentelemetry&&color=f5a800&label=Latest%20release)](https://github.com/open-telemetry/semantic-conventions/releases/latest)
[![Specification Version](https://img.shields.io/badge/OTel_specification_version-v1.44.0-blue?logo=opentelemetry&color=f5a800)](https://github.com/open-telemetry/opentelemetry-specification/releases/tag/v1.44.0)

Semantic Conventions define a common set of (semantic) attributes which
provide meaning to data when collecting, producing and consuming it.

## Read the docs

The human-readable version of the semantic conventions resides in the [docs](docs/README.md) folder.
Major parts of these Markdown documents are generated from the YAML definitions located in the [model](model/README.md) folder.

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md)

Approvers ([@open-telemetry/specs-semconv-approvers](https://github.com/orgs/open-telemetry/teams/specs-semconv-approvers)):

- [Alexandra Konrad](https://github.com/trisch-me), Elastic
- [Daniel Dyla](https://github.com/dyladan), Dynatrace
- [Ted Young](https://github.com/tedsuo), Lightstep

_Find more about the approver role in [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#approver)._

Maintainers ([@open-telemetry/specs-semconv-maintainers](https://github.com/orgs/open-telemetry/teams/specs-semconv-maintainers)):

- [Alexander Wert](https://github.com/AlexanderWert), Elastic
- [Armin Ruech](https://github.com/arminru), Dynatrace
- [Joao Grassi](https://github.com/joaopgrassi), Dynatrace
- [Johannes Tax](https://github.com/pyohannes), Grafana Labs
- [Josh Suereth](https://github.com/jsuereth), Google
- [Liudmila Molkova](https://github.com/lmolkova), Microsoft
- [Trask Stalnaker](https://github.com/trask), Microsoft

Emeritus Approvers:

- [Christian Neumüller](https://github.com/Oberon00)
- [James Moessis](https://github.com/jamesmoessis)
- [Sean Marciniak](https://github.com/MovieStoreGuy)

Emeritus Maintainers:

- [Reiley Yang](https://github.com/reyang)

_Find more about the maintainer role in [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#maintainer)._",VRAI
open-telemetry/weaver,Toolkit,DevOPs,2025-05-15T00:26:47Z,2025-04-08T23:41:18Z,0,19,0,0,0,0,0,0,2024-02-07T18:14:56Z,2025-04-07T22:15:39Z,7702,117,Rust,VRAI,42,FAUX,68,"codegen,documentation,observability,opentelemetry,policy,semconv",68,"OTel Weaver lets you easily develop, validate, document, and deploy semantic conventions",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,21,"# OpenTelemetry Weaver

[![build](https://github.com/open-telemetry/weaver/actions/workflows/ci.yml/badge.svg)](https://github.com/open-telemetry/weaver/actions/workflows/ci.yml)
[![codecov](https://codecov.io/gh/open-telemetry/weaver/graph/badge.svg?token=tmWKFoMT2G)](https://codecov.io/gh/open-telemetry/weaver)
[![build](https://github.com/open-telemetry/weaver/actions/workflows/audit.yml/badge.svg)](https://github.com/open-telemetry/weaver/actions/workflows/audit.yml)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Slack](https://img.shields.io/badge/Slack-OpenTelemetry_Weaver-purple)](https://cloud-native.slack.com/archives/C0697EXNTL3)
----

[Getting started](#getting-started) | [Main commands](#main-commands) | [Generate Doc & Code](crates/weaver_forge/README.md) | [Architecture](docs/architecture.md) | [Change log](CHANGELOG.md) | [Contributing](CONTRIBUTING.md) | [Links](#links) |

> [!NOTE]
> Codegen authors, please refer to the following documentation to learn how to
> use Weaver for generating code from semantic conventions:
> - [Templates organization, integrated Jinja2 engine, list of supported filters/functions/tests](crates/weaver_forge/README.md)
> - [Weaver Configuration](docs/weaver-config.md)

## What is OpenTelemetry Weaver?

**OTel Weaver** is a comprehensive tool designed to enable developers to
easily develop, validate, document, and deploy semantic conventions (phase 1)
and application telemetry schemas (phase 2). As an **open, customizable, and
extensible platform**, it aims to serve both as a standalone developer tool
and as an integral component within CI/CD pipelines—whether for the
OpenTelemetry project itself, other open-source projects, vendor solutions,
or even large-scale enterprise deployments leveraging OpenTelemetry.

## Semantic Conventions and Application Telemetry Schema

- **Semantic conventions** enable SMEs to define a catalog of well-defined and reusable
  attributes and signals. OpenTelemetry maintains an official Semantic Convention
  Registry that any project can leverage for consistent instrumentation.
  Open-source projects, vendors, and enterprises can also implement their own
  registries for specific needs, which Weaver can import and resolve to cover all
  instrumented components of complex systems.
- **Application Telemetry Schema** allows developers to specify the semantic
  convention registries and custom attributes and signals supported by their
  applications. The vision behind this concept is detailed in this [document](https://github.com/open-telemetry/oteps/blob/main/text/0243-app-telemetry-schema-vision-roadmap.md),
  with implementation planned for Weaver's phase 2.

## Design Principles

Weaver is built upon principles of extensibility, customizability, scalability,
robustness, reliability, and cross-platform compatibility.

## Key Features

- **Schema Resolution**: The Weaver Resolver sub-project resolves references,
  extends clauses, and overrides in semantic convention registries and application
  telemetry schemas, producing self-contained, easy-to-process, and shareable
  resolved schemas.
- **Policy Enforcement**: The Weaver Checker sub-project ensures the quality,
  maintainability, and extensibility of registries and schemas by checking them
  against a set of declarative policies using the popular rego policy language.
- **Documentation and Code Generation**: The Weaver Forge sub-project generates
  documentation and code from registries and schemas. It leverages a `jq-compatible`
  expression language for data transformation and a `jinja2-compatible` template
  engine for output generation.
- **WASM-based Plugin System (future plan)**: A plugin system based on WASM will
  be implemented to extend the Weaver platform. Plugins could be designed to
  download registries and schemas from custom systems, feed data catalog solutions,
  or configure dashboard systems, among other functionalities.

## Scalability

- Built with Rust, Weaver offers performance comparable to C or C++ implementation.
  The entire OpenTelemetry semantic convention registry can be downloaded, resolved,
  and documented in under 2 seconds.
- Semantic Convention Registry Git repositories can be efficiently cached locally.
- Registry and schema validation, as well as documentation and code generation,
  are parallelized for optimal performance.

## Robustness and Reliability

- **Memory Safety**: Rust ensures memory safety, preventing common vulnerabilities.
- **Comprehensive Error Reporting**: Weaver reports as many errors as possible in
  a single pass, providing developers with comprehensive feedback.
- **Quality Assurance**: Code coverage, Cargo deny, Dependabot, and automated security
  audits.

## Cross-Platform Compatibility

- Tested Platforms: Weaver is manually tested on Linux, macOS, and Windows.
- Future Plans: Automated tests will be implemented for broader platform coverage.

## Getting started

Currently, weaver can be consumed in one of three ways:

- [Pre-built Binaries](#pre-built-binaries)
- [Docker Image](#docker-image)
- [Building from source](#building-from-source)

### Pre-built Binaries

Weaver release attach pre-built binaries for supported platforms with every release.
Instructions for installing are included in these release notes.

See: [Weaver Releases](https://github.com/open-telemetry/weaver/releases)

### Docker Image

Weaver deploys a docker image for development to [docker hub](https://hub.docker.com/r/otel/weaver).

Instructions for using the docker image can be found [here](docs/docker-guide.md).

### Building from source

To install the tool from source. you need to have Rust installed on your
system (see [Install Rust](https://www.rust-lang.org/tools/install)).

To build the tool:

- In debug mode, run the following command:
  ```
  cargo build
  ```
- In release mode, run the following command:
  ```
  cargo build --release
  ```

The generated `weaver` binary will be located in the `target/debug` directory
for debug mode or the `target/release` directory for release mode.

To run a registry check, use the following command:
```
cargo run -- registry check
```

This command will check the OpenTelemetry Semantic Convention Registry by
default.

To check a set of policies against the registry, use the following command:
```
cargo run -- registry check -b path/to/policies
```

An example of a policy file can be found here [schemas/otel_policies.rego](schemas/otel_policies.rego).

## Main commands

In phase 1, the only supported commands are related to the management of
Semantic Convention Registries. The following commands are available:

| Command                                                                   | Description                                 |
|---------------------------------------------------------------------------|---------------------------------------------|
| [weaver registry check](docs/usage.md#registry-check)                     | Check the validity of a semconv registry    |
| [weaver registry resolve](docs/usage.md#registry-resolve)                 | Resolve a semconv registry                  |
| [weaver registry diff](docs/usage.md#registry-diff)                       | Generate a diff report between two versions |
| [weaver registry generate](docs/usage.md#registry-generate)               | Generate artifacts from a semconv registry  |
| [weaver registry update-markdown](docs/usage.md#registry-update-markdown) | Update semconv snippet-based markdown files |
| [weaver registry stats](docs/usage.md#registry-stats)                     | Generate statistics on a semconv registry   |

Phase 2 will introduce commands related to the management of Application
Telemetry Schemas.

## Documentation

- [Weaver Architecture](docs/architecture.md): A document detailing the architecture of the project.
- [Weaver Configuration](docs/weaver-config.md): A document detailing the configuration options available.
- [Weaver Forge](crates/weaver_forge/README.md): An integrated template engine designed to generate
  documentation and code based on semantic conventions.
- [Weaver Checker](crates/weaver_checker/README.md): An integrated policy
  engine for enforcing policies on semantic conventions.
- [Schema Changes](docs/schema-changes.md): A document describing the data model
  used to represent the differences between two versions of a semantic convention registry.
- [Application Telemetry Schema OTEP](https://github.com/open-telemetry/oteps/blob/main/text/0243-app-telemetry-schema-vision-roadmap.md):
  A vision and roadmap for the concept of Application Telemetry Schema.
- Presentation slides from the Semantic Convention SIG meeting on October 23,
  2023 [here](https://docs.google.com/presentation/d/1nxt5VFlC1mUjZ8eecUYK4e4SxThpIVj1IRnIcodMsNI/edit?usp=sharing).

## Examples of Code and Documentation Generation

The following examples show how to generate code and documentation for various languages using Weaver.

> Note: A more out of the box experience will be available in the future.

**Java**
From GitHub repo main branch.

```bash
weaver registry generate \
--templates https://github.com/open-telemetry/semantic-conventions-java.git[buildscripts/templates] \
java
```

**C++**
From a specific release using the corresponding archive.

```bash
weaver registry generate \
--templates https://github.com/open-telemetry/opentelemetry-cpp/archive/refs/tags/v1.20.0.zip[buildscripts/semantic-convention/templates] \
./ \
--param filter=any \
--param output=output/ \
--param schema_url=https://opentelemetry.io/schemas/v1.32.0
```

**Python**
From GitHub repo main branch.

```bash
weaver registry generate \
--templates https://github.com/open-telemetry/opentelemetry-python.git[scripts/semconv/templates] \
--param output=./ \
--param filter=any \
./
```

**Markdown (attribute registry)**
From a specific release using a GitHub archive release.

```bash
weaver registry generate \
--templates https://github.com/open-telemetry/semantic-conventions/archive/refs/tags/v1.32.0.zip[templates] \
markdown
```

**Templates in local path**

```bash
weaver registry generate \
--templates custom-templates/ \
markdown
```

## Experimental

- [Component Telemetry Schema](docs/component-telemetry-schema.md) (proposal)
- [Resolved Telemetry Schema](docs/resolved-telemetry-schema.md) (proposal)
- OpenTelemetry Telemetry Schema
  v1.2.0 [Draft](https://github.com/lquerel/oteps/blob/app-telemetry-schema-format/text/0241-telemetry-schema-ext.md) (
  not yet ready).

## Shell Completions

Weaver supports shell completions via
[clap_complete](https://docs.rs/clap_complete/latest/clap_complete/) for the
following shells: bash, elvish, fish, powershell, and zsh. Installing and using
the completion depends on your shell, and your setup.

For example, to install the completion for bash, you can run the following
command:

```bash
weaver completion bash > $BASH_COMPLETION_USER_DIR/weaver
```

To enable the completion for zsh, you can run the following command:

```bash
weaver completion zsh > ${fpath[1]}/_weaver
```

That will install the completion in the first directory in your `fpath` array.

## Links

- [OpenTelemetry Semantic Convention File Format](https://github.com/open-telemetry/build-tools/blob/main/semantic-conventions/syntax.md)
- [OpenTelemetry Schema File Format v1.1.0](https://opentelemetry.io/docs/specs/otel/schemas/file_format_v1.1.0/)
- Meta/Facebook's [positional paper](https://research.facebook.com/publications/positional-paper-schema-first-application-telemetry/)
  presenting a similar approach but based on Thrift+Annotations+Automations.

## Contributing

Pull requests are welcome. For major changes, please open an issue
first to discuss what you would like to change. For more information, please
read [CONTRIBUTING](CONTRIBUTING.md).

## License

OpenTelemetry Weaver is licensed under Apache License Version 2.0.",VRAI
openshift/managed-cluster-config,Application System,Application System,2025-05-15T18:17:50Z,2025-04-11T23:08:34Z,0,0,0,0,0,0,0,0,2019-02-26T21:44:58Z,2025-04-08T07:35:31Z,68608,57,HTML,VRAI,215,FAUX,7,osdv4,7,Static deployable artifacts for managed OSD clusters,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,204,"# managed-cluster-config repository

This repo contains static configuration specific to a ""managed"" OpenShift Dedicated (OSD) cluster.
## How to use this repo
https://issues.redhat.com/browse/SDE-2786 has change the repo slightly: /deploy holds the sources of truth, and /generated_deploy holds the configurations that will be applied by Hive.
To add a new SelectorSyncSet, add your yaml manifest to the `deploy` dir, then run the `make` command.

Alternatively you can enable GitHub Actions on your fork and `make` will be ran automatically. Additionally,
the action will create a new commit with the generated files.

To add an ACM (Governance) Policy
- If the manifest of the object you want to convert to policy already exists in `deploy` : in the object config.yaml, add a field `policy: `destination: ""acm-policies""` (example: https://github.com/openshift/managed-cluster-config/blob/master/deploy/backplane/cee/config.yaml) 
- If the manifest of the object does not exist: add your manifests with a config.yaml file. If you only want this object to be deployed as Policy, see [this example](https://github.com/openshift/managed-cluster-config/tree/bad140663d088cbce06edaf2527f69651db5a80b/deploy/hs-mgmt-route-monitor-operator)

`make` will look for `config.yaml` files, runs it with the PolicyGenerator binary and save the output to `generated_deploy/acm-policies` directory. `make` will then automatically
add the policy as a new SelectorySyncSet.

# Building

## Dependencies

- oyaml: `pip install oyaml`

# Configuration

All resources in `generated_deploy/` are bundled into a template that is used by config management to apply to target ""hive"" clusters.  The configuration is deployed to the ""hive"" cluster inside a SelectorSyncSet.

SelectorSyncSet deployment supports resources that are synced down to OSD clusters.  Each are explained in detail here.  The general configuration is managed in a `config.yaml` file in each deploy directory.  Key things of note:

* This file is now mandatory in the scope of OSD-15267 and have been added to all folders. In case it is not define, `make` will fail
```
+ scripts/generate_template.py -t scripts/templates/ -y deploy -d /Users/bdematte/git/managed-cluster-config/hack/ -r managed-cluster-config
ERROR : Missing config.yaml for resource defined in deploy/acm-policies
Some config.yaml files are missing, exiting...
make: *** [generate-hive-templates] Error 1
```
* Configuration is _not_ inherited by sub-directories!  Every (EVERY) directory in the `deploy/` hierarchy must define a `config.yaml` file.

You must specify a `deploymentMode` property in `config.yaml`.

* `deploymentMode` (optional, default = `""SelectorSyncSet""`) - either ""Policy"" or ""SelectorSyncSet"".

## Direct Deployment

Direct deployments to Hive clusters should be done via [app-interface](https://gitlab.cee.redhat.com/service/app-interface#manage-openshift-resources-via-app-interface-openshiftnamespace-1yml).

## SelectorSyncSet Deployment

In the `config.yaml` file you define a top level property `selectorSyncSet`.  Within this configuration is supported for `matchLabels`, `matchExpressions`, `matchLabelsApplyMode`, `resourceApplyMode` and `applyBehavior`.

* `matchLabels` (optional, default: `{}`) - adds additional `matchLabels` conditions to the SelectorSyncSet's `clusterDeploymentSelector`
* `matchExpressions` (optional, default: `[]`) - adds `matchExpressions` conditions to the SelectoSyncSet's `clusterDeploymentSelector`
* `resourceApplyMode` (optional, default: `""Sync""`) - sets the SelectorSyncSet's `resourceApplyMode`
* `matchLabelsApplyMode` (optional, default: `""AND""`) - When set as `""OR""` generates a separate SSS per `matchLabels` conditions. Default behavior creates a single SSS with all `matchLabels` conditions.  This is to tackle a situation where we want to apply configuration for one of many label conditions.
* `applyBehavior` (optional, default: None, [see hive default](https://github.com/openshift/hive/blob/master/config/crds/hive.openshift.io_selectorsyncsets.yaml)) - sets the SelectorSyncSet's `applyBehavior`

You can also define a top level property `policy` to specify the behaviour of `./scripts/generate-policy-config.py` for the resource. Supported sub-properties :
* `complianceType` (optional, default: `""mustonlyhave""`, [see operator values](https://github.com/open-cluster-management-io/config-policy-controller/blob/main/api/v1/configurationpolicy_types.go) - select the compliance type for the policy when used by `./scripts/generate-policy-config.py`)
* `metadataComplianceType` (optional, default: `""musthave""`, [see operator values](https://github.com/open-cluster-management-io/config-policy-controller/blob/main/api/v1/configurationpolicy_types.go) - select the compliance type for metadata for the policy when used by `./scripts/generate-policy-config.py`)

Example to apply a directory for any of a set of label conditions using Upsert:
```yaml
deploymentMode: ""SelectorSyncSet""
selectorSyncSet:
    matchLabels:
        myAwesomeLabel: ""some value""
        someOtherLabel: ""something else""
    resourceApplyMode: ""Upsert""
    matchLabelsApplyMode: ""OR""
policy:
    complianceType: ""mustonlyhave""
    metadataComplianceType: ""musthave""
```

# Selector Sync Sets included in this repo

## Prometheus

A set of rules and alerts that SRE requires to ensure a cluster is functioning.  There are two categories of rules and alerts found here:

1. SRE specific, will never be part of OCP
2. Temporary addition until made part of OCP

## Prometheus and Alertmanager persistent storage

Persistent storage is configured using the configmap `cluster-monitoring-config`, which is read by the cluster-monitoring-operator to generate PersistentVolumeClaims and attach them to the Prometheus and Alertmanager pods.

## Curated Operators

Initially OSD will support a subset of operators only.  These are managed by patching the OCP shipped OperatorSource CRs.  See `deploy/osd-curated-operators`.

NOTE that ClusterVersion is being patched to add overrides.  If other overrides are needed we'll have to tune how we do this patching.  It must be done along with the OperatorSource patching to ensure CVO doesn't revert the OperatorSource patching.

## Console Branding

In OSD, managed-cluster-config sets a [key named `branding` to `dedicated`](https://github.com/openshift/managed-cluster-config/blob/master/deploy/osd-console-branding/osd-branding.console.Patch.yaml) in the [Console operator](https://github.com/openshift/api/blob/master/operator/v1/types_console.go#L89-L135). This value is in turn read by code that applies the [logo](https://github.com/openshift/console/blob/1572a985cc0753d7e2630984c5163170765e9487/frontend/public/components/masthead.jsx) and [other branding elements](https://github.com/openshift/console/search?p=2&q=dedicated) predefined for that value.

## OAuth Templates

Docs TBA.

## Resource Quotas

Refer to [deploy/resource-quotas/README.md](deploy/resource-quotas/README.md).

## Image Pruning

Docs TBA.

# Dependencies

pyyaml


# Additional Scripts

There are additional scripts in this repo as a holding place for a better place or a better solution / process.",FAUX
opensource-observer/oss-directory,Application System,Documentations,2025-05-15T11:42:14Z,2025-05-07T08:13:32Z,0,0,0,0,0,1,0,0,2023-08-02T23:50:31Z,2025-04-04T13:19:13Z,9344,66,Python,VRAI,164,FAUX,4,"data,github,open-source,public-goods,research",4,A curated directory of open source software (OSS) projects and their associated artifacts,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,107,"# OSS Directory [![License: Apache 2.0][license-badge]][license]

[license]: https://opensource.org/license/apache-2-0/
[license-badge]: https://img.shields.io/badge/License-Apache2.0-blue.svg

This repository contains a curated directory of open source software (OSS) projects and their associated artifacts. Artifacts include git repositories, npm packages, smart contracts, Open Collective collectives, accounts used for managing grant funds, and more. Groups of related projects are organized into collections.

The OSS Directory serves as the ""source of truth"" for the projects and collections that are discoverable on [Open Source Observer](https://www.opensource.observer).

While the directory may never be complete, it is actively maintained. We welcome community contributions of new projects and collections, as well as updates to existing entries.

This directory is a public good, free to use and distribute. We hope it serves the needs of researchers, developers, foundations, and other users looking to better understand the OSS ecosystem!

## How to contribute

Currently the main way to contribute is by submitting a pull request. You can update any `.yaml` file under `./data/` or submit a new one. Fork this repository, commit your changes, and open a pull request from your fork to this repository.

Detailed instructions are available in the [latest docs](https://docs.opensource.observer/docs/projects).

If you are adding a new project, please make sure to include a unique project name to identify the project and at least one GitHub url. In most cases, we adopt the GitHub organization name as the project name. Our project naming conventions are described in more detail [here](https://docs.opensource.observer/docs/projects#project-names) - please try to follow them!

Submissions will be validated to ensure they conform to the schema and don't contain any artifacts that are already in the directory. If you are unsure or have additional questions about contributing, please open an issue or message us on [Discord](https://www.opensource.observer/discord).

### Setup for local development

You can install dependencies with `pnpm`.

```bash
pnpm install
```

### Validation

Our GitHub actions CI will reject any contributions that do not conform to the schema defined in `./src/resources/schema`.

To check for validation errors locally, run

```bash
pnpm run validate
```

### Publish

#### NPM

First bump the version number in `package.json`. Then build and publish

```bash
pnpm build
npm publish
```

If you did not log into npm yet, you'll first need to run `npm login`.

#### PyPI

First bump the version number in `pyproject.toml`. Then build and publish

```bash
poetry build
poetry publish
```

If you did not log into PyPI yet, you'll first need to
[generate an API Token](https://pypi.org/manage/account/)
and configure the CLI `poetry config pypi-token.pypi API_TOKEN`.

## Using as a library

We have also published this repository as a library that you can use in your own projects. This is useful if you want to build a tool that uses the data in this repository or perform your own custom analysis.

We have libraries for JavaScript and Python. We don't store the entire dataset with the package. Under the hood, this will clone the repository into a temporary directory, read all the data files, validate the schema, and return the objects. This way, you know you're getting the latest data, even if the package hasn't been updated in a while.

_Note: These do not work in a browser-environment_

### JavaScript library

[npm page](https://www.npmjs.com/package/oss-directory)

#### Installation

Install the library

```bash
npm install oss-directory
# OR yarn add oss-directory
# OR pnpm add oss-directory
```

#### Fetch all of the data

You can fetch all of the latest data in this repo with the following:

```js
import { Project, Collection, fetchData } from ""oss-directory"";

const data = await fetchData();
const projects: Project[] = data.projects;
const collections: Collection[] = data.collections;
```

You can pass in the following options to `fetchData`:

```js
const data = await fetchData({
  // The branch to check out from the repo
  branch: ""main"",
  // The commit to check out from the repo
  commit: ""066e5ad612d6ef67c0516b55b0c3be789282e6b6""
  // Do not strictly validate the data coming from GitHub
  skipValidation: true,
});
```

_Note: skipValidation is really useful if you don't want this integration
to force an error everytime we update the schema.
However, this will necessarily lead to getting data that does not
conform to the types your application expects as we add new fields._

#### Utility functions

We also include functions for casting and validating data:

- `validateProject`
- `validateCollection`
- `safeCastProject`
- `safeCastCollection`

### Python library

[PyPI page](https://pypi.org/project/oss-directory/)

#### Installation

Install the library

```bash
pip install oss-directory
# OR poetry add oss-directory
```

#### Fetch all of the data

You can fetch all of the data in this repo with the following:

```python
from ossdirectory import fetch_data
from ossdirectory.fetch import OSSDirectory

data: OSSDirectory = fetch_data()
projects: List[dict] = data.projects;
collections: List[dict] = data.collections;
```

## Organization

The directory is organized into two main folders:

- `./data/projects` - each file represents a single open source project and contains all of the artifacts for that project.
  - See `./src/resources/schema/project.json` for the expected JSON schema
  - Files should be named by the project ""name""
  - Project names must be globally unique. If there is a conflict in chosen name, we will give priority to the project that has the associated GitHub organization
  - In most cases, we adopt the GitHub organization name as the `name`.
- `./data/collections` - each file represents a collection of projects that have some collective meaning (e.g. all projects in an ecosystem).
  - See `./src/resources/schema/collection.json` for the expected JSON schema
  - Projects are identified by their unique project `name`.

## Scripting changs

Sometimes you need to make a bunch of changes all at once. We have a framework that supports 2 types of such changes:

1. **Migrations**: If you are changing the schema of the data files, you'll need to write a migration that changes all data files to adhere to the new schema
2. **Transformations**: If you want to write a one-off transformation that does not change the schema, use this

### Changing the schema with migrations

🚨⚠️ **Note**: Please use this sparingly. In the current setup, all upstream dependents of this package will break anytime there is a schema change. This is because older versions of the library will try to get the latest data from the main branch and break when validating it against an older schema ️️🚨⚠️

All files under `./data` must conform to schemas defined in `./src/resources/schema`.

If you want to change the schema, you'll need to write a migration:

1. Update the schema in `src/resources/schema/`
2. Add a [version]-[desc].ts file to the `src/migrations/` folder, exporting functions that migrate each file type.
3. Add the migration functions to the MIGRATIONS array in `src/migrations/index.ts`.
4. You can run the migration by running `pnpm migrate`.
5. Please run `pnpm validate` to make sure your migration adheres to the schema. We will not accept any PRs where the data does not conform to the schemas.
6. Commit and submit a pull request with all of the resulting changes.
7. Publish a new version of the npm package. Remember to bump the version number in `package.json`. If you don't do this, you'll break all downstream dependents, because they're fetching the latest from GitHub.
8. Notify all upstream dependents of this package that there is a new major version number and they need to update. Schema changes **will** break any dependent builds until they upgrade.

The framework will run migrations in sequence, so you are guaranteed that your data is valid as of the previous version.
Note: we currently only support migrating in one direction (and not reverting)

### Running a one-off transformation

If you need to make a wide-ranging change that does not affect the schema, use these steps to script the change

1. Add a [transformName].ts file to the `src/transformations/` folder, exporting functions that transform each file type.
2. Add the transformation functions to the TRANSFORMATIONS array in `src/transformations/index.ts`.
3. You can run the transformation by running `pnpm transform --name <transformName>`
4. Please run `pnpm validate` to make sure your migration adheres to the schema. We will not accept any PRs where the data does not conform to the schemas.
5. Commit and submit a pull request with all of the resulting changes.

## Making onchain attestations about projects

### EAS schemas

We've created a schema for making project attestations with [EAS](https://attest.sh/). These attestations create an onchain link between a GitHub repo and its blockchain addresses (ie, that smart contracts and contract factories under its control).

You can view the schemas here:

- optimism-goerli: [Schema 168](https://optimism-goerli-bedrock.easscan.org/schema/view/0x739257b1bf8533a29a5c59a6dda5905c50f7c2bf436d709cd9ea7bfabbe5172b)

- optimism: [Schema 86](https://optimism.easscan.org/schema/view/0x739257b1bf8533a29a5c59a6dda5905c50f7c2bf436d709cd9ea7bfabbe5172b)

### EAS attestations

You can make a series of attestations about a project by running the following:

```bash
ts-node src/scripts/optimism-attestation.ts data/projects/P/PROJECT.yaml optimism # OR optimism-goerli
```

Make sure you have `.env` file that contains your `PRIVATE_KEY` for an account with some ETH on the appropriate network.",VRAI
oracle-devrel/cd3-automation-toolkit,Toolkit,Toolkit,2025-05-14T17:40:04Z,2025-04-04T06:19:07Z,0,31,0,0,0,0,0,0,2022-10-10T16:30:17Z,2025-04-04T14:27:55Z,87111,58,Python,VRAI,27,FAUX,0,,0,The CD3 Automation Toolkit generates Terraform modules from a design spec in an Excel sheet.  It also enables a user to export an OCI tenancy into Excel and corresponding TF modules (and associated tfvars),FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,9,"[![License: UPL](https://img.shields.io/badge/license-UPL-green)](https://img.shields.io/badge/license-UPL-green) [![Quality gate](https://sonarcloud.io/api/project_badges/quality_gate?project=oracle-devrel_cd3-automation-toolkit)](https://sonarcloud.io/dashboard?id=oracle-devrel_cd3-automation-toolkit)

<br>
  
# CD3 Automation Toolkit

<br>

  [What's New](https://github.com/oracle-devrel/cd3-automation-toolkit/releases/tag/v2024.4.3) &nbsp;•&nbsp;[Excel Templates](https://oracle-devrel.github.io/cd3-automation-toolkit/latest/excel-templates/) &nbsp;•&nbsp;[CD3 Docs](https://oracle-devrel.github.io/cd3-automation-toolkit/)&nbsp;•&nbsp; [Watch & Learn](https://www.youtube.com/playlist?list=PLPIzp-E1msrbJ3WawXVhzimQnLw5iafcp) &nbsp;•&nbsp;[Blogs & Tutorials](https://oracle-devrel.github.io/cd3-automation-toolkit/latest/tutorials/) &nbsp;•&nbsp;[Livelabs](https://apexapps.oracle.com/pls/apex/f?p=133:180:112501098061930::::wid:3724) &nbsp;•&nbsp;[Slack Channel](https://oracle-devrel.github.io/cd3-automation-toolkit/latest/queries) 
  
<br>


CD3 stands for **Cloud Deployment Design Deliverable**. The CD3 Automation toolkit enables you to effortlessly Build, Export and Manage OCI (Oracle Cloud Infrastructure) resources by converting Excel templates to fully functional Terraform modules within minutes ⚡️⚡️ .

Additionally, the toolkit also supports seamless resource management using OCI DevOps GIT service and Jenkins Pipelines.

<br>

🚀 Click the below button to quickly launch CD3 toolkit container in Oracle Cloud and start managing your Infra as Code. 
<br>

[![Deploy_To_OCI](https://oci-resourcemanager-plugin.plugins.oci.oraclecloud.com/latest/deploy-to-oracle-cloud.svg)](https://cloud.oracle.com/resourcemanager/stacks/create?zipUrl=https://github.com/oracle-devrel/cd3-automation-toolkit/archive/refs/heads/main.zip)

<br>

<h2>📌 Toolkit Supported OCI Services</h2>

<table>
  
  <tr>
    <td>IAM/Identity</td>
    <td>Tagging</td>
    <td>Quotas</td>
  </tr>
  <tr>
    <td>Network</td>
    <td>DNS Management</td>
    <td>Load Balancers</td>
  </tr>
  <tr>
    <td>OCI Network Firewall</td>
    <td>KMS</td>
    <td>Policy Enforcement using OPA</td>
    
  </tr>
  <tr>
    <td>Compute</td>
    <td>Storage</td>
    <td>Database</td>    
  </tr>
  <tr>
    <td>OKE</td>
    <td>Resource Manager</td>
    <td>SDDCs</td>    
  </tr>
  <tr>
    <td>Logging Services</td>
    <td>Monitoring</td>
    <td>Budgets</td>
  </tr>
  <tr>    
    <td>Cloud Guard</td>
    <td>SHOWOCI report</td>    
    <td>CIS Landing Zone<br>Compliance</td>
    
  </tr>
</table>
</tr>


</body>

<br>

## Why CD3?


⏳ For Enterprise infrastructures, manual resource provisioning is tedious and error-prone.

📝 Creating Terraform Code for each module/resource can be cumbersome and requires Terraform expertise.

🔁 Manually created infrastructure is hard to rebuild for different environments or regions.

<br>

##  How CD3 works?


The toolkit transforms input data from Excel files into Terraform files, enabling seamless creation of infrastructure in OCI.

**CD3 isn't just about creation!!!** ⬅️ Reverse engineer existing infrastructure back into Excel and IaC(terraform) and continue to manage your OCI resources using CD3.

📜 The generated Terraform code can be used by the OCI Resource Manager or can be integrated into organization's existing DevOps CI/CD ecosystem.

<br>

## 👥 Who can use the toolkit??

  Anyone who wants to create/modify new resources in OCI or export existing resources from OCI and manage IaC without much effort. 

<br>

## 💡 Benefits of CD3:


   ✅ Time saving ⏰ 
  
   ✅ Faster infrastructure provisioning 🚀
  
   ✅ Scalability 📈
  
   ✅ Operational efficiency ⚙️

<br>

   **There's more!!**

**Secure architecture 🛡️:** CD3 toolkit helps customers deploy secure standardization across OCI tenancies by providing CIS-compliant Excel templates. It also enables native execution of the CIS Compliance Checker script against your tenancy.

**DevOps-oriented 🔄:** The toolkit facilitates integration of consistent output Terraform files in module format with any continuous integration and delivery (CI/CD) solution. The Terraform code can be reused to build similar workloads in different OCI regions and tenancies, which helps in quicker adoption of OCI.

**Platform independent 🌐:** CD3 is packaged as a container that can be hosted on any platform.
 
<br>


## Where to get started?

[Click here](https://oracle-devrel.github.io/cd3-automation-toolkit/) to learn how to setup the toolkit and start managing your OCI Infra !!

<br>

## ⭐️ Contributing
This project is open source.  Please submit your contributions by raising an <b>Issue</b> or through <b>Discussion topic</b> in this repository. Currently, we do not accept any pull requests. Oracle appreciates any contributions that are made by the open source community.

<br>

## ⚠️ License
Copyright (c) 2024 Oracle and/or its affiliates.

Licensed under the Universal Permissive License (UPL), Version 1.0.

See [LICENSE](LICENSE.txt) for more details. 

ORACLE AND ITS AFFILIATES DO NOT PROVIDE ANY WARRANTY WHATSOEVER, EXPRESS OR IMPLIED, FOR ANY SOFTWARE, MATERIAL OR CONTENT OF ANY KIND CONTAINED OR PRODUCED WITHIN THIS REPOSITORY, AND IN PARTICULAR SPECIFICALLY DISCLAIM ANY AND ALL IMPLIED WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.  FURTHERMORE, ORACLE AND ITS AFFILIATES DO NOT REPRESENT THAT ANY CUSTOMARY SECURITY REVIEW HAS BEEN PERFORMED WITH RESPECT TO ANY SOFTWARE, MATERIAL OR CONTENT CONTAINED OR PRODUCED WITHIN THIS REPOSITORY. IN ADDITION, AND WITHOUT LIMITING THE FOREGOING, THIRD PARTIES MAY HAVE POSTED SOFTWARE, MATERIAL OR CONTENT TO THIS REPOSITORY WITHOUT ANY REVIEW. USE AT YOUR OWN RISK.",FAUX
oscal-compass/compliance-to-policy,Toolkit,Toolkit,2025-04-03T00:54:35Z,2024-05-02T11:59:12Z,0,0,0,0,0,0,0,0,2023-03-31T20:38:36Z,2025-04-03T00:54:41Z,2446,24,Go,VRAI,19,FAUX,12,,12,Compliance-to-Policy (C2P) provides the framework to bridge the gap between compliance and policy administration.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,6,,FAUX
oss-compass/compass-projects-information,Application System,Documentations,2025-05-10T03:05:22Z,2025-03-15T09:55:43Z,0,0,0,0,0,1,0,0,2022-09-29T09:46:29Z,2025-04-05T21:30:29Z,14563,18,,VRAI,27,FAUX,9,,9,Meta Data && Feedback for OSS Compass,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,29,"# OSS Compass 项目信息 

欢迎来到 OSS Compass 项目信息仓库，本仓库作为 OSS Compass 平台的元数据和反馈仓库。

当您在 OSS Compass 网站上提交一个项目时，它会自动在本仓库上创建一个 pull request，以便您提供有关项目的更多信息，
并从 OSS Compass 社区中获得反馈。

本仓库包含以下信息：

1. `all_repositories.csv` - 包含所有在 OSS Compass 网站上有报告的仓库信息的 CSV 文件。
2. `single-repositories` - 包含用户提交的针对单个仓库的分析请求的目录。
3. `communities` - 包含用户提交的针对社区的分析请求的目录。
4. `collections` - 包含在 OSS Compass 网站上展示的项目集合的目录。

## 贡献

有两种方式可以为 OSS Compass 项目信息仓库做贡献：

1. 在 OSS Compass 网站上访问 [提交您的项目](https://compass.gitee.com/submit-your-project)直接提交您的项目。
2. Fork 本仓库并遵循我们所要求的格式创建一个包含您的修改的 pull request。

请确保您的 pull request 遵循本仓库的格式规范，并包含清晰的修改描述。

## 鸣谢

OSS Compass 项目信息仓库由 OSS Compass 团队创建和维护。我们要感谢所有为改进平台做出贡献的贡献者，以及开源社区的支持。

我们也要特别感谢 ChatGPT 帮助改进了本 README.md 文件。

## 许可证

OSS Compass 项目信息仓库使用 GNU Affero General Public License v3.0 许可证。请参阅 [LICENSE](LICENSE) 文件以获取
更多信息。",VRAI
ossf/alpha-omega,Application System,Application System,2025-05-14T16:51:53Z,2025-04-24T15:54:12Z,0,949,0,0,0,0,0,0,2021-11-09T21:36:55Z,2025-04-04T04:32:52Z,11752,95,Open Policy Agent,VRAI,55,FAUX,59,"open-source-security,opensource,security",59,Our mission is to catalyze sustainable improvements to critical open source software projects and ecosystems.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,38,"# Alpha-Omega

Alpha-Omega's mission is to catalyze sustainable security improvements to critical open source projects and ecosystems. 
We accomplish this in various ways, such as funding security staff at organizations like the Rust Foundation and the
Eclipse Foundation, security improvements to projects like Homebrew, security audits of projects like OpenSSL, and
security features in projects like Rustls. We also sponsor work to identify serious vulnerabilities across a large
set of open source projects, such as our work through OpenRefactory. 

Since 2022, Alpha-Omega has been working hard, ""turning money into security"". Learn more at [alpha-omega.dev](https://alpha-omega.dev)
or read our latest [Annual Report](https://alpha-omega.dev/wp-content/uploads/sites/22/2024/02/Alpha-Omega-Annual-Report-2023.pdf).

## Alpha

We have active engagements with the following projects:

* [Eclipse Foundation](/alpha/engagements/2024/Eclipse%20Foundation)
* [FreeBSD Foundation](/alpha/engagements/2024/FreeBSD)
* [Homebrew](/alpha/engagements/2024/Homebrew)
* [Node.js](/alpha/engagements/2024/NodeJS)
* [OpenSSL](/alpha/engagements/2024/OpenSSL)
* [OpenRefactory](/alpha/engagements/2024/OpenRefactory)
* [Prossimo](/alpha/engagements/2024/Prossimo)
* [Python Software Foundation](/alpha/engagements/2024/Python%20Software%20Foundation)
* [Rust Foundation](/alpha/engagements/2024/Rust%20Foundation)

[Learn more about Alpha](alpha/README.md).

## Project Information

#### Meeting times

We usually meet on the first Wednesday of each month at 9:00am PT. You can find the meeting
invite link on the [OpenSSF Community Calendar](https://calendar.google.com/calendar?cid=czYzdm9lZmhwNWk5cGZsdGI1cTY3bmdwZXNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ).

#### Core Team

The Alpha-Omega core team members include:

* [Michael Scovetta](https://linkedin.com/in/scovetta) (Microsoft)
* [Bob Callaway](https://www.linkedin.com/in/bobcallaway) (Google)
* [Henri Yandell](https://www.linkedin.com/in/flamefew/) (AWS)
* [Miaolai Zhou](https://www.linkedin.com/in/miaolaizhou) (AWS)
* [Michael Winser](https://www.linkedin.com/in/michaelw/) (Technical Strategist)
* [Michelle Martineau](https://www.linkedin.com/in/michelle-martineau/) (Linux Foundation)

#### Get Involved

You can get involved by engaging with us in various ways:

* **Slack:** We watch the [#alpha_omega](https://openssf.slack.com/archives/C02LUUWQZNK) Slack channel.
* **Monthly Meeting:** Come and talk to us directly.
* **Mailing List:** Join the [alpha-omega-announcements](https://lists.openssf.org/g/alpha-omega-announcements) mailing list to be notified of upcoming developments.
* **Contact Us:** [Let us know](https://alpha-omega.dev/membership-inquiries/) you'd like to get involved.",FAUX
OT-CONTAINER-KIT/redis,Application System,Toolkit,2025-02-28T02:28:04Z,2023-12-18T01:46:36Z,1,0,0,0,0,0,0,0,2020-03-25T08:05:22Z,2025-04-03T05:59:20Z,100,26,Shell,VRAI,56,FAUX,10,"hacktoberfest,hacktoberfest-accepted",10,A production optimized redis docker image,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,20,"<p align=""left"">
  <img src=""./img/redis.png"">
</p>

# Ot-Container-Kit (Redis)

I am a repo which have a production based Redis and Redis Exporter docker image codebase.

## Features

This image provides you below features:-
- [X] **Lightweight nature:-** Images are quite low in terms of size which will improve your deployment process time.
- [X] **Security Compliant:-** Images are security compliant i.e. It doesn't hold any vulnerable package.
- [X] **Best Practices:-** We have tried to follow the best practices for writing the Docker images.

## Pre-requisites

Here are the list of pre-requisites which is required for development and setup purpose.

- **Docker Engine**
- **Docker Compose**

## Image Compatibility

The following table shows the compatibility between the Operator Version, Redis Image, Sentinel Image, and Exporter Image:

| Operator Version | Redis Image | Sentinel Image | Exporter Image |
|------------------|-------------|----------------|----------------|
| v0.15.2          | v7.0.13     | v7.0.13        | v1.48.0        |
| v0.15.1          | v7.0.12     | v7.0.12        | v1.48.0        |
| v0.15.0          | v7.0.11     | v7.0.11        | v1.48.0        |
| v0.14.0          | v7.0.7      | v7.0.7         | v1.48.0        |
| v0.13.0          | v6.2.5      | nil            | v1.48.0        |

That's it

> Note : latest tag would be comptabile with latest operator version.

## Building Image

#### Redis Docker Image

```shell
make build-redis
```

#### Redis Exporter Docker Image

```shell
make build-redis-exporter
```

## Running Setup

#### For standalone server

```shell
make setup-standalone-server-compose
```

#### For cluster setup

```shell
make setup-cluster-compose
```",VRAI
oxyno-zeta/s3-proxy,Toolkit,Toolkit,2025-05-13T20:15:10Z,2025-03-03T17:26:25Z,0,1,0,0,0,0,0,0,2019-09-22T14:17:39Z,2025-04-07T21:14:27Z,3151,338,Go,VRAI,38,FAUX,13,"basic-authentication,opa,openid-connect,reverse-proxy,s3,s3-bucket,s3-proxy,serve-static",13,"S3 Reverse Proxy with GET, PUT and DELETE methods and authentication (OpenID Connect and Basic Auth)",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,8,"<h1 align=""center""><img width=""350"" height=""350"" src=""https://raw.githubusercontent.com/oxyno-zeta/s3-proxy/master/docs/logo/logo.png"" /></h1>

<p align=""center"">
  <a href=""https://github.com/avelino/awesome-go"" rel=""noopener noreferer"" target=""_blank""><img src=""https://awesome.re/mentioned-badge.svg"" alt=""Mentioned in Awesome Go"" /></a>
  <a href=""http://godoc.org/github.com/oxyno-zeta/s3-proxy"" rel=""noopener noreferer"" target=""_blank""><img src=""https://img.shields.io/badge/godoc-reference-blue.svg"" alt=""Go Doc"" /></a>
  <a href=""https://github.com/oxyno-zeta/s3-proxy/actions/workflows/ci.yml"" rel=""noopener noreferer"" target=""_blank""><img src=""https://github.com/oxyno-zeta/s3-proxy/actions/workflows/ci.yml/badge.svg"" alt=""Github Actions"" /></a>
  <a href=""https://goreportcard.com/report/github.com/oxyno-zeta/s3-proxy"" rel=""noopener noreferer"" target=""_blank""><img src=""https://goreportcard.com/badge/github.com/oxyno-zeta/s3-proxy"" alt=""Go Report Card"" /></a>
</p>
<p align=""center"">
  <a href=""https://coveralls.io/github/oxyno-zeta/s3-proxy?branch=master"" rel=""noopener noreferer"" target=""_blank""><img src=""https://coveralls.io/repos/github/oxyno-zeta/s3-proxy/badge.svg?branch=master"" alt=""Coverage Status"" /></a>
  <a href=""https://hub.docker.com/r/oxynozeta/s3-proxy"" rel=""noopener noreferer"" target=""_blank""><img src=""https://img.shields.io/docker/pulls/oxynozeta/s3-proxy.svg"" alt=""Docker Pulls"" /></a>
  <a href=""https://github.com/oxyno-zeta/s3-proxy/blob/master/LICENSE"" rel=""noopener noreferer"" target=""_blank""><img src=""https://img.shields.io/github/license/oxyno-zeta/s3-proxy"" alt=""GitHub license"" /></a>
  <a href=""https://github.com/oxyno-zeta/s3-proxy/releases"" rel=""noopener noreferer"" target=""_blank""><img src=""https://img.shields.io/github/v/release/oxyno-zeta/s3-proxy"" alt=""GitHub release (latest by date)"" /></a>
</p>

---

## Features

- Multi S3 bucket proxy
- Index document (display index document instead of listing when found)
- Custom templates
- Custom S3 endpoints supported
- Basic Authentication support
- Multiple Basic Authentication support
- OpenID Connect Authentication support
- Multiple OpenID Connect Provider support
- Redirect to original host and path with OpenID Connect authentication
- Bucket mount point configuration with hostname and multiple path support
- Authentication by path and http method on each bucket
- Prometheus metrics
- Allow to publish files on S3 bucket
- Allow to delete files on S3 bucket

And many others.

## Documentation

There is an online documentation generated for this project.

You can find it here: [https://oxyno-zeta.github.io/s3-proxy/](https://oxyno-zeta.github.io/s3-proxy/)

## Advanced interfaces

Looking for more advanced interfaces. Take a look on this project: [s3-proxy-interfaces](https://github.com/oxyno-zeta/s3-proxy-interfaces).

## Want to contribute ?

- Read the [CONTRIBUTING guide](./CONTRIBUTING.md)

## Inspired by

- [pottava/aws-s3-proxy](https://github.com/pottava/aws-s3-proxy)

## Thanks

- My wife BH to support me doing this

## Author

- Oxyno-zeta (Havrileck Alexandre)

## License

Apache 2.0 (See in LICENSE)",FAUX
pachyderm/pachyderm,Documentations,DevOPs,2025-01-09T19:41:45Z,2024-10-25T00:53:58Z,0,0,0,0,0,0,0,9,2014-09-04T07:50:02Z,2025-04-07T08:11:12Z,656481,6218,Go,VRAI,568,FAUX,931,"analytics,big-data,containers,data-analysis,data-science,distributed-systems,docker,go,kubernetes,pachyderm",931,Data-Centric Pipelines and Data Versioning,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,195,"<p align=""center"">
	<img src='./Pachyderm_Icon-01.svg' height='225' title='Pachyderm'>
</p>

[![GitHub release](https://img.shields.io/github/release/pachyderm/pachyderm.svg?style=flat-square)](https://github.com/pachyderm/pachyderm/releases)
[![GitHub license](https://img.shields.io/badge/license-Pachyderm-blue)](https://github.com/pachyderm/pachyderm/blob/master/LICENSE)
[![GoDoc](https://godoc.org/github.com/pachyderm/pachyderm?status.svg)](https://pkg.go.dev/github.com/pachyderm/pachyderm/v2/src/client)
[![Go Report Card](https://goreportcard.com/badge/github.com/pachyderm/pachyderm)](https://goreportcard.com/report/github.com/pachyderm/pachyderm)
[![Slack Status](https://img.shields.io/badge/slack-pachyderm-brightgreen.svg?logo=slack)](https://www.pachyderm.com/slack)
[![CLA assistant](https://cla-assistant.io/readme/badge/pachyderm/pachyderm)](https://cla-assistant.io/pachyderm/pachyderm)

# Pachyderm – Automate data transformations with data versioning and lineage


Pachyderm is cost-effective at scale, enabling data engineering teams to automate complex pipelines with sophisticated data transformations across any type of data. Our unique approach provides parallelized processing of multi-stage, language-agnostic pipelines with data versioning and data lineage tracking. Pachyderm delivers the ultimate CI/CD engine for data. 

## Features

- Data-driven pipelines automatically trigger based on detecting data changes.
- Immutable data lineage with data versioning of any data type. 
- Autoscaling and parallel processing built on Kubernetes for resource orchestration.
- Uses standard object stores for data storage with automatic deduplication.  
- Runs across all major cloud providers and on-premises installations.


## Getting Started
To start deploying your end-to-end version-controlled data pipelines, run Pachyderm [locally](https://docs.pachyderm.com/latest/set-up/local-deploy/) or you can also [deploy on AWS/GCE/Azure](https://docs.pachyderm.com/latest/set-up/cloud-deploy) in about 5 minutes. 

You can also refer to our complete [documentation](https://docs.pachyderm.com) to see tutorials, check out example projects, and learn about advanced features of Pachyderm.

If you'd like to see some examples and learn about core use cases for Pachyderm:
- [Examples](https://github.com/pachyderm/examples)
- [Use Cases](https://www.pachyderm.com/use-cases/)
- [Case Studies](https://www.pachyderm.com/case-studies/)

## Documentation

[Official Documentation](https://docs.pachyderm.com/)

## Community
Keep up to date and get Pachyderm support via:
- [![Twitter](https://img.shields.io/twitter/follow/pachyderminc?style=social)](https://twitter.com/pachyderminc) Follow us on Twitter.
- [![Slack Status](https://badge.slack.pachyderm.io/badge.svg)](https://slack.pachyderm.io) Join our community [Slack Channel](https://www.pachyderm.com/slack) to get help from the Pachyderm team and other users.

## Contributing
To get started, sign the [Contributor License Agreement](https://cla-assistant.io/pachyderm/pachyderm).

You should also check out our [contributing guide](https://docs.pachyderm.com/latest/contributing/setup/).

Send us PRs, we would love to see what you do! You can also check our GH issues for things labeled ""help-wanted"" as a good place to start. We're sometimes bad about keeping that label up-to-date, so if you don't see any, just let us know.

## Usage Metrics

Pachyderm automatically reports anonymized usage metrics. These metrics help us
understand how people are using Pachyderm and make it better.  They can be
disabled by setting the env variable `METRICS` to `false` in the pachd
container.",VRAI
PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition,Documentations,Documentations,2025-03-05T01:15:47Z,2024-08-16T22:05:40Z,0,12,0,0,0,0,0,4,2023-06-07T05:13:40Z,2025-04-05T22:00:45Z,23303,86,Shell,VRAI,54,FAUX,1,,1,"Kubernetes – An Enterprise Guide, Third Edition - Published by Packt",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,7,"# Kubernetes-An-Enterprise-Guide-Third-Edition
Welcome to the repo for the book Kubernetes – An Enterprise Guide, Third Edition - Published by Packt.
Written by Marc Boorshtein and Scott Surovich.  

# Extra's Directory  
We have created a directoryb called Extras.  This contains content that we wanted to add for readers that is outside of the book content.  
It contains add-ons that we find useful for additional testing and are provided as-is without any support.  

Currently, it container one add-on, dnsmasq.  You can used dnsmasq to create your own domain for Ingress and ISTIO Virtual Services rather than using nip.io like the book exercises use.  
If you decide to use dndmasq for a domain, the scripts in the exercises will still create nip.io URL's, so you will need to change the domains after the scripts create them with your dnsmasq domain instead of nip.io  

dnsmasq was added for readers that may want to used the scripts on other Kubernetes clusters like a kubeadm cluster - or if you use your KinD cluster offline, where nip.io domains will not resolve due to not having an Internet connection.  
  
## Troubleshooting and Getting Help

If labs don't work, take a look at [TROUBLESHOOTING.md](TROUBLESHOOTING.md).  If you're still having issues, please open an issue int his repo and we'll be happy to help!",FAUX
PacktPublishing/Terraform-Cookbook-Second-Edition,Documentations,Documentations,2025-01-24T07:08:07Z,2023-08-22T09:46:47Z,3,2,0,0,0,0,0,0,2022-07-26T12:00:19Z,2025-04-02T21:05:05Z,672,96,HCL,VRAI,104,FAUX,2,"arm-templates,azure-infrastructure-automation,azure-resource-manager,ci-cd-book,hashicorp-terraform,iac,infrastructure-as-code,infrastructure-automation,ms-azure,terraform-aws,terraform-azure,terraform-in-action,terraform-up-and-running",2,"Code Repository for Terraform Cookbook Second Edition, Published by Packt",FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,7,"# Terraform Cookbook, Second Edition
This is the code repository for [Terraform Cookbook, Second Edition](https://www.packtpub.com/product/terraform-cookbook-second-edition/9781804616420), published by Packt.

**Provision, run, and scale cloud architecture with real-world examples using Terraform**

The author of this book is -[Mikael Krief](https://www.linkedin.com/in/mikael-krief/)

## About the book

HashiCorp Configuration Language (HCL) has changed how we define and provision data center infrastructure with the launch of Terraform, a top-tier product for building Infrastructure as Code (IaC). Terraform Cookbook shows you how to leverage Terraform to manage complex infrastructure with ease.

This new edition has been updated to include real-world examples for provisioning Azure, AWS and GCP infrastructure with Terraform. You'll delve into manual and automated testing with Terraform configurations, creating and managing a balanced, efficient, and reusable infrastructure with Terraform modules. You'll learn how to automate the deployment of Terraform configuration with continuous integration and continuous delivery (CI/CD).
Besides that, several new chapters have been added that describe the use of Terraform for Docker and Kubernetes, examine advanced topics on GitOps practices, and explain how to test Terraform configurations using different tools to check code and security compliance. The final chapter covers troubleshooting common Terraform issues and provides solutions for frequently encountered errors.

By the end of this book, you'll have developed the skills needed to get the most value out of Terraform and to effectively manage your infrastructure.


## Key Takeaways
- Use Terraform to build and run cloud and Kubernetes infrastructure using IaC best practices
- Adapt the Terraform command line adapted to appropriate use cases
- Automate the deployment of Terraform confi guration with CI/CD
- Discover manipulation of the Terraform state by adding or removing resources
- Explore Terraform for Docker and Kubernetes deployment, advanced topics on GitOps practices, and Cloud Development Kit (CDK)
- Add and apply test code and compliance security in Terraform configuration
- Debug and troubleshoot common Terraform errors

## New Edition v/s Previous Edition:  
-	Real-world examples for provisioning Azure, AWS, and GCP infrastructure with Terraform.
-	New chapters covering Terraform for Docker and Kubernetes, advanced GitOps practices, and testing Terraform configurations using different tools to check code and security compliance.
-	A complete chapter dedicated to mastering Terraform Cloud.
-	Troubleshooting common Terraform issues and solutions for frequently encountered errors.



## What's New 
-	Real-world examples for provisioning Azure, AWS, and GCP infrastructure with Terraform.
-	New chapters covering Terraform for Docker and Kubernetes, advanced GitOps practices, and testing Terraform configurations using different tools to check code and security compliance.
-	A complete chapter dedicated to mastering Terraform Cloud.
-	Troubleshooting common Terraform issues and solutions for frequently encountered errors.


## Outline and Chapter Summary

This new edition has been updated to include real-world examples for provisioning Azure, AWS and GCP infrastructure with Terraform. You'll delve into manual and automated testing with Terraform configurations, creating and managing a balanced, efficient, and reusable infrastructure with Terraform modules. You'll learn how to automate the deployment of Terraform configuration with continuous integration and continuous delivery (CI/CD).
Besides that, several new chapters have been added that describe the use of Terraform for Docker and Kubernetes, examine advanced topics on GitOps practices, and explain how to test Terraform configurations using different tools to check code and security compliance. The final chapter covers troubleshooting common Terraform issues and provides solutions for frequently encountered errors.

1. Chapter 1, [Setting Up the Terraform Environment](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP01)
2. Chapter 2, [Writing Terraform Configurations](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP02)
3. Chapter 3, [Scaling Your Infrastructure with Terraform](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP03)
4. Chapter 4, [Using Terraform with External Data](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP04)
5. Chapter 5, [Managing Terraform State](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP05)
6. Chapter 6, [Applying a Basic Terraform Workflow](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP06)
7. Chapter 7, [Sharing Terraform Configuration with Modules](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP07)
8. Chapter 8, [Provisioning Azure Infrastructure with Terraform](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP08)
9. Chapter 9, [Getting Starting to Provisioning AWS and GCP Infrastructure Using Terraform](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP09)
10. Chapter 10, [Using Terraform for Docker and Kubernetes Deployment](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP10)
11. Chapter 11, [Running Test and Compliance Security on Terraform Configuration](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP11)
12. Chapter 12, [Deep-Diving into Terraform](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP12)
13. Chapter 13, [Automating Terraform Execution in CI/CD Pipeline](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP13)
14. Chapter 14, [Using Terraform Cloud to Improve Team Collaboration](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP14)
15. Chapter 15, [Troubleshooting Terraform Errors](https://github.com/PacktPublishing/Terraform-Cookbook-Second-Edition/tree/main/CHAP15)

### Chapter 01, Setting Up the Terraform Environment
This chapter delves into the diverse methods of manually installing Terraform, encompassing manual installations, script-based installations, and utilization within Docker containers. The chapter commences by advocating comprehension of Infrastructure as Code (IaC) principles for effective configuration composition. Subsequently, it elucidates the creation of a localized developmental environment enabling the crafting of Terraform configuration files and their subsequent application.
The chapter progresses to offer pivotal insights into both IaC and Terraform best practices. It commences with elucidating the acquisition and manual installation of Terraform on Windows systems. It extends the discourse to encompass scripted installations for both Windows and Linux systems. Furthermore, the chapter elaborates on the utilization of Terraform within Docker containers, providing practical guidance for its integration. The chapter culminates with a focus on enhancing the Terraform ecosystem through the upgrading of Terraform providers, thereby enveloping a comprehensive array of practices pertinent to Terraform's establishment and utilization.

**Key Insights**:
- **IaC Best Practices First**: Before embarking on Terraform configuration, it's essential to grasp Infrastructure as Code (IaC) best practices. These principles guide the creation of well-structured, readable, and maintainable configuration code.
- **Diverse Installation Approaches**: The chapter outlines multiple approaches for installing Terraform to accommodate various needs. These methods include manual installation on Windows and Linux, script-based installations, and utilizing Docker containers. This flexibility ensures compatibility with different operating systems and preferences.
- **Customized Development Environment**: Setting up a local development environment is crucial. It allows developers to write and test Terraform configuration files before deploying changes. This practice enhances code quality and minimizes errors.
- **IaC and Terraform Best Practices**: The chapter covers both Infrastructure as Code and Terraform best practices. This dual focus equips readers with essential knowledge for efficient configuration management and infrastructure provisioning.
- **Windows and Linux Installation**: Detailed steps are provided for downloading and installing Terraform manually on both Windows and Linux systems. Additionally, script-based installations, like using Chocolatey on Windows and APT on Linux, are explained to streamline the setup process.
- **Docker Integration**: The chapter introduces Docker as a platform for running Terraform. Readers learn how to utilize Terraform within Docker containers, enhancing portability and isolation of infrastructure deployments.
- **Provider Upgrades**: Highlighting the dynamic nature of Terraform, the chapter emphasizes the significance of upgrading Terraform providers. This practice ensures access to new features and improvements, maintaining the efficacy of infrastructure management.
- **Version Control**: The importance of maintaining version control for Terraform configurations is underscored. This practice enables collaboration, rollback options, and tracking changes over time.
- **Integrated Development Environment (IDE)**: Readers learn to write Terraform configurations using Visual Studio Code, a popular integrated development environment. This enhances the development experience by providing code editing features and automation.


### Chapter 02, Writing Terraform Configurations
The chapter delves into the intricate process of crafting effective Terraform configurations by addressing a range of essential components and techniques. The Terraform language's versatility becomes evident from the outset, enabling robust manipulation for various scenarios. Through practical recipes, readers are guided in utilizing Terraform's language to its fullest potential in real-world business applications.
The recipes systematically cover diverse aspects. Initial attention is given to configuring Terraform and specifying provider versions. Techniques for enhancing flexibility are explored, such as employing aliases to utilize multiple instances of a single provider and leveraging variables and outputs for dynamic code. The integration of built-in functions and conditional expressions into configurations is examined, empowering users to craft more adaptive and intelligent setups. Furthermore, the chapter delves into resource interdependencies, imparting strategies to manage these relationships effectively. An in-depth exploration of custom pre- and postconditions offers insight into rigorous checks that can be applied to ensure robust provisioning, and readers are introduced to validation techniques for the provisioned infrastructure.

**Key Insights**:
-	**Language Richness and Manipulation Potential**: Terraform's language offers a wealth of capabilities for configuration manipulation. It becomes evident that the language's richness empowers users to efficiently design and customize their configurations to meet specific business needs.
-	**Provider Management**: The chapter emphasizes the importance of managing providers effectively. It guides readers on specifying provider versions and introduces the concept of using aliases to handle multiple instances of the same provider. This insight enhances configuration flexibility and scalability.
-	**Dynamic Configurations**: Variables and outputs are introduced as vital components for crafting dynamic configurations. By leveraging variables, users can create adaptable and reusable code structures. Outputs, on the other hand, expose provisioned data for downstream consumption, enhancing configuration interoperability.
-	**Functionality and Conditionals**: Built-in functions and conditional expressions are explored as powerful tools for enhancing configuration adaptability. Readers gain the ability to introduce custom logic and decision-making into their Terraform setups, increasing their sophistication and responsiveness.
-	**Dependency Management and Validation**: The chapter delves into managing resource dependencies, a critical aspect of configuration design. By understanding and controlling these relationships, users can ensure their infrastructure provisions are coherent and efficient. Furthermore, the concept of custom pre- and postconditions is introduced as a means to rigorously validate the infrastructure, elevating the overall reliability of the configurations.


### Chapter 03, Scaling Your Infrastructure with Terraform
This chapter delves into the remarkable advantages of Infrastructure as Code (IaC), notably its swift provisioning of extensive infrastructure in contrast to manual methods. Emphasizing the synergy between IaC and established software development principles, the chapter underscores the application of principles like Don't Repeat Yourself (DRY). This principle advocates against code duplication, urging developers to streamline their codebase. A thorough exploration of the DRY principle is provided through an external link, shedding light on its essence and practicality. The chapter transitions to the heart of its content by unveiling the efficacy of leveraging expressions from the Terraform language. Through techniques like count, maps, collections, arrays, and dynamic blocks, the chapter elucidates how these tools empower the creation of concise Terraform configurations. This, in turn, facilitates the deployment of diverse infrastructure across multiple environments, eradicating the need for redundant code and fostering efficiency. The chapter culminates by introducing a set of recipes that include provisioning infrastructure across varied environments, utilizing count for multiple resource provisioning, harnessing maps, iteratively navigating map objects, generating dynamic blocks, and implementing map filters, collectively providing a robust framework for scalable infrastructure deployment with Terraform.


**Key Insights**:
- **Accelerated Infrastructure Deployment**: Infrastructure as Code (IaC) offers a significant advantage by enabling rapid and large-scale provisioning of infrastructure compared to manual processes. This efficiency stems from the automation and consistency IaC brings to the provisioning process.
-  **Software Development Principles in IaC**: Applying software development principles to IaC, such as the ""Don't Repeat Yourself"" (DRY) principle, leads to cleaner, more maintainable code. DRY emphasizes avoiding code duplication, which in turn reduces errors and streamlines updates across the infrastructure.
-  **Terraform Expressions**: The chapter introduces various expressions from the Terraform language, including count, maps, collections, arrays, and dynamic blocks. These expressions enhance the versatility of Terraform configurations, allowing for dynamic provisioning and eliminating the need for redundant code.
-  **Efficient Multi-Environment Deployment**: The use of Terraform expressions empowers users to deploy infrastructure seamlessly across multiple environments. This capability simplifies the management of diverse infrastructures while maintaining a single, concise codebase.
-  **Practical Recipes for Scalability**: The chapter provides a set of practical recipes that exemplify the concepts discussed. From provisioning infrastructure in different environments to generating dynamic blocks and filtering maps, these recipes offer actionable insights into optimizing infrastructure deployment using Terraform.

### Chapter 04, Using Terraform with External Data
This chapter delves into leveraging Terraform configurations to address scenarios where information about pre-existing external resources or infrastructures is necessary. The focal point involves utilizing data sources to extract data from external systems and probing these resources via queries. This enhances Terraform's capabilities beyond its conventional provisioning scope. Additionally, the chapter elucidates how Terraform can engage with local operations, including managing local files and orchestrating the execution of locally installed programs. A comprehensive exploration of the Terraform Shell provider is also undertaken, exemplifying the execution of shell scripts through Terraform. The chapter encompasses a diverse array of topics, encapsulating data retrieval from external systems, local file manipulation, and versatile script execution.
The chapter unfolds through an assortment of practical recipes, each serving to broaden the reader's toolkit. Starting with the acquisition of external data through data sources, the text advances into the realm of querying external data using Terraform's capabilities. Subsequently, it navigates the terrain of local operations, elucidating how Terraform can adeptly oversee local file manipulations and the execution of locally available programs. The culmination of the chapter is the introduction of the Terraform Shell provider, a mechanism that facilitates the execution of shell scripts orchestrated by Terraform. By encompassing a spectrum of techniques, from sourcing external data to orchestrating local and shell operations, this chapter equips readers with a well-rounded proficiency in expanding Terraform's purview beyond conventional provisioning.

**Key Insights**:
- **Data Source Utilization**: The chapter underscores the significance of data sources in Terraform, showcasing their utility in retrieving information about external resources that were not provisioned through Terraform itself. This empowers users to integrate pre-existing assets seamlessly into their Terraform configurations.
- **Enhanced Terraform Scope**: The chapter highlights how Terraform's capabilities extend beyond provisioning, demonstrating its ability to query external resources. This versatility allows users to gain insights and interact with external systems, broadening the tool's applicability.
- **Local Operations**: A central theme of the chapter revolves around Terraform's aptitude for local operations. It elucidates how Terraform can effectively manipulate local files, operate locally installed programs, and thereby integrate with the user's environment.
- **Script Execution with Terraform Shell Provider**: The Terraform Shell provider emerges as a powerful tool in the chapter, enabling the execution of shell scripts through Terraform. This mechanism enhances automation by allowing users to seamlessly incorporate custom scripts into their Terraform workflows.
- **Recipe-Based Learning**: The chapter's organization into practical recipes facilitates a structured learning approach. Each recipe tackles a specific aspect, ranging from obtaining external data to executing shell scripts. This approach enables readers to grasp concepts incrementally and apply them in a modular manner.

### Chapter 05, Managing Terraform State
This chapter delves into the significance of the Terraform state file, a crucial artifact within the Terraform ecosystem. Serving as a repository for all executed configurations, this file underpins the entire Terraform workflow. While direct interaction with the state file is typically unnecessary, instances may arise where manipulation becomes imperative—whether for resource exploration, removal, import, or relocation. While one might be tempted to manually edit this JSON file, such an approach is ill-advised due to the potential introduction of errors that could disrupt future Terraform operations. To mitigate these risks, HashiCorp offers dedicated tools like the Terraform CLI and configuration blocks tailored to the Terraform system.
The chapter comprehensively guides readers through secure methods of manipulating Terraform state. The Terraform CLI takes center stage in this process, enabling a spectrum of tasks including resource listing, deletion, synchronization, importing, and movement. Additionally, the text explores the integration of external resources from disparate state files, highlighting the versatility of the Terraform ecosystem. The chapter culminates in the presentation of practical scenarios, or ""recipes,"" elucidating diverse aspects of Terraform state manipulation. These recipes span from leveraging local Terraform state instances to refining configuration through the Terraform move block, thereby equipping readers with a nuanced understanding of effective state management.

**Key Insights**:
- **Terraform State's Significance**: The Terraform state file is a pivotal artifact in the Terraform workflow, encapsulating all executed configurations. It's critical for maintaining the infrastructure's desired state.
- **Avoid Manual State Manipulation**: Although Terraform state may require manipulation for various reasons, manually editing the state file is discouraged due to the potential introduction of errors that might disrupt future Terraform operations.
- **HashiCorp's Tooling**: HashiCorp provides dedicated tools like the Terraform CLI and specific configuration blocks to safely manipulate the Terraform state, ensuring that operations are conducted accurately and without compromising future workflows.
- **Range of State Manipulation Tasks**: The Terraform CLI offers a wide array of tasks for handling Terraform state, including resource listing, deletion, synchronization, importing existing resources, and relocating resources.
- **Versatility of State Management**: The chapter emphasizes the adaptability of the Terraform ecosystem, showing how external resources can be integrated from other state files. This highlights the flexibility and extensibility of Terraform's state management capabilities.

 
### Chapter 06, Applying a Basic Terraform Workflow
The chapter delves into the practical utilization of the Terraform CLI to navigate the fundamental workflow of Terraform. It begins by emphasizing the significance of presenting a well-structured configuration and elucidates methods to validate its syntax. The chapter proceeds with a comprehensive exploration of pivotal Terraform commands and options. This includes elucidating the process of efficiently destroying infrastructure resources and revealing the providers employed within the configuration. Additionally, the utilization of workspaces as a means to manage diverse environments is expounded upon.
The chapter further elucidates diverse techniques for enhanced interaction with Terraform, such as generating a single cross-compatible lock file for both Windows and Linux. It expounds upon strategies for copying Terraform modules and introduces the concept of ""tainting"" resources. The practical aspects of producing a dependency graph are laid out, along with insights into managing distinct Terraform configuration directories. Importantly, the chapter provides in-depth guidance on testing and evaluating Terraform expressions, facilitating users in optimizing their configurations. It culminates in a comprehensive guide to debugging Terraform execution, equipping readers with tools to address and resolve potential issues effectively.

**Key Insights**:
- **Terraform Workflow and CLI Usage**: The chapter highlights the core Terraform workflow and introduces users to the Terraform Command-Line Interface (CLI). It emphasizes the importance of maintaining a well-structured configuration and explains how to leverage the CLI to validate configuration syntax.
- **Resource Management and Destruction**: Readers gain insights into effectively managing infrastructure resources by learning how to destroy them using the Terraform CLI. This essential process helps users clean up resources that are no longer needed, maintaining a lean and efficient infrastructure.
- **Provider Identification and Workspaces**: The chapter elucidates techniques to identify and display the providers utilized within a Terraform configuration. It also introduces the concept of workspaces, enabling users to manage multiple environments within a single configuration, enhancing flexibility and organization.
- **Advanced Techniques and Enhancements**: The chapter extends the understanding of Terraform usage by addressing advanced topics. It covers cross-compatibility by generating a single lock file suitable for both Windows and Linux. It also provides insights into copying Terraform modules, tainting resources for debugging, and generating dependency graphs for visualizing infrastructure relationships.
- **Expression Testing and Debugging**: A significant portion of the chapter is dedicated to optimizing Terraform configurations. It provides guidance on evaluating Terraform expressions, facilitating efficient testing. Furthermore, it equips readers with essential debugging techniques to diagnose and address potential issues that may arise during Terraform execution.


### Chapter 07, Sharing Terraform Configuration with Modules
The chapter addresses the recurring challenge developers confront regarding effective and seamless code reuse in various programming languages. To tackle this, the software industry has witnessed the rise of reusable language constructs, frameworks, and software packages, like NuGet, NPM, Bower, PyPI, RubyGems, and more, that enable code sharing across applications and teams. The application of this principle extends to Infrastructure as Code (IaC), where similar concerns about code structure, standardization, and sharing within organizations arise.

**Key Insights**:
- Developers commonly encounter code reuse challenges, leading to the emergence of reusable language constructs, frameworks, and software packages.
- Infrastructure as Code (IaC) faces similar issues of code structure and sharing, prompting the adoption of reusable practices.
- Terraform modules support various stages in their lifecycle, including creation, basic usage, loop integration, and publication.
- The chapter covers diverse scenarios like local module utilization, public registry integration, Git repository sharing, and advanced techniques such as using the Terrafile pattern and testing with Terratest.


### Chapter 08, Provisioning Azure Infrastructure with Terraform
This chapter delves into the practical utilization of Terraform's versatile array of providers, specifically focusing on provisioning cloud infrastructure within the Azure environment. The preceding chapters of this book laid the foundation by exploring fundamental Terraform concepts and command-line intricacies, along with module-based configuration sharing. Though the configuration examples revolved around the ""azurerm"" provider, their applicability extends universally to all Terraform providers. 
Here, the chapter commences by elucidating the integration of Terraform with Azure Cloud Shell and addressing security concerns through robust authentication mechanisms and safeguarding the Terraform state file in an Azure storage account. The narrative progresses to intricate scenarios wherein Azure Resource Manager (ARM) templates and Azure CLI scripts are executed seamlessly through Terraform. Moreover, the chapter navigates through acquiring Azure resource listings with Terraform and adeptly shielding sensitive data within Azure Key Vault. Two pragmatic case studies further exemplify Terraform's prowess in Azure: one outlining the provisioning and configuration of an Infrastructure as a Service (IaaS) architecture comprising Virtual Machines (VMs), and another elucidating the deployment of a Platform as a Service (PaaS) infrastructure. The chapter culminates with advanced topics, encompassing the estimation of Azure resource expenses based on Terraform configurations, and the innovative application of the AzAPI provider for generating Terraform configurations from pre-existing infrastructures.

**Key Insights**:
- **Azure Integration**: Learn how Terraform seamlessly integrates with Azure Cloud Shell, enhancing your ability to manage cloud resources within the Azure environment.
- **Resource Management**: Explore the execution of Azure Resource Manager (ARM) templates and Azure CLI commands through Terraform, allowing for efficient orchestration of diverse cloud resources.
- **Security and Compliance**: Understand the importance of secure authentication and the protection of sensitive information, as well as how to utilize Azure Key Vault with Terraform to ensure secure secret management.
- **Practical Scenarios**: Gain practical insights through case studies, covering the provisioning and configuration of both IaaS and PaaS infrastructure, demonstrating Terraform's effectiveness in different Azure deployment scenarios.
- **Advanced Techniques**: Delve into advanced techniques such as cost estimation using Infracost and innovative use of the AzAPI Terraform provider, enabling optimization of costs and enhanced configuration management.


### Chapter 09, Getting Starting to Provisioning AWS and GCP Infrastructure Using Terraform
In this chapter, the spotlight turns towards familiarizing readers with fundamental techniques for utilizing Terraform to provision infrastructure within the realms of both AWS and GCP. For each respective cloud provider, the chapter delves into critical aspects such as the establishment of Terraform credentials, configuring providers, and safeguarding the state file through remote backends like AWS S3 and GCP Storage. Particularly noteworthy is the comprehensive exploration of Google Cloud Shell, an integrated command-line interface, enabling users to execute Terraform operations devoid of local machine setup. The chapter concludes by presenting an array of instructive recipes that span across various topics, including initial steps with Terraform on AWS, integration of S3 backend functionality within AWS, initiation of Terraform usage on GCP, implementation of Google Cloud Storage (GCS) backend within the GCP context, and adept execution of Terraform via Google Cloud Shell.

**Key Insights**:
- **Azure Integration**: Learn how Terraform seamlessly integrates with Azure Cloud Shell, enhancing your ability to manage cloud resources within the Azure environment.
- **Resource Management**: Explore the execution of Azure Resource Manager (ARM) templates and Azure CLI commands through Terraform, allowing for efficient orchestration of diverse cloud resources.
- **Security and Compliance**: Understand the importance of secure authentication and the protection of sensitive information, as well as how to utilize Azure Key Vault with Terraform to ensure secure secret management.
- **Practical Scenarios**: Gain practical insights through case studies, covering the provisioning and configuration of both IaaS and PaaS infrastructure, demonstrating Terraform's effectiveness in different Azure deployment scenarios.
- **Advanced Techniques**: Delve into advanced techniques such as cost estimation using Infracost and innovative use of the AzAPI Terraform provider, enabling optimization of costs and enhanced configuration management.

### Chapter 10, Using Terraform for Docker and Kubernetes Deployment
This chapter delves into the diverse capabilities of Terraform beyond its conventional usage for Infrastructure as Code on major cloud platforms like Azure, AWS, and GCP. While Terraform is commonly associated with cloud resource provisioning, it encompasses a wide array of providers that extend its automation potential to various domains, including non-cloud entities like files, Docker, and Kubernetes. The preceding chapter established a foundation for Terraform application in cloud contexts; however, this chapter transitions to an alternative domain: employing Terraform for orchestrating Docker and Kubernetes. This transition is driven by Terraform's straightforward approach, positioning it as a pivotal tool for unifying the automation of resource and component deployment, all under the umbrella of a consistent Infrastructure as Code language.
Throughout this chapter, the focus shifts towards harnessing Terraform's capabilities to effectively manipulate Docker images and facilitate application deployment within Kubernetes environments. Key concepts encompass the utilization of the kubernetes and helm providers to facilitate Kubernetes resource management and Helm chart deployment, respectively. The chapter also introduces the integration of GitOps methodologies with Terraform, which is achieved through the employment of the Kubernetes Terraform control",FAUX
Parsifal-M/backstage-opa-plugins,Toolkit,Toolkit,2025-05-09T11:39:34Z,2025-03-21T07:55:36Z,0,5,0,0,0,0,0,0,2023-02-25T15:51:40Z,2025-04-04T22:32:28Z,11851,53,TypeScript,VRAI,10,FAUX,4,"backstage,backstage-opa-backend,backstage-plugin,opa,open-source,permissions,plugins,rbac",4,Open Policy Agent (OPA) Plugins for Backstage,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,7,"# Welcome to the OPA Plugins Repository for Backstage

[![codecov](https://codecov.io/gh/Parsifal-M/backstage-opa-plugins/graph/badge.svg?token=IHZGVSXZY7)](https://codecov.io/gh/Parsifal-M/backstage-opa-plugins)

This repository contains a collection of plugins for [Backstage](https://backstage.io) that integrate with [Open Policy Agent](https://www.openpolicyagent.org/).

## Blogs

- [Going Backstage with OPA](https://www.styra.com/blog/going-backstage-with-opa/)

## Talks

- [Can It Be Done? Building Fine-Grained Access Control for Backstage with OPA](https://www.youtube.com/watch?v=N0n_czYo_kE&list=PLj6h78yzYM2P4KPyeDFexAVm6ZvfAWMU8&index=15&ab_channel=CNCF%5BCloudNativeComputingFoundation%5D)

## Plugins

- [backstage-opa-backend](./plugins/backstage-opa-backend/README.md) - A Backend Plugin that the [backstage-opa-entity-checker](./plugins/backstage-opa-entity-checker/README.md) consumes to evaluate policies.
- [plugin-permission-backend-module-opa-wrapper](./plugins/permission-backend-module-opa-wrapper/README.md) - An isolated OPA Client and a Policy Evaluator that integrates with the Backstage permissions framework and uses OPA to evaluate policies, making it possible to use OPA for permissions (like RBAC). Does not require the `backstage-opa-backend` plugin!
- [backstage-opa-entity-checker](./plugins/backstage-opa-entity-checker/README.md) - A frontend plugin that provides a component card that displays if an entity has the expected entity metadata according to an opa policy.
- [backstage-opa-policies](./plugins/backstage-opa-policies/README.md) - A frontend component designed to be added to entity pages to fetch and display the OPA policy that entity uses based on a URL provided in an annotation in the `catalog-info.yaml` file.

## Beta Plugins

### Authz

- [backstage-opa-authz-react](./plugins/opa-authz-react/README.md) - A frontend plugin that allows you to control the visibility of components based on the result of an OPA policy evaluation.

### Entity Checker Processor

- [catalog-backend-module-opa-entity-checker-processor](./plugins/catalog-backend-module-opa-entity-checker-processor) - A Backstage catalog processor that validates entities at ingestion time using the `backstage-opa-backend` plugin and adds an annotation based on the OPA policy evaluation result which can be `error`, `warning` or `info`

## Additional Documentation

Each Plugin has its own documentation in the [Plugins](./plugins/) Folder, I am however, slowly moving things to [Github pages](https://parsifal-m.github.io/backstage-opa-plugins/#/). Feel free to help out!

## Local Development

Step by step guide to developing locally:

1. Clone this repository
2. Create an `app-config.local.yaml` file in the root of the repository copying the contents from `app-config.yaml`
3. Create a PAT (Personal Access Token) for your GitHub account with these scopes: `read:org`, `read:user`, `user:email`. This token should be placed under `integrations.github.token` in the `app-config.local.yaml` file.
4. Run `yarn install --immutable` in the root of the repository
5. Use `docker-compose up -d` to start the OPA server and postgres database (this will also load the two policies in the `example-opa-policies` folder automatically)
6. Update the OPA rbac policy in here [rbac_policy.rego](./example-opa-policies/rbac_policy.rego), or use your own! If you want to use the default policy, you'll have to update `is_admin if ""group:twocodersbrewing/maintainers"" in claims` to what ever your user entity claims are.
7. Run `yarn dev` or `yarn debug` in the root of the repository to start the Backstage app (use debug if you want to see what is happening in the OPA plugin)

## Ecosystem

- [PlaTT Policy Template](https://github.com/ap-communications/platt-policy-template) contains policy templates that will work with the [plugin-permission-backend-module-opa-wrapper](./plugins/permission-backend-module-opa-wrapper/README.md) plugin!

## Contributing

Contributions are welcome! However, still figuring out the best approach as this does require user and group entities to be in the system.

Please open an issue or a pull request. You can also contact me on mastodon at [@parcifal](https://hachyderm.io/@parcifal).

Please remember to sign your commits with `git commit -s` so that your commits are signed!",FAUX
patternfly-labs/react-form-wizard,Toolkit,Toolkit,2025-05-13T01:10:13Z,2025-03-04T01:04:00Z,0,0,0,0,3,0,0,0,2021-11-04T18:39:26Z,2025-04-08T01:06:05Z,8669,8,TypeScript,VRAI,12,FAUX,24,"patternfly,react,wizard",24,A Patternfly react wizard framework.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,11,"# PatternFly Labs React Form Wizard [![GitHub package.json version](https://img.shields.io/github/package-json/v/patternfly-labs/react-form-wizard)](https://www.npmjs.com/package/@patternfly-labs/react-form-wizard)

An opinionated framework for wizards using [PatternFly](https://www.patternfly.org/).

[Demo](https://patternfly-labs.github.io/react-form-wizard/)

## Installation

### Install dependencies

#### Using npm

```sh
npm install @patternfly-labs/react-form-wizard @patternfly/react-core @patternfly/react-styles
```

#### Using yarn

```
yarn add @patternfly-labs/react-form-wizard @patternfly/react-core @patternfly/react-styles
```

### Setup Patternfly CSS

Import css from patternfly before importing react-form-wizard.

```typescript
import '@patternfly/react-core/dist/styles/base.css'
import '@patternfly/react-styles/css/components/Wizard/wizard.css'
```

## Concepts

### Wizard structure

A wizard contains steps which contain sections which contain inputs.

```tsx
import { WizardPage, Step, Section, TextInput, Select } from '@patternfly-labs/react-form-wizard'

function Example() {
   return (
      <WizardPage title=""My Wizard"">
         <Step label=""Details"" id=""details-step"">
            <Section label=""Details"">
               <TextInput label=""Name"" path=""name"" required />
               <Select label=""Namespace"" path=""namespace"" options={['default', 'namespace-1']} />
            </Section>
         </Step>
      </WizardPage>
   )
}
```

### Item Context

The wizard works by setting an item context which inputs use as a data source.
Inputs then get value or set value in the item context using [path](https://github.com/jonschlinkert/set-value#object-paths) notation.

```tsx
function Example() {
   return (
      <TextInput label=""Name"" path=""metadata.name"" required />
   )
}
```

Some inputs can change the item context, such as the `ArrayInput`.

```tsx
function Example() {
   return (
      <ArrayInput path=""resources"" placeholder=""Add new resource"">
         <TextInput label=""Name"" path=""metadata.name"" required />
         <Select label=""Namespace"" path=""metadata.namespace"" options={['default']} required/>
      </ArrayInput>
   )
}
```

### Working with an array of items

The root data can either be an object or an array of objects.
When working with an array of objects an`ItemSelector` can be used to set the item context specific item.

```tsx
function Example() {
   return (
      <ItemSelector selectKey=""kind"" selectValue=""Application"">
         <TextInput label=""Name"" path=""metadata.name"" required />
         <Select label=""Namespace"" path=""metadata.namespace"" options={['default']} required/>
      </ItemSelector>
   )
}
```

`ArrayInput` can also be used to work with a subset of items in this case.

```tsx
function Example() {
   return (
      <ArrayInput path={null} filter={(item) => item.kind === 'Subscription'}>
         <TextInput label=""Name"" path=""metadata.name"" required />
         <Select label=""Namespace"" path=""metadata.namespace"" options={['default']} required/>
      </ArrayInput>
   )
}
```

### Input common properties

- **label** - The label for the input.
- **path** - The [path](https://github.com/jonschlinkert/set-value#object-paths) the input is getting and setting value to, in the current item context.
- **id** - Optional id of the input control. Used for testing. If not set, defaults to a sanitized version of the path.
- **validation** - Optional validation function that takes in the current item context and input value. It should return an error string if there is an error.
- **hidden** - Optional hidden function that takes in the current item context and returns true if the input should be hidden.

### Validation

Inputs take an optional validation function. The validation function takes in the current item context and input value. It should returns a validation error string if the validation fails.

### Conditional hiding

Inputs take an optional hidden function. The hidden function takes in the current item context, and returns true if the input should be hidden.

`Steps` and `Sections` automatically hide if all its inputs are hidden. This makes it easy to make a wizard with conditional flow.

### Examples

See the [wizards](https://github.com/patternfly-labs/react-form-wizard/tree/main/wizards) directory for example wizards.

## Development

> If you plan on contributing, please fork the repo and create a pull request using your fork.

1. Clone the repo

   ```
   git clone git@github.com:patternfly-labs/react-form-wizard.git
   ```

2. Install dependencies

   ```
   npm ci
   ```

3. Start the project

   ```
   npm start
   ```",FAUX
pbom-dev/OSCAR,Application System,Application System,2025-02-11T13:49:57Z,2023-07-18T09:28:41Z,0,0,0,0,0,1,0,0,2023-02-11T14:37:50Z,2025-04-04T23:44:14Z,519,92,Python,VRAI,25,FAUX,20,,20,"A comprehensive, systematic and actionable way to understand attacker behaviors and techniques with respect to the software supply chain",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,9,"# OSC&R

## What is OSC&R?

OSC&R stands for Open Software Supply Chain Attack Reference. It is a comprehensive, systematic, and actionable way to understand attacker behaviors and techniques with respect to the software supply chain.",FAUX
percona/percona-postgresql-operator,DevOPs,DevOPs,2025-05-14T09:13:47Z,2025-04-08T09:02:42Z,0,0,0,0,2,0,0,0,2021-03-04T13:34:50Z,2025-04-07T08:22:10Z,74863,315,Go,VRAI,61,FAUX,30,"kubernetes-operator,percona,postgresql",30,Percona Operator for PostgreSQL,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,92,"# Percona Operator for PostgreSQL

![Percona Kubernetes Operators](kubernetes.svg)

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
![Docker Pulls](https://img.shields.io/docker/pulls/percona/percona-postgresql-operator)
![Docker Image Size (tag)](https://img.shields.io/docker/image-size/percona/percona-postgresql-operator/2)
![GitHub tag (latest by SemVer)](https://img.shields.io/github/v/tag/percona/percona-postgresql-operator?include_prereleases&sort=semver)
![GitHub go.mod Go version](https://img.shields.io/github/go-mod/go-version/percona/percona-postgresql-operator)
[![Go Report Card](https://goreportcard.com/badge/github.com/percona/percona-postgresql-operator)](https://goreportcard.com/report/github.com/percona/percona-postgresql-operator)

## Introduction

[Percona Operator for PostgreSQL](https://docs.percona.com/percona-operator-for-postgresql/2.0/index.html) automates and simplifies deploying and managing open source PostgreSQL clusters on Kubernetes. It is based on [Postgres Operator](https://github.com/CrunchyData/postgres-operator) developed by Crunchy Data.

Whether you need to get a simple PostgreSQL cluster up and running, need to deploy a high availability, fault tolerant cluster in production, or are running your own database-as-a-service, the Operator provides the essential features you need to keep your clusters healthy:

- PostgreSQL cluster provisioning
- High availability and disaster recovery
- Automated user management with password rotation
- Automated updates
- Support for both asynchronous and synchronous replication
- Scheduled and manual backups
- Integrated monitoring with [Percona Monitoring and Management](https://www.percona.com/software/database-tools/percona-monitoring-and-management)

While the Percona Operator is primarily managed through the command line, you can also use **[Percona Everest](https://docs.percona.com/everest/index.html)** for a web-based user interface. This open-source tool provides a streamlined experience for provisioning and managing your databases, simplifying day-to-day tasks and reducing administrative overhead. Learn more about Percona Everest in the [documentation](https://docs.percona.com/everest/index.html) or jump right in with the [quickstart guide](https://docs.percona.com/everest/quickstart-guide/quick-install.html).

## Architecture

Percona Operators are based on the [Operator SDK](https://github.com/operator-framework/operator-sdk) and leverage Kubernetes primitives to follow best CNCF practices.

Learn more about [architecture and design decisions](https://docs.percona.com/percona-operator-for-postgresql/2.0/architecture.html).

# Documentation

To learn more about the Operator, check the [Percona Operator for PostgreSQL documentation](https://docs.percona.com/percona-operator-for-postgresql/2.0/index.html). 

## Quickstart installation

Ready to try out the Operator? Check the [Quickstart tutorial](https://docs.percona.com/percona-operator-for-postgresql/2.0/quickstart.html) for easy-to follow steps. 

Below is one of the ways to deploy the Operator using `kubectl`.

### kubectl

1. Deploy the operator from `deploy/bundle.yaml`

```sh
kubectl apply --server-side -f https://raw.githubusercontent.com/percona/percona-postgresql-operator/main/deploy/bundle.yaml
```

2. Deploy the database cluster itself from `deploy/cr.yaml`

```sh
kubectl apply -f https://raw.githubusercontent.com/percona/percona-postgresql-operator/main/deploy/cr.yaml
```

# Need help?

**Commercial Support**  | **Community Support** |
:-: | :-: |
| <br/>Enterprise-grade assistance for your mission-critical PostgreSQL deployments with the Percona Operator for PostgreSQL. Get expert guidance for complex tasks like multi-cloud replication, database migration and building platforms.<br/><br/>  | <br/>Connect with our engineers and fellow users for general questions, troubleshooting, and sharing feedback and ideas.<br/><br/>  | 
| **[Get Percona Support](https://hubs.ly/Q02ZTH9s0)** | **[Visit our Forum](https://forums.percona.com/c/postgresql/percona-kubernetes-operator-for-postgresql/68)** |

# Contributing

Percona welcomes and encourages community contributions to help improve Percona Operator for PostgreSQL.

See the [Contribution Guide](CONTRIBUTING.md) on how you can contribute.

## Roadmap

We have a public roadmap which can be found [here](https://github.com/orgs/percona/projects/10). Please feel free to contribute and propose new features by following the roadmap [guidelines](https://github.com/percona/roadmap).

## Submitting Bug Reports

If you find a bug in Percona Docker Images or in one of the related projects, please submit a report to that project's [JIRA](https://jira.percona.com/browse/K8SPG) issue tracker or [create a GitHub issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/creating-an-issue#creating-an-issue-from-a-repository) in this repository. 

Learn more about submitting bugs, new features ideas and improvements in the [Contribution Guide](CONTRIBUTING.md).",FAUX
permitio/opal,Toolkit,DevOPs,2025-03-03T16:29:56Z,2024-12-04T05:17:51Z,0,7,0,0,0,0,0,0,2021-02-14T11:37:31Z,2025-04-08T13:24:00Z,6182,5241,Python,VRAI,208,FAUX,85,"authorization,cedar,hacktoberfest,microservices,opa,opal,open-policy-agent,openfga,policy,policy-as-code,pubsub,realtime,websocket",85,"Policy and data administration, distribution, and real-time updates on top of Policy Agents (OPA, Cedar, ...)",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,69,"<p  align=""center"">
 <img src=""https://github.com/permitio/opal/assets/4082578/4e21f85f-30ab-43e2-92de-b82f78888c71"" height=170 alt=""opal"" border=""0"" />
</p>
<h1 align=""center"">
⚡OPAL⚡
</h1>

<h2 align=""center"">
Open Policy Administration Layer
</h2>

<a href=""https://github.com/permitio/opal/actions?query=workflow%3ATests"" target=""_blank"">
    <img src=""https://github.com/permitio/opal/workflows/Tests/badge.svg"" alt=""Tests"">
</a>
<a href=""https://pypi.org/project/opal-server/"" target=""_blank"">
    <img src=""https://img.shields.io/pypi/v/opal-server?color=%2331C654&label=OPAL%20Server%20%28PyPi%29"" alt=""Package"">
</a>
<a href=""https://pypi.org/project/opal-client/"" target=""_blank"">
    <img src=""https://img.shields.io/pypi/v/opal-client?color=%2331C654&label=OPAL%20Client%20%28PyPi%29"" alt=""Package"">
</a>
<a href=""https://pepy.tech/project/opal-server"" target=""_blank"">
    <img src=""https://static.pepy.tech/personalized-badge/opal-client?period=total&units=international_system&left_color=black&right_color=blue&left_text=Downloads"" alt=""Downloads"">
</a>

<a href=""https://hub.docker.com/r/permitio/opal-server"" target=""_blank"">
    <img src=""https://img.shields.io/docker/pulls/permitio/opal-client?label=Docker%20pulls"" alt=""Docker pulls"">
</a>

<a href=""https://bit.ly/permit-slack"" target=""_blank"">
    <img src=""https://img.shields.io/badge/Slack%20Community-4A154B?logo=slack&logoColor=white"" alt=""Join our Slack!"">
</a>
<a href=""https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fpublish.twitter.com%2F%3FbuttonType%3DFollowButton%26query%3Dhttps%253A%252F%252Ftwitter.com%252Fpermit_io%26widget%3DButton&ref_src=twsrc%5Etfw&region=follow_link&screen_name=permit_io&tw_p=followbutton""><img src=""https://img.shields.io/twitter/follow/permit_io?label=Follow%20%40permit_io&style=social"">
</a>

## What is OPAL?

OPAL is an administration layer for Policy Engines such as <a target=""_blank"" href=""https://www.openpolicyagent.org/"">Open Policy Agent (OPA)</a>, and <a target=""_blank"" href=""https://github.com/permitio/cedar-agent"">AWS' Cedar Agent</a> detecting changes to both policy and policy data in realtime and pushing live updates to your agents. OPAL brings open-policy up to the speed needed by live applications.

As your app's data state changes (whether it's via your APIs, DBs, git, S3 or 3rd-party SaaS services), OPAL will make sure your services are always in sync with the authorization data and policy they need (and only those they need).

Check out OPAL's main site at <a target=""_blank"" href=""https://opal.ac"">OPAL.ac</a>

## OPAL Use Cases

OPAL is the easiest way to keep your solution's authorization layer up-to-date in realtime. It aggregates policy and data from across the field and integrates them seamlessly into the authorization layer, and is microservices and cloud-native.

Here are some of the main use cases for using OPAL:
* **End-to-End [Fine-Grained Authorization](https://www.permit.io/blog/what-is-fine-grained-authorization-fga) service** that can be used with any policy language or data store
* [Google-Zanzibar](https://www.permit.io/blog/what-is-google-zanzibar) support for Policy as Code engines such as OPA and AWS Cedar
* Streamline permissions in microservice architectures using [centralized policy configuration with decentralized data](https://www.permit.io/blog/best-practices-for-implementing-hybrid-cloud-security) sources and policy engines
* Manage and automate the deployment of multiple Open Policy Agent engines in a Cloud-Native environment

<img src=""https://github.com/permitio/opal/assets/4082578/99d3dd95-a7ff-45c2-805e-3d533f8b1e8c"" alt=""simplified"" border=""0"">

OPAL  uses a client-server stateless architecture. OPAL-Servers publish policy and data updates over a lightweight (websocket) PubSub Channel, which OPAL-clients subscribe to via topics. Upon updates, each client fetches data directly (from the source) to load it into its managed Policy Engine instance.


### OPA + OPAL == 💜

While OPA (Open Policy Agent) decouples policy from code in a highly-performant and elegant way, the challenge of keeping policy agents up-to-date remains.
This is especially true in applications, where each user interaction or API call may affect access-control decisions.
OPAL runs in the background, supercharging policy agents and keeping them in sync with events in real time.

### AWS Cedar + OPAL == 💪

Cedar is a very powerful policy language, which powers AWS' AVP (Amazon Verified Permissions) - but what if you want to enjoy the power of Cedar on another cloud, locally, or on premise?
This is where [Cedar-Agent](https://github.com/permitio/cedar-agent) and OPAL come in.

This [video](https://youtu.be/tG8jrdcc7Zo) briefly explains OPAL and how it works with OPA, and a deeper dive into it at [this OWASP DevSlop talk](https://www.youtube.com/watch?v=1_Iz0tRQCH4).

## Who's Using OPAL?
OPAL is being used as the core engine of Permit.io Authorization Service and serves in production:
* \> 10,000 policy engines deployment
* \> 100,000 policy changes and data synchronizations every day
* \> 10,000,000 authorization checks every day

Besides Permit, OPAL is being used in Production in **Tesla**, **Walmart**, **The NBA**, **Intel**, **Cisco**, **Live-Oak Bank**, and thousands of other development teams and companies of all sizes.

## Documentation

- 📃 &nbsp; [Full documentation is available here](https://docs.opal.ac)
- 💡 &nbsp; [Intro to OPAL](https://docs.opal.ac/getting-started/intro)
- 🚀 &nbsp; Getting Started:

  OPAL is available both as **python packages** with a built-in CLI as well as pre-built **docker images** ready-to-go.

  - [Play with a live playground environment in docker-compose](https://docs.opal.ac/getting-started/quickstart/opal-playground/overview)
  <!-- - this tutorial is great for learning about OPAL core features and see what OPAL can do for you. -->
  - [Try the getting started guide for containers](https://docs.opal.ac/getting-started/running-opal/overview)
  <!-- - this tutorial will show you how to configure OPAL to your specific needs and run the official docker containers locally or in production. -->

  - [Check out the Helm Chart for Kubernetes](https://github.com/permitio/opal-helm-chart)

- A video demo of OPAL is available [here](https://www.youtube.com/watch?v=IkR6EGY3QfM)

- You can also check out this webinar and Q&A about OPAL [on our YouTube channel](https://www.youtube.com/watch?v=A5adHlkmdC0&t=1s)
  <br>

- 💪 &nbsp; TL;DR - This one command will download and run a working configuration of OPAL server and OPAL client on your machine:

```
curl -L https://raw.githubusercontent.com/permitio/opal/master/docker/docker-compose-example.yml \
> docker-compose.yml && docker compose up
```

<p>
  <a href=""https://asciinema.org/a/409288"" target=""_blank"">
    <img src=""https://asciinema.org/a/409288.svg"" />
  </a>
</p>

- 🧠 &nbsp; ""How-To""s

  - [How to get started with OPAL (Packages and CLI)](https://docs.opal.ac/getting-started/running-opal/as-python-package/overview)

  - [How to get started with OPAL (Container Images)](https://docs.opal.ac/getting-started/running-opal/overview)

  - [How to trigger Data Updates via OPAL](https://docs.opal.ac/tutorials/trigger_data_updates)

  - [How to extend OPAL to fetch data from your sources with FetchProviders](https://docs.opal.ac/tutorials/write_your_own_fetch_provider)

  - [How to configure OPAL (basic concepts)](https://docs.opal.ac/tutorials/configure_opal)

  - [How to Use OPAL with Cedar in a Multi-Language Project](https://www.permit.io/blog/scaling-authorization-with-cedar-and-opal)

- 🎨 &nbsp; [Key concepts and design](https://docs.opal.ac/overview/design)
- 🏗️ &nbsp; [Architecture](https://docs.opal.ac/overview/architecture)
<br>

📖 For further reading, check out our [Blog](https://io.permit.io/opal-readme-blog)

## Community

 We would love to chat with you about OPAL. [Join our Slack community](https://io.permit.io/opal-readme-slack) to chat about authorization, open-source, realtime communication, tech, or anything else!

You can raise questions and ask for features to be added to the road-map in our [**Github discussions**](https://github.com/permitio/opal/discussions), report issues in [**Github issues**](https://github.com/permitio/opal/issues)
</br>
</br>
If you like our project, please consider giving us a ⭐️
</br>

[![Button][join-slack-link]][badge-slack-link] </br> [![Button][follow-twitter-link]][badge-twitter-link]

## Contributing to OPAL

We would love for you to contribute to this project and help make it even better than it is today! 💎

As a contributor, here are the guidelines we would like you to follow:
 - [Code of Conduct](CODE_OF_CONDUCT.md)
 - [Question or Problem?](CONTRIBUTING.md#question)
 - [Issues and Bugs](CONTRIBUTING.md#issue)
 - [Feature Requests](CONTRIBUTING.md#feature)
 - [Development Guidelines](CONTRIBUTING.md#development)

[join-slack-link]: https://i.ibb.co/wzrGHQL/Group-749.png
[badge-slack-link]: https://io.permit.io/join_community
[follow-twitter-link]: https://i.ibb.co/k4x55Lr/Group-750.png
[badge-twitter-link]: https://twitter.com/opal_ac

## There's more!

- Check out [OPToggles](https://github.com/permitio/OPToggles), which enables you to create user targeted feature flags/toggles based on Open Policy managed authorization rules!
- Check out [Cedar-Agent](https://github.com/permitio/cedar-agent), the easiest way to deploy & run AWS Cedar.",VRAI
permitio/opal-example-policy-repo,Documentations,Documentations,2025-01-28T21:52:44Z,2021-04-07T00:01:53Z,0,2,0,1,0,0,0,0,2021-04-06T23:43:56Z,2025-03-22T10:33:54Z,48,28,Open Policy Agent,VRAI,173,FAUX,1,example,1,An example repository for OPAL policy tracking,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,6,"<p  align=""center"">
 <img src=""https://i.ibb.co/BGVBmMK/opal.png"" height=170 alt=""opal"" border=""0"" />
</p>
<h2 align=""center"">
OPAL Example Policy Repo
</h2>

[Check out OPAL main repo here.](https://github.com/permitio/opal)

### What's in this repo?
This repo contain an example git repo containing a basic [OPA](https://www.openpolicyagent.org/docs/latest/) policy written in [Rego](https://www.openpolicyagent.org/docs/latest/policy-language/#what-is-rego).

This repo is used in [OPAL](https://github.com/permitio/opal)'s Getting Started [tutorial](https://opal.ac/getting-started/quickstart/docker-compose-config/overview) to **demonstrate** how OPAL keeps your OPA agents in sync with policy and data changes. When commits are affecting this repo, the OPAL server will immediately push updates (over websockets pub/sub interface) to the connected OPAL clients, and they in turn will push the updated policy and data to OPA.

If you follow [the tutorial](https://opal.ac/getting-started/quickstart/docker-compose-config/overview), you will see how this repo is used by OPAL in a real example running in docker-compose. The entire tutorial is also available as [video](https://asciinema.org/a/5IMzZRPltUiFdsNnZ81t14ERk?t=1).

#### The policy in this repo
This repo has a very simple [RBAC policy](https://en.wikipedia.org/wiki/Role-based_access_control):
- each user is granted certain roles
- a user can perform an action on a resource, only if:
  - one of his roles has permission to do so
  - the user ""location"" is in the US (a special **twist** that is **non-standard** to RBAC, but is useful for the tutorial)
- a user with admin role can do anything

### About OPA (Open Policy Agent)

#### Why use OPA?
OPA enables decoupling policy from code in your applications, and enables you to evolve your application and your authorization policies (i.e: ""permissions logic"") separately.

#### Who uses OPA?
Companies like [Netflix](https://www.youtube.com/watch?v=R6tUNpRpdnY) and [Pinterest](https://www.youtube.com/watch?v=LhgxFICWsA8) built their authorization layer using OPA

### About OPAL (Open Policy Administration Layer)
[OPAL](https://github.com/permitio/opal) is an administration layer for Open Policy Agent (OPA), detecting changes to both policy and policy data in realtime and pushing live updates to your agents.

OPAL brings open-policy up to the speed needed by live applications. As your application state changes (whether it's via your APIs, DBs, git, S3 or 3rd-party SaaS services), OPAL will make sure your services are always in sync with the authorization data and policy they need (and only those they need).

Check out OPAL's main site at [OPAL.ac](https://opal.ac).

<img src=""https://i.ibb.co/CvmX8rR/simplified-diagram-highlight.png"" alt=""simplified"" border=""0"">",FAUX
PlanB-Network/bitcoin-educational-content,Application System,Documentations,2025-05-16T06:14:45Z,2025-05-04T23:01:57Z,0,0,0,0,0,20,0,0,2023-04-29T13:10:06Z,2025-04-08T14:52:31Z,1459190,54,Python,VRAI,140,FAUX,326,,326,,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,54,"<div align=""center"">
  <a href=""https://planb.network"">
    <picture>
      <source srcset=""docs/assets/black_and_orange_gradient.png"" media=""(prefers-color-scheme: light)"">
      <source srcset=""docs/assets/horizontal_logo.png"" media=""(prefers-color-scheme: dark)"">
      <img src=""docs/assets/black_and_orange_gradient.png"" alt=""PBN Logo"" width=""37%"">
    </picture>
  </a>
</div>

<div align=""center"">
  <h1 style=""margin-top: 0;""></h1>
  <a href=""https://planb.network/en/node-network"">
    <img alt=""Website"" src=""https://img.shields.io/website?up_message=online&down_message=offline&url=https%3A%2F%2Fstart9.com&logo=website&label=%F0%9F%8C%90%20Become a P₿N Node&color=ff5c00"">
  </a>
  <a href=""https://twitter.com/planb_network"">
    <img alt=""X (formerly Twitter) Follow"" src=""https://img.shields.io/twitter/follow/planb_network"">
  </a>
  <a href=""https://t.me/PlanBNetwork_ContentBuilder"">
    <img alt=""Static Badge"" src=""https://img.shields.io/badge/community-telegram-blue?logo=telegram"">
  </a>
  <a href=""https://planb.network"">
    <img alt=""Website"" src=""https://img.shields.io/website?up_message=online&down_message=offline&url=https%3A%2F%2Fstart9.com&logo=website&label=%F0%9F%8C%90%20P₿N Platform&color=ff5c00"">
  </a>
  <a href=""https://weblate.planb.network/engage/planb-network-website/"">
    <img src=""https://weblate.planb.network/widget/planb-network-website/website-elements/svg-badge.svg"" alt=""Translation status"" />
  </a>
  <a href=""https://github.com/PlanB-Network/bitcoin-educational-content"">
    <img src=""https://img.shields.io/github/contributors/DecouvreBitcoin/sovereign-university-data?label=Contributors&color=ff5c00"" alt=""Contributors"">
  </a>
</div>

<br />
<div align=""center"">
  <h3>
    Welcome to the Plan ₿ Network Content Repo!
  </h3>
  <p>
    Welcome to the Plan ₿ Network Content Repo! If you are here, there's a chance you want to contribute to this larger-than-us project, which aims at consolidating the first multilingual and open-source e-learning platform focused on Bitcoin.
  </p>
</div>
<br />
<p align=""center"">
  <a href=""https://planb.network"">
    <img src=""docs/assets/PBN-banner.png"" alt=""P₿N banner"" width=""100%"">
  </a></p>
<br />

---

If you're just lost on GitHub, visit our [website](https://planb.network) to learn more about Bitcoin.

However, we hope you're here to contribute, so here we will present the inner workings of this content management system and how you can assist us -- either by adding new material or translating/proofreading content in your language.

Thank you for your time, involvement, and effort in this project. Here, we believe in a [value-for-value model](https://dergigi.com/2021/12/30/the-freedom-of-value/) and we'll do our best to reciprocate your contribution on the basis of your Proof-of-Work. Moreover, by participating in the creation of valuable Bitcoin resources for your local community, you could also get some useful insight into how to spread that knowledge locally.

# Why do we do it?

Plan ₿ Network was born from our belief in the transformative power of Bitcoin to create a decentralized future. We are inspired by the Cypherpunk Manifesto, which emphasizes the importance of knowledge as the foundation of freedom and change. Our ultimate goal is to empower individuals all over the world with the knowledge they have been deprived of, in order to revolutionize the world.

# How does it work?

Plan ₿ Network helps communities worldwide get organized to provide better education. The platform develops the necessary tools to achieve this goal through the [online academy](https://planb.network/courses), the [tutorials portal](https://planb.network/tutorials), and the [resource portal](https://planb.network/resources).

Before getting onto the platform, contents are published here, in this repository, as a central public
place for Bitcoin Education. Any piece of content is specifically formatted to
facilitate data parsing and (so) rendering on websites or apps.

## Repository Structure

This repo is organized around three main directories, which are:

- `courses`, comprising all the courses about Bitcoin, Lightning, Cryptography, Mining, and so on.
- `resources`, consisting of various types of resources about Bitcoin, such as books, videos, or podcasts.
- `tutorials`, containing ""how-to"" articles, categorized into themes like exchange, merchant, node, privacy, and so on.

Each individual piece of content, which is a markdown file, is defined by its location in this tree structure and by its language (indicated in the name code). For instance, the Italian tutorial about Nerd-Miner would have the following path:

```
./tutorials/mining/nerd-miner/it.md
```

If you want to learn more about our repo structure and content format, check out our [documentation](./docs/README.md).

Instead, if you want to have a bird-eye view of our published content and proposals, check the below:

- [Course Dashboard](https://github.com/orgs/PlanB-Network/projects/5)
- [Tutorial Dashboard](https://github.com/orgs/PlanB-Network/projects/6)
- [Resources Dashboard](https://github.com/orgs/PlanB-Network/projects/4)

## Incentive model: Value-4-Value

We believe in the power of Proof Of Work and how it enables a value-4-value model. In the words of one of the greatest poets of our times, DerGigi, we consider time, talent and treasure. Thus, we are committed to give back to those who enrich us with their time and talent.

In practical terms, it means that any task (content creation, peer-reviewing,
proofreading), explicitly as an individual Github issue, is correlated to a reward in sats. When the issue is closed, the reward is sent to the contributor who completed the task.

If you want to learn more about it, go have a [look here](#why-value-4-value-PoW-model?)

# How can you contribute?

Bitcoin is for everyone, and so is Plan ₿ Network, which means that everyone who has a specific talent can contribute to the growth of the network. Whether you are a nerdy, super coder or a social media influencer, your knowledge and skills are essential to spread Bitcoin knowledge across the world.

## Be a node of the network

Another way to contribute might be to become a Plan ₿ Network Node. A node is a Bitcoin community that has a physical hub and wants to move to the next step to accelerate local Bitcoin adoption! There are no requirements regarding the size of its community: what is important is its dedication and the Proof-of-Work already done for the local peers.

At Plan ₿ Network, we aim to improve the Bitcoin education ecosystem by providing the right tools to educate others and create local projects. So, by becoming a node, your community will get access to a large range of resources, services, special training, and more.

If you want your community to be a reference in the Bitcoin world, learn more about how to become a node [here](https://planb.network/en/node-network).

## Be a Bitcoin Educator

If you have created some Bitcoin educational content, whether in the form of a video, book, podcast, blog, or even an entire course, Plan ₿ Network will grant you the right visibility on our global platform. We can provide the tools to translate your teaching materials and share them with people who speak many different languages.

As an educator, your role is crucial. In fact, without your dedication and passion, Bitcoin only remains a piece of information. Instead, we want people to understand that Bitcoin is precious and the knowledge of it will pass through generations.

To propose your content directly, you can first reach out via the
[Telegram Group](https://t.me/PlanBNetwork_ContentBuilder) to get assistance.

## Join the Proofreading Team

The key to making education powerful is to make it accessible to everybody in the world. Not everyone speaks English, or every other language on Earth! However, we are sure that together, as a network, we can make Bitcoin educational resources as open-source as the seven seas, by translating them into many languages.

If you are confident that you can help us proofread our content, we are eagerly looking for you! Just reach out via the [Telegram Group](https://t.me/PlanBNetwork_ContentBuilder).

Literally EVERY LANGUAGE is needed!

<a href=""https://weblate.planb.network/engage/planb-network-website/"">
<img src=""https://weblate.planb.network/widget/planb-network-website/website-elements/multi-auto.svg"" alt=""Translation status"" />
</a>

The image above displays the percentage of static elements on our website that have been translated, per language. Feel free to reach out if you want to add your language to the platform!

### How to add a new language on PBN

If you are enthusiastic about this initiative and you would like to help us, please add your local language to the github repo and the website. Kindly follow the following steps:

1. Join the [Contributors TG group](https://t.me/PlanBNetwork_ContentBuilder) and present the languages you speak.
2. You will be contacted by our team member and maybe added to a specific group dedicated to one language.
3. The first step will be to [add the new language](https://planb.network/tutorials/contribution/content/weblate-add-new-language-eef2f5c0-1aba-48a3-b8f0-a57feb761d86) on the Weblate app (an account on Weblate is required).
4. Then, you can [start translating the static elements](https://planb.network/tutorials/contribution/content/weblate-translate-front-end-8213b931-650f-4efd-8f4e-9a8ae5ce6295) of the website.
5. When you finish, go back to the [TG group](https://t.me/PlanBNetwork_ContentBuilder) or reach out to the translation coordinator to inform them you have completed the task.
6. Thanks to the [LLM-Translator](https://github.com/Asi0Flammeus/LLM-Translator), or other tools, we will translate the BTC101 course to benchmark with you the automatic translation accuracy.
7. [Start proofreading](https://planb.network/tutorials/contribution/content/contribution-proofreading-review-tutorial-1ee068ca-ddaf-4bec-b44e-b41a9abfdef6) BTC101 on GitHub (an account on GitHub is required) in the corresponding language after the approval of the coordinator.
8. After you finish and we merge the PR, we will send you the reward in sats for both contributions, using a Lightning Network invoice or LN address.
9. If the automatic translation is good enough, we will translate all the educational content of Plan ₿ Network and publish it on the website.
10. Then, anyone will be able to choose any content in that language to proofread and receive the associated reward.

# Why do we use the value-4-value PoW model?

Well, because we don’t see any other way. What is the problem with the internet nowadays? It's that we think we are getting everything for free, but, in reality, we are the product. In fact, our data is used for the benefit of others. Since we believe in the freedom to choose how we handle our data and knowledge, we ensure that everything we share, build, and contribute in this open-source model is paid for in a value-for-value fashion.

## How do we do it?

Our first goal is to gather all the amazing content that has already been created (you can find everything on our website).

But there is a catch: we want to reach every corner of the world! For this reason, we need everyone to collaborate with their own language skills. We can't get to all the people in Bangladesh if we don't have the content in Bengali.

Therefore, we have implemented this incentive model. First, you will get paid per task (content creation,proofreading): there is a whole payment system set in place by Asi0 -- if
you are curious about it, you will find some details in the [documentation](./docs/value-4-value-model.md). In short,
each content type has a formula based on various parameters like the number of words, the content difficulty, and other criteria, so the reward associated to each task is
defined in the assigned issue.

## What's next?

It's simple: as people read or use your content, they will rate it. The better the rating you get as a proofreader or verifier, the more proofreadings you will receive. This is what we like to call a proof-of-work structure.

The better the quality of your work, the more content you will get. The more value you receive, the more value WE all receive.

**LONG LIVE VALUE-FOR-VALUE!**

# End Goal: Overcome the language barriers in Bitcoin Education

So that's it, that's our plan to take over the world! We seek every role, in every language: proofreaders, educators, and most importantly, individuals that are eager to learn.

If you believe you are one of them, [reach out](https://t.me/PlanBNetwork_ContentBuilder). We want to collaborate!

<br />
<p align=""center"">
  <a href=""https://planb.network"">
    <img src=""docs/assets/doing-my-part.jpg"" alt=""Doing my Part meme"" width=""75%"">
  </a></p>
<br />",VRAI
plexsystems/konstraint,Toolkit,Application System,2025-05-10T18:36:16Z,2025-02-22T21:12:31Z,0,63,0,0,0,0,0,26,2019-09-13T03:02:46Z,2025-03-31T00:23:44Z,1369,386,Go,VRAI,51,FAUX,12,"conftest,gatekeeper,kubernetes,opa,open-policy-agent,policy,rego",12,A policy management tool for interacting with Gatekeeper,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,27,"# Konstraint

[![Go Report Card](https://goreportcard.com/badge/github.com/plexsystems/konstraint)](https://goreportcard.com/report/github.com/plexsystems/konstraint)
[![GitHub release](https://img.shields.io/github/release/plexsystems/konstraint.svg)](https://github.com/plexsystems/konstraint/releases)

![logo](images/logo.png)

Konstraint is a CLI tool to assist with the creation and management of templates and constraints when using [Gatekeeper](https://github.com/open-policy-agent/gatekeeper).

## Installation

```text
go install github.com/plexsystems/konstraint@latest
```

A docker image is also provided for each release:

```text
docker run -v $PWD:/konstraint ghcr.io/plexsystems/konstraint create /konstraint/examples
```

## Usage

To create the Gatekeeper resources, use `konstraint create <policy_dir>`.

To generate the accompanying documentation, use `konstraint doc <policy_dir>`.

Both commands support the `--output` flag to specify where to save the output. For more detailed usage documentation, see the [CLI Documentation](docs/cli/konstraint.md).

## Why this tool exists

### Automatically copy Rego to the ConstraintTemplate

When writing policies for Gatekeeper, the Rego must be added to [ConstraintTemplates](https://github.com/open-policy-agent/gatekeeper#constraint-templates) in order for Gatekeeper to enforce the policy. This creates a scenario in which the Rego is written in a `.rego` file, and then copied into the ConstraintTemplate. When a change is needed to be made to the Rego, both instances must be updated.

### Automatically update all ConstraintTemplates with library changes

Gatekeeper supports importing _libraries_ into `ConstraintTemplates` with the `libs` field. If a change is required to the imported library, every template must be updated to include this new change.

### Enable writing the same policies for Conftest and Gatekeeper

With Gatekeeper, policies are evaluated in the context of an [AdmissionReview](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#webhook-request-and-response). This means that policies are typically written with a prefix of `input.review.object`.

With [Conftest](https://github.com/open-policy-agent/conftest), policies are written against `yaml` files.

This creates a scenario where the policy needs to be written differently depending upon the context in which the policy is being evaluated in.

### Kubernetes Libraries

In the [examples/lib](examples/lib) directory, there are multiple libraries that enable policies to be written for both Conftest and Gatekeeper.

You can include as little or as many of these libraries into your policies as desired.

#### Purpose

By first validating the Kubernetes manifests with `Conftest` on a local machine, we can catch manifests that would otherwise violate policy without needing to deploy to a cluster running Gatekeeper.

## FAQ

**Konstraint ran without error, but I don't see any new files.**

This typically means no policies were found, or the policies did not have any `violation[]` rules, so they are not compatible with Gatekeeper.

For more information, see [How Constraints are Created](docs/constraint_creation.md).

**My ConstraintTemplates are missing the input parameters**

Input parameters can be specified by using one or more `@parameter <name> <type>` tags in the comment header block. If you use input parameters, Konstraint will skip generating the `Constraint` resource for that policy.

For more information, see [Using Input Parameters](docs/constraint_creation.md#using-input-parameters).",VRAI
pokepay/aws-sdk-lisp,Application System,Toolkit,2024-12-17T08:19:24Z,2022-12-19T19:55:46Z,0,0,0,0,0,0,13,0,2017-08-30T08:22:33Z,2025-02-17T07:30:47Z,27871,89,Common Lisp,VRAI,20,FAUX,13,"aws,aws-sdk,common-lisp",13,AWS-SDK for Common Lisp,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,7,"# AWS-SDK for Common Lisp

## Warning

This software is still ALPHA quality. The APIs will be likely to change.

## Usage

AWS-SDK provides interfaces for each AWS services as individual systems under ""aws-sdk/services/*"".

Here's an example to send an SMS via Amazon Simple Notification Service:

```common-lisp
(ql:quickload '(:aws-sdk :aws-sdk/services/sns))

(setf aws:*session* (aws:make-session))

;; Sending 
(aws/sns:publish :phone-number ""+8190xxxxxxxx"" :message ""Hi, there"")
```

## Configuring the SDK

### Configuring Credentials

Before using the SDK, you'll need to set AWS credentials for AWS services. AWS-SDK supports multiple methods to configure AWS credentials.

* Environment Credentials: Set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`
* Shared Credentials file (~/.aws/credentials).
* EC2 Instance Role Credentials

It's also can be configured via `aws:*session*`:

```common-lisp
(setf aws:*session*
      (aws:make-session :credentials
                        (aws:make-credentials
                         :access-key-id ""XXXXXXXXXXXXXXXXXXXX""
                         :secret-access-key ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
                         :session-token ""xxxxxxxxxxx"")))
```

### Configuring AWS Region

For making AWS API request, AWS region is also required. There're multiple methods to configure it.

* Environment variable: `AWS_REGION`
* Shared configuration file (~/.aws/config).

It's also can be configured via `aws:*session*`:

```common-lisp
(setf aws:*session* (aws:make-session :region ""us-west-2""))
```

## Development note

### Generating all services

```
$ lake
```

## Author

* Eitaro Fukamachi (e.arrows@gmail.com)

This product is developed with the generous support of [Pocket Change, K.K.](https://www.pocket-change.jp/)

## Copyright

Copyright (c) 2017 Eitaro Fukamachi (e.arrows@gmail.com)

## License

Licensed under the BSD 2-Clause License.",VRAI
portefaix/portefaix-hub,Application System,Documentations,2025-05-02T15:13:39Z,2025-02-04T17:28:29Z,0,0,0,0,12,0,0,0,2020-12-02T11:50:41Z,2025-04-01T21:31:03Z,4055,6,Mustache,VRAI,12,FAUX,12,"galactus,helm,kubernetes,portefaix",12,Portefaix Helm charts,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,16,"# Portefaix Hub

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![](https://github.com/portefaix-hub/charts/workflows/Release%20Charts/badge.svg?branch=master)](https://github.com/portefaix-hub/charts/actions)
[![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/portefaix-hub)](https://artifacthub.io/packages/search?repo=portefaix-hub)

Charts are available in the following formats:

- [OCI Artifacts](https://helm.sh/docs/topics/registries/)

### Installing from an OCI Registry

Charts are available in OCI format. The list of available charts can be found [here](https://github.com/orgs/portefaix/packages).

Install one of the available charts:

```shell
$ helm upgrade -i oci://ghcr.io/portefaix/portefaix-hub/<chart_name> --version=<version>
```

## Charts

See [Artifact Hub](https://artifacthub.io/packages/search?repo=portefaix-hub) for a complete list.

## SLSA

All _artifacts_ provided by this repository meet [SLSA L3](https://slsa.dev/spec/v1.0/levels#build-l3)

### Verify SLSA provenance

Using the [Github CLI]():

```shell
$ gh attestation verify oci://ghcr.io/portefaix/charts/fake:0.1.0 --repo portefaix/portefaix-hub
Loaded digest sha256:84440dd6e696ed153a43490bdfdf9190d640d041fb4201f326578a4be829e811 for oci://ghcr.io/portefaix/charts/fake:0.1.0
Loaded 1 attestation from GitHub API
✓ Verification succeeded!

sha256:84440dd6e696ed153a43490bdfdf9190d640d041fb4201f326578a4be829e811 was attested by:
REPO                     PREDICATE_TYPE                  WORKFLOW
portefaix/portefaix-hub  https://slsa.dev/provenance/v1  .github/workflows/chart-release-manual.yml@refs/heads/feat/sign
```

## Contributing

See [CONTRIBUTING.md](./CONTRIBUTING.md)

## License

[Apache 2.0 License](./LICENSE)",VRAI
prancer-io/prancer-compliance-test,Toolkit,Documentations,2024-06-13T07:44:17Z,2023-10-02T05:32:44Z,0,384,0,0,0,0,0,0,2020-05-15T03:23:07Z,2024-06-13T07:44:23Z,15660,39,Open Policy Agent,VRAI,12,FAUX,7,"devops,devsecops,iac,iac-security,opa,policy,rego",7,This repository includes cloud security policies for IaC and live resources.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,20,"# Prancer Compliance test repository

## Introduction
Prancer is a pre-deployment and post-deployment multi-cloud security platform for your Infrastructure as Code (IaC) and live cloud environment. It shifts the security to the left and provides end-to-end security scanning based on the Policy as Code concept. DevOps engineers can use it for static code analysis on IaC to find security drifts and maintain their cloud security posture with continuous compliance features. you can get more information from our website at : https://www.prancer.io

## How to use the repository
The easiest way to get up and running is to make sure you can run the scenario we are explaining in the [Hello World example](https://www.prancer.io/guidance/). after being able to run that simple scenario, you can use this repository to do more advanced security tests.

## Repository structure
The repository consists of 4 high level folders representing each supported type:
 - AWS
 - Azure
 - Google
 - Kubernetes

Under each top level directory, we have `cloud`, `iac` and `terraform` folders which hold the `rego` files respectively.
 - `cloud` folder holds all the rego files related to post deployment tests. These tests contribute to have continuous compliance in the cloud
 - `iac` folder holds all the rego files related to IaC Security Scan. These tests contribute to shift security to left concept
 - `terraform` folder holds all the rego files related to Terraform Infrastructure as Code

## Prerequisites
Make sure you have the following prerequisites available:
 - Linux distribution
 - Python 3.6.8 / 3.8 / 3.9
 - Prancer Basic [How to install prancer basic](https://docs.prancer.io/installation/)
 - OPA [How to install OPA binary](https://www.openpolicyagent.org/docs/latest/#running-opa)
 > Note: We recommend moving `opa` to a directory included in your system's `PATH` (i.e `/usr/local/bin/`)

 ## Sample scenario
 There are lots of use cases available for the [Prancer Platform](https://www.prancer.io/introduction/?section=use-case-scenarios). Here I will show you a sample scenario to IaC Scan Azure ARM template.

 > The complete code is available in the [Hello World](https://github.com/prancer-io/prancer-hello-world) repository

The easiest way is to clone our [Hello World](https://github.com/prancer-io/prancer-hello-world) repository, change the parameters and run it.

### step 1 - Clone the `Hello World` sample repo
`git clone https://github.com/prancer-io/prancer-hello-world`
`cd prancer-hello-world`

### step 2 - Change the connector file to point to your IaC code repo
You can use the available connector and change the `gitProvider` attribute to point to your own repo. (https://github.com/prancer-io/prancer-hello-world/blob/master/gitConnectorArmRemoteStructure.json)

`cat gitConnectorArmRemoteStructure.json`
```
{
    ""fileType"": ""structure"",
    ""type"": ""filesystem"",
    ""companyName"": ""prancer"",
    ""gitProvider"": ""https://github.com/prancer-io/prancer-armof.git"",
    ""branchName"": ""master"",
    ""private"": false
}
```

### step 3 - Verify Master snapshot and Master test files
The next step is to verify master snapshot and master test files which are pointed to this repository. The complete code is already available in our `Hello World` sample repository and you don't need to change anything. You can find the sample codes here:
https://github.com/prancer-io/prancer-hello-world/tree/master/validation/scenario-arm-remote

`cat validation/scenario-arm-remote/master-snapshot.json`
```
{
    ""$schema"": """",
    ""contentVersion"": ""1.0.0.0"",
    ""fileType"": ""masterSnapshot"",
    ""connector"": ""gitConnectorRemote"",
    ""remoteFile"": ""azure/iac/master-snapshot.json"",
    ""connectorUsers"": [
      {
        ""id"": ""USER_1"",
        ""testUser"": ""user1"",
        ""source"": ""gitConnectorArmRemoteStructure""
      }
    ]
}
```

`cat validation/scenario-arm-remote/master-test.json`
```
{
    ""contentVersion"": ""1.0.0.0"",
    ""notification"": [],
    ""masterSnapshot"": ""master-snapshot"",
    ""fileType"": ""mastertest"",
    ""connector"": ""gitConnectorRemote"",
    ""remoteFile"": ""azure/iac/master-compliance-test.json""
}
```

### step 4 - running the IaC Scan
run the command `prancer --crawler scenario-arm-remote` to get all the files available in your repository. And then `prancer scenario-arm-remote` to complete the IaC security tests for Azure ARM templates.",VRAI
Privado-Inc/privado,Toolkit,Application System,2025-04-22T08:52:46Z,2024-10-21T06:18:20Z,0,0,0,0,0,1,0,0,2021-11-13T11:46:25Z,2025-03-30T01:27:06Z,30950,528,Python,VRAI,62,FAUX,33,"android-privacy-tools,appsec,compliance,devprivops,devsecops,gdpr,gdpr-compliant,hacktoberfest,play-store-data-safety,privacy-by-design,privacy-engineering,privacy-labels,privacy-policy,static-analysis",33,"Open Source Static Scanning tool to detect data flows in your code, find data security vulnerabilities & generate accurate Play Store Data Safety Report.",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,33,"# What is Privado?
[![slack](https://img.shields.io/badge/slack-privado-5A34D9.svg?logo=slack)](https://join.slack.com/t/privado-community/shared_invite/zt-yk5zcxh3-gj8sS9w6SvL5lNYZLMbIpw)
[![docs](https://img.shields.io/badge/docs-gitbook-brightgreen.svg?logo=gitbook)](https://docs.privado.ai)

Privado is an open-source static code analysis tool to discover data flows in the code. It detects more than 110 [personal data elements](docs/extra/data%20element%20list.csv) being processed and further maps the data flow from the point of collection to ""sinks"" such as external third parties, databases, logs, and internal APIs.

<img src=""https://user-images.githubusercontent.com/80044360/186333819-779bfff5-d7a2-4bba-88e9-0ca866e1ee81.gif"" width=""600px"">

# Supported languages
We support Java and Python in GA. Our Enterprise offering covers all programming languages, and we're working on adding support for more languages to OSS. Support for JS/TS is coming soon!



# Quick Start

First, make sure you have [Docker](https://docs.docker.com/get-docker/) installed on your system, then follow these simple steps to get started with Privado.

### Download the Privado CLI

```
curl -o- https://raw.githubusercontent.com/Privado-Inc/privado-cli/main/install.sh | bash
```

### Clone our test repo
We recommend using [this sample app](https://github.com/saurabh-sudo/BankingSystem-Backend) to get started with Privado.
```
git clone git@github.com:saurabh-sudo/BankingSystem-Backend.git
```

### Scan your repository

```
privado scan <source directory>
```


### Get results

The results are generated at `<source directory>/.privado/privado.json` and a preview will be shown in your terminal.

### Visualize results

To visualize the results and generate reports, you can create a free account at the end of a successful scan. Once a scan is complete, it will ask your permission to synchronize the generated results with Privado Cloud Dashboard. Note that **no code is sent to the cloud**–only the JSON output generated by the scan. Upon successful sync, you can view the results on our free platform.

<img src=""https://user-images.githubusercontent.com/80044360/186335775-82139291-4edc-4750-85bf-18b26d5655d3.png"" width=""600px"">

# Who is it for?
1. Privacy Engineers
2. Data Protection Engineers
3. Data Governance Engineers
4. Security Engineers
5. Mobile App Developers
6. Developers 
    
# How does it help?
Privado lets Engineers ask contextual questions about the usage of sensitive data at scale.  
Examples:

<img src=""https://user-images.githubusercontent.com/80044360/186449832-5799854c-a8e0-43c6-bf43-4b47ff5d23a5.jpeg"" width=""600px"">

# Use cases
1. Generate and maintain Data maps and Record of Processing Activity Reports ( RoPA / Article-30 Reports )
2. Automate the generation of the data-flow diagrams
3. Identify and remove data leaks
4. Improve data storage security by identifying and fixing insecure practices
5. Finding and fixing unaccounted third-party sharing of data
6. Establish and enforce Data Protection and Governance policies
7. Generate Android Data Safety Report
8. Incorporate various GDPR, CCPA, SOC, ISO, HIPAA, and PCI controls
9. Do continuous monitoring for privacy and data issues
10. Implement Privacy by Design

# How does Privado work?
Privado can be run locally on your computer or in your CI/CD pipeline. Privado creates a knowledge graph during the scanning process that contextually answers thousands of questions about sensitive data. Since the scan is local, you never have to worry about your code leaving your machine. An output file is stored in JSON format, and the results can be viewed on Privado Cloud.

# What does the scan discover?
Privado will discover the following information in the code during scanning and present it in a dashboard for your review.

-   Data Elements
-   Data Flow Diagrams
-   Data Inventory
-   Code Analysis
-   Issues
    
# What can I do with Privado?
Apart from getting a comprehensive outlook of your data practices for Privacy Audits, you can also use the tool to generate various privacy reports to comply with privacy laws like GDPR and CCPA.

## Record of Processing Activity ( ROPA ) Report

Our free cloud platform can be used to generate RoPA reports for one or more synced repositories.

## Data Safety Report 
A Data Safety Report is a privacy form needed to publish any Android app on the Play Store. Most of the time, filling out a report means developers asking around the team to find what data they're collecting, spending hours reading SDK docs to see where information gets shared and navigating the complex Playstore form. With our scan, we pre-fill data types that are collected and shared, and our wizard guides you through generating the report.

# Contribute
Please check out our [contribution page](https://docs.privado.ai/extra/contributing) if you love this project and would like to contribute.",VRAI
projectcontour/contour,Toolkit,DevOPs,2025-05-15T20:08:47Z,2025-04-28T03:50:08Z,0,0,0,0,0,0,0,11,2017-10-26T20:35:22Z,2025-04-08T04:03:15Z,36755,3773,Go,VRAI,691,FAUX,120,"cncf,contour,envoy,gateway-api,hacktoberfest,http-proxy,ingress-controller,kubernetes",120,Contour is a Kubernetes ingress controller using Envoy proxy.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,233,"# Contour

![GitHub release](https://img.shields.io/github/release/projectcontour/contour.svg) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![Slack](https://img.shields.io/badge/slack-join%20chat-e01563.svg?logo=slack)](https://kubernetes.slack.com/messages/contour)

![Build and Test Pull Request](https://github.com/projectcontour/contour/workflows/Build%20and%20Test%20Pull%20Request/badge.svg) [![Go Report Card](https://goreportcard.com/badge/github.com/projectcontour/contour)](https://goreportcard.com/report/github.com/projectcontour/contour) [![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/projectcontour/contour/badge)](https://securityscorecards.dev/viewer/?uri=github.com/projectcontour/contour) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/4141/badge)](https://bestpractices.coreinfrastructure.org/projects/4141)


![Contour is fun at parties!](contour.png)

## Overview

Contour is an [ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) for Kubernetes that works by deploying the [Envoy proxy](https://www.envoyproxy.io/) as a reverse proxy and load balancer.
Contour supports dynamic configuration updates out of the box while maintaining a lightweight profile.

Contour supports multiple configuration APIs in order to meet the needs of as many users as possible:

- **[Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)** - A stable upstream API that enables basic ingress use cases.
- **[HTTPProxy](https://projectcontour.io/docs/main/config/fundamentals/)** - Contour's Custom Resource Definition (CRD) which expands upon the functionality of the Ingress API to allow for a richer user experience as well as solve shortcomings in the original design.
- **[Gateway API](https://gateway-api.sigs.k8s.io/)** - A new CRD-based API managed by the [Kubernetes SIG-Network community](https://github.com/kubernetes/community/tree/master/sig-network) that aims to evolve Kubernetes service networking APIs in a vendor-neutral way.

## Prerequisites

See the [compatibility matrix](https://projectcontour.io/resources/compatibility-matrix/) for the Kubernetes versions Contour is supported with.

RBAC must be enabled on your cluster.

## Get started

Getting started with Contour is as simple as one command.
See the [Getting Started](https://projectcontour.io/getting-started) document.

## Troubleshooting

If you encounter issues, review the Troubleshooting section of [the docs](https://projectcontour.io/docs), [file an issue](https://github.com/projectcontour/contour/issue), or talk to us on the [#contour channel](https://kubernetes.slack.com/messages/contour) on the Kubernetes Slack server.

## Contributing

Thanks for taking the time to join our community and start contributing!

- Please familiarize yourself with the [Code of Conduct](/CODE_OF_CONDUCT.md) before contributing.
- See [CONTRIBUTING.md](/CONTRIBUTING.md) for information about setting up your environment, the workflow that we expect, and instructions on the developer certificate of origin that we require.
- Check out the [open issues](https://github.com/projectcontour/contour/issues).
- Join our Kubernetes Slack channel: [#contour](https://kubernetes.slack.com/messages/contour/)
- Join the **Contour Community Meetings** - [schedule, notes, and recordings can be found here](https://projectcontour.io/community)
- Find GOVERNANCE in our [Community repo](https://github.com/projectcontour/community)
## Roadmap

See [Contour's roadmap](https://github.com/projectcontour/community/blob/main/ROADMAP.md) to learn more about where we are headed.

## Security

### Security Audit

A third party security audit was performed by Cure53 in December of 2020. You can see the full report [here](Contour_Security_Audit_Dec2020.pdf).

### Reporting security vulnerabilities

If you've found a security related issue, a vulnerability, or a potential vulnerability in Contour please let the [Contour Security Team](mailto:cncf-contour-maintainers@lists.cncf.io) know with the details of the vulnerability. We'll send a confirmation email to acknowledge your report, and we'll send an additional email when we've identified the issue positively or negatively.

For further details please see our [security policy](SECURITY.md).

## Changelog

See [the list of releases](https://github.com/projectcontour/contour/releases) to find out about feature changes.",VRAI
PTG-Kitware/angel_system,Application System,Documentations,2025-01-06T15:42:29Z,2024-11-21T21:42:06Z,3,0,0,0,0,0,0,0,2022-01-05T16:44:58Z,2025-01-06T15:42:35Z,214639,8,Jupyter Notebook,VRAI,17,FAUX,24,,24,,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,28,"# PTG ANGEL System

Initial repo for PTG project.

This repo contains:
* Experimental Unity app (and companion [HL2SS] plugin) for
  transmitting appropriate sensor data off of the HoloLens2 platform,
  and for running the ANGEL ARUI.
* ROS 2 system for receiving sensor data, processing analytics, and pushing 
  results out. 

Note: This repository contains submodules that will need to be initialized upon first checkout:
```bash
$ git submodule update --init --recursive
```

# Table of Contents
<!--
    Updating this section is currently a manual process (or use an LLM?).
    New sections at the top and second level should be added here manually.
-->
* [Windows Development for HL2](#Windows-Development-for-HL2)
  * [Tool versions used](#Tool-versions-used)
  * [First time build/deploy instructions](#First-time-builddeploy-instructions)
  * [ROS Unity Setup](#ROS-Unity-Setup)
  * [Running application without a development environment](#Running-application-without-a-development-environment)
  * [Misc. notes](#Misc-notes)
* [ROS 2 System](#ROS-2-System)
  * [Provision Files](#Provision-Files)
  * [Developing with Submodules](#Developing-with-Submodules)
  * [Docker-based Workflow](#Docker-based-Workflow)
  * [Run Configurations -- Tmuxinator](#Run-Configurations----Tmuxinator)
  * [Configuring ROS nodes that utilize SMQTK-Core](#Configuring-ROS-nodes-that-utilize-SMQTK-Core)
  * [Setting up the foot pedal for annotations](#Setting-up-the-foot-pedal-for-annotations)
  * [ANGEL System Python Package](#ANGEL-System-Python-Package)
  * [Lessons Learned](#Lessons-Learned)

# Windows Development for HL2

## Tool versions used

Unity Hub - 3.0.0 - https://unity.com/unity-hub  
Unity - 2020.3.25f1 - https://unity3d.com/get-unity/download/archive  
Visual Studio 2019 - 16.11.8 - https://visualstudio.microsoft.com/vs/older-downloads/  
Mixed Reality Feature Tool - 1.02111-Preview - https://www.microsoft.com/en-us/download/details.aspx?id=102778  
HoloLens 2 headset - OS version 20438.1432  
Anaconda/Miniconda - py3.9  
HL2SS plugin - 1.0.15 - https://github.com/jdibenes/hl2ss

## First time build/deploy instructions

1) Open the unity/ARUI project with Unity Hub.
2) Open the arui_engineering scene in the Unity editor (unity\ARUI\Assets\my_scene).
3) Modify the default project build settings (``File -> Build Settings...``).
   - Ensure the ""Scenes/arui_engineering"" scene is checked for inclusion in the build
     (others may be unchecked).
   - Under the Universal Windows Platform tab
     - Target Device = HoloLens  
     - Architecture = ARM64  
     - Minimum Platform Version = 10.0.19041.0  
     - Build and Run on = USB Device  
   - Click ``Switch Platform`` after applying the new settings.  
4) Follow the instructions in the [ROS Unity setup section](#ros-unity-setup) to generate the message files and set the endpoint IP.
5) Create a build folder to place completed Unity builds (e.g. unity/ARUI/Build).
6) Click ``Build`` and specify your desired build folder. After the build completes, a new file explorer windows will pop up with the location of the build.
7) Open the .sln file with Visual Studio.  
8) In Visual Studio, switch ``Solution Configurations`` to Release and ``Solution Platforms`` to ARM64.  
9) Open the project properties window (In the ``Solution Explorer`` pane, right-click ``Angel_ARUI`` and select ``Properties``) and switch to the ``Configurations -> Debugging`` tab. Enter the IP address of your HoloLens in the Machine Name field.
10) Deploy the app to the Hololens by clicking ``Build -> Deploy Solution``
11) After deployment completes, open the Windows menu in the Hololens and select All Apps, and then click on the Angel_ARUI application.

## ROS Unity Setup
### ROS IP configuration
1) In Unity, click ``Robotics -> ROS Settings`` to open the ROS Settings menu.
2) Set the protocol to ``ROS2``.
3) Enter the IP address of the machine the TCP endpoint node will be running on (i.e. the machine the HoloLens 2 will be connecting to) and close the ROS Settings menu.
### ROS message C# script generation
1) In Unity, click ``Robotics -> Generate ROS Messages...`` to open the ROS Message Browser.
2) Set the ROS message path to the directory containing the ROS2 messages for the project (../../ros/angel_msgs for this project's ANGEL message folder). The Message Browser should display the .msg files it found in the ROS message path.
3) Click ``Build msgs`` and wait for message generation to finish. You should now see new files in the Built message path location (default location is unity/ARUI/Assets/RosMessages).
4) Close the ROS Message Browser.

## Running application without a development environment
See [Unity README.md](unity/README.md) for instructions on creating an application package and installing it via the HoloLens 2 device portal.

## Misc. notes

- Research mode must be enabled in the HoloLens headset (see ""Enabling Research Mode"" section here https://docs.microsoft.com/en-us/windows/mixed-reality/develop/advanced-concepts/research-mode).  
- The first time you start your application on the HoloLens, you may have to allow camera/microphone access to the app. Click yes on the popups and then restart the application.
- Due to issues accessing the depth and PV camera simultaneously, the HoloLens OS version should be less than 20348.1501. Version 20348.1432 has been tested with the app and confirmed to work. See this [issue](https://github.com/microsoft/HoloLens2ForCV/issues/133) for more information. To install an older OS version on the HoloLens, follow the instructions [here](https://docs.microsoft.com/en-us/hololens/hololens-recovery#clean-reflash-the-device).
- Although the short throw depth camera data provided by research mode is provided as UINT16 data, the max valid value is 4090 (values above 4090 signify an invalid reading from the camera). So, the true range of the short throw depth camera is [0, 4090].

# ROS 2 System
ROS 2 Foxy is utilized to support our system of streaming analytics and
report-back to the HoloLens2 platform.

Workspace root: `./ros/`

System requirements
* ansible
* docker
* `apt install docker-compose-plugin`
  OR
  `pip install docker-compose`

Some files required from the `https://data.kitware.com` Girder service require
authentication due to their protected nature.
The environment variable `GIRDER_API_KEY` must be defined with a valid API key,
otherwise an authentication token cannot be retrieved.

We currently require a pip-installed `docker-compose` tool, as opposed to a
system package-manager installed `docker-compose-plugin` package.
The package manager docker plugin behaves a little differently that our current
docker-compose configuration and scripting does not yet handle.

## Provision Files
External large files should be provisioned by running the ansible tool:

    ./angel-provision-files.sh

This may include large files for running the system, like ML model files, or
other files required for building docker images.

This provisioning may require additional configuration and variables set in
your environment in order to satisfy some permissions:
* `GIRDER_API_KEY` will need to be set in order to acquire protected files from
  `data.kitware.com`.

The configuration that controls what is staged and where is located
in the `ansible/roles/provision-files/vars/main.yml` file.

## Developing with Submodules
We incorporate various submodules in this repository to reference externally
developed or maintained packages.

Some of these packages however are things that we have authored to support this
system but in a modular way to decouple their usage and functionality from this
system.
We may desire to develop or fix aspects of those submodules while developing or
testing aspects of ANGEL System.
We can test/develop/fix these submodules right in their checked-out locations
within this repository.
* `cd` into the submodule in question.
* [Optional] Update the current, or checkout any alternative branch as desired.
* [Optional] Check out a new branch to house development for the submodule's
  repository.
* Modify submodule content as desired.
* Run `./angel-workspace-shell.sh`, build and test as desired.


## Docker-based Workflow
**Intention**: Use containerization to standardize development and runtime
environment and practice.
* Docker is utilized for containerization.
* Docker-compose is used to manage container build and run  
  configurations.

Docker functionality is located under the `./docker/` directory.
* `workspace-base-dev` provides a base environment that supports building and
  running our workspace.
* `workspace-build` provides a build of our workspace.

### Building Docker Images
Quick-start
```bash
./angel-docker-build.sh
```

### Developing within Docker
Quick-start
```bash
# Start the containerized environment shell.
./angel-workspace-shell.sh
# Build your local workspace for use.
./workspace_build.sh
```

To develop adequately in docker, the container environment needs to have access
to the source code that is being edited, likely by an IDE on the host.
Running `./angel-workspace-shell.sh` will create a temporary docker container
and drop into a bash shell.
This script has some usage output that can be seen by passing a `-h`/`--help`
option.
By default, the `workspace-shell-dev-gpu` service will be run.
The definition of this service is found in the `docker/docker-compose.yml`
configuration.

This will mount the `./ros/` subtree on the host system into the
`/angel_workspace/src/` directory in the run container.

This ""workspace"" context additionally mounts build, install and log output
directories to a spot on the host in order to:
* persist between shell start-ups and shutdowns
* facilitate build sharing between multiple workspace shells

Due to this, there will not be a build available upon first shell start-up.
The script `/angel_workspace/workspace_build.sh` is available to run.
This script is used during the image build process, so using this script
ensures that the same build method is performed.

Other directories and files are mounted into the container environment for
development purposes and external file sharing.

This shell will **_NOT_** have the local installation sourced in order to
facilitate further safe build actions.
`ros2 run` actions should be performed in a separate shell from where builds
are performed.

**_NOTE_** that any new system requirements may of course be installed locally,
but these will be lost upon container shutdown.
* Additional core requirements should be reflected in the
  `./docker/workspace-build/` image definition.
* Changes to the workspace build process should be reflected in the
  `./docker/workspace-build/` image definition.
* Additional development only dependencies should be added to the
  `./docker/workspace-base-dev` image definition.

### Container Cyclone DDS configuration
A basic template config may be auto-generated to specify which host network
interface for it to use by uncommenting and setting the `CYCLONE_DDS_INTERFACE`
in the `docker/.env` or exporting it on your commandline to the desired value.

## Run Configurations -- Tmuxinator
The ""standard"" way to set up and run collections of nodes in ROS2 is via launch
files.
This is all well and good and provides great features, but what it doesn't do
is provide access to individual components at runtime.
During development, and often in the field as well, it is important to be able
to get at individual components to see what might be going wrong or to
restart/tweak one thing without impacting everything else that is running.
We utilize tmux, configured and run by tmuxinator, to manage multiple
windows/panes to host individual components.

Tmuxinator configurations are stored in the `./tmux/` directory.

### Example: Object detection system fragment
```bash
./angel-workspace-shell.sh -r -- tmuxinator start fragment_object_detection_debug
```
Anatomy of the call:
1) (Re)uses `angel-workspace-shell.sh` to run a command in a docker container.
2) `-r` option to script sets ""use our built workspace"" flag.
3) Things to the right of `--` are run inside the container.
4) `tmux` directory is (by default) in the same directory as the working
   directory the container starts in.
5) `tmuxinator` command creates a new server session defined by the given
   config.

To stop the system after using the above command, we simply need to exit the
tmux instance or kill the tmux session.
This can be done by providing the tmux keyboard command `<Ctrl-B, D>`.
Alternatively, in a new or existing pane, running `tmux kill-session` or
`tmux kill-server`.
When using the above example ""start"" command, exiting the tmux session will
also shut down the container.

If starting an `angel-workspace-shell.sh -r` first, dropping into a bash
terminal, and _**then**_ calling `tmuxinator start fragment_object_detection_debug`,
we have the option to exit the tmux session and stop it using another
tmuxinator command:
```bash
tmuxinator stop fragment_object_detection_debug
```

## Configuring ROS nodes that utilize SMQTK-Core
Some ROS nodes utilize the smqtk-core plugin system so specification of what
algorithm is utilized is determined based on a configuration file.

Configuration files are currently located in: `ros/angel_system_nodes/configs/`
* `default_object_det_config.json`
  * Determines the image object detection plugin used in the
    `angel_system_nodes object_detector.py` node.
* `default_activity_det_config.json`
  * Determines the activity detection plugin used in the
    `angel_system_nodes activity_detector.py` node.

## Setting up the foot pedal for annotations
The `annotation_event_monitor` ROS node uses the up and down arrow keys to
generate `AnnotationEvent` messages. These messages can be used to determine
when the beginning and end of an activity or error occur during a recording.

To help with this process, a foot pedal can be used to map foot pedal presses
to keyboard presses. For this system, we are using the [Infinity 3 USB foot pedal].

To configure your Linux system to recognize foot pedal presses as keyboard
presses, see this [guide].

In the .hwdb file, make sure to map the keyboard presses to the up and down arrow keys.

## ANGEL System Python Package

`angel_system/` contains the interfaces and implementations for the
various components in the ANGEL system python package.

### Running PTG evaluation
See `angel_system/eval/README.md` for details.

## Lessons Learned
### `rosdep`
References to the lists that rosdep uses to resolve names:
/etc/ros/rosdep/sources.list.d/20-default.list


[Infinity 3 USB foot pedal]: https://www.amazon.com/Infinity-Digital-Control-Computer-USB2/dp/B002MY6I7G?th=1
[guide]: https://catswhisker.xyz/log/2018/8/27/use_vecinfinity_usb_foot_pedal_as_a_keyboard_under_linux/
[hl2ss]: https://github.com/jdibenes/hl2ss",VRAI
pulumi/automation-api-examples,Documentations,Documentations,2025-02-10T22:51:30Z,2022-10-04T14:09:50Z,0,0,19,0,0,0,0,0,2020-09-01T21:20:06Z,2025-04-07T00:43:41Z,56143,229,Go,VRAI,58,FAUX,23,,23,Examples for the Pulumi Automation API https://pkg.go.dev/github.com/pulumi/pulumi/sdk/v3/go/auto?tab=doc,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,25,"# automation-api-examples

This repo provides full end to end examples and walk-throughs for the Pulumi Automation API. The Automation API is available for `Go`, `Node.js`, `Python`, `C#`, and `Java`.

Full docs for automation API can be found here:
- [Go](https://pkg.go.dev/github.com/pulumi/pulumi/sdk/v3/go/auto?tab=doc)
- [Node.js](https://www.pulumi.com/docs/reference/pkg/nodejs/pulumi/pulumi/automation/)
- [Python](https://www.pulumi.com/docs/reference/pkg/python/pulumi/#module-pulumi.automation)
- [C#](https://www.pulumi.com/docs/reference/pkg/python/pulumi/#module-pulumi.automation)
- [Java](https://www.pulumi.com/docs/reference/pkg/java/com/pulumi/automation/package-summary.html)

## Content

Take a look at our examples grouped by language.

### Go Examples

Example   | Description |
--------- | --------- |
[Git Repo](go/git_repo_program) | Use Automation API with a Pulumi program from a git repo. In this case a static S3 website from the [Pulumi examples repo](https://github.com/pulumi/examples/tree/master/aws-go-s3-folder).
[Inline Program](go/inline_program) | Use Automation API with an `inline` Pulumi program. Inline programs are self contained in a single `main.go` and support full debugging capabilities. In this demo we deploy the same static S3 website adapted from the [Pulumi examples repo](https://github.com/pulumi/examples/tree/master/aws-go-s3-folder).
[Local Program](go/local_program) | This example shows how to use Automation API with an existing traditional CLI-driven Pulumi program. We add an Automation API deployment program to our Fargate program that deploys a web service via a Fargate task behind a load balancer.
[Inline/Local Hybrid Program](go/inline_local_hybrid) | This example shows how to refactor your infrastructure to get the best of both worlds, a debuggable `inline` program that can still be driven by the Pulumi CLI for convenience (one-off deployments, inspecting the stack, retrieving outputs, etc). In this example we deploy an S3 static website. The `automation/main.go` is fully debuggable, including the shared deployment function. The stack can also be managed via the CLI program in `cli/main.go`.
[Multi-Stack Orchestration](go/multi_stack_orchestration) | This example shows how to use Automation API to tame the complexity of multiple stacks with dependent stack outputs. We decompose our S3 static website into two stacks, one that manages the bucket, and another that manages the `index.html` file. Both of these are defined as inline programs, and are deployed and destroyed together via a single `main.go`
[Pulumi Over HTTP - Infra as RESTful resources](go/pulumi_over_http) | This application demonstrates how to run Automation API in an HTTP server to expose infrastructure as RESTful resources. In our case, we've defined and exposed a static website `site` that exposes all of the `CRUD` operations plus list. Users can hit our REST endpoint and create custom static websites by specifying the `content` field in the `POST` body. All of our infrastructure is defined in `inline` programs that are constructed and altered on the fly based on input parsed from user-specified `POST` bodies.
[Database Migration](go/database_migration) | This example provisions an AWS Aurora SQL database and executes a database ""migration"" using the resulting connection info. This migration creates a table, inserts a few rows of data, and reads the data back to verify the setup. This is all done in a single program using an `inline` Pulumi program. With Automation API you can orchestrate complex workflows that go beyond infrastructure provisioning and into application management, database setup, etc.
[Cloud-backed Secret Provider](go/inline_secrets_provider) | This example demonstrates an inline program using a cloud-backed (KMS) secret provider.
[Passphrase Secret Provider](go/inline_passphrase_secrets_provider) | This example demonstrates an inline program using a passphrase secret provider.
[Remote Deployment](go/remote_deployment) | This example demonstrates how to use Automation API to run Pulumi programs remotely with Pulumi Deployments. In this case a static S3 website from the [Pulumi examples repo](https://github.com/pulumi/examples/tree/master/aws-ts-s3-folder).


### Node.js Examples

Example  | Toolchain | Description |
--------- | --------- | --------- |
[Inline Program](nodejs/inlineProgram-tsnode) | Typescript + ts-node | Use Automation API with an `inline` Pulumi program. Inline programs are self-contained in a single `index.ts` and support full debugging capabilities. In this demo we deploy the same static S3 website adapted from the [Pulumi examples repo](https://github.com/pulumi/examples/tree/master/aws-ts-s3-folder). This example uses `typescript` with `ts-node` as an execution environment.
[Inline Program](nodejs/inlineProgram-ts) | Typescript (tsc) + node | Use Automation API with an `inline` Pulumi program. Inline programs are self-contained in a single `index.ts` and support full debugging capabilities. In this demo we deploy the same static S3 website adapted from the [Pulumi examples repo](https://github.com/pulumi/examples/tree/master/aws-ts-s3-folder). This example uses `typescript` compiled into `javascript` via `tsc` and executed via `node`.
[Inline Program](nodejs/inlineProgram-js) | Javascript + node | Use Automation API with an `inline` Pulumi program. Inline programs are self-contained in a single `index.js` and support full debugging capabilities. In this demo we deploy the same static S3 website adapted from the [Pulumi examples repo](https://github.com/pulumi/examples/tree/master/aws-js-s3-folder). This example uses plain `javascript` executed via `node`.
[Local Program](nodejs/localProgram-tsnode) | Typescript + ts-node | This example shows how to use Automation API with an existing traditional CLI-driven Pulumi program. We add an Automation API deployment program to our existing CLI-driven S3 website program. This example uses `typescript` with `ts-node` as an execution environment.
[Cross-Language Program](nodejs/crossLanguage-tsnode) | Typescript + ts-node | This example shows how to use Automation API in `typescript` with an existing traditional CLI-driven Pulumi program written in a __different__ language, in this case `go`. We add an Automation API deployment program to our Fargate program that deploys a web service via a Fargate task behind a load balancer. This automation program uses `typescript` with `ts-node` as an execution environment.
[Pulumi Over HTTP - Infra as RESTful resources](nodejs/pulumiOverHttp-ts) | Typescript (tsc) + node | This application demonstrates how to run Automation API in an HTTP server to expose infrastructure as RESTful resources. In our case, we've defined and exposed a static website `site` that exposes all of the `CRUD` operations plus list. Users can hit our REST endpoint and create custom static websites by specifying the `content` field in the `POST` body. All of our infrastructure is defined in `inline` programs that are constructed and altered on the fly based on input parsed from user specified `POST` bodies.
[Database Migration](nodejs/databaseMigration-ts) | Typescript (tsc) + node | This example provisions an AWS Aurora SQL database and executes a database ""migration"" using the resulting connection info. This migration creates a table, inserts a few rows of data, and reads the data back to verify the setup. This is all done in a single program using an `inline` Pulumi program. With Automation API you can orchestrate complex workflows that go beyond infrastructure provisioning and into application management, database setup, etc.
[Local Program with mocha tests](nodejs/localProgram-tsnode-mochatests) | Typescript + ts-node | This example shows how to use Automation API with an existing traditional CLI-driven Pulumi program alongside some mocha-based integration tests to ensure that the infrastructure was set up properly. This example uses `typescript` with `ts-node` as an execution environment, with `mocha` being used to run the tests.
[Remote Deployment](nodejs/remoteDeployment-tsnode) | Typescript + ts-node | This example demonstrates how to use Automation API to run Pulumi programs remotely with Pulumi Deployments. In this case a static S3 website from the [Pulumi examples repo](https://github.com/pulumi/examples/tree/master/aws-ts-s3-folder).

### Python Examples

Example  | Description |
--------- | --------- |
[Inline Program](python/inline_program) | Use Automation API with an `inline` Pulumi program. Inline programs are self contained in a single `main.py` and support full debugging capabilities. In this demo we deploy the same static S3 website adapted from the [Pulumi examples repo](https://github.com/pulumi/examples/tree/master/aws-py-s3-folder).
[Cross-Language Program](python/cross_language) | This example shows how to use Automation API in `python` with an existing traditional CLI-driven Pulumi program written in a __different__ language, in this case `go`. We add an Automation API deployment program to our Fargate program that deploys a web service via a Fargate task behind a load balancer.
[Database Migration](python/database_migration) | This example provisions an AWS Aurora SQL database and executes a database ""migration"" using the resulting connection info. This migration creates a table, inserts a few rows of data, and reads the data back to verify the setup. This is all done in a single program using an `inline` Pulumi program. With Automation API you can orchestrate complex workflows that go beyond infrastructure provisioning and into application management, database setup, etc.
[Local Program](python/local_program) | This example shows how to use Automation API with an existing traditional CLI-driven Pulumi program. We add an Automation API deployment program to our existing CLI-driven app described in the [aws-py-voting-app](https://github.com/pulumi/examples/tree/master/aws-py-voting-app) example.
[Pulumi Over HTTP - Infra as RESTful resources](python/pulumi_over_http) | This application demonstrates how to run Automation API in an HTTP server to expose infrastructure as RESTful resources. In our case, we've defined and exposed a static website `site` that exposes all of the `CRUD` operations plus list. Users can hit our REST endpoint and create custom static websites by specifying the `content` field in the `POST` body. All of our infrastructure is defined in `inline` programs that are constructed and altered on the fly based on input parsed from user specified `POST` bodies.
[Pulumi Via Jupyter](python/pulumi_via_jupyter) | This example explores running Pulumi through a Jupyter Notebook.
[Remote Deployment](python/remote_deployment) | This example demonstrates how to use Automation API to run Pulumi programs remotely with Pulumi Deployments. In this case a static S3 website from the [Pulumi examples repo](https://github.com/pulumi/examples/tree/master/aws-ts-s3-folder).

### .NET Examples

Example  | Description |
--------- | --------- |
[Inline Program](dotnet/InlineProgram) | Use Automation API with an `inline` Pulumi program. Inline programs are self contained in a .NET console application and support full debugging capabilities. In this demo we deploy the same static S3 website adapted from the [Pulumi examples repo](https://github.com/pulumi/examples/tree/master/aws-cs-s3-folder).
[Local Program](dotnet/LocalProgram) | This example shows how to use Automation API with an existing traditional CLI-driven Pulumi program. We add an Automation API deployment program to our existing CLI-driven S3 website program.
[Cross-Language Program](dotnet/CrossLanguage) | This example shows how to use Automation API in `dotnet` with an existing traditional CLI-driven Pulumi program written in a __different__ language, in this case `go`. We add an Automation API deployment program to our Fargate program that deploys a web service via a Fargate task behind a load balancer.
[Database Migration](dotnet/DatabaseMigration) | This example provisions an AWS Aurora SQL database and executes a database ""migration"" using the resulting connection info. This migration creates a table, inserts a few rows of data, and reads the data back to verify the setup. This is all done in a single program using an `inline` Pulumi program. With Automation API you can orchestrate complex workflows that go beyond infrastructure provisioning and into application management, database setup, etc.
[Remote Deployment](dotnet/RemoteDeployment) | This example demonstrates how to use Automation API to run Pulumi programs remotely with Pulumi Deployments. In this case a static S3 website from the [Pulumi examples repo](https://github.com/pulumi/examples/tree/master/aws-ts-s3-folder).


### Java Examples

Example  | Description |
--------- | --------- |
[Inline Program](java/inlineProgram) | Use Automation API with an `inline` Pulumi program. Inline programs are self contained in a Java console application and support full debugging capabilities. In this demo we deploy a static S3 website.
[Local Program](java/localProgram) | This example shows how to use Automation API with an existing traditional CLI-driven Pulumi program. We add an Automation API deployment program to our existing CLI-driven S3 website program.
[Database Migration](java/databaseMigration) | This example provisions an AWS Aurora SQL database and executes a database ""migration"" using the resulting connection info. This migration creates a table, inserts a few rows of data, and reads the data back to verify the setup. This is all done in a single program using an `inline` Pulumi program. With Automation API you can orchestrate complex workflows that go beyond infrastructure provisioning and into application management, database setup, etc.

## Other projects using Automation API

Project | Description |
--- | ---
[Ploy](https://github.com/jaxxstorm/ploy) | Ploy is a CLI used to deploy a local Docker image to an EKS cluster.
[Halloumi](https://github.com/pulumi/halloumi) | Pulumi + Heroku = Halloumi. You write your application, we run it in the cloud.
[Self Service Platyform](https://github.com/komalali/self-service-platyform) | A webapp skeleton for building your own Infrastructure Platform using Python and Flask.

If you have a project using Automation API that you'd like to showcase here please submit a PR!",FAUX
pulumi/docs,Toolkit,Documentations,2025-05-15T23:38:49Z,2025-05-14T07:41:11Z,0,0,185,0,0,0,0,0,2017-09-21T15:27:51Z,2025-04-08T01:51:36Z,2329537,134,HTML,VRAI,238,FAUX,563,,563,All things Pulumi docs!,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,435,"<p align=""center"">
  <a href=""https://www.pulumi.com?utm_campaign=pulumi-docs-github-repo&utm_source=github.com&utm_medium=top-logo"" title=""Pulumi Documentation - Build and Deploy Infrastructure as Code Solutions on Any Cloud"">
    <img src=""https://www.pulumi.com/images/logo/logo-on-white-box.svg?"" width=""350"">
   </a>

  [![Slack](http://www.pulumi.com/images/docs/badges/slack.svg)](https://slack.pulumi.com?utm_campaign=pulumi-docs-github-repo&utm_source=github.com&utm_medium=slack-badge)
  [![GitHub Discussions](https://img.shields.io/github/discussions/pulumi/pulumi)](https://github.com/pulumi/pulumi/discussions)
  [![License](https://img.shields.io/github/license/pulumi/pulumi)](LICENSE)
  ![Deployment Status](https://github.com/pulumi/docs/actions/workflows/build-and-deploy.yml/badge.svg?branch=master&event=push)
  ![Examples Tests](https://github.com/pulumi/docs/actions/workflows/scheduled-test.yml/badge.svg?branch=master)

# Pulumi Documentation Site

## Table of contents

- :blue_book: [View Pulumi Docs](https://pulumi.com/docs/?utm_campaign=pulumi-docs-github-repo&utm_source=github.com&utm_medium=docs-toc)
- :clap: [Contributing](#Contributing)
- :toolbox:	[Setup and Development](#setup-and-development)
  - [Generating SDK and CLI documentation](#generating-sdk-and-cli-documentation)
- :busts_in_silhouette: [Pulumi Community](#community)
- :blue_book: [Pulumi Developer Resources](#pulumi-resources)
- :compass:	[Pulumi Roadmap](#pulumi-roadmap)

## About this repository

This repository hosts all of the hand-crafted documentation, guides, tutorials, blogs, and landing pages that you see on [https://pulumi.com](https://pulumi.com?utm_campaign=pulumi-docs-github-repo&utm_source=github.com&utm_medium=about-docs), as well as all of the assets and templates we use to render the Pulumi website. It also houses the documentation that we generate for the Pulumi CLI and language SDKs, and it's responsible for building and deploying the website (with Pulumi, of course!).

### What's not in this repository

* Pulumi AI: You'll find the open-source components of the Pulumi AI project at https://github.com/pulumi/pulumi-ai.
* Pulumi Registry: You'll find everything related to the Registry at https://github.com/pulumi/registry.

## Contributing

We welcome all contributions to this repository. Be sure to read our [contributing guide](CONTRIBUTING.md) and [code of conduct](CODE-OF-CONDUCT.md) first, then [submit a pull request](https://github.com/pulumi/docs/pulls) here on GitHub. If you see something that needs fixing but don't have time to contribute, you can also [file an issue](https://github.com/pulumi/docs/issues).

See also:

* [Publishing a Pulumi blog post](./BLOGGING.md)
* [Documentation and coding style guide](./STYLE-GUIDE.md)

# Setup and Development

### Toolchain

We build the Pulumi website with Hugo, manage our dependencies with Node.js and Yarn, and write our documentation in Markdown. Below is a list of the tools you'll need if you'd like to work on the website (e.g., to contribute docs content, a blog post, etc.):

* [Hugo](https://gohugo.io/installation/) (>= 0.126.0)
  * Hugo 0.126.0 is highly recommended. This is the version we use in our deployment pipelines.
* [Node.js](https://nodejs.org/en/download/package-manager) (>= 18)
* [Yarn](https://classic.yarnpkg.com/lang/en/docs/install) (1.x)

Additionally, to build the SDK and CLI documentation, you'll also need:

* [Go](https://golang.org/) (>= 1.21)
* [Python](https://www.python.org) (>= 3.7)
* [.NET](https://dotnet.microsoft.com/en-us/download) (>= 6)
* [Pulumi](https://www.pulumi.com/docs/install)
* [Pulumi ESC](https://www.pulumi.com/docs/install/esc)

### Repository layout

* **Documentation and page content**: We generally follow Hugo's [directory-structure conventions](https://gohugo.io/getting-started/directory-structure/), with Markdown files in `./content`, layout files (including partials and shortcodes) in `./layout`, and data files in `./data`. There are also several [Hugo templates](https://gohugo.io/content-management/archetypes/) available in `./archetypes` for bootstrapping common content types like blog posts and Learn modules.

* **CSS and JavaScript**: We build our CSS and JavaScript bundles separately from Hugo and check in the built artifacts at `./assets`. We use [Tailwind](https://tailwindcss.com/) for CSS, [Stencil](https://stenciljs.com/) for web components, and [jQuery](https://jquery.com/) for wiring things together in general. Source files for these are in `./theme`.

* **Examples**: Many of the examples we include in our documentation are maintained as full Pulumi programs and tested daily. You'll find them all at `./static/programs`.

* **Infrastructure**: We deploy the website as a statically built artifact to a unique Amazon S3 bucket on every commit to the base branch of this repo. The Pulumi program that handles this is located in `./infrastructure`. This is also where you'll find the CloudFront configuration that handles proxying [Pulumi AI](https://pulumi.com/ai) and the [Pulumi Registry](https://pulumi.com/registry).

### Using the Makefile

The `Makefile` exposes a number of useful helpers for authoring:

* `make ensure` resolves and installs all dependencies
* `make lint` checks all Markdown files for correctness
* `make format` formats all applicable files to ensure they conform to style guidelines
* `make serve` runs the Hugo server locally at http://localhost:1313 and watches for changes. You can set `BUILD_FUTURE=false` to simulate production behavior by excluding future-dated content (e.g., `BUILD_FUTURE=false make serve`)
* `make serve-all` does the same as `make serve`, but also watches for changes to CSS and JS source files
* `make build` generates the website and writes it to `./public`
* `make build-assets` builds only the CSS and JavaScript asset bundles
* `make serve-static` runs a local HTTP server that serves the contents of `./public`
* `make test` tests all of the programs in `./static/programs` (see `./scripts/programs/test.sh` for options)
* `make generate` builds the TypeScript, Python, and Pulumi CLI documentation
* `make new-blog-post` scaffolds a new, bare-bones blog post with placeholder content
* `make new-tutorial` scaffolds a new single-page tutorial
* `make new-tutorial-module` scaffolds a new multi-page tutorial
* `make new-tutorial-topic` scaffolds a new tutorial topic and adds it to an existing multi-page tutorial
* `make new-example-program` generates a new multi-language set of examples at `./static/programs`
* `make new-dev-stack` creates a new dev stack (in the `pulumi` organization, which you must belong to)
* `make deploy-dev-stack` runs a build, deploys to S3, runs the tests, and deploys to the selected dev stack

As a content contributor, the commands you'll use most often are these:

```bash
make ensure    # Install or update dependencies.
make serve     # Run the development server locally on http://localhost:1313.
make lint      # Identify any Markdown or code-formatting issues so you can fix them.
```

## Generating SDK and CLI documentation

We generate two kinds of reference documentation with this repository: language-specific SDK docs (for a subset of Pulumi packages) and CLI docs (for command-line tools like `pulumi` and `esc`). Instructions for generating both types of docs are listed below.

### SDK docs

We build and host language-specific SDK documentation for the following Pulumi packages:

* [pulumi](https://github.com/pulumi/pulumi)
* [pulumi-awsx](https://github.com/pulumi/pulumi-awsx)
* [pulumi-kubernetesx](https://github.com/pulumi/pulumi-kubernetesx)
* [pulumi-policy](https://github.com/pulumi/pulumi-policy)
* [pulumi-terraform](https://github.com/pulumi/pulumi-terraform)

The Node.js, Python, and .NET versions of these docs are built using language-specific tooling and checked into the repository as stand-alone docsets. (Go versions are sourced directly from GitHub and hosted at [pkg.go.dev](/github.com/pulumi/pulumi/sdk/v3/go/pulumi).)

To build the docs for these packages yourself, you'll first need to clone each package into a sibling directory. The easiest way to do this is to use the `make update-repos` helper:

```bash
# Clone and update all of the repositories above into sibling directories of this repo.
make update-repos
```

Once you've done this, you can generate the docs for each package.

### Generating the Node.js and Python SDK docs

The Node and Python SDK docs are built with [TypeDoc](http://typedoc.org/) and [Pydocgen](https://pypi.org/project/pydocgen/). The easiest way to generate these docs is to use the `make generate` helper:

```bash
make ensure          # Install dependencies.
make update-repos    # Clone and update all package repositories.
make generate        # Generate the Node.js and Python docs for all packages.
```

Generated docs are rendered into the `./static-prebuilt/nodejs` and `./static-prebuilt/python` folders. At deploy-time, we copy the contents of these folders into `./docs/reference/pkg` to make them available on pulumi.com -- for example, [here](https://www.pulumi.com/docs/reference/pkg/nodejs/pulumi/pulumi) and [here](https://www.pulumi.com/docs/reference/pkg/python/pulumi).

See below to learn how to view these rendered docs locally.

### Generating the .NET SDK docs

The .NET SDK docs are built with [Docfx](https://github.com/dotnet/docfx). To generate these, you'll need both `dotnet` and `docfx` installed and on your PATH. For example, assuming you've already [installed the `dotnet` executable](https://dotnet.microsoft.com/en-us/download) for your platform, you can:

```bash
make ensure                     # Install dependencies.
make update-repos               # Clone and update all package repositories.
dotnet tool install -g docfx    # Install docfx globally, following the instructions to ensure it's on your PATH.
docfx build docfx/docfx.json    # Generate the .NET docs.
```

### CLI docs

The `make generate` helper also generates the Pulumi CLI documentation. If you'd prefer not to use that helper (e.g., to avoid having to clone all the repos and generate SDK docs), you can build them directly using the `pulumi` and `esc` CLIs:

```bash
pulumi gen-markdown ./content/docs/cli/commands    # Generate Pulumi CLI documentation.
esc gen-docs ./content/docs/esc-cli/commands       # Generate Pulumi ESC CLI documentation.
```

Generated docs reflect the functionality of the currently installed CLI, so make sure you've installed the latest public version of each one ([`pulumi`](https://github.com/pulumi/pulumi/releases), [`esc`](https://github.com/pulumi/esc/releases)) before running these commands and submitting your PR.

### Viewing rendered SDK and CLI docs locally

After building the SDK and/or CLI docs, you can view them locally with `make build` and `make serve-static`.

For example, from a fresh clone of this repository, you can install all dependencies and generate and browse the Node.js, Python, and Pulumi CLI docs using the following sequence:

```bash
make ensure          # Install dependencies.
make update-repos    # Clone and update all package repositories.
make generate        # Generate the Node.js, Python, and Pulumi CLI docs.
make build           # Build the website, copying all generated docs into place.
make serve-static    # Serve the built website statically to make sure everything looks right.
```

With `make serve-static` running, you can browse to the docs by navigating to http://localhost:8080/docs. Then, from the left-hand menu:

* Choose Languages &amp; SDKs followed by your language of choice, then scroll to the bottom of the page to find the package you're interested in
* Choose Pulumi CLI or Pulumi ESC CLI, then Commands

### Checking in generated docs

All generated docs, including all Node.js, Python, and .NET SDK docs and all Pulumi and Pulumi ESC CLI docs, get checked into this repository.

## Search

We use [Algolia](https://www.algolia.com/) for search, and we update the Algolia search index [on every deployment](https://github.com/pulumi/docs/blob/master/scripts/ci-push.sh#L13) of the website. Whether you're adding a new page or updating an existing one, your changes will be reflected in search results within a few seconds of release.

### Creating findable content

We currently index every page of the website, including the blog and the Registry. However, we do not index all of the content of every page &mdash; we only index certain properties of the page. These include:

* Page titles (specifically the `title` and `h1` frontmatter params)
* Page descriptions (specifically the `meta_desc` param)
* Second-level headings (e.g., those prefixed with `##` in Markdown files)
* Keywords, if any (via the `search.keywords` param)
* Authors, if any (via the `authors` param)
* Tags, if any (via the `tags` param)

Because of this, it's important to be thoughtful about the terms you use for these fields, especially titles, keywords, descriptions, and H2 headings. If you want your content to be findable by specific terms, you must make sure those terms exist in one or more of the fields listed above.

For example, if you were writing a guide to building an ETL pipeline with Redshift, and you wanted to make sure the page would be surfaced for queries like `redshift data warehouse etl`, you might construct the page's frontmatter in the following way:

```yaml
title: Build an ETL pipeline with Redshift and AWS Glue
meta_desc: Learn how to combine AWS Glue and Amazon Redshift to build a fully-automated ETL pipeline with Pulumi.
search:
    keywords:
        - data warehouse
```

In this case, the optional `search.keywords` field is included to cover the terms `data warehouse`, as those terms don't exist in the page's title or description. If it weren't, queries for `data warehouse` would fail to match this particular page.

Certain fields also rank higher than others in terms of their overall relevance. (Titles and keywords, for example, are considered more relevant than descriptions.) For a full list of these rankings, along with all of the rules we apply to the search index, see the [search app in pulumi/docs](https://github.com/pulumi/docs/blob/master/scripts/search/settings.js).

### Keeping pages out of search results

To keep a page from showing up in search results (including on Google, etc.), use the `block_external_search_index` frontmatter parameter:

```yaml
title: My page
...
block_external_search_index: true
```
## Community

Engage with our community to elevate your developer experience:

- **Join our online [Pulumi Community on Slack](https://slack.pulumi.com/?utm_campaign=pulumi-docs-repo&utm_source=github.com&utm_medium=welcome-slack)** - Interact with thousands of Pulumi developers for collaborative problem-solving and knowledge-sharing!
- **Join a [Local Pulumi User Groups (PUGs)](https://www.meetup.com/pro/pugs/)**-  Attend tech-packed meetups and hands-on virtual or in-person workshops.
- **Follow [@PulumiCorp](https://twitter.com/PulumiCorp) on X (Twitter)** - Get real-time updates, technical insights, and sneak peeks into the latest features.
- **Subscribe to our YouTube Channel, [PulumiTV](https://www.youtube.com/@PulumiTV)** - Learn about AI / ML essentials, launches, workshops, demos and more.
- **Follow our [LinkedIn](https://www.linkedin.com/company/pulumi/?utm_campaign=pulumi-docs-github-repo&utm_source=github.com&utm_medium=docs-community)** - Uncover company news, achievements, and behind-the-scenes glimpses.

## Pulumi developer resources

Delve deeper into Pulumi with additional resources:

- [Get Started with Pulumi](https://www.pulumi.com/docs/get-started/?utm_campaign=pulumi-docs-github-repo&utm_source=github.com&utm_medium=docs-resources): Deploy a simple application in AWS, Azure, Google Cloud, or Kubernetes using Pulumi.
- [Registry](https://www.pulumi.com/registry/?utm_campaign=pulumi-docs-github-repo&utm_source=github.com&utm_medium=docs-resources): Search for packages and learn about the supported resources you need. Install the package directly into your project, browse the API documentation, and start building.
- [Pulumi Blog](https://www.pulumi.com/blog/?utm_campaign=pulumi-docs-github-repo&utm_source=github.com&utm_medium=docs-resources) - Stay in the loop with our latest tech announcements, insightful articles, and updates.
- [Try Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-docs-github-repo&utm_source=github.com&utm_medium=docs-resources) - Use natural-language prompts to generate Pulumi infrastructure-as-code programs in any language.

## Pulumi roadmap

Review the planned work for the upcoming quarter and a selected backlog of issues that are on our mind but not yet scheduled on the [Pulumi Roadmap.](https://github.com/orgs/pulumi/projects/44)",VRAI
pulumi/examples,Documentations,Documentations,2025-05-13T23:50:52Z,2025-04-03T03:10:29Z,0,0,274,0,0,0,0,0,2017-10-27T19:50:31Z,2025-04-08T04:26:13Z,155029,2459,TypeScript,VRAI,878,FAUX,158,"aws,azure,gcp,kubernetes,serverless",158,"Infrastructure, containers, and serverless apps to AWS, Azure, GCP, and Kubernetes... all deployed with Pulumi",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,187,"<p align=""center"">
  <a href=""https://www.pulumi.com?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=top-logo"" title=""Pulumi Examples - Build and Deploy Infrastructure as Code Solutions on Any Cloud"">
    <img src=""https://www.pulumi.com/images/logo/logo-on-white-box.svg?"" width=""350"">
  </a>
</p>

  [![Slack](http://www.pulumi.com/images/docs/badges/slack.svg)](https://slack.pulumi.com?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=slack-badge)
  [![GitHub Discussions](https://img.shields.io/github/discussions/pulumi/pulumi)](https://github.com/pulumi/pulumi/discussions)
  [![License](https://img.shields.io/github/license/pulumi/pulumi)](LICENSE)

**Pulumi** is the easiest way to build and deploy infrastructure, for any architecture and on any cloud, using programming languages that you already know and love. Code and ship infrastructure faster with your favorite languages and tools, and embed IaC anywhere with [Automation API](https://www.pulumi.com/docs/guides/automation-api/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=about-pulumi).

Pulumi is open source under the [Apache 2.0 license](https://github.com/pulumi/pulumi/blob/master/LICENSE), supports many languages and clouds, and is easy to extend.

## Table of contents

- :rocket: [About This Repo](#about-the-pulumi-examples-repo)
- :toolbox:	[All Pulumi Examples](#all-pulumi-examples)
- :clap: [Contributors](#contributors)
- :busts_in_silhouette: [Pulumi Community](#community)
- :blue_book: [Pulumi Developer Resources](#pulumi-developer-resources)
- :compass:	[Pulumi Roadmap](#pulumi-roadmap)

# About the Pulumi Examples Repo
[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/pulumi/examples)

This repository contains examples of using Pulumi to build and deploy cloud applications and infrastructure across major programming languages.

Each example has a two-part prefix, `<cloud>-<language>`, to indicate which `<cloud>` and `<language>` it pertains to. For example, `<cloud>` could be:
 - `aws` for [Amazon Web Services](https://github.com/pulumi/pulumi-aws) 
 - `azure` for [Microsoft Azure](https://github.com/pulumi/pulumi-azure)
 - `gcp` for [Google Cloud Platform](https://github.com/pulumi/pulumi-gcp) 
 - `kubernetes` for [Kubernetes](https://github.com/pulumi/pulumi-kubernetes) 
 - `cloud` for [Pulumi's cross-cloud programming framework](https://github.com/pulumi/pulumi-cloud).

See the [Pulumi documentation](https://www.pulumi.com/docs/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) for more details on getting started with Pulumi.

## Checking out a single example

You can checkout only the example(s) you want by using a [sparse checkout](https://git-scm.com/docs/git-sparse-checkout). The following commands show how checkout only the `aws-go-fargate` example. Replace `aws-go-fargate` with your example of interest. 

```bash
$ mkdir examples && cd examples
$ git init
$ git remote add origin -f https://github.com/pulumi/examples/
$ git config core.sparseCheckout true
$ echo ""aws-go-fargate"" >> .git/info/sparse-checkout ## update this
$ git pull origin master
```

Don't see an example listed? [Try Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) and use natural-language prompts to generate Pulumi infrastructure-as-code programs in _any_ language.

## All Pulumi examples

- [AWS](#aws)
    - [TypeScript](#typescript)
    - [JavaScript](#javascript)
    - [Python](#python)
    - [Go](#go)
    - [C#](#c)
    - [F#](#f)
- [Azure](#azure)
    - [TypeScript](#typescript-1)
    - [Python](#python-1)
    - [Go](#go-1)
    - [C#](#c-1)
    - [F#](#f-1)
- [GCP](#gcp)
    - [TypeScript](#typescript-2)
    - [JavaScript](#javascript-1)
    - [Python](#python-2)
    - [Go](#go-2)
    - [C#](#c-2)
- [Kubernetes](#kubernetes)
    - [TypeScript](#typescript-3)
    - [JavaScript](#javascript-2)
    - [Python](#python-3)
    - [Go](#go-3)
    - [C#](#c-3)
- [Openstack](#openstack)
- [OVHCloud](#ovhcloud)
- [Cloud](#cloud)
- [DigitalOcean](#digitalocean)
- [Multicloud](#multicloud)
- [F5](#f5)
- [Twilio](#twilio)
- [Linode](#linode)
- [Testing](#testing)
- [Automation API](https://github.com/pulumi/automation-api-examples)

## AWS

### TypeScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[API Gateway](aws-ts-apigateway) | Deploy a simple REST API that counts the number of times a route has been hit.
[API Gateway HTTP API with routes](aws-ts-apigatewayv2-http-api) | Deploy a HTTP API that invokes a Lambda.
[API Gateway HTTP API quickstart](aws-ts-apigatewayv2-http-api-quickcreate) | Deploy a very simple HTTP API that invokes a Lambda.
[API Gateway V1 with EventBridge and Lambda](aws-ts-apigateway-eventbridge) | Deploy a REST API that uses EventBridge to target a Lambda function. Includes API Gateway model validation and custom integration-response mapping.
[API Gateway V2 with EventBridge and Lambda](aws-ts-apigatewayv2-eventbridge) | Deploy an HTTP API that uses EventBridge to target a Lambda function.
[Apigateway - Auth0](aws-ts-apigateway-auth0) | Deploy a simple REST API protected by Auth0.
[AppSync](aws-ts-appsync) | Deploy a basic GraphQL endpoint in AWS AppSync.
[AssumeRole](aws-ts-assume-role) | Use AssumeRole to create resources.
[Containers](aws-ts-containers) | Provision containers on Fargate.
[EKS - Dashboard](aws-ts-eks) | Deploy an EKS Kubernetes cluster with an EBS-backed StorageClass, then the Kubernetes Dashboard into the cluster.
[EKS - Hello World](aws-ts-eks-hello-world) | Deploy an EKS Kubernetes cluster with an EBS-backed StorageClass, then a Kubernetes namespace and nginx deployment into the cluster.
[EKS - Migrate Node Groups](aws-ts-eks-migrate-nodegroups) | Create an EKS cluster and node group to use for workload migration with zero downtime.
[Fargate](aws-ts-hello-fargate) | Build, deploy, and run a Dockerized app using ECS, ECR, and Fargate.
[Lambda Thumbnailer](aws-ts-lambda-thumbnailer) | Create a video thumbnail extractor using serverless functions.
[Miniflux](aws-ts-pulumi-miniflux) | Stand up an RSS Service using Fargate and RDS.
[Pulumi Webhooks](aws-ts-pulumi-webhooks) | Create a Pulumi `cloud.HttpEndpoint` that receives webhook events delivered by Pulumi Cloud, then echos the event to Slack.
[RDS and Airflow](aws-ts-airflow) | Deploy a RDS Postgres instance and containerized Airflow.
[Resources](aws-ts-resources) | Create various resources, including `cloudwatch.Dashboard`, `cloudwatch.EventRule`, `cloudwatch.LogGroup`, and `sqs.Queue`.
[Ruby on Rails](aws-ts-ruby-on-rails) | Create a single EC2 virtual machine instance with a local MySQL database.
[S3 Lambda](aws-ts-s3-lambda-copyzip) | Set up two AWS S3 Buckets and a single Lambda that listens to one and, upon each new object arriving in it, zips it up and copies it to the second bucket.
[Serverless Application](aws-ts-serverless-raw) | Deploy a complete serverless C# application using raw resources from `@pulumi/aws`.
[Serverless Datawarehouse](aws-ts-serverless-datawarehouse) | Deploy a serverless data warehouse.
[Slackbot](aws-ts-slackbot) | Create a simple slackbot that posts a notification to a specific channel any time you're @mentioned anywhere.
[Stack Reference](aws-ts-stackreference) | Create a ""team"" EC2 Instance with tags set from upstream stacks.
[Static Website](aws-ts-static-website) | Serve a static website using S3, CloudFront, Route53, and Certificate Manager.
[Step Functions](aws-ts-stepfunctions) | Use Step Functions with a Lambda function.
[Thumbnailer](aws-ts-thumbnailer) | Create a video thumbnail extractor using serverless functions and containers.
[Twitter](aws-ts-twitter-athena) | Query Twitter every 2 minutes, store the results in S3, and set up an Athena table and query.
[URL Shortener](aws-ts-url-shortener-cache-http) | Create a serverless URL shortener that uses high-level components.
[Voting App](aws-ts-voting-app) | Create a simple voting app using Redis and Python Flask.
[Web Server](aws-ts-webserver) | Deploy an EC2 Virtual machine using TypeScript to run a Python web server.
[Web Server with Manual Provisioning](aws-ts-ec2-provisioners) | Use Pulumi dynamic providers to accomplish post-provisioning configuration steps.
[LangServe - Hello OpenAI](aws-ts-langserve) | Deploy a LangServe app that uses OpenAI's on AWS ECS.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### JavaScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Containers](aws-js-containers) | Provision containers on Fargate.
[S3 Folder Component](aws-js-s3-folder-component) | Serve a static website on S3 from a component.
[S3 Folder](aws-js-s3-folder) | Serve a static website on S3.
[Servless SQS to Slack](aws-js-sqs-slack) | Wire up a serverless AWS Lambda to an AWS SQS queue and post a message to Slack.
[Web Server - Component](aws-js-webserver-component) | Deploy an EC2 instance using a common module for creating an instance.
[Web Server](aws-js-webserver) | Deploy an EC2 Virtual machine running a Python web server.
[LangServe - Hello OpenAI](aws-js-langserve) | Deploy a LangServe app that uses OpenAI's on AWS ECS.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### Python

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[API Gateway HTTP API quickstart](aws-py-apigatewayv2-http-api-quickcreate) | Deploy a very simple HTTP API that invokes a Lambda.
[API Gateway V2 with EventBridge and Lambda](aws-py-apigatewayv2-eventbridge) | Deploy an HTTP API that uses EventBridge to target a Lambda function.
[AppSync](aws-py-appsync) | Deploy a basic GraphQL endpoint in AWS AppSync.
[AssumeRole](aws-py-assume-role) | Use AssumeRole to create resources.
[Fargate](aws-py-fargate) | Provision a full ECS Fargate cluster running a load-balanced nginx web server.
[Resources](aws-py-resources) | Create various resources, including `cloudwatch.Dashboard`, `cloudwatch.EventRule`, `cloudwatch.LogGroup`, and `sqs.Queue`.
[S3 Folder](aws-py-s3-folder) | Serve a static website on S3.
[Stack Reference](aws-py-stackreference) | Create a ""team"" EC2 Instance with tags set from upstream stacks.
[Step Functions](aws-py-stepfunctions) | Use Step Functions with a Lambda function.
[Web Server](aws-py-webserver) | Deploy an EC2 instance and open port 80.
[LangServe - Hello OpenAI](aws-py-langserve) | Deploy a LangServe app that uses OpenAI's on AWS ECS.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### Go

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[AssumeRole](aws-go-assume-role) | Use AssumeRole to create resources.
[Fargate](aws-go-fargate) | Provision a full ECS Fargate cluster running a load-balanced nginx web server.
[Lambda](aws-go-lambda) | Create a lambda that does a simple `ToUpper` on the string input and returns it.
[S3 Folder](aws-go-s3-folder) | Serve a static website on S3.
[Web Server](aws-go-webserver) | Deploy an EC2 Virtual machine running a Python web server.
[LangServe - Hello OpenAI](aws-go-langserve) | Deploy a LangServe app that uses OpenAI's on AWS ECS.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### C#

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[AssumeRole](aws-cs-assume-role) | Use AssumeRole to create resources.
[Fargate](aws-cs-fargate) | Build, deploy, and run a Dockerized app using ECS, ECR, and Fargate.
[Lambda](aws-cs-lambda) | Create a lambda that does a simple `ToUpper` on the string input and returns it.
[S3 Folder](aws-cs-s3-folder) | Serve a static website on S3.
[Web Server](aws-cs-webserver) | Deploy an EC2 instance and open port 80.
[LangServe - Hello OpenAI](aws-cs-langserve) | Deploy a LangServe app that uses OpenAI's on AWS ECS.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### F#

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Lambda Web Server](aws-fs-lambda-webserver) | Create a web server in AWS lambda using the Giraffe web server.
[S3 Folder](aws-fs-s3-folder) | Serve a static website on S3.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## Azure

### TypeScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Azure Container Apps](azure-ts-containerapps) | Run a Docker image on Azure Container Apps.
[Azure Container Instance](azure-ts-aci) | Run Azure Container Instances on Linux.
[Azure Kubernetes Service](azure-ts-aks) | Create an Azure Kubernetes Service (AKS) Cluster.
[Azure App Service](azure-ts-appservice) | Build a web application hosted in App Service and provision Azure SQL Database and Azure Application Insights.
[Azure App Service with Docker](azure-ts-appservice-docker) | Build a web application hosted in App Service from Docker images.
[App Service in Virtual Network](azure-ts-webapp-privateendpoint-vnet-injection) | Deploy two App Services - Front web app with VNet injection and Back web app with a Private Endpoint.
[Azure Cosmos DB and LogicApp](azure-ts-cosmosdb-logicapp) | Define Cosmos DB, API connections, and link them to a logic app.
[Azure Functions](azure-ts-functions) | Deploy a Node.js serverless function to Azure Functions.
[Azure Functions - Many](azure-ts-functions-many) | Deploy several kinds of Azure Functions created from raw deployment packages.
[Azure SDK integration](azure-ts-call-azure-sdk) | Call Azure SDK functions from a Pulumi program.
[Static Website](azure-ts-static-website) | Configure static website hosting in Azure Storage.
[Azure Synapse](azure-ts-synapse) | Starting point for enterprise analytics solutions based on Azure Synapse.
[Web Server](azure-ts-webserver) | Provision a Linux web server in an Azure Virtual Machine.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### Python

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Azure Container Apps](azure-py-containerapps) | Run a Docker image on Azure Container Apps.
[Azure Container Instance](azure-py-aci) | Run Azure Container Instances on Linux.
[Azure Kubernetes Service](azure-py-aks) | Create an Azure Kubernetes Service (AKS) Cluster.
[Azure App Service](azure-py-appservice) | Build a web application hosted in App Service and provision Azure SQL Database and Azure Application Insights.
[Azure App Service with Docker](azure-py-appservice-docker) | Build a web application hosted in App Service from Docker images.
[Azure SDK integration](azure-py-call-azure-sdk) | Call Azure SDK functions from a Pulumi program in Python.
[Azure Cosmos DB and LogicApp](azure-py-cosmosdb-logicapp) | Define Cosmos DB, API connections, and link them to a logic app.
[Minecraft Server](azure-py-minecraft-server) | Deploy an Azure Virtual Machine and provision a Minecraft server.
[Static Website](azure-py-static-website) | Configure static website hosting in Azure Storage.
[Azure Synapse](azure-py-synapse) | Starting point for enterprise analytics solutions based on Azure Synapse.
[Virtual Data Center](azure-py-virtual-data-center) | Deploy Azure Virtual Data Center (VDC) hub-and-spoke network stacks in Azure, complete with ExpressRoute and VPN Gateways, Azure Firewall guarding a DMZ, and Azure Bastion.
[Web Server](azure-py-webserver) | Provision a Linux web server in an Azure Virtual Machine.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### Go

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Azure Container Apps](azure-go-containerapps) | Run a Docker image on Azure Container Apps.
[Azure Container Instance](azure-go-aci) | Run Azure Container Instances on Linux.
[Azure Kubernetes Service](azure-go-aks) | Create an Azure Kubernetes Service (AKS) Cluster.
[Azure App Service with Docker](azure-go-appservice-docker) | Build a web application hosted in App Service from Docker images.
[Static Website](azure-go-static-website) | Configure static website hosting in Azure Storage.
[Azure SDK integration](azure-go-call-azure-sdk) | Call Azure SDK functions from a Pulumi programin Go.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### C#

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
Cluster.
[Azure Container Apps](azure-cs-containerapps) | Run a Docker image on Azure Container Apps.
[Azure Container Instance](azure-cs-aci) | Run Azure Container Instances on Linux.
[Azure Kubernetes Service](azure-cs-aks) | Create an Azure Kubernetes Service (AKS) Cluster.
[AKS web app with .NET 5](azure-cs-net5-aks-webapp) | Create an Azure Kubernetes Service (AKS) cluster and deploy a web app to it using .NET 5 and C# 9.
[AKS + Cosmos DB](azure-cs-aks-cosmos-helm) | A Helm chart deployed to AKS that stores TODOs in an Azure Cosmos DB MongoDB API.
[Azure App Service](azure-cs-appservice) | Build a web application hosted in App Service and provision Azure SQL Database and Azure Application Insights.
[Azure App Service with Docker](azure-cs-appservice-docker) | Build a web application hosted in App Service from Docker images.
[Azure API integration](azure-cs-call-azure-api) | Call additional Azure API endpoints from a Pulumi program.
[Azure Cosmos DB and LogicApp](azure-cs-cosmosdb-logicapp) | Define Cosmos DB, API connections, and link them to a logic app.
[Azure Functions](azure-cs-functions) | Deploy a Node.js serverless function to Azure Functions.
[Static Website](azure-cs-static-website) | Configure static website hosting in Azure Storage.
[Azure Synapse](azure-cs-synapse) | Starting point for enterprise analytics solutions based on Azure Synapse.
[Azure SQL Server](azure-cs-sqlserver) | An example of a SQLServer on Azure PaaS.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## GCP

### TypeScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Cloud Run](gcp-ts-cloudrun) | Deploy a custom Docker image into Google Cloud Run service.
[Functions - Raw](gcp-ts-serverless-raw) | Deploy two Google Cloud Functions implemented in Python and Go.
[Functions](gcp-ts-functions) | Deploy an HTTP Google Cloud Function endpoint.
[GKE - Hello World](gcp-ts-gke-hello-world) | Deploy a GKE cluster, then a Kubernetes namespace and nginx deployment into the cluster.
[GKE](gcp-ts-gke) | Provision a Google Kubernetes Engine (GKE) cluster, then a Kubernetes Deployment.
[Ruby on Rails](gcp-ts-k8s-ruby-on-rails-postgresql) | Deliver a containerized Ruby on Rails application.
[Slackbot](gcp-ts-slackbot) | Create a simple slackbot that posts a notification to a specific channel any time you're @mentioned anywhere.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### JavaScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Web Server](gcp-js-webserver) | Build a web server in Google Cloud.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### Python

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Functions - Raw](gcp-py-serverless-raw) | Deploy two Google Cloud Functions implemented in Python and Go.
[Functions](gcp-py-functions) | Deploy a Python-based Google Cloud Function.
[GKE](gcp-py-gke) | Provision a Google Kubernetes Engine (GKE) cluster, then a Kubernetes Deployment.
[Network Component](gcp-py-network-component) | Use a reusable component to create a Google Cloud Network and instance.
[nginx Server](gcp-py-instance-nginx) | Build a nginx server in Google Cloud.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### Go

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Functions](gcp-go-functions) | Deploy a Go-based Google Cloud Function.
[Functions - Raw](gcp-py-serverless-raw) | Deploy a Google Cloud Function implemented in Python.
[Web Server](gcp-go-webserver) | Build a web server in Google Cloud.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### C#

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Functions - Raw](gcp-py-serverless-raw) | Deploy a Google Cloud Function implemented in Python.
[Functions](gcp-go-functions) | Deploy a Go-based Google Cloud Function.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## Kubernetes

### TypeScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[App Rollout via ConfigMap](kubernetes-ts-configmap-rollout) | Enable a change in a ConfigMap to trigger a rollout of an nginx Deployment.
[App Rollout via S3 Data Change](kubernetes-ts-s3-rollout) | Enable a change in data in S3 to trigger a rollout of an nginx deployment.
[Expose Deployment](kubernetes-ts-exposed-deployment) | Deploy nginx to a Kubernetes cluster, and publicly explose it using a Kubernetes Service.
[Guestbook](kubernetes-ts-guestbook) | Build and deploy a simple, multi-tier web application using Kubernetes and Docker.
[Jenkins](kubernetes-ts-jenkins) | Deploy a container running the Jenkins continuous integration system onto a running Kubernetes cluster.
[Multicloud](kubernetes-ts-multicloud) | Create managed Kubernetes clusters using AKS, EKS, and GKE, and deploy the application on each cluster.
[nginx server](kubernetes-ts-nginx) | Deploy a replicated nginx server to a Kubernetes cluster, using TypeScript and no YAML.
[Sock Shop](kubernetes-ts-sock-shop) | Deploy a version of the standard Sock Shop microservices reference app.
[Staged App Rollout](kubernetes-ts-staged-rollout-with-prometheus) | Create a staged rollout gated by checking that the P90 response time reported by Prometheus is less than some amount.
[Wordpress Helm Chart](kubernetes-ts-helm-wordpress) | Use the Helm API to deploy v2.1.3 of the Wordpress Helm Chart to a Kubernetes cluster.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### Python

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Guestbook](kubernetes-py-guestbook) | Build and deploy a simple, multi-tier web application using Kubernetes and Docker.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### C#

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Guestbook](kubernetes-cs-guestbook) | Build and deploy a simple, multi-tier web application using Kubernetes and Docker.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### Go

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Guestbook](kubernetes-go-guestbook) | Build and deploy a simple, multi-tier web application using Kubernetes and Docker.
[App Rollout via ConfigMap](kubernetes-go-configmap-rollout) | Enable a change in a ConfigMap to trigger a rollout of an nginx Deployment.
[Wordpress Helm Chart](kubernetes-go-helm-wordpress) | Use the Helm API to deploy v9.6.0 of the Wordpress Helm Chart to a Kubernetes cluster.
[Expose Deployment](kubernetes-go-exposed-deployment) | Deploy nginx to a Kubernetes cluster, and publicly expose it using a Kubernetes Service.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## Openstack

### Python

[🔝 Back to the list](#all-pulumi-examples)

[Web Server](openstack-py-webserver) | Deploy an Openstack instance and open port 8000.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## OVHCloud

### Go

| Example                              | Description |
|--------------------------------------| --------- |
| [Kubernetes](ovhcloud-go-kubernetes) | A sample to deploy a managed Kubernetes cluster on OVHcloud |

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## Cloud

### TypeScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[URL Shortener - Cache and HttpServer](cloud-ts-url-shortener-cache-http) | Create a simple URL shortener SPA that uses the high-level `cloud.Table` and `cloud.HttpServer` components.
[URL Shortener - Cache](cloud-ts-url-shortener-cache) | Create a simple URL shortener SPA that uses the high-level `cloud.Table` and `cloud.API` components.
[URL Shortener](cloud-ts-url-shortener) | Create a complete URL shortener web application that uses the high-level `cloud.Table` and `cloud.HttpServer` components.
[Voting App](cloud-ts-voting-app) | Create a simple voting app using Redis and Python Flask.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### JavaScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[API on AWS](cloud-js-api) | Create a simple REST API that counts the number of times a route has been hit.
[Containers](cloud-js-containers) | Provision containers on Fargate.
[HttpServer](cloud-js-httpserver) | Create a simple REST API that counts the number of times a route has been hit.
[Thumbnailer - Machine Learning](cloud-js-thumbnailer-machine-learning) | Create a video thumbnail extractor using serverless functions, containers, and AWS Rekognition.
[Thumbnailer](cloud-js-thumbnailer) | Create a video thumbnail extractor using serverless functions and containers.
[Twitter](cloud-js-twitter-athena) | Query Twitter every 2 minutes, store the results in S3, and set up an Athena table and query.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## DigitalOcean

### TypeScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Droplets](digitalocean-ts-loadbalanced-droplets) | Build sample architecture.
[Kubernetes](digitalocean-ts-k8s) | Provision a DigitalOcean Kubernetes cluster.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### Python

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Droplets](digitalocean-py-loadbalanced-droplets) | Build sample architecture.
[Kubernetes](digitalocean-py-k8s) | Provision a DigitalOcean Kubernetes cluster.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

### C#

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Droplets](digitalocean-cs-loadbalanced-droplets) | Build sample architecture.
[Kubernetes](digitalocean-cs-k8s) | Provision a DigitalOcean Kubernetes cluster.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## Multicloud

### TypeScript

[🔝 Back to the list](#all-pulumi-examples)

[Try Pulumi Copilot](https://app.pulumi.com/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) and use natural-language prompts to generate Pulumi example programs in _any_ language.

Example   | Description |
--------- | --------- |
[Buckets](multicloud-ts-buckets) | Use a single Pulumi program to provision resources in both AWS and GCP.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## F5

### TypeScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[BigIP Local Traffic Manager](f5bigip-ts-ltm-pool) | Provide load balancing via an F5 BigIP appliance to backend HTTP instances.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## Twilio

### TypeScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Component](twilio-ts-component) | Create a custom Component Resource to parse incoming messages from Twilio.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## Linode

### JavaScript

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
--------- | --------- |
[Web Server](linode-js-webserver) | Build a web server on Linode.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## Testing

[🔝 Back to the list](#all-pulumi-examples)

Example   | Description |
-----          | --------- |
[Unit Tests in TypeScript](testing-unit-ts)      | Mock-based unit tests in TypeScript.
[Unit Tests in Python](testing-unit-py)          | Mock-based unit tests in Python.
[Unit Tests in Go](testing-unit-go)              | Mock-based unit tests in Go.
[Unit Tests in C#](testing-unit-cs)              | Mock-based unit tests in C#.
[Testing with Policies](testing-pac-ts)          | Tests based on Policy-as-Code in TypeScript.
[Integration Testing in Go](testing-integration) | Deploy-check-destroy tests in Go.

[Use Pulumi AI](https://www.pulumi.com/ai/?utm_campaign=pulumi-examples-github-repo&utm_source=github.com&utm_medium=pulumi-examples) to build a new example in _any_ language.

## Automation API

[🔝 Back to the list](#all-pulumi-examples)

[Automation API Examples](https://github.com/pulumi/automation-api-examples)

## Community

Engage with our community to elevate your developer experience:

- **Join our online [Pulumi Community on Slack](https://slack.pulumi.com/?utm_campaign=pulumi-pulumi-examples-repo&utm_source=github.com&utm_medium=welcome-sla",FAUX
pulumi/pulumi,Toolkit,Application System,2025-05-15T22:24:14Z,2025-05-12T10:10:16Z,0,0,1,0,0,0,0,0,2016-10-31T21:02:47Z,2025-04-07T21:39:55Z,163874,22795,Go,VRAI,1179,FAUX,2297,"aws,azure,cloud,cloud-computing,containers,csharp,dotnet,fsharp,gcp,go,golang,iac,infrastructure-as-code,javascript,kubernetes,python,serverless,typescript",2297,Pulumi - Infrastructure as Code in any programming language 🚀,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,314,"<p align=""center"">
    <a href=""https://www.pulumi.com/?utm_source=github.com&utm_medium=referral&utm_campaign=pulumi-pulumi-github-repo&utm_content=top+logo"" title=""Pulumi - Modern Infrastructure as Code - AWS Azure Kubernetes Containers Serverless"">
        <img src=""https://www.pulumi.com/images/logo/logo-on-white-box.svg?"" width=""350"">
    </a>
</p>

[![Slack](http://www.pulumi.com/images/docs/badges/slack.svg)](https://slack.pulumi.com/)
[![GitHub Discussions](https://img.shields.io/github/discussions/pulumi/pulumi)](https://github.com/pulumi/pulumi/discussions)
[![NPM version](https://badge.fury.io/js/%40pulumi%2Fpulumi.svg)](https://npmjs.com/package/@pulumi/pulumi)
[![Python version](https://badge.fury.io/py/pulumi.svg)](https://pypi.org/project/pulumi)
[![NuGet version](https://badge.fury.io/nu/pulumi.svg)](https://badge.fury.io/nu/pulumi)
[![GoDoc](https://godoc.org/github.com/pulumi/pulumi?status.svg)](https://godoc.org/github.com/pulumi/pulumi)
[![License](https://img.shields.io/github/license/pulumi/pulumi)](LICENSE)

# Infrastructure as Code in any Programming Language

<a href=""https://www.pulumi.com/docs/iac/get-started/"">
    <img src=""https://www.pulumi.com/images/get-started.svg?"" align=""right"" width=""120"">
</a>

**Pulumi Infrastructure as Code** is the easiest way to build and deploy infrastructure, of any architecture and on any cloud, using programming languages that you already know and love. Code and ship infrastructure faster with your favorite languages and tools, and embed IaC anywhere with [Automation API](https://www.pulumi.com/docs/iac/using-pulumi/automation-api/).

Simply write code in your favorite language and Pulumi automatically provisions and manages your resources on
[AWS](https://www.pulumi.com/docs/iac/clouds/aws/),
[Azure](https://www.pulumi.com/docs/iac/clouds/azure/),
[Google Cloud Platform](https://www.pulumi.com/docs/iac/clouds/gcp/), 
[Kubernetes](https://www.pulumi.com/docs/iac/clouds/kubernetes/), and [120+ providers](https://www.pulumi.com/registry/) using an
[infrastructure-as-code](https://www.pulumi.com/what-is/what-is-infrastructure-as-code/) approach.
Skip the YAML, and use standard language features like loops, functions, classes,
and package management that you already know and love.

For example, create three web servers:

```typescript
const aws = require(""@pulumi/aws"");
const sg = new aws.ec2.SecurityGroup(""web-sg"", {
    ingress: [{ protocol: ""tcp"", fromPort: 80, toPort: 80, cidrBlocks: [""0.0.0.0/0""] }],
});
for (let i = 0; i < 3; i++) {
    new aws.ec2.Instance(`web-${i}`, {
        ami: ""ami-7172b611"",
        instanceType: ""t2.micro"",
        vpcSecurityGroupIds: [sg.id],
        userData: `#!/bin/bash
            echo ""Hello, World!"" > index.html
            nohup python -m SimpleHTTPServer 80 &`,
    });
}
```

Or a simple serverless timer that archives Hacker News every day at 8:30AM:

```typescript
const aws = require(""@pulumi/aws"");

const snapshots = new aws.dynamodb.Table(""snapshots"", {
    attributes: [{ name: ""id"", type: ""S"", }],
    hashKey: ""id"", billingMode: ""PAY_PER_REQUEST"",
});

aws.cloudwatch.onSchedule(""daily-yc-snapshot"", ""cron(30 8 * * ? *)"", () => {
    require(""https"").get(""https://news.ycombinator.com"", res => {
        let content = """";
        res.setEncoding(""utf8"");
        res.on(""data"", chunk => content += chunk);
        res.on(""end"", () => new aws.sdk.DynamoDB.DocumentClient().put({
            TableName: snapshots.name.get(),
            Item: { date: Date.now(), content },
        }).promise());
    }).end();
});
```

Many examples are available spanning containers, serverless, and infrastructure in
[pulumi/examples](https://github.com/pulumi/examples).

Pulumi is open source under the [Apache 2.0 license](https://github.com/pulumi/pulumi/blob/master/LICENSE), supports many languages and clouds, and is easy to extend.  This
repo contains the `pulumi` CLI, language SDKs, and core Pulumi engine, and individual libraries are in their own repos.

## Welcome

<img align=""right"" width=""400"" src=""https://www.pulumi.com/images/docs/quickstart/console.png"" />

* **[Get Started with Pulumi](https://www.pulumi.com/docs/iac/get-started/)**: Deploy a simple application in AWS, Azure, Google Cloud, or Kubernetes using Pulumi.

* **[Learn](https://www.pulumi.com/tutorials/)**: Follow Pulumi learning pathways to learn best practices and architectural patterns through authentic examples.

* **[Examples](https://github.com/pulumi/examples)**: Browse several examples across many languages,
  clouds, and scenarios including containers, serverless, and infrastructure.

* **[Docs](https://www.pulumi.com/docs/)**: Learn about Pulumi concepts, follow user-guides, and consult the reference documentation.

* **[Registry](https://www.pulumi.com/registry/)**: Find the Pulumi Package with the resources you need. Install the package directly into your project, browse the API documentation, and start building.

* **[Secrets Management](https://www.pulumi.com/product/secrets-management/)**: Tame secrets sprawl and configuration complexity securely across all your cloud infrastructure and applications with Pulumi ESC.

* **[Pulumi Roadmap](https://github.com/orgs/pulumi/projects/44)**: Review the planned work for the upcoming quarter and a selected backlog of issues that are on our mind but not yet scheduled.

* **[Community Slack](https://slack.pulumi.com/)**: Join us in Pulumi Community Slack. All conversations and questions are welcome.

* **[GitHub Discussions](https://github.com/pulumi/pulumi/discussions)**: Ask questions or share what you're building with Pulumi.

## <a name=""getting-started""></a>Getting Started

[![Watch the video](/youtube_preview_image.png)](https://www.youtube.com/watch?v=6f8KF6UGN7g)

See the [Get Started](https://www.pulumi.com/docs/iac/get-started/) guide to quickly get started with
Pulumi on your platform and cloud of choice.

Otherwise, the following steps demonstrate how to deploy your first Pulumi program, using AWS
Serverless Lambdas, in minutes:

1. **Install**:

    To install the latest Pulumi release, run the following (see full
    [installation instructions](https://www.pulumi.com/docs/iac/download-install/) for additional installation options):

    ```bash
    $ curl -fsSL https://get.pulumi.com/ | sh
    ```

2. **Create a Project**:

    After installing, you can get started with the `pulumi new` command:

    ```bash
    $ mkdir pulumi-demo && cd pulumi-demo
    $ pulumi new hello-aws-javascript
    ```

    The `new` command offers templates for all languages and clouds.  Run it without an argument and it'll prompt
    you with available projects.  This command created an AWS Serverless Lambda project written in JavaScript.

3. **Deploy to the Cloud**:

    Run `pulumi up` to get your code to the cloud:

    ```bash
    $ pulumi up
    ```

    This makes all cloud resources needed to run your code.  Simply make edits to your project, and subsequent
    `pulumi up`s will compute the minimal diff to deploy your changes.

4. **Use Your Program**:

    Now that your code is deployed, you can interact with it.  In the above example, we can curl the endpoint:

    ```bash
    $ curl $(pulumi stack output url)
    ```

5. **Access the Logs**:

    If you're using containers or functions, Pulumi's unified logging command will show all of your logs:

    ```bash
    $ pulumi logs -f
    ```

6. **Destroy your Resources**:

    After you're done, you can remove all resources created by your program:

    ```bash
    $ pulumi destroy -y
    ```

To learn more, head over to [pulumi.com](https://pulumi.com/) for much more information, including
[tutorials](https://www.pulumi.com/tutorials/), [examples](https://github.com/pulumi/examples), and
details of the core Pulumi CLI and [programming model concepts](https://www.pulumi.com/docs/iac/concepts/).

## <a name=""platform""></a>Platform

### Languages

|    | Language | Status | Runtime | Versions |
| -- | -------- | ------ | ------- | -------- |
| <img src=""https://www.pulumi.com/logos/tech/logo-js.png"" height=38 />     | [JavaScript](https://www.pulumi.com/docs/iac/languages-sdks/javascript/) | Stable  | Node.js | [Current, Active and Maintenance LTS versions](https://nodejs.org/en/about/previous-releases)  |
| <img src=""https://www.pulumi.com/logos/tech/logo-ts.png"" height=38 />     | [TypeScript](https://www.pulumi.com/docs/iac/languages-sdks/javascript/) | Stable  | Node.js | [Current, Active and Maintenance LTS versions](https://nodejs.org/en/about/previous-releases)  |
| <img src=""https://www.pulumi.com/logos/tech/logo-python.svg"" height=38 /> | [Python](https://www.pulumi.com/docs/iac/languages-sdks/python/)     | Stable  | Python | [Supported versions](https://devguide.python.org/versions/#versions) |
| <img src=""https://www.pulumi.com/logos/tech/logo-golang.png"" height=38 /> | [Go](https://www.pulumi.com/docs/iac/languages-sdks/go/)             | Stable  | Go | [Supported versions](https://go.dev/doc/devel/release#policy) |
| <img src=""https://www.pulumi.com/logos/tech/dotnet.svg"" height=38 />      | [.NET (C#/F#/VB.NET)](https://www.pulumi.com/docs/iac/languages-sdks/dotnet/)     | Stable  | .NET | [Supported versions](https://dotnet.microsoft.com/en-us/platform/support/policy/dotnet-core#lifecycle)  |
| <img src=""https://www.pulumi.com/logos/tech/java.svg"" height=38 />      | [Java](https://www.pulumi.com/docs/iac/languages-sdks/java/)     | Public Preview  | JDK | 11+  |
| <img src=""https://www.pulumi.com/logos/tech/yaml.svg"" height=38 />      | [YAML](https://www.pulumi.com/docs/iac/languages-sdks/yaml/)     | Stable  | n/a  | n/a  |

### EOL Releases

The Pulumi CLI v1 and v2 are no longer supported. If you are not yet running v3, please consider migrating to v3 to continue getting the latest and greatest Pulumi has to offer! :muscle:

* To migrate from v2 to v3, please see our [v3 Migration Guide](https://www.pulumi.com/docs/iac/download-install/migrating-3.0/).

### Clouds

Visit the [Registry](https://www.pulumi.com/registry/) for the full list of supported cloud and infrastructure providers.

## Contributing

Visit [CONTRIBUTING.md](https://github.com/pulumi/pulumi/blob/master/CONTRIBUTING.md) for information on building Pulumi from source or contributing improvements.",VRAI
pulumi/pulumi-policy,Toolkit,Toolkit,2025-05-12T07:01:33Z,2024-05-10T08:08:48Z,0,0,56,0,0,0,0,0,2019-05-23T22:47:52Z,2025-03-19T19:18:32Z,627,35,TypeScript,VRAI,5,FAUX,62,"javascript,policy,policy-as-code,pulumi,python,typescript",62,"Pulumi's Policy as Code SDK, CrossGuard. Define infrastructure checks in code to enforce security, compliance, cost, and other practices, enforced at deployment time.",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,23,"![Build Status](https://github.com/pulumi/pulumi-policy/actions/workflows/main.yml/badge.svg)

# Pulumi Policy SDK

## Overview

Define and manage policy for cloud resources deployed through Pulumi.

Policy rules run during `pulumi preview` and `pulumi up`, asserting that cloud resource definitions
comply with the policy immediately before they are created or updated. Policies may optionally define
remediations that automatically fix policy violations rather than issue warnings.

During `preview`, every rule is run on every resource, and policy violations are batched up
into a final report. During the update, the first policy violation will halt the deployment.

Policy violations can have enforcement levels that are **advisory**, which results in a printed
warning, or **mandatory**, which results in an error after `pulumi preview` or `pulumi up` completes.
The enforcement level **remediate** is stronger than both and enables automatic remediations.

## Getting Started

Please see [Get Started with Policy as Code](https://www.pulumi.com/docs/get-started/crossguard/) to get
started authoring and enforcing policies.

## Documentation

For additional documentation, guides, best practices, and FAQs, see [Policy as Code](https://www.pulumi.com/docs/guides/crossguard/).

## Examples

Looking for examples? Please refer to the [examples repo](https://github.com/pulumi/examples/tree/master/policy-packs).

## Languages

Policies can be written in TypeScript/JavaScript (Node.js) or Python and can be applied to Pulumi stacks written in any language.

|    | Language | Status |
| -- | -------- | ------ |
| <img src=""https://www.pulumi.com/logos/tech/logo-ts.png"" height=38 />     | [TypeScript](./sdk/nodejs) | Stable      |
| <img src=""https://www.pulumi.com/logos/tech/logo-js.png"" height=38 />     | [JavaScript](./sdk/nodejs) | Stable      |
| <img src=""https://www.pulumi.com/logos/tech/logo-python.png"" height=38 /> | [Python](./sdk/python)     | Preview     |
| <img src=""https://www.pulumi.com/logos/tech/dotnet.png"" height=38 />      | .NET                       | Coming Soon |
| <img src=""https://www.pulumi.com/logos/tech/logo-golang.png"" height=38 /> | Go                         | Coming Soon |",FAUX
pulumi/pulumi-self-hosted-installers,Toolkit,Documentations,2025-05-16T01:37:37Z,2025-04-29T13:20:13Z,0,0,116,0,0,0,0,0,2019-11-21T23:08:25Z,2025-04-04T16:46:35Z,2562,45,TypeScript,VRAI,9,FAUX,29,,29,Repository for getting started with self-hosted Pulumi Service.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,23,"# Self-Hosted Pulumi Service Installers

This repository contains installation guides for deploying the [Self-Hosted Pulumi Service](https://www.pulumi.com/product/self-hosted/) into a variety of different target environments.  The components of the Pulumi Service and general guidance on deploying and operating the service are documented in the [Self-Hosted Pulumi Service documentation](https://www.pulumi.com/docs/guides/self-hosted/).  Each guide details how to deploy the set of supporting cloud infrastructure on which the Pulumi Service can run, as well as how to deploy the container images needed to run the Pulumi Service.

The following guides are currently available:
* Quickstart ([Docker Compose](./quickstart-docker-compose))
* AWS ([EKS](./eks-hosted) or [ECS](./ecs-hosted))
* Azure ([AKS](./aks-hosted))
* Docker ([Docker Engine](./local-docker))
* Google Cloud ([GKE](./gke-hosted))
* VMware (Coming soon!)

Learn more about how to self-host Pulumi for your organization [here](https://www.pulumi.com/docs/guides/self-hosted/).",VRAI
pulumi/templates,Documentations,Documentations,2025-05-13T16:37:08Z,2024-10-02T12:53:56Z,0,0,69,0,0,0,0,0,2018-03-09T18:21:12Z,2025-03-23T03:58:06Z,2346,113,Go,VRAI,70,FAUX,113,,113,Templates used by `pulumi new`,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,75,"![Build Status](https://github.com/pulumi/templates/actions/workflows/ci.yml/badge.svg?branch=master)

# Pulumi Templates

This repo contains the templates for `pulumi new`, which make it easy to quickly get started building new Pulumi projects.

## Adding a new template

1. Create a new directory for the template, e.g. `my-template-javascript`. By convention, hyphens are used to separate words and the language is included as a suffix.

1. Add template files in the new directory. Note that when new projects are created from templates, all of the files contained in the template directory are copied into the resulting Pulumi project. Be sure to exclude any unnecessary files.

   Also note that dependency lockfiles like `package-lock.json` and `go.sum` are deliberately git-ignored to ensure that new projects always track with the latest Pulumi and provider SDKs.

1. If the template is an architecture template, include the requisite supplemental metadata at `./metadata`:

   - If the template adds to an existing template group (for example, if it's a new a `static-website-aws` template), add a new line for the template in the `templates` section of that group:

     ```diff
       name: AWS Static Website
       ...
       templates:
         ...
         - static-website-go
         - static-website-csharp
     -   - static-website-yaml
     +   - static-website-yaml
     +   - static-website-java
     ```

   - If the template introduces a new architecture, make a new entry in `./metadata/architectures.yaml` using the others as a guide (the keys and `slug` values correspond with these templates' eventual paths at <https://pulumi.com/templates>), then add a new file that lists the new template at `./metadata/groups/{architecture}-{cloud}-{language}.yaml`. Set the new group's `parent` property to match the key/name of the item you added to `architectures.yaml`.

1. Ensure the template applies sensible, conservative defaults for all configuration values. Ideally, users should be able to run `pulumi new --yes` with your template and get an immediately deployable project out of the box.

1. Ensure the template supports the _minimum_ runtime version for its associated language. Consult the [Languages & SDKs documentation](https://www.pulumi.com/docs/iac/languages-sdks/) for reference. (This is why our CI workflows use older runtimes. Every template in this repository should comply with this requirement.)

1. Request a review from the @pulumi/content-engineering team.

## Text replacement

The following special strings can be included in any template file; these will be replaced by the CLI when laying down the template files.

- `${PROJECT}` - The name of the project.
- `${DESCRIPTION}` - The description of the project.",FAUX
radius-project/bicep-types-aws,Toolkit,Toolkit,2025-04-04T17:38:28Z,2025-02-03T19:21:28Z,0,0,0,0,0,0,0,0,2022-08-30T22:38:05Z,2025-04-04T17:38:32Z,10310,8,TypeScript,VRAI,9,FAUX,6,,6,Bicep type support for AWS,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,15,,FAUX
RafaySystems/getstarted,Documentations,Documentations,2025-04-25T21:19:58Z,2024-05-15T13:46:14Z,0,0,0,0,0,0,0,56,2021-11-20T00:24:15Z,2025-03-11T15:13:06Z,18985,11,HCL,VRAI,24,FAUX,3,,3,Repo for Getting Started,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,10,"# Get Started
Git Repo with functioning examples to help you Get Started with the Rafay Kubernetes Operations Platform. 

## GPU on Amazon EKS
- Provision an Amazon EKS cluster with GPU worker node groups in minutes 
- Configure and provision a custom cluster blueprint with the Nvidia GPU Operator
- Deploy a GPU workload to the EKS cluster and view the integrated GPU dashboard 

## Amazon EKS with Cloudwatch 
- Provision an Amazon EKS Cluster with a custom cluster blueprint with the Cloudwatch addon 
- View metrics in Container Insights 

## Amazon EKS with Karpenter
- Provision an Amazon EKS Cluster with a custom cluster blueprint with the Karpenter cluster auto scaler",FAUX
RafaySystems/terraform-provider-rafay,DevOPs,Toolkit,2025-04-21T11:03:10Z,2025-03-27T15:24:28Z,0,0,0,0,0,0,0,1,2021-07-09T05:46:21Z,2025-04-08T06:59:33Z,7310,8,Go,VRAI,12,FAUX,35,,35,Rafay terraform provider ,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,67,"# terraform-provider-rafay
Rafay terraform provider 

## Authentication

The Rafay provider offers a flexible means of providing credentials for
authentication. The following methods are supported, in this order, and
explained below:

- Environment variables
- Credentials/configuration file


### Environment Variables

You can provide your credentials via the `RCTL_REST_ENDPOINT`, `RCTL_API_KEY`, and `RCTL_PROJECT` environment variables, representing your Rafay
Console Endpoint, Rafay Access Key and Rafay Project respectively.


```terraform
provider ""rafay"" {}
```

Usage:

```sh
$ export RCTL_API_KEY=""rafayaccesskey""
$ export RCTL_REST_ENDPOINT=""console.rafay.dev""
$ export RCTL_PROJECT=""defaultproject""
$ terraform plan
```
>! Note: For `RCTL_API_KEY`, use the entire output of the generated API key.

### Credentials/configuration file

You can use an [Rafay credentials or configuration file](https://docs.rafay.co/cli/config/#config-file) to specify your credentials. You can specify a location of the configuration file in the Terraform configuration by providing the `provider_config_file`  

Usage:

```terraform
provider ""rafay"" {
  provider_config_file = ""/Users/tf_user/rafay_config.json""
}
```


## Build provider

Run the following command to build the provider

```shell
$ make
```

## Test sample configuration

First, build and install the provider.

```shell
$ make install
```

Then, navigate to the `examples` directory. 

```shell
$ cd examples/resources/rafay_project/
```

Run the following command to initialize the workspace and apply the sample configuration.

```shell
$ terraform init && terraform apply
```

## Debug

```shell
export TF_LOG=TRACE
export TF_LOG_PATH=log.txt
```

List terraform states
```shell
terraform state
terraform state show <name>
```",VRAI
raffaelespazzoli/openshift-enablement-exam,Documentations,Documentations,2024-09-12T10:18:22Z,2024-08-29T14:37:36Z,0,0,0,0,7,0,0,2,2016-10-10T14:04:07Z,2024-09-12T10:18:34Z,11735,64,Shell,VRAI,50,FAUX,9,,9,,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,9,"# openshift-enablement-exam

The following instructions will setup an OpenShift OCP 3.3 environment on Google Cloud compliant with the following reference architecture.

![GCP reference architecture](./media/OSE-on-GCE-Architecture-v0.3.png)


attention: this is all deprecated now.

## 3.6 changes

updated to 3.6
service catalog does not seem to install so it's disabled
you can now choose to install gluster with the following variable GLUSTER=yes


## Setup

Clone this project

```
git clone https://github.com/raffaelespazzoli/openshift-enablement-exam
cd openshift-enablement-exam
```

Create a [new google cloud project](https://cloud.google.com/resource-manager/docs/creating-project).

Install the [command line tool](https://cloud.google.com/sdk/downloads).

[Initialize and authenticate in gcloud](https://cloud.google.com/sdk/docs/authorizing).

In order to run this provisioning script you will need to be able to run 34vCPU, 3 global static IP addresses, and 10 in-use IP addresses in the US central region. You may need to increase your [resource quota](https://cloud.google.com/compute/docs/resource-quotas).

Enable your project to use the compute api by visiting the [compute engine](https://console.cloud.google.com/home) menu item (there is probably a better way to do it).

Set your google project configuration
```
export GCLOUD_PROJECT=<your project>
```
set your dns zone (es: exam.example.com). The name of the zone should be the same name of your google project.
your master will be available at `master.exam.example.com` and the routes will have the form `*.apps.exam.example.com`.

you need to externally configure your domain to point to google cloud dns. More explanations [here] (https://cloud.google.com/dns/update-name-servers)
```
export DNS_DOMAIN=<your domain>
```
Set you RHN account credentials.
```
export RHN_USERNAME=rhn-gps-rspazzol
export RHN_PASSWORD=xxx
```
define which key you want to use, this key must be available to the following ssh commands
```
export SSH_PUB_KEY=<the ssh pub key you want to use> #usually $HOME/.ssh/id_rsa.pub
```
Set the RHEL pool you want to use:
```
export RHN_SUB_POOL=8a85f9843e3d687a013e3ddd471a083e
```
I recommend having a script that sets up all your variables:
```
export GCLOUD_PROJECT=openshift-enablement-exam2
export DNS_DOMAIN=gc2.raffa.systems
export RHN_USERNAME=rhn-gps-rspazzol
export RHN_PASSWORD=XXXX
export SSH_PUB_KEY=$HOME/.ssh/id_rsa.pub
export RHN_SUB_POOL=8a85f9843e3d687a013e3ddd471a083e
```
If you are courageous you can just run:
```
./allinone.sh
```
but at least the first time, I recommend following the below scripts.

Another option is to deploy using google cloud deployment (a declarative way of creating resources).
This is still a work in progress. Cd to `cloud-deployment` and run:
```
./gcp-cloud-provision.sh
```

Provisioning Gluster CNS

if you desire to provision Gluster CNS export the following variable
```
export GLUSTER=yes
```
This will create an addtional disk of 200GB in each of the nodes and deploy Gluster CNS there.


## Gcloud provisioning

Run the provisioning script.

```
./provision-gcp.sh
```
This will take some time.

## Prepare the bastion host

I've switched to preemptible instances and preemptible instances don't always start when provisioned (a bug?). Go to your google console and make sure all the instances are stared.

Run the prepare bastion script.
```
./prepare-bastion.sh
```

## Prepare the cluster

Shell in the bastion host
```
ssh -o SendEnv=RHN_USERNAME -o SendEnv=RHN_PASSWORD -o SendEnv=DNS_DOMAIN -o SendEnv=RHN_SUB_POOL -o SendEnv=GLUSTER `gcloud compute addresses list | grep ose-bastion | awk '{print $3}'`
```
Run the prepare cluster script
```
cd openshift-enablement-exam
./prepare-cluster.sh
```

## Setup openshift

Run the ansible playbook
```
ansible-playbook -v -i hosts /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml
```

## Creating new users

The admin/admin user is created by the installer.
You still have to give it permissions, for example (from one of the masters):
```
oc adm policy add-cluster-role-to-user cluster-admin admin
```
If you need to add more users, from the bastion host run the following
```
ansible 'masters' -i hosts -b -m shell -a ""htpasswd -b /etc/origin/master/htpasswd <username> <password>""
```
## Clean up

To clean up your Google Cloud project type the following:
```
./cleanup-gcp.sh
```
This may take some time.",FAUX
rancher/charts,Application System,Documentations,2025-05-16T04:33:54Z,2025-04-16T20:08:54Z,0,0,0,0,0,0,0,45,2018-04-02T03:21:06Z,2025-04-08T09:41:38Z,357203,402,Smarty,FAUX,720,FAUX,17,,17,Github based Helm Chart Index Repository providing charts crafted for Rancher Manager,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,142,"## Built-in Helm Chart Repository

This repository contains [Helm Charts](https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/helm-charts-in-rancher) served by the Rancher Manager - Apps & Marketplace.

It is currently maintained by the Release Team.

More information on [Wiki](https://github.com/rancher/charts/wiki)

> [!NOTE]
> [SUSE Rancher Prime](https://www.rancher.com/products/rancher-platform) subscription includes [SUSE Application Collection](https://apps.rancher.io); a curated, trusted, and up-to-date collection of developer and infrastructure applications built, packaged, tested, and distributed by SUSE through a platform with advanced security and automation features.",VRAI
rancher/kubewarden-ui,Application System,Application System,2025-05-13T08:27:19Z,2025-04-11T23:10:27Z,0,0,0,0,1,0,0,0,2021-11-08T15:54:48Z,2025-04-07T19:36:10Z,77231,12,TypeScript,VRAI,16,FAUX,22,"dashboard,hacktoberfest,kubernetes,kubernetes-security,rancher",22,Kubewarden's User Interface ,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,16,"[![Build & Release](https://github.com/rancher/kubewarden-ui/actions/workflows/build-extension-charts.yml/badge.svg)](https://github.com/rancher/kubewarden-ui/actions/workflows/build-extension-charts.yml)

[![E2E](https://github.com/rancher/kubewarden-ui/actions/workflows/playwright.yml/badge.svg?event=schedule)](https://github.com/rancher/kubewarden-ui/actions/workflows/playwright.yml?query=event%3Aschedule)
[![Unit tests](https://github.com/rancher/kubewarden-ui/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/rancher/kubewarden-ui/actions/workflows/unit-tests.yml)

# Kubewarden UI

This is an extension for [Rancher Manager](https://github.com/rancher/rancher) (`v2.7.0`) which allows you to interact with Kubewarden.

View the [Rancher UI Extension documentation](https://docs.kubewarden.io/next/howtos/ui-extension/install) for more in-depth info on how to use the UI. Visit the [Kubewarden docs](https://docs.kubewarden.io) for more insight on how to use Kubewarden.

## Installation

> This extension requires a Rancher version of `v2.7.0` or later, you can find the latest releases [here](https://github.com/rancher/rancher/releases).

The official [Rancher documentation](https://ranchermanager.docs.rancher.com/integrations-in-rancher/rancher-extensions#installing-extensions) has an extensive walkthrough on how to install an Extension. However, there are multiple ways of installing the Kubewarden Extension:

- Installing the Helm chart housed in the [Rancher Extension Repository](https://github.com/rancher/ui-plugin-charts) (Described in the [Rancher docs](https://ranchermanager.docs.rancher.com/integrations-in-rancher/rancher-extensions#installing-extensions))
- Installing the Helm chart within this repository (This method allows installing all Release Candidate builds)
- Deploying the Extension Catalog Image to mirror a Helm repository (Necessary for Air-Gapped clusters)

### Installing from the `Kubewarden/UI` repository

1. Navigate to the Extensions page from the side-nav and Enable the Extension support **without** adding the Rancher Extension Repository.
2. Once the Extension Operator has been installed click on the Action Menu in the top right of the screen and select ""Manage Repositories"".
3. Create a new Helm repository with the ""Target"" as a https URL pointing to the published domain for the UI Github Repository:

```console
https://rancher.github.io/kubewarden-ui/
```

4. Navigate back to the extensions screen and a card for Kubewarden should appear with available options for versions to install.

### Deploying the Extension Catalog Image

> This requires a Rancher Manager version of `v2.7.5` or greater

The Extension Catalog Image (ECI) is comprised of a hardened [SLE BCI](https://registry.suse.com/bci/bci-base-15sp4/index.html) image running an NGINX service which supplies the Helm charts as well as the minified extension files.More information can be found [here](https://rancher.github.io/dashboard/extensions/advanced/air-gapped-environments).

Released ECIs for Kubewarden can be found within the [packages](https://github.com/kubewarden/ui/pkgs/container/kubewarden-ui) of the UI repository. You will be able to deploy the ECI either from `ghcr.io` (e.g. `ghcr.io/kubewarden/kubewarden-ui:1.1.0`) or by mirroring the image into a registry that is accessible to the Rancher cluster.

#### Mirror the ECI into a Private Registry

1. Pull the image.

```console
docker pull ghcr.io/kubewarden/kubewarden-ui:1.1.0
```

2. Tag the image with the registry name

```console
docker tag ghcr.io/kubewarden/kubewarden-ui:1.1.0 my-registry.com/kubewarden/kubewarden-ui:1.1.0
```

3. Push the image to the registry

```console
docker push my-registry.com/kubewarden/kubewarden-ui:1.1.0
```

#### Deploy the ECI from the UI:

> Any Authentication needed for the registry **_MUST_** be created as a secret under the `cattle-ui-plugin-system` namespace.

1. Navigate to the Extensions page from the side-nav and Enable the Extension.
2. Once the Extension Operator has been installed click on the Action Menu in the top right of the screen and select ""Manage Extension Catalogs"".
3. Click on the ""Import Extension Catalog"" button to open the dialog.
4. Input the ECI reference including the version number into the ""Catalog Image Reference"" input.
5. Select any ""Pull Secrets"" necessary for the ECI to be pulled.
6. Once imported navigate back to the main Extensions and install Kubewarden extension as normal.

A more in-depth guide on importing an Extension Catalog Image can be found in the [Rancher Dashboard documentation](https://rancher.github.io/dashboard/extensions/advanced/air-gapped-environments#importing-the-extension-catalog-image).

## Developing

You will need to point the UI to a running instance of Rancher, here's a [quickstart guide](https://docs.ranchermanager.rancher.io/pages-for-subheaders/rancher-on-a-single-node-with-docker) for setting up Rancher in a Docker container.

### Run the dev environment

1. From the root directory, install the packages.

```sh
yarn install
```

2. Run the dashboard locally.

```sh
API=https://<rancher-host> yarn dev
```

### To test the build

1. Build the plugin.

```sh
yarn build-pkg kubewarden
```

3. In another terminal, serve the package.

```sh
yarn serve-pkgs kubewarden
```

4. Run the dashboard locally.

```sh
API=https://<rancher-host> yarn dev
```

Once your environment is running you will be able to load the plugin by navigating to the Extensions page from the Side Nav.

5. Load the plugin by choosing the ""Developer Load"" option from the Action Menu (3 dots), then inputing the provided url from the `serve-pkgs` command into the Extension URL input.",FAUX
rancher/rancher,DevOPs,Application System,2025-05-15T16:30:44Z,2025-05-07T13:15:57Z,0,0,0,0,0,2,0,0,2014-11-07T20:49:31Z,2025-04-08T08:06:17Z,163344,24002,Go,VRAI,3025,FAUX,3049,"cattle,containers,docker,kubernetes,orchestration,rancher",3049,Complete container management platform,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,374,"# Rancher

[![Docker Pulls](https://img.shields.io/docker/pulls/rancher/rancher.svg)](https://store.docker.com/community/images/rancher/rancher)
[![Go Report Card](https://goreportcard.com/badge/github.com/rancher/rancher)](https://goreportcard.com/report/github.com/rancher/rancher)

Rancher is an open source container management platform built for organizations that deploy containers in production. Rancher makes it easy to run Kubernetes everywhere, meet IT requirements, and empower DevOps teams.

## Stable Release

* v2.10
  * Stable - v2.10.3 - `rancher/rancher:v2.10.3` / `rancher/rancher:stable` - Read the full release [notes](https://github.com/rancher/rancher/releases/tag/v2.10.3).
* v2.9
  * Stable - v2.9.3 - `rancher/rancher:v2.9.3` - Read the full release [notes](https://github.com/rancher/rancher/releases/tag/v2.9.3).
* v2.8
  * Stable - v2.8.5 - `rancher/rancher:v2.8.5` - Read the full release [notes](https://github.com/rancher/rancher/releases/tag/v2.8.5).

To get automated notifications of our latest release, you can watch the announcements category in our [forums](http://forums.rancher.com/c/announcements), or subscribe to the RSS feed `https://forums.rancher.com/c/announcements.rss`.

## Quick Start

    sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher

Open your browser to https://localhost

## Installation

See [Installing/Upgrading Rancher](https://ranchermanager.docs.rancher.com/v2.8/pages-for-subheaders/installation-and-upgrade) for all installation options.

### Minimum Requirements

* Operating Systems
  * Please see [Support Matrix](https://rancher.com/support-matrix/) for specific OS versions for each Rancher version. Note that the link will default to the support matrix for the latest version of Rancher. Use the left navigation menu to select a different Rancher version. 
* Hardware & Software
  * Please see [Installation Requirements](https://ranchermanager.docs.rancher.com/v2.8/pages-for-subheaders/installation-requirements) for hardware and software requirements.

### Using Rancher

To learn more about using Rancher, please refer to our [Rancher Documentation](https://ranchermanager.docs.rancher.com/v2.8).

## Source Code

This repo is a meta-repo used for packaging and contains the majority of Rancher codebase. For other Rancher projects and modules, [see go.mod](https://github.com/rancher/rancher/blob/release/v2.8/go.mod) for the full list.

Rancher also includes other open source libraries and projects, [see go.mod](https://github.com/rancher/rancher/blob/release/v2.8/go.mod) for the full list.

## Build configuration

Refer to the [build docs](docs/build.md) on how to customize the building and packaging of Rancher.

## Support, Discussion, and Community
If you need any help with Rancher, please join us at either our [Rancher forums](http://forums.rancher.com/) or [Slack](https://slack.rancher.io/) where most of our team hangs out at.

Please submit any Rancher bugs, issues, and feature requests to [rancher/rancher](https://github.com/rancher/rancher/issues).

For security issues, please first check our [security policy](https://github.com/rancher/rancher/security) and email security-rancher@suse.com instead of posting a public issue in GitHub.  You may (but are not required to) use the GPG key located on [Keybase](https://keybase.io/rancher).

# License

Copyright (c) 2014-2025 [SUSE](http://rancher.com)

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.",VRAI
redhat-ai-services/ai-accelerator,Application System,Documentations,2025-05-09T19:24:05Z,2025-03-10T17:36:50Z,0,0,0,0,9,0,0,0,2024-03-01T23:43:59Z,2025-04-03T09:33:23Z,2906,49,Python,VRAI,88,FAUX,11,"gitops,openshift,rhoai",11,The AI Accelerator is a template project for setting up Red Hat OpenShift AI using GitOps,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,18,"# Red Hat AI Accelerator

Welcome to the AI Accelerator project source code. This project is designed to initialize an OpenShift cluster with a recommended set of operators and components that aid with training, deploying, serving and monitoring Machine Learning models.

This repo is intended to provide a core set of OpenShift features that would commonly be used for a Data Science environment, but can also be highly customized for specific scenarios. When starting out we recommend making a copy or a fork of this project on your Git based instance, since it utilizes the process of automating IT infrastructure using infrastructure as code and software development best practices such as Git, code review, and CI/CD - known as GitOps.

Once the initial components are deployed, several ArgoCD Application objects are created which are then used to install and manage the install of the operators on the cluster.

![AI Accelerator Overview](documentation/diagrams/AI_Accelerator.drawio.png)

## Additional Documentation and Info

* [Overview](documentation/overview.md) - what's inside this repository?
* [Installation Guide](documentation/installation.md) - containing step by step instructions for executing this installation sequence on your cluster

### Operators

* [Authorino Operator](components/operators/authorino-operator/)
* [NVIDIA GPU Operator](components/operators/gpu-operator-certified/)
* [Node Feature Discovery Operator](components/operators/nfd/)
* [OpenShift AI](components/operators/openshift-ai/)
* [OpenShift Pipelines](components/operators/openshift-pipelines/)
* [OpenShift Serverless](components/operators/openshift-serverless/)
* [OpenShift ServiceMesh](components/operators/openshift-servicemesh/)

### Applications

* OpenShift GitOps: [ArgoCD](components/argocd/)
* S3 compatible storage: [MinIO](components/apps/minio)

### Configuration

* [Bootstrap Overlays](bootstrap/overlays/)
* [Cluster Configuration Sets](clusters/overlays/)

### Tenants

* [Tenant Examples](tenants/)",VRAI
redhat-appstudio/infra-deployments,DevOPs,Documentations,2025-05-15T18:42:39Z,2025-05-12T21:33:40Z,0,0,0,0,6,0,0,0,2021-09-28T13:19:51Z,2025-04-08T15:44:33Z,9840,43,Shell,FAUX,266,FAUX,43,,43,"This repository is an initial set of Argo-CD-based deployments of AppStudio components to a cluster, plus a script to bootstrap Argo CD onto that cluster (to drive these Argo-CD-based deployments, via OpenShift GitOps).",FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,166,"# StoneSoup Infrastructure Deployments

For the full documentation click [here](https://redhat-appstudio.github.io/infra-deployments/docs/introduction/about.html)

When working on documentation updates, you may want to enable GitHub pages for your forked repository,
see the [GitHub docs: Publishing from a branch](https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-from-a-branch).
The site will be available at `https://<username>.github.io/<repository>` (`https://<username>.github.io/infra-deployments` if you didn't rename yor fork).

This repository is an initial set of Argo-CD-based deployments of StoneSoup components to a cluster, plus a script to bootstrap Argo CD onto that cluster (to drive these Argo-CD-based deployments, via OpenShift GitOps).

This repository is structured as a GitOps monorepo (e.g. the repository contains the K8s resources for *multiple* applications), using [Kustomize](https://kustomize.io/).

The contents of this repository are not owned by any single individual, and should instead be collectively managed and maintained through PRs by individual teams. More information about that can be found in the documentation section about how to [Extend The Service](https://redhat-appstudio.github.io/infra-deployments/docs/deployment/extending-the-service.html).",VRAI
redhat-cop/rego-policies,Documentations,Documentations,2025-05-15T09:15:20Z,2025-04-25T14:57:41Z,0,60,0,0,0,0,0,0,2020-05-11T02:59:34Z,2025-03-31T10:16:36Z,3319,165,Shell,VRAI,36,FAUX,5,"conftest,container-cop,gatekeeper,opa,rego",5,Rego policies collection,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,13,"[![Lint policies with OPA/Regal](https://github.com/redhat-cop/rego-policies/actions/workflows/regal-lint.yaml/badge.svg)](https://github.com/redhat-cop/rego-policies/actions/workflows/regal-lint.yaml)
[![Run conftest-unittests.sh](https://github.com/redhat-cop/rego-policies/actions/workflows/conftest-unittests.yaml/badge.svg)](https://github.com/redhat-cop/rego-policies/actions/workflows/conftest-unittests.yaml)
[![Run gatekeeper-k8s-integrationtests.sh](https://github.com/redhat-cop/rego-policies/actions/workflows/gatekeeper-k8s-integrationtests.yaml/badge.svg)](https://github.com/redhat-cop/rego-policies/actions/workflows/gatekeeper-k8s-integrationtests.yaml)
[![Run pre-commit](https://github.com/redhat-cop/rego-policies/actions/workflows/precommit-validate.yml/badge.svg)](https://github.com/redhat-cop/rego-policies/actions/workflows/precommit-validate.yml)
[![Scorecard supply-chain security](https://github.com/redhat-cop/rego-policies/actions/workflows/scorecard.yml/badge.svg)](https://github.com/redhat-cop/rego-policies/actions/workflows/scorecard.yml)

# rego-policies
[Rego](https://www.openpolicyagent.org/docs/latest/policy-language/) policies collection.

## Policies
For a full list of policies, see the auto-generated [POLICIES.md](POLICIES.md)

The naming of the policies follows the Gatekeeper format, as described [here.](https://github.com/plexsystems/konstraint/blob/main/docs/constraint_creation.md#resource-naming)

Want to run the policies on a k8s/OCP cluster? See [TESTING.md](TESTING.md)

## Tools
### Conftest
conftest is a CLI to execute rego policies. It can be used to test locally before pushing to [OPA](https://www.openpolicyagent.org/).
- [https://www.conftest.dev/install](https://www.conftest.dev/install/)

### OPA Playground
OPA provides a web based playground, which can highlight which lines have been activated. Having issues with your policy? check it out with ""Coverage"" enabled:
- [https://play.openpolicyagent.org](https://play.openpolicyagent.org)

### Slack for all things
Stuck on a problem?
- [https://slack.openpolicyagent.org/](https://slack.openpolicyagent.org/)",FAUX
redhat-na-ssa/hobbyist-guide-to-rhoai,Toolkit,Application System,2025-04-15T14:17:29Z,2024-07-17T19:45:53Z,0,0,0,0,2,0,0,0,2024-06-14T20:34:25Z,2025-04-03T18:18:41Z,40213,10,Shell,VRAI,8,FAUX,0,,0,This is the Hobbyist Guide to Installing and Configuring RHOAI for customers. Bring your towel.,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,4,"# Red Hat OpenShift AI BootCamp

[![Spelling](https://github.com/redhat-na-ssa/hobbyist-guide-to-rhoai/actions/workflows/spellcheck.yml/badge.svg)](https://github.com/redhat-na-ssa/hobbyist-guide-to-rhoai/actions/workflows/spellcheck.yml)

**Acronyms**:

- RHOCP = Red Hat OpenShift Container Platform
- RHOAI = Red Hat OpenShift AI
- ODH = Open Data Hub
- CR = Custom Resource

## About RHOAI

Red Hat OpenShift AI is a platform for data scientists and developers of artificial intelligence and machine learning applications.

OpenShift AI provides an environment to develop, train, serve, test, and monitor AI/ML models and applications on-premise or in the cloud. [More Info](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.13/html/introduction_to_red_hat_openshift_ai/index)

- Learn more about features and dependencies [(link)](/docs/info-features.md)

## Cluster Setup Steps

0. [Prerequisite](/docs/00-prerequisite.md)
1. [Add administrative user](/docs/01-add-administrative-user.md)
1. [Enable GPU support for RHOAI](/docs/02-enable-gpu-support.md)
1. [(Optional) Run sample GPU application](/docs/03-run-sample-gpu-application.md)
1. [Configure GPU dashboards](/docs/04-configure-gpu-dashboards.md)
1. [Configure GPU sharing method](/docs/05-configure-gpu-sharing-method.md)
1. [Install RHOAI Kserve dependencies](/docs/06-install-kserve-dependencies.md)
1. [Install RHOAI Operator and Components](/docs/07-install-rhoai-operator.md)
1. [Configure RHOAI / Data Science Pipelines](/docs/08-configure-rhoai.md)
1. [Configure distributed workloads](/docs/09-configure-distributed-workloads.md)

### Automation Key

To run all steps, from this repo's root directory, run below command

```sh
./scripts/setup.sh -s 0
./scripts/setup.sh -s 9
```

The following will setup RHOAI with GPUs on an OpenShift cluster on AWS

```sh
until oc apply -k https://github.com/redhat-na-ssa/demo-ai-gitops-catalog/demos/overlays/rhoai-workshop-ready?ref=v0.14 ; do : ; done
```

> For more comprehensive gitops functionality, check out below repository:
> [**demo-ai-gitops-catalog**](https://github.com/redhat-na-ssa/demo-ai-gitops-catalog)

## Demo Instructions

1. [Distributed Workloads](/docs/10-demo-distributed_workloads.md)

1. [Fraud Detection](/docs/11-demo-fraud-detection.md)",VRAI
redhat-openshift-ecosystem/certified-operators,Application System,Documentations,2025-05-16T01:43:35Z,2025-05-09T12:14:50Z,0,0,0,0,144,0,0,0,2021-09-27T17:16:03Z,2025-04-08T08:08:18Z,70330,52,Dockerfile,VRAI,479,FAUX,26,,26,Production catalog for Red Hat Certified Operator Bundles ,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,505,"# Red Hat certified operators production catalog
Production catalog for Red Hat Certified Operator Bundles

See our [documentation](https://redhat-openshift-ecosystem.github.io/operator-pipelines/) to
find out more about Certified operators and contribution.


# Operator bundle submission
A new operator bundle submission needs to follow a predefined directory
structure that is described below in this section. The new submission is
done through Github pull requests which can be either open by the [CI pipeline][ci-pipeline] or
manually by a user. The recommended way is to use the CI pipeline because it makes
sure all necessary data are provided in the correct format. Documentation about how
to use the pipeline is available in the pipeline repository.

## Pull request
A pull request with a new operator bundle needs to follow given rules to pass a tests:
 * Title format: `operator package-name (version)`
 * Pull request can only modify one operator
 * Pull request can't modify already merged operators
 * Associated images need to be pinned to specific image digest - tags are not allowed

Rules mentioned above are always followed when pull request is opened using CI pipeline.

## Directory structure
This repository contains an `operator` directory where all certified operators
are stored. Each operator has its directory with its package name. Inside the directory,
you have to provide a `ci.yaml` file that stores the necessary metadata for a successful
certification process. The format of the file is described below.

```bash
operators
└── kogito-operator
    ├── 1.6.0
    │   ├── manifests
    │   │   ├── app.kiegroup.org_kogitobuilds.yaml
    │   │   ├── app.kiegroup.org_kogitoinfras.yaml
    │   │   ├── app.kiegroup.org_kogitoruntimes.yaml
    │   │   ├── app.kiegroup.org_kogitosupportingservices.yaml
    │   │   ├── kogito-operator.clusterserviceversion.yaml
    │   │   └── kogito-operator-controller-manager_v1_serviceaccount.yaml
    │   └── metadata
    │       └── annotations.yaml
    └── ci.yaml
```

### Format of ci.yaml file
In the file, user needs to provide necessary details for the operator certification process.
Currently, the certification process supports the following options:

* `cert_project_id` - Identifier of certification project created through Red Hat Connect.
* `merge` - A boolean value that determines whether a new operator is automatically
merged when passed all required checks. (optional, default: `True`)

[ci-pipeline]:   https://github.com/redhat-openshift-ecosystem/operator-pipelines",VRAI
redhat-openshift-ecosystem/community-operators-prod,Toolkit,Documentations,2025-05-16T04:20:27Z,2025-05-14T16:47:31Z,0,0,0,0,0,0,0,0,2021-06-15T17:03:54Z,2025-04-08T13:46:47Z,222126,114,Dockerfile,VRAI,556,FAUX,5,,5,community-operators metadata backing OpenShift OperatorHub,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,928,"# Openshift Community Operators
[![License](http://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)

## About this repository

This repo is the canonical source for Kubernetes Operators that appear on [OpenShift Container Platform](https://openshift.com) and [OKD](https://www.okd.io/).

**NOTE** The index catalog `registry.redhat.io/redhat/community-operator-index:v<OCP Version>` is built from this repository and it is
consumed by Openshift and OKD to create their sources and built their catalog. To know more about how
Openshift catalog are built see the [documentation](https://docs.openshift.com/container-platform/4.14/operators/understanding/olm-rh-catalogs.html#olm-rh-catalogs_olm-rh-catalogs).

See our [documentation](https://redhat-openshift-ecosystem.github.io/operator-pipelines/) to find out more about Community operators and contribution.

## Add your Operator

We would love to see your Operator added to this collection. We currently use automated vetting via continuous integration plus manual review to curate a list of high-quality, well-documented Operators. If you are new to Kubernetes Operators start [here](https://sdk.operatorframework.io/build/).

## Contributing Guide

- [Prerequisites](https://redhat-openshift-ecosystem.github.io/operator-pipelines/users/contributing-prerequisites/)
- [Where to place operator](https://redhat-openshift-ecosystem.github.io/operator-pipelines/users/contributing-where-to/)
- [Creating pull request (PR)](https://redhat-openshift-ecosystem.github.io/operator-pipelines/users/contributing-via-pr/)
- [Operator Publishing / Review settings](https://redhat-openshift-ecosystem.github.io/operator-pipelines/users/operator-ci-yaml/)
- [OKD/OpenShift Catalogs criteria and options](https://redhat-openshift-ecosystem.github.io/operator-pipelines/users/packaging-required-criteria-ocp/)

## IMPORTANT NOTICE

Some APIs versions are deprecated and are OR will no longer be served. See https://redhat-openshift-ecosystem.github.io/operator-pipelines/#important-notice

## Reporting Bugs

Use the issue tracker in this repository to report bugs.",VRAI
replicatedhq/kURL,Toolkit,Toolkit,2025-05-10T01:36:55Z,2025-02-24T22:11:37Z,0,8,0,0,0,0,0,0,2019-07-25T20:41:29Z,2025-04-06T02:18:42Z,45599,777,Shell,VRAI,79,FAUX,70,"airgapped,bash,ceph,contour,kubeadm,kubernetes,kubernetes-cluster,kubernetes-installation,kubernetes-installer,kurl,rook",70,"Production-grade, airgapped Kubernetes installer combining upstream k8s with overlays and popular components",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,93,"<div align=""center"">
  <img alt=""Kurl-logo"" src=""https://kurl.sh/kurl_logo@2x.png"" />
</div>
<br/>

# kURL

kURL is a Kubernetes installer for airgapped and online clusters.

kURL relies on `kubeadm` to bring up the Kubernetes control plane, but there are a variety of tasks a system administrator must perform both before and after running kubeadm init in order to have a production-ready Kubernetes cluster, such as installing Docker, configuring Pod networking, or installing kubeadm itself.
The purpose of this installer is to automate those tasks so that any user can deploy a Kubernetes cluster with a single script.

## Getting Started

For more information please see [kurl.sh/docs/](https://kurl.sh/docs/)

## Community

For questions about using kURL, there's a [Replicated Community](https://help.replicated.com/community) forum, and a [#kurl channel in Kubernetes Slack](https://kubernetes.slack.com/channels/kurl).

## Notifications

kURL offers several optional [add-ons](https://kurl.sh/add-ons) for Kubernetes cluster creation.
These open-source technology add-ons are distributed under various open-source licenses.

One optional add-on available for object storage is [MinIO](https://github.com/minio/minio).
Use of MinIO is governed by the GNU AGPLv3 license that can be found in their [License](https://github.com/minio/minio/blob/master/LICENSE) file.

One optional add-on available for Metrics & Monitoring is Prometheus via the [Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator), which includes Grafana.
Use of Grafana is currently governed by the GNU AGPL v3 license that can be found in their [License](https://github.com/grafana/grafana/blob/main/LICENSE) file.

## Contributing

Contributions are greatly appreciated. See [CONTRIBUTING.md](CONTRIBUTING.md) or more details. 
Before starting any work, please either comment on an existing issue, or file a new one.

## Releases

For details on each release, see the [changelog](https://github.com/replicatedhq/kURL/releases).
For Replicated vendors, detailed release notes are available at [Kubernetes Installer Release Notes](https://docs.replicated.com/release-notes/rn-kubernetes-installer) on the Replicated documentation site.

Release assets and changelog are available on the [GitHub Releases](https://github.com/replicatedhq/kURL/releases) page.

Releases are created by a GitHub Workflow when a tag is pushed.
The tag should follow the date format `vYYYY.MM.DD-#`.

A new release, from HEAD, can be tagged by running the following command:

```shell
make tag-and-release
```

To tag and release a specific commit:

```shell
make COMMIT_ID=<GITHUB_SHA> tag-and-release
```

The `tag-and-release` Make task enforces the git tree to be clean and a tag to be created against
the `main` branch. To override this behavior call the underlying script directly:

```shell
./bin/tag-and-release.sh --commit-id=<GITHUB_SHA> --no-main --outdated
```

## Software Bill of Materials

Signed SBOMs for kURL Go and Javascript dependencies are combined into a tar file and are included with each release.

- **kurl-sbom.tgz** contains SBOMs for Go  and Javascript dependencies
- **kurl-sbom.tgz.sig** is the digital signature for kurl-sbom.tgz
- **key.pub** is the public key from the key pair used to sign kurl-sbom.tgz

The following example illustrates using [cosign](https://github.com/sigstore/cosign) to verify that **kurl-sbom.tgz** has
not been tampered with.

```shell
$ cosign verify-blob --key key.pub --signature kurl-sbom.tgz.sig kurl-sbom.tgz
Verified OK
```",FAUX
reply-fr/sustainable-personal-accounts,DevOPs,Application System,2024-09-19T14:54:08Z,2023-10-31T14:44:40Z,0,0,0,0,0,0,0,0,2021-11-25T12:37:51Z,2025-04-02T09:06:43Z,17867,46,Python,VRAI,7,FAUX,19,"accounts,aws,aws-codebuild,aws-control-tower,aws-finops,aws-incident-manager,aws-lambda-python,aws-organizations,aws-tags,innovation,management,python",19,"automate the control, the purge and the management of AWS accounts assigned permanently to selected employees - foster innovation from cloud teams",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,4,"# Sustainable Personal Accounts

## Sustainable Personal Accounts (SPA) automates AWS accounts assigned permanently to employees

With Sustainable Personal Accounts (SPA) you can control, purge and manage AWS accounts assigned permanently to employees of your enterprise.

![sustainable-personal-accounts](./media/sustainable-personal-accounts.drawio.png)

As a CTO/CCoE Leader/R&D Director:

- I recognize that innovative companies distribute sandbox AWS accounts to their staff
- I provide AWS accounts to cloud practitioners of my enterprise
- I manage these corporate resources at scale with SPA

As a person who benefits from a personal AWS account:

- After authentication, I connect to my account with `AWSAdministratorAccess` permission,
- A budget is set automatically to alert me by email,
- CodeBuild projects are executed on my behalf for purging cloud resources and for alignment with corporate policies.

As a DevOps in charge of SPA deployment:

- I drive the behavior of SPA from a single YAML file with all settings,
- I can adjust the Codebuild project used for account preparation to evolving corporate policies
- I can adjust the Codebuild project used for account purge to evolving corporate policies
- I can select when maintenance transactions are triggered,
- I can define settings for each organizational unit for easy scaling
- I can define settings for individual accounts for fine-grained tuning
- I can integrate tags from CSV files communicated by other teams (e.g., FinOps, Security, Enterprise Architecture)

As a SRE in charge of SPA operations:

- I can manage preventive controls on personal accounts with SCP
- An automated CloudWatch dashboard covers technical and functional monitoring in one place
- SPA activities, aka, console logins, account on-boarding, account maintenance, are reported as CSV files in S3 bucket every month
- FinOps reports are produced every month as CSV and Excel files in S3 bucket
- Management exceptions are managed interactively in Incident Manager
- Instant cost report is attached to incident records for contextual analysis
- Inventories of accounts are produced as CSV files in S3 bucket every week

As a FinOps engineer:

- I receive cost reports every month in my mailbox
- The Excel report consolidates costs per cost center and per organizational unit for easy analysis
- The CSV report is processed with custom software for automated show-back and charge-back

## SPA has been designed for AWS Organizations

Mandatory requirements:

- SPA is leveraging AWS Organization for events management and for account management across AWS accounts. The deployment of AWS Organizations can be managed by Amazon Control Tower, but Amazon Control Tower itself is not mandatory.
- SPA should be deployed on a dedicated AWS account. This facilitates the management of permissions accross a large number of AWS accounts, and contributes to the separation of concerns.
- SPA needs an assume role with permissions on the AWS Organization
- SPA needs an assume role with permissions within each AWS account that it manages

We recommend to deploy Amazon Control Tower and to benefit from cloud automation at scale on top of AWS Organizations, of AWS Service Catalog and of AWS IAM Identity Center (successor to AWS SSO). However, this is not a requirement for SPA, and you can deploy and use the solution without Amazon Control Tower.

## Get started with Sustainable Personal Accounts

A set of workbooks are available for common operations on Sustainable Personal Accounts, such as:

- [Full Setup of SPA](./workbooks/full-setup-of-spa.md) - Start here for initial installation
- [Manage preparation tasks](./workbooks/manage-preparation-tasks.md) - Customize the preparation of accounts
- [Inspect a SPA system](./workbooks/inspect-a-production-system.md) - Review the behavior of SPA components
- [Manage preventive controls](./workbooks/manage-preventive-controls.md) - Define guardrails for personal accounts
- [Create a personal account](./workbooks/create-a-personal-account.md) - Add manually an account handled by SPA
- [Add Microsoft Teams webhook](./workbooks/add-microsoft-teams-webhook.md) - Forward SPA notifications to ChatOps channels
- [Transmit reports over email](./workbooks/transmit-reports-over-email.md) - Forward SPA reports to selected email recipients
- [Manage account states](./workbooks/manage-account-states.md) - Handle account tags by yourself
- [Release all managed accounts](./workbooks/release-managed-accounts.md) - Set all state machines to RELEASED state
- [Reset all managed accounts](./workbooks/reset-managed-accounts.md) - Set all state machines to VANILLA state
- [Manage account costs](./workbooks/manage-account-costs.md) - Automate some FinOps activities with SPA
- [Setup continuous deployment of SPA](./workbooks/setup-continuous-deployment.md) - Use GitOps principles to operate SPA

## Sustainable Personal Accounts features an event-driven serverless architecture

Persistence of states is done with tags set on AWS accounts. State transitions are handled with Eventbridge, Lambda, CodeBuild and DynamoDB. The management of the solution is done with CloudWatch, SSM Incident Manager, Cost Explorer and S3 reporting bucket.

![architecture](./media/reference-architecture.drawio.png)

Sustainable Personal Accounts has been designed with following principles:

- the entire solution is configured with one single YAML file
- the entire infrastructure is deployed with python code and AWS CDK
- the configuration of organizational units and accounts is store in SSM Parameter Store
- states of the state machine are implemented with tags on AWS accounts -- no database for state management
- computing resources are entirely serverless, powered by Lambda functions and CodeBuild projects
- processing is driven by events, powered by AWS EventBridge
- budget alerts are consolidated over SQS
- data persistence is provided with DynamoDB tables
- the preparation of an AWS account is done with a customizable Codebuild project -- adapt it to your corporate policy
- the purge of an AWS account is done with a customisable Codebuild project -- adapt it to your FinOps best practices
- monitoring is implemented with a CloudWatch dashboard deployed automatically
- system can be extended to specific needs via custom event processing

## Sustainable Personal Accounts provides an integrated monitoring dashboard

![cloudwatch-dashboard](./media/cloudwatch-dashboard.png)

The monitoring dashboard provides a combination of business indicators and of technical indicators:

- daily costs per cost center
- daily transactions per cost center
- daily transactions per label
- events by label
- exceptions by label
- invocations of Lambda functions
- durations of Lambda executions
- Lambda errors
- read units for DynamoDB tables
- write units for DynamoDB tables
- errors with DynamoDB tables

## Sustainable Personal Accounts interacts with human beings on exceptions

![management-by-exception](./media/management-by-exception.drawio.png)

Sustainable Personal Accounts provides a monitoring dashboard that can be checked when you want. But you do not need to keep your eyes on the monitoring dashboard. On an exceptional situation, SPA can post a message in a Microsoft Teams channel. It can also send you an alert over e-mail. You can come back anytime to SSM Incident Manager, where SPA will push incident records with tags and with contextualised cost reports.

## Sustainable Personal Accounts integrates ticket management

![incident-record](./media/incident-record.png)

When an exceptional situation is encountered, Sustainable Personal Accounts creates an incident record in SSM Incident Manager. Examples of exceptions:

- budget alert
- console login with IAM User or with root credentials
- failed maintenance transaction

On exception, SPA queries cost explorer and produces a report for on-going costs. The report is attached to the incident record for easy access. Also, each incident record is tagged so that you can use information items for problem management, or for transverse analysis of past incident records.

## Sustainable Personal Accounts automates reporting

![automated-reporting](./media/automated-reporting.drawio.png)

Sustainable Personal Accounts produces following kinds of reports:

- costs - These reports in CSV and in Excel formats are produced for managers of the service, and for managers of cost centers.

- activities such as on-boarding, maintenance and console login - These reports in CSV format are provided to feed a downward system such as a database. They can be used for service analytics.

- inventories of accounts - These reports in CSV format mention cost centers, Organizational Units, dates of last login, and account state. They can be used to detect unused accounts, and accounts that have not be released for some reason.

Reports are put on a S3 bucket for easy storage. Summary reports can also be transmitted over email to selected recipients, for example to the FinOps mailbox.

## Sustainable Personal Accounts contributes to FinOps

![cost-and-usage-reports](./media/cost-and-usage-reports.png)

FinOps teams can get cost details per cost centers and per Organizational Units. Each AWS account is tagged with one cost center, and is placed in one Organizational Unit. This structure has proven very convenient, since it leverages the structure put in AWS Organisations itself.

## Sustainable Personal Accounts is driven by configuration files

![collaborative-configuration](./media/collaborative-configuration.drawio.png)

Sustainable Personal Accounts is provisioned and configured from a single YAML file. Settings cover every aspect of the solution, for example the cron expression that triggers the maintenance window, the buildspec files for CodeBuild tasks on accounts.

In addition, part of the solution configuration can be delegated to CSV files. This architecture allows extended contributions to the settings of SPA

## Any other question?

Please check the [Frequent Asked Questions page](./FAQ.md), maybe your point has already been addressed there.",VRAI
Resourcely-Inc/cloud-guardrails,Documentations,Documentations,2024-10-25T16:44:50Z,2024-10-10T17:01:37Z,0,0,0,0,0,0,0,0,2024-10-10T16:59:49Z,2025-04-04T16:38:37Z,95,125,,VRAI,10,FAUX,0,,0,"Open-source best practices for protecting a secure, sensible cloud platform",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,5,"# Cloud Guardrails
There are many best practices for operating public cloud footprints effectively.

Some of these practices help with security, some help workloads be more reliable, and some are just generally good best practices.

There are various sources of best practices, including:
 - word of mouth
 - prior experience (good and bad)
 - conference talks
 - architecture whitepapers
 - benchmarks (CIS, etc.)

But it can be hard to get a high-level view quickly, see best practices in a table format, and filter to practices that are most applicable for your technology/cloud/maturity.

The Cloud Guardrails project aims to be an open-source reference of best practices to help spread knowledge and socialize what works.

Check it out at [cloudguardrails.com](https://www.cloudguardrails.com)!

## Format
Each guardrail is contained in a YAML file. We designed it this way so you can easily skim the best practices and see structured information about them. We have also built a tool to display them in a simple table format that allows sorting.

## Open source
Since the point of this project is to share knowledge, we created under the MIT license. Any future use should be shared back to the community.

## Contributing
Do you have some best practices you'd like to share? Awesome! Please see our [contributing](CONTRIBUTING.md) guidelines. We welcome your suggestions/additions/corrections.

## Where did this come from?
The initial authors are:
 - [Mark Andersen](https://www.linkedin.com/in/markandersen94/)
 - [William Bengtson](https://www.linkedin.com/in/william-bengtson/)
 - [Adam Cotenoff](https://www.linkedin.com/in/adam-cotenoff/)
 - [Houston Hopkins](https://www.linkedin.com/in/houstonhopkins/)
 - [Travis McPeak](https://www.linkedin.com/in/travismcpeak/)
 - [Nicholas Siow](https://www.linkedin.com/in/nicholas-siow-960740201/)",FAUX
RHsyseng/telco-operations,Documentations,Documentations,2024-06-14T09:50:07Z,2022-10-17T12:12:04Z,0,2,0,0,5,0,0,1,2022-01-20T18:00:16Z,2025-01-19T05:47:43Z,2778,19,Python,VRAI,17,FAUX,0,,0,Repository used by the Telco Operations Team ,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,6,"<div align=""center"">

# Telco Operations Team Repository

<p align=""center"">
  <img alt=""Red Hat Telco Logo"" src=""https://raw.githubusercontent.com/RHsyseng/telco-operations/main/assets/rh-telco-logo.png"" height=""140"" />
  <h3 align=""center"">Telco Operations Team</h3>
</p>

| :warning: **Unless specified otherwise, everything contained in this repository is unsupported by Red Hat.** |
| --- |

[Documentation](#documentation) •
[Labs](#labs) •
[Others](#others)

</div>

## Documentation

* [ABI](./abi/README.md)
* [Assisted Installer](./assisted-installer/README.md)
* [Contrail CNI](./contrail/README.md)
* [Fedora CoreOS](./fcos/README.md)
* [Gateway API](./gateway-api/README.md)
* [HyperShift](./hypershift/README.md)
* [Knative](./knative/README.md)
* [Loki](./loki/README.md)
* [Metallb](./metallb/README.md)
* [Microshift](./microshift/README.md)
* [mirror-registry](./mirror-registry/README.md)
* [OPA](./opa/README.md)
* [OpenShift](./openshift/README.md)
* [RHACM](./rhacm/README.md)
* [ROSA](./rosa/README.md)
* [SNO](./sno/README.md)
* [ZTP](./ztp/README.md)

## Labs

Nothing yet!

## Others

* [Tools/Apps created by the team](./tooling/README.md)",FAUX
rond-authz/rond,Toolkit,Toolkit,2025-05-07T09:41:18Z,2025-01-07T09:15:42Z,0,5,0,0,0,0,0,0,2022-06-03T10:03:42Z,2025-03-10T11:35:31Z,1386,163,Go,VRAI,7,FAUX,35,"authorization,hacktoberfest,openpolicyagent,rbac,security",35,A lightweight container for distributed security policy evaluation,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,21,"<div align=""center"">

  <img alt=""Rönd Logo"" src=""https://github.com/rond-authz/.github/blob/58bf35733bb43143cfb6ad9b05b93e20d6729633/profile/img/Rond_Logo_Full-Lockup_Horizontal.png#gh-light-mode-only"" width=""300"" />
  <img alt=""Rönd Logo"" src=""https://github.com/rond-authz/.github/blob/58bf35733bb43143cfb6ad9b05b93e20d6729633/profile/img/Rond_Logo_Full-Lockup_Horizontal-White.png#gh-dark-mode-only""  width=""300"">
  <br/><br/>
  
[![Build Status][github-actions-svg]][github-actions]
[![Coverage Status][coverall-svg]][coverall-io]
[![Go Report Card][go-report-card-badge]][go-report-card]
[![Go Sec][security-badge-svg]][security-badge]

[![Docs][docs-badge]][docs]

[![Mia-Platform][mia-platform-badge]][mia-platform]

# Rönd

Rönd is a lightweight container that distributes security policy enforcement throughout your application.
</div>

Rönd is based on [OpenPolicy Agent](https://www.openpolicyagent.org) and allows you to define security policies to be executed during API invocations. Rönd runs in your Kubernetes cluster as a sidecar container of your Pods.
Rönd intercepts the API traffic, applies your policies and, based on the policy result, forwards the request to your application service or rejects the API invocation.

## Why Rönd?

Find out more [here][why-rond].

## Features

Rönd supports three policy types:

1. Allow or reject request
2. Query generation during the request flow
3. Response body patching

## RBAC capabilities

Rönd natively allows you to build an RBAC solution based on Roles and Bindings saved in MongoDB.

## Who is using Rönd

Here is a list of awesome people using Rönd, if you're using it but do not appear in this list feel free to open a PR!

 * [Cattolica Assicurazioni](https://www.cattolica.it/)
 * [MDConcierge](https://www.mdconcierge.it/)
 * [Mia-Care](https://mia-care.io/)
 * [Mia-Platform](https://mia-platform.eu)
 * [PreviDigital](https://previdigital.com/)

## Local development

For local development you need to have Go installed locally, checkout the [go.mod](./go.mod#L3) file to know the currently used language version.

### Run tests

```sh
make test
```

Please note that in order to run tests you need Docker to be installed; tests need a local instance of MongoDB to be up and running, the `make test` command will take care of it by creating a new `mongodb` container. The container is auomatically removed at the end of tests; if it remains leaked simply run `make clean`.

#### With coverage

To run test with coverage file in output, run

```sh
make coverage
```

### Contributing

Please read [CONTRIBUTING.md](./CONTRIBUTING.md) for further details about the process for submitting pull requests.

[github-actions]: https://github.com/rond-authz/rond/actions/workflows/test.yml
[github-actions-svg]: https://github.com/rond-authz/rond/actions/workflows/test.yml/badge.svg
[coverall-svg]: https://coveralls.io/repos/github/rond-authz/rond/badge.svg
[coverall-io]: https://coveralls.io/github/rond-authz/rond
[security-badge-svg]: https://github.com/rond-authz/rond/actions/workflows/security.yml/badge.svg
[security-badge]: https://github.com/rond-authz/rond/actions/workflows/security.yml
[go-report-card-badge]: https://goreportcard.com/badge/github.com/rond-authz/rond
[go-report-card]: https://goreportcard.com/report/github.com/rond-authz/rond
[mia-platform-badge]: https://img.shields.io/badge/Supported%20by-Mia--Platform-green?style=for-the-badge&link=https://mia-platform.eu/&color=3d86f4&labelColor=214147
[mia-platform]: https://mia-platform.eu/?utm_source=referral&utm_medium=github&utm_campaign=rond
[docs-badge]: https://img.shields.io/badge/-Read%20the%20Docs-green?style=for-the-badge&color=3d86f4&labelColor=214147
[docs]: https://rond-authz.io/?utm_source=referral&utm_medium=github&utm_campaign=rond
[why-rond]: https://github.com/rond-authz#why-r%C3%B6nd",FAUX
run-ai/fake-gpu-operator,Toolkit,Application System,2025-04-30T08:05:45Z,2024-08-20T13:18:10Z,0,0,0,0,0,0,0,0,2022-05-17T17:58:12Z,2025-04-02T11:34:15Z,533,100,Go,VRAI,22,FAUX,4,,4,,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,20,,VRAI
russolsen/ham_radio_question_pool,Application System,Documentations,2024-10-28T13:45:15Z,2019-10-20T18:03:31Z,0,0,0,0,0,2,0,0,2019-12-16T15:23:23Z,2025-02-23T23:41:06Z,2597,26,Ruby,VRAI,10,FAUX,3,,3,All of the current questions for the FCC amateur radio license exams in machine readable format.,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,4,"# Ham Radio Question Pool

Amateur radio question pools in various formats.

In the US most anyone can
[get a license](https://www.fcc.gov/wireless/bureau-divisions/mobility-division/amateur-radio-service)
to operate an amateur radio station (i.e. HAM radio) by taking a multiple choice test.
There are no age restrictions and the test questions
are publically available.

There are actually three tests of increaing difficulty:
* Technician
* General
* Amateur Extra

Passing the technican test means that you get a technican
license which allows you to operate a station in various
specific radio bands, mostly in the VHF and UHF regions of
the radio spectrum.

Passing the general test in addition to the technician
test gives you access to more frequencies while passing
the amateur extra test gives you even more.

You do need to pass the tests in order, first technician,
then general, then amateur extra, but there's no requirement
that you keep going - you can stop at technican or general.

In any case, while the questions are publically available at
[NCVEC](https://www.ncvec.org/), they are delivered
in Word or PDF format. This project aims to make the questions
accessable in various machine readable formats, including:
* JSON
* YAML
* CSV
* Simple plain text

## Usage

The JSON version looks like this:

```json
[
  {
    ""id"": ""T1A01"",
    ""correct"": 2,
    ""refs"": ""[97.1]"",
    ""question"": ""Which of the following is part of the Basis and Purpose of the Amateur Radio Service?"",
    ""answers"": [
      ""Providing personal radio communications for as many citizens as possible"",
      ""Providing communications for international non-profit organizations"",
      ""Advancing skills in the technical and communication phases of the radio art"",
      ""All these choices are correct""
    ]
  },
  //...
]
```
Note that the correct answer index is *zero* based, so that the
correct answer to question `T1A01` is the third one,
_Advancing skills in the technical and communication phases of the radio art_.

Given all that, the YAML version is about what you would expect:

```yaml
---
- :id: T1A01
  :correct: 2
  :refs: ""[97.1]""
  :question: Which of the following is part of the Basis and Purpose of the Amateur
    Radio Service?
  :answers:
  - Providing personal radio communications for as many citizens as possible
  - Providing communications for international non-profit organizations
  - Advancing skills in the technical and communication phases of the radio art
  - All these choices are correct
```

And the CSV:

```
id,correct,question,a,b,c,d,refs
T1A01,2,Which of the following is part of the Basis and Purpose of the Amateur Radio Service?,Providing personal radio communications for as many citizens as possible,Providing communications for international non-profit organizations,Advancing skills in the technical and communication phases of the radio art,All these choices are correct,[97.1]
```

And last but certainly not least, the plain text version looks like this:

```
T1A01 (C) [97.1]
Which of the following is part of the Basis and Purpose of the Amateur Radio Service?
A. Providing personal radio communications for as many citizens as possible
B. Providing communications for international non-profit organizations
C. Advancing skills in the technical and communication phases of the radio art
D. All these choices are correct

...
```

Note that the correct answer (`(C)`) is coded in the question header along with the question ID
and references to the appropriate regulations.
Also note that there is a blank line after each question, including the last question.

## Why?

I set out to do this work because I wanted to keep track of my progress with
some simple tools of my own.

This project largely overlaps the fine work done by Jason Staten (https://github.com/statianzo)
at https://github.com/statianzo/hampool.

I started doing my version before I found
Jason's. I decided to put this one out there because the version of the General exam question
pool in Jason's repo is now out of date.

In any case, hope someone gets some use from this.

## See Also

[Wikipedia](https://en.wikipedia.org/wiki/Amateur_radio_licensing_in_the_United_States)
has a good introduction to the whole topic of US amateur radio licensing.

The original source of the questions is  [NCVEC](http://www.ncvec.org/page.php?id=338).

[Jason Staten's](https://github.com/statianzo)
fine work is at 
[https://github.com/statianzo/hampool](https://github.com/statianzo/hampool).

You can find online amateur radio testing at https://hamexam.org and https://arrlexamreview.appspot.com.",VRAI
SamYuan1990/Probe,Application System,Application System,2024-12-30T07:05:12Z,2024-05-31T13:58:20Z,0,3,0,0,0,0,0,0,2020-10-14T12:21:14Z,2024-12-30T07:05:17Z,3822,36,JavaScript,VRAI,12,FAUX,3,"fabric,hyperledger-fabric,performance,performance-analysis,performance-visualization,tool,tps,ui",3,"Probe is a web GUI application with for Hyperledger Fabric maintainer, user, research to find the best block config logic for specific chain-code.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,3,"# Probe

[![Build Status](https://dev.azure.com/yy19902439/yy19902439/_apis/build/status/SamYuan1990.Probe?branchName=main)](https://dev.azure.com/yy19902439/yy19902439/_build/latest?definitionId=8&branchName=main)

[![Issues](https://img.shields.io/github/issues/SamYuan1990/Probe?color=0088ff)](https://github.com/SamYuan1990/Probe/issues)

[![GitHub pull requests](https://img.shields.io/github/issues-pr/SamYuan1990/Probe?color=0088ff)](https://github.com/SamYuan1990/Probe/pulls)

[中文](README_ZH.MD)


## What is Probe

![Probe](/doc/Probe.png)

According to [blockchain-performance-metrics](https://www.hyperledger.org/learn/publications/blockchain-performance-metrics), Probe is a web GUI application for Hyperledger Fabric mantianer, user, research. Recently, aims at providing a way to control both SUT and LGC to find the best block config logic for specific chaincode for specific fabric network. Meanwhile has a better understanding of how block config impacts performance.

- Probe provides loop test control for given block parameter arrays.
- Probe provides TPS result review via GUI.
- Probe provides sample chaincode for some test cases.

Long term goal for Probe is a coordinator between Test Harness or LGC, SUT.

- Probe will allow you design shell scripts to schedule SUT and Test Harness.
- Probe will allow you investigate performance matrix with GUI in customer way.

**You can use Probe to ...**[HowToConfigFabricParameters](doc/HowToConfigFabricParameters.md)

---
**Sample run of [Probe](https://www.bilibili.com/video/BV1Kz4y1179L)**

---

## Table Of Content

* [Prerequisites](#prerequisites)
* [Quick_Start](#Quick_Start)
* [Contributing](#contributing)
* [License](#license)
* [Contact](#contact)
* [Regards](#thanks-for-choosing)

---
## Prerequisites
1. Install this project `npm install`
1. Install fabric-sample environment
`curl -vsS https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh | bash`
**Note**
If the above curl command fails, it may be that the old version of curl cannot handle redirection or cannot be redirected due to network reasons in

some countries and regions. At this time, users can download the [bootstrap.sh](https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh) script then run it.
1. Get tape `docker pull ghcr.io/hyperledger-twgc/tape`
1. Apply the bridge file to adjust block parameters for test network `cp sample/prepareConfig.sh fabric-samples/test-network`
1. `npm start`

---

## Quick_Start

1. Access localhost:3000, click ![TestNetworkSample](doc/quick_sample.png)
1. Click![submit](doc/quicksample2.png)
1. Access `localhost:3000/result/BatchTimeout` to see TPS relationship with BatchTimeout
1. Access `localhost:3000/result/MaxMessageCount` to see TPS relationship with MaxMessageCount

--- 

## Docker
If you want to try with docker usage, welcome to use
`docker run -d --rm --name Probe -p 3000:3000 -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd):/home/probe \
  19902439/probe:latest`
it is based on alphine, and which means, you may need to make sure your minifab/fabric sample works instead, as install for `configtxgen` or able to use docker in docker.

---
## Contributing
Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are **greatly appreciated**.

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -s`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

If your want to contribute Probe with new feature, bug fixing please create a new issue, of course with PR is best.
Also welcome for documentatoin, learning course, etc.
For development, please complete `Quick start` above.
For Frontend, please use `npm run build`.
For backend, please develop `--DryRun` as mock for unit test and adding real cases in CI

---
## License
Hyperledger Project source code files are made available under the Apache License, Version 2.0 (Apache-2.0), located in the [LICENSE](LICENSE) file.

---
## Contact

* [Maintainers](MAINTAINERS.md)

---

## Why Probe

As discussed with [TWGC performance work group](https://github.com/Hyperledger-TWGC/fabric-performance-wiki), we found out that different block parameters, networks, chaincode language and chaincode logic having influence final TPS.

To answer, the best parameter for specific fabric network and fabric chaincode, this project been created.

## Probe is not

- Real time time monitor, for real time tps monitor, pls use Prometheus. (But Probe has demo for it with test network [here](https://www.bilibili.com/video/BV1x54y1x78Z))
- Auto test framework for Fabric performance, as in probe, we will invoke as byfn or minifab for your network up/down/cleanup.
- Performance test tool for Fabric, for this we using tape.
- GUI for tape, tape focus on once off time performance testing.


### THANKS FOR CHOOSING",FAUX
sapcc/helm-charts,Application System,Documentations,2025-05-15T14:14:50Z,2025-05-15T09:44:22Z,0,5,0,0,0,0,0,24,2016-11-01T17:54:30Z,2025-04-07T17:15:26Z,45380,37,Smarty,VRAI,72,FAUX,286,"helm,kubernetes,openstack",286,Helm charts for SAP Converged Cloud managing openstack on kubernetes,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,231,"# SAP Converged Charts

This repository contains Helm charts required by SAP Converged Cloud.

## Structure

Charts are grouped logically into:

  * `common`: Reusable charts
  * `global`: Singletons that only exist once in a global context
  * `openstack`: Openstack and dependent or related services
  * `prometheus-exporters`: A curated collection of Prometheus exporters
  * `prometheus-rules`: Prometheus alert- & aggregation rules
  * `system`: Infrastructure required by the control plane

This structure is just a logical grouping, it does not represent deployable
units or imply other semantics.

## Charts

On the second level we expect a chart. This can be a single chart or
a meta-chart that describe a dependent set of compononents. Meta-charts contain
sub-charts or reference charts from other repositories using Helm dependencies.

```
.
└── system
    ├── dns
    │   └── charts
    │       ├── bind
    │       └── unbound
    ├── kube-system
    │   └── charts
    │       ├── ingress
    │       └── dashboard
    └── prometheus
        └── charts
            ├── kube-state-metrics
            ├── prometheus-collector
            └── prometheus-frontend
```

We imply that the highest chart will be deployed as a Helm release. In this
example, releasing `dns` will install/update `bind` and `unbound`.

In order to be able to relate charts to running Kubernetes pods, we also imply
that a chart will be deployed in a namespace with the same name.

```
$ kubectl get pods --all-namespaces                                                                                                                 0 ↵
NAMESPACE         NAME                                               READY     STATUS    RESTARTS   AGE
dns               bind1-2290429089-joidj                             2/2       Running   0          5d
dns               bind2-3590597799-1vcv0                             2/2       Running   0          5d
dns               unbound1-3007389427-shh2y                          1/1       Running   0          9d
dns               unbound1-3577488147-ld1rd                          1/1       Running   0          5d
kube-system       ingress-controller-d3snv                           1/1       Running   4          13d
kube-system       ingress-controller-j9bpf                           1/1       Running   2          18d
```

This has the benefits that:

  * Values required for releasing a chart can be found at the same place in `cc/regions`
  * Cleanup of a failed release, is as easy as deleting the namespace.
  * For testing a chart can deployed in a seperate testing namespace.
  * Pods and other Kubernetes primitives are reflected at a known place in
      Kubernetes

### Test a Chart

Opening a PR to this repository triggers the Helm chart tests which are described in detail [here](./ci/README.md).

### Install/Update of a Chart/Release

Per convention we use the name of the meta-chart as namespace and name of the
release. Values are pulled in from a secret repository.

```
helm upgrade dns ./system/dns --namespace dns --values ../secrets/staging/system/dns.yaml --install
```",VRAI
SAP/InfraBox,DevOPs,DevOPs,2024-11-05T15:50:50Z,2024-02-21T08:23:36Z,0,24,0,0,0,0,0,0,2018-03-22T14:06:46Z,2025-04-04T03:38:18Z,19525,272,Python,VRAI,63,FAUX,54,"ci,docker,docker-compose,kubernetes,open-source",54,InfraBox is a cloud native continuous integration system,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,46,"# <img src=""src\dashboard-client\static\logo_compact_transparent.png"" width=""200"">
[![REUSE status](https://api.reuse.software/badge/github.com/SAP/InfraBox)](https://api.reuse.software/info/github.com/SAP/InfraBox)


InfraBox is a continuous integration system. It's well suited for cloud native applications and has [native support for kubernetes](https://github.com/SAP/infrabox-examples/tree/master/kubernetes). Watch our short introduction video:

[![Introduction to InfraBox](https://img.youtube.com/vi/O8N2U7d404I/0.jpg)](https://www.youtube.com/watch?v=O8N2U7d404I)

Some of InfraBox's features are:

- [Static and dynamic workflows](https://github.com/SAP/infrabox-examples)
- [Set resource limits (CPU and memory) for each task](https://github.com/SAP/infrabox-examples)
- [GitHub integration](docs/install/configure/github.md)
- [Gerrit integration](docs/install/configure/gerrit.md)
- [LDAP support](docs/install/configure/ldap.md)
- [Periodically schedule builds](docs/cronjobs.md)
- [and many more, see our examples](https://github.com/SAP/infrabox-examples)


## Documentation
All our documentation can be found [here](docs/README.md). You can also look at our [example repository](https://github.com/SAP/infrabox-examples) on how to make use of the different features InfraBox provides.

## How to obtain support
If you need help please post your questions to [Stack Overflow](https://stackoverflow.com/questions/tagged/infrabox).
In case you found a bug please open a [Github Issue](https://github.com/SAP/infrabox/issues).

## Roadmap

### 1.2
- Bugfixes
- Improve Documentation (installation, job definition & API)
- Add support for SAML
- Quota management on user and project level

### 1.x
- [knative](https://cloud.google.com/knative/) integration
- Service for creating EKS K8s Clusters
- More to come, _your idea here_

## Contribute
Any contribution is highly appreciated. See our [developer's documentation](docs/dev.md) for details.

## License
Copyright (c) 2018 SAP SE or an SAP affiliate company. All rights reserved.
This file is licensed under the Apache Software License, v. 2 except as noted otherwise in the [LICENSE file](LICENSE).",FAUX
saturnhead/blog-examples,Documentations,Documentations,2024-03-15T13:26:10Z,2023-09-25T13:24:05Z,0,1,0,0,0,0,0,0,2023-07-06T12:36:09Z,2024-08-29T14:08:03Z,91,5,HCL,VRAI,17,FAUX,0,,0,,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,4,# blog-examples,FAUX
Scalr/sample-tf-opa-policies,Documentations,Documentations,2024-07-05T09:13:22Z,2020-11-30T13:28:03Z,0,62,0,0,0,0,0,0,2020-01-13T12:52:23Z,2025-03-24T22:47:35Z,178,166,Open Policy Agent,VRAI,73,FAUX,6,,6,,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,11,"# Terraform OPA policies examples

[![OPA tests](https://github.com/Scalr/sample-tf-opa-policies/workflows/OPA/badge.svg)](https://github.com/Scalr/sample-tf-opa-policies/actions?query=workflow%3AOPA)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/Scalr/sample-tf-opa-policies/pulls)
[![Documentation Status](https://readthedocs.com/projects/scalr-athena/badge/?version=latest)](https://docs.scalr.com/en/latest/opa.html)

A sample collection of OPA policies to test against Terraform run in Scalr.

## Resources

* Blog series for getting started with OPA for Terraform in Scalr: https://scalr.com/blog/opa-series-part-1-open-policy-agent-and-terraform/
* Documentation for using OPA with Scalr: https://docs.scalr.com/en/latest/opa.html
* OPA documentation: https://www.openpolicyagent.org/docs/latest/

## Policy Files

Each sample policy comprises 3 files.

- `*.rego`: The OPA policy
- `*.mock.json`: Input data for testing the policy with `opa test`
- `*.test.rego`: Definition of the tests that `opa test` should run and expected results

Each folder also includes an example `scalr-policy.hcl` file. These files are used by Scalr to implement enforcement levels for each policy (`hard-mandatory`, `soft-mandatory`, `advisory`). See [Enabling and Enforcing Policy](https://docs.scalr.com/en/latest/opa.html#enabling-and-enforcing-policy) for more details.

## Policy Evaluation

You can evaluate a policy against your own terraform plans using the Terraform CLI and `opa eval` as follows.

```bash
$ terraform plan --out planfile
$ terraform show -json planfile > plan.json
$ opa eval --format pretty --data policy.rego -i plan.json data.terraform.deny
```

## Policies

Summary descriptions of each policy. Detailed descriptions of each rule can be found as comments in the policy file.
Many policies contain arrays of values that are checked against resources. The arrays and reason messages are of course customisable.

| Policy                                 | Description                                                              |
| -------------------------------------- | ------------------------------------------------------------------------ |
| [aws/enforce_aws_iam_and_workspace.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_aws_iam_and_workspace/enforce_aws_iam_and_workspace.rego) | Checks valid IAM roles for provider and workspace.                        |
| [aws/enforce_aws_resource.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_aws_resource/enforce_aws_resource.rego) | Check resource types against an allowed list.                            |
| [aws/enforce_cidr.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_cidr/enforce_cidr.rego) | Check security group CIDR blocks contain allowed CIDR's.             |
| [aws/enforce_ebs_del_on_term.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_ebs_del_on_term/enforce_ebs_del_on_term.rego) | Check `delete_on_termination = true` is set for EBS volumes.             |
| [aws/enforce_iam_instance_profiles.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_iam_instance_profiles/enforce_iam_instance_profiles.rego) | Check IAM instance profile is in allowed list.                          |
| [aws/enforce_instance_subnets.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_instance_subnet/enforce_instance_subnet.rego) | Check instances are using allowed subnets |
| [aws/enforce_kms_key_names.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_kms_key_names/enforce_kms_key_names.rego) | Check KMS keys (by name) against allowed list.                           |
| [aws/enforce_lb_subnets.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_lb_subnets/enforce_lb_subnets.rego) | Check Loadbalancers are using allowed subnets |
| [aws/enforce_s3_buckets_encryption.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_s3_buckets_encryption/enforce_s3_buckets_encryption.rego) | Check encryption is set for S3 buckets.                                  |
| [aws/enforce_s3_private.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_s3_private/enforce_s3_private.rego) | Check S3 buckets are not public.                                  |
| [aws/enforce_sec_group.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_sec_group/enforce_sec_group.rego) | Check security groups have been specified and are in allowed list.       |
| [aws/enforce_rds_subnets.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/aws/enforce_rds_subnets/enforce_rds_subnets.rego) | Check RDS clusters are using allowed subnets |
| [cost/limit_monthly_cost.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/cost/limit_monthly_cost/limit_monthly_cost.rego) | Check estimated cost against an upper limit.                             |
| [external_data/random_decision.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/external_data/random_decision/random_decision.rego) | Example of using external data (HTTP GET) in a policy.                   |
| [gcp/enforce_gcs_private.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/gcp/enforce_gcs_private/enforce_gcs_private.rego) | Check GCS buckets are not public.                   |
| [management/denied_provisioners.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/management/denied_provisioners/denied_provisioners.rego) | Checks provisioner types against an allowed list.                        |
| [management/enforce_ami_owners.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/management/enforce_ami_owners/enforce_ami_owners.rego) | Checks AMI's being used belong to allowed list of AMI owners.            |
| [management/enforce_var_desc.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/management/enforce_var_desc/enforce_var_desc.rego) | Checks variables have descriptions.            |
| [management/instance_types.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/management/instance_types/instance_types.rego) | Checks instance types/sizes against allowed list. AWS, Azure and GCP.    |
| [management/resource_tags.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/management/resource_tags_mock/resource_tags.rego) | Checks required tags are configured for all clouds.                      |
| [management/whitelist_ami.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/management/whitelist_ami_mock/whitelist_ami.rego) | Checks AMI against allowed list or configured from data source.          |
| [management/workspace_name.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/management/workspace_name/workspace_name.rego) | Simple example of using `tfrun` data and validating a workspace name.    |
| [management/workspace_environment.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/management/workspace_environment_type/workspace_environment_type.rego) | Checks workspace environment type and enforces cost limits based on environment. |
| [management/workspace_destroy.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/management/workspace_destroy/workspace_destroy.rego) | Checks workspace has an active state and denies its destroy, if active state is present.    |
| [management/workspace_tags.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/management/workspace_tags/workspace_tags.rego) | Checks workspace is tagged with provider name.                           |
| [modules/pin_module_version.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/modules/pin_module_version/pin_module_version.rego) | Enforces use of specific module versions.                                |
| [modules/required_modules.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/modules/required_modules/required_modules.rego) | Checks resources are only be created via specific modules.               |
| [placement/cloud_location.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/placement/cloud_location/cloud_location.rego) | Checks resources are deployed to specific regions in each cloud.         |
| [providers/blacklist_provider.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/providers/blacklist_provider/blacklist_provider.rego) | Implements a provider blacklist.                                         |
| [user/user.rego](https://github.com/Scalr/sample-tf-opa-policies/blob/master/user/check_user/user.rego) | Restricts which users can trigger terraform runs. Works for CLI and VCS. |

## Contributions

We welcome contributions in many ways!

### Report Bugs

Submit bug reports at https://github.com/Scalr/sample-tf-opa-policies/issues.

Be sure to include the following.

* Link to the policy file.
* The `terraform plan` data in JSON format you tested against.
* The `opa eval` command used to reproduce the problem and it's output.

### Feedback and Suggestions

Submit feed back and suggestions at https://github.com/Scalr/sample-tf-opa-policies/issues.

Be sure to include the following.

* Detailed description of how you expect a policy to work.
* Sample `terraform plan` data the policy should work against.

### Pull Requests

Better still have a go at fixing bug or implementing new policy examples yourself and submit a Pull Request.

If you submit a new policy you must include the following files.

* The `*.rego` file with the policy code.
* `*_mock.json` containing test data mocks. You should include data for both valid and invalid evaluation of each rule in the policy.
* `*_test.rego` defining the tests to be run and expected results when the PR checks are performed.

To submit a PR follow the standard process.

1. Fork the repo
2. Clone locally and create a new branch
3. Commit and push
4. Submit pull request

Before submitting the PR for the new policy or bug fix you should confirm it works using `opa eval` as shown above and validate the mock based tests work using `opa test`.

Example test

```
# opa test enforce_sec_group.* -v
data.terraform.test_valid: PASS (7.390418ms)
data.terraform.test_invalid: PASS (603.837µs)
data.terraform.test_missing: PASS (483.272µs)
--------------------------------------------------------------------------------
PASS: 3/3
```

## License

The examples are licensed under the [MIT License](https://github.com/Scalr/sample-tf-opa-policies/blob/master/LICENSE).",FAUX
schrutek/SPG_POS,Documentations,Documentations,2024-03-11T11:59:36Z,2022-03-02T20:37:29Z,1,0,0,0,0,0,0,0,2020-04-21T14:56:51Z,2024-03-18T13:21:48Z,89351,13,C#,VRAI,5,FAUX,49,,49,Spengergasse POS,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,2,,FAUX
ScottLogic/blog,Application System,Documentations,2025-05-14T16:06:46Z,2025-04-28T12:06:56Z,0,0,0,0,0,1,0,0,2022-05-31T13:56:19Z,2025-04-04T09:35:39Z,1123677,9,JavaScript,VRAI,96,FAUX,16,,16,The Scott Logic blog,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,129,"# Scott Logic Blogs

If you are looking to write a blog post, then you don't _need_ to read any further, as you can author posts using
SiteLeaf: see our [company intranet][confluence-getting-started] for instructions on how to create a new page and view
it before publication on the blog.

The below instructions allow more control over the process, though they do require a smidgen of git knowledge and a
GitHub account.

## Technical Stack

The blog is a static website, designed to be hosted on [GitHub pages][github-pages].

The underlying content is generated through a series of Ruby gems and libraries, starting with a dedicated github-pages
[gem][ruby-github-pages].

Within that stack, [Jekyll][jekyll-docs] is used as the static content generation engine,
consuming template files written in either **HTML** or **Markdown** (syntax extended by [Kramdown][kramdown-syntax]).
Common content or structure can be further injected or managed using the [Liquid][ruby-liquid] templating language.

## Authoring

To write a blog post, it is recommended to fork the repo, then perform a sparse checkout that excludes all previous
posts and all authors other than yourself. This will result in the quickest build.

The alternative is to perform a full checkout and build all posts (more than 15 years' worth!) which can take five
minutes or more to complete. If you are working on technical issues or improvements, then you may need some blog posts
to see the result of your changes; in this case, there is little alternative but to perform a full checkout.

### Fork the repository

At the top of the [ScottLogic blog GitHub repo][scottlogic-blog-repo] you will find the ""fork"" button. You
will need to be logged into your GitHub account first, then clicking the button will generate your own fork and navigate
to the resulting fork. Then click the <code style=""white-space: nowrap"">&lt;&gt; Code</code> button at the top to see
your clone options; HTTPS clone is the simplest. Copy the URL, then:

```shell
# Clone your fork without checking anything out:
git clone --depth 1 --filter=blob:none --no-checkout YOUR-REPO-URL-HERE
cd blog

# Optional: tell sparse checkout what to include (excludes _posts by default).
# Use your Scott Logic username in the below command:
git sparse-checkout set _data _includes _layouts assets category scripts scss shell YOUR-SL-USERNAME-HERE
git checkout gh-pages
```

### First-time authors

If this is your first post, you'll need to set yourself up as an author. For this, you will need a directory in the repo
root named after your Scott Logic username. Within this you will need a set of files: `atom.xml`, `feed.xml`,
`index.html`, and an image file of yourself. Just copy an existing author's files and modify their contents: it should
be obvious what needs changing. Then add yourself to `_data/authors.yml`, again using an existing author as a template.
You will need to add

- an entry under `authors`
- your username under `active-authors`

Finally, if you performed a _sparse checkout_ as recommended, you will need to add directory `_posts` in the root of
your local copy.

### Adding a new post

Below is a summary for getting started; for more comprehensive instructions on authoring posts, including markdown
syntax and linking to images, see the pages on our [company intranet][confluence-getting-started].

Within the `_posts` directory, add a markdown file for your new post, named with date and title similar to existing
posts, e.g. `2024-10-31-Your-snappy-post-title.md`. Copy the headers from a recent post, and modify the values for
yours. You may add as many tags (keywords) as you like, but a maximum of two Categories; see `_data/categories.yml` for
our current categories.

Note that Jekyll will not display your post unless the header date is in the past, so if you do not see your post when
running locally, check the date (and time) first. As you can probably guess, this is how you can set a ""Go Live"" date
in the future, if you don't want your post to appear immediately.

Once you have your skeleton file in place, you can run the blog and start writing. Saving changes should trigger a
rebuild.

### Run the blog locally

By far the easiest route is to use Docker: if you have it installed, you can [skip ahead][run-docker] now!

The blog consists of static HTML pages with content generated using:
- [github-pages][ruby-github-pages] for deployment hooks
- [Jekyll][jekyll-docs] for static site generation generator
- [Kramdown][kramdown-syntax] for an extended markdown syntax
- [Liquid][ruby-liquid] for templating functionality
- [Nokogiri][ruby-nokogiri] for efficient XML and HTML handling, relying on:
  - Native XML Parsers
- [Bundler][ruby-bundler] to manage gems and dependencies
- [Ruby][ruby-downloads].

_In theory_, once you've installed Ruby and Bundler,
given that the project contains a valid [Gemfile][project-gemfile],
then using Bundler should bring in most of the dependencies automatically.
However, due to Nokogiri's reliance on Native XML parsers you may require additional steps.
Thorough instructions for setting up your development environment are detailed below.

#### Prerequisites

First, install Ruby and (if on Linux) a few build dependencies for Nokogiri.

On Linux:

```shell
sudo apt-get install ruby2.3 ruby2.3-dev build-essential dh-autoreconf libxslt-dev libxml2-dev zlib1g-dev
```

On Windows, if you use Chocolatey, simply run `choco install ruby` in a PowerShell instance
with elevated priveleges. If you don't use Chocolatey, you can use [RubyInstaller][rubyinstaller]
or see the Ruby website for [alternative ways to install Ruby][ruby-installation-instructions].
You don't need to install any other dependencies on Windows at this stage.

Secondly, update Gem and install the Jekyll, Bundler and Nokogiri gems.

On Linux:

```shell
sudo gem update
sudo gem install jekyll bundler nokogiri
```

On Windows, in a PowerShell instance with elevated privileges:

```shell
gem update
gem install jekyll bundler nokogiri
```

Thirdly, configure Bundler to store project dependencies in `vendor/bundle`, and,
when in the root directory of your clone of the blog, install the project dependencies.

```shell
bundle config path vendor/bundle
cd PATH/TO/BLOG
bundle install
```

Finally, run `jekyll -v` to check whether Jekyll is working. If so, you're good to run the blog!

#### Running in the native environment

Once you've got all the prerequisites for your operating system, you can run the blog.
Navigate to the root directory of your clone of the blog and execute Jekyll using Bundler.

```shell
bundle exec jekyll serve
```

The blog will then be available on [localhost][localhost].

If you are working on fixes or new features, and need to re-compile the scripts or SCSS, you can use these npm scripts:

```shell
npm ci
npm run scripts
npm run style
```

### Running with Docker

Use a bash-compatible shell; Git bash on Windows should work fine.

#### Install gem dependencies

First, we output gem dependencies to directory `container_gem_cache` on the host machine. This is analogous to running
""npm install"" for an npm package:

```shell
./shell/docker-gem-install.sh
```

#### Run in watch mode

Now we can serve the blog with live reloading. Replace ""jbloggs"" with your ScottLogic username:

```shell
BLOG_USERNAME=jbloggs ./shell/docker-dev-watch.sh
```

It'll take a while to build first time, but once it's done you should see message ""done in XXX.YYY seconds"".
Then you can navigate to [localhost][localhost] in your browser.

Note that if you performed a _sparse checkout_ as recommended, and if this is your first post, then you won't see any
blog posts when the site loads unless you've already added a file for your new blog post.

## CI/CD

We use GitHub Actions for CI/CD. The workflow definitions are in YAML files
in `.github/workflows`.

### Compress Images Once a Month

Uses the [calibreapp/image-actions][calibreapp-image-actions] Action to
automatically compress images. The compression algorithm is near-lossless. It
compresses all images in the repo once per month, and creates a Pull Request to
merge in the resulting changes.

### Check accessibility of changed content

Runs [pa11y-ci][pa11y-ci] with the aXe test runner to detect some common
accessibility problems. It serves the blog locally and runs the tests on the
rendered webpages. It only checks pages and blog posts which have changed, but
doesn’t take any interest in changes to layouts or includes, so changes to
those should be tested across the whole site separately. This workflow runs on
Pull Requests, pushes to `gh-pages` and on manual dispatches.

### Generate Read More related

Generates Read More links on blog pages across the blog, using the OpenAI API
to determine which blog posts are on similar themes. This workflow runs only on
manual dispatches on the `gh-pages` branch and creates a Pull Request to merge
in the resulting changes.

### Remove Unused Images

For each image in the repo, searches all the blog posts, pages, YAML data files
and JavaScript scripts for any occurrences of its filename. If the filename
occurs nowhere, deletes the image. Then makes a Pull Request to merge in its
changes. This workflow runs only on a manual dispatch on the `gh-pages` branch.

[calibreapp-image-actions]: https://github.com/calibreapp/image-actions
[confluence-getting-started]: https://scottlogic.atlassian.net/wiki/spaces/INT/pages/3577479175/Getting+started+with+the+Scott+Logic+blog
[sparse-checkout-guide]: https://github.blog/2020-01-17-bring-your-monorepo-down-to-size-with-sparse-checkout/#sparse-checkout-and-partial-clones

[github-pages]: https://pages.github.com/
[github-pages-docs]: https://docs.github.com/en/pages
[run-docker]: #running-with-docker
[jekyll-docs]: https://jekyllrb.com/docs/
[kramdown-syntax]: https://kramdown.gettalong.org/syntax.html
[localhost]: http://localhost:4000
[ruby-github-pages]: https://rubygems.org/gems/github-pages
[ruby-bundler]: https://bundler.io/
[rubyinstaller]: https://rubyinstaller.org/
[ruby-installation-instructions]: https://www.ruby-lang.org/en/documentation/installation
[ruby-nokogiri]: https://nokogiri.org/
[ruby-liquid]: https://shopify.github.io/liquid/
[ruby-downloads]: https://www.ruby-lang.org/en/downloads/
[pa11y-ci]: https://github.com/pa11y/pa11y-ci
[project-gemfile]: ./Gemfile
[scottlogic-blog-repo]: https://github.com/ScottLogic/blog",VRAI
semaphoreio/semaphore,DevOPs,DevOPs,2025-05-15T10:19:18Z,2025-04-23T15:12:16Z,0,6,0,0,0,0,0,0,2025-01-21T12:37:52Z,2025-04-08T02:19:13Z,49000,957,Elixir,VRAI,31,FAUX,48,"build-pipelines,cd,ci,continuous-delivery,continuous-integration,elixir,golang",48,Semaphore is an open source CI/CD platform. Self-host Semaphore on your own servers or on a cloud provider.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,16,"<div align=""center"">
  
# Semaphore - Open Source CI/CD Platform
[![CI/CD, Semaphore, Continuous Integration](files/hero.png)](https://www.semaphore.io/)

<!-- Badges -->

  
[![Build Status](https://semaphore.semaphoreci.com/badges/semaphore/branches/main.svg?key=e8fbbbbc-ba9f-43cd-8438-e55c61ca6308)](https://semaphore.semaphoreci.com/projects/semaphore)
[![Stars](https://img.shields.io/github/stars/semaphoreio/semaphore)](https://github.com/semaphoreio/semaphore/stargazers)
[![Discord](https://img.shields.io/discord/1097422014735732746?label=Discord&logo=discord&color=5865F2&style=social)](https://discord.gg/FBuUrV24NH)
[![Twitter Follow](https://img.shields.io/twitter/follow/semaphoreci?style=social)](https://twitter.com/semaphoreci)

Semaphore CI/CD helps product teams ship software faster, with quality and security.

</div>

___

## 🚀 Features

- **Blazing-fast CI/CD** – Get your builds & deployments done in record time.  
- **Simple yet powerful** – YAML-based config, parallel execution, and more!  
- **Scales with you** – From solo developers to massive engineering teams.  
- **Built for the modern cloud** – Works seamlessly with containers, Kubernetes, and multi-cloud environments.  

[![CI/CD, Semaphore, Continuous Integration](files/screenshot.png)](https://www.semaphore.io/)

___

## Installation
Installing and running Semaphore is easy and only takes **10-30 minutes** ⏱️

Choose your preferred installation method from our detailed guides:

### 🎡 Kubernetes Cluster

[![GKE](https://img.shields.io/badge/Google_Kuberenetes_Engine_(GKE)-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)](https://docs.semaphoreci.com/CE/getting-started/install-gke) 
[![EKS](https://img.shields.io/badge/Amazon_Elastic_Kubernetes_Service_(EKS)-FF9900?style=for-the-badge&logo=amazonwebservices&logoColor=black)](https://docs.semaphoreci.com/CE/getting-started/install-eks)

### 💻 Single Machine

[![Ubuntu VM](https://img.shields.io/badge/Ubuntu_Machine-E95420?style=for-the-badge&logo=ubuntu&logoColor=white)](https://docs.semaphoreci.com/CE/getting-started/install-ubuntu)
[![Google Cloud Compute (VM)](https://img.shields.io/badge/Google_Cloud_Compute-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)](https://docs.semaphoreci.com/CE/getting-started/install-gcompute)
[![EC2](https://img.shields.io/badge/AWS_Compute_(EC2)-FF9900?style=for-the-badge&logo=amazonwebservices&logoColor=black)](https://docs.semaphoreci.com/CE/getting-started/install-aws-ec2)

___

## Semaphore Versions

Semaphore comes in three flavors to fit your needs:

### 🌱 Community Edition
Free and open source under [Apache 2.0 license](LICENSE). This is what you'll find in this repository (everything outside the `ee/` folder). Perfect for teams who want to run Semaphore on their own infrastructure with community support.

### 🏢 Enterprise Edition
Our enhanced version with extra features for larger organizations, available under a commercial license. You'll find this code in the `ee/` directory. Comes with professional support to keep your CI/CD running smoothly.

### ☁️ Semaphore Cloud
Don't want to manage your own infrastructure? Our hosted version at [semaphoreci.com](https://semaphoreci.com) gives you all the power of Semaphore without the setup. From free plans for small projects to enterprise-scale solutions.

___
## Important Links

- 📚 [Documentation](https://docs.semaphoreci.com/CE/getting-started/about-semaphore) - Learn how to use Semaphore
- 🗺️ [Roadmap](ROADMAP.md) - See what's planned for future releases
- 🏛️ [Governance](GOVERNANCE.md) - How the project is managed and decisions are made
- 📋 [Project Board](https://github.com/orgs/semaphoreio/projects/1) - Track development progress on our Kanban board
- 🔒 [Security](SECURITY.md) - Our security policies and reporting procedures
___

## Contributing

We're excited to welcome contributions to Semaphore! All contributors are expected to follow our [Code of Conduct](CODE_OF_CONDUCT.md).

[![GitHub Discussions](https://img.shields.io/github/discussions/semaphoreio/semaphore?label=Discussions&logo=github)](https://github.com/semaphoreio/semaphore/discussions) 
[![GitHub Issues](https://img.shields.io/github/issues/semaphoreio/semaphore?label=Issues&logo=github)](https://github.com/semaphoreio/semaphore/issues)
[![GitHub Contributors](https://img.shields.io/github/contributors/semaphoreio/semaphore?logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIyNCIgdmVyc2lvbj0iMS4xIiB3aWR0aD0iMjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgeG1sbnM6Y2M9Imh0dHA6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL25zIyIgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPjxnIHRyYW5zZm9ybT0idHJhbnNsYXRlKDAgLTEwMjguNCkiPjxwYXRoIGQ9Im03IDEwMzEuNGMtMS41MzU1IDAtMy4wNzg0IDAuNS00LjI1IDEuNy0yLjM0MzEgMi40LTIuMjc4OCA2LjEgMCA4LjVsOS4yNSA5LjggOS4yNS05LjhjMi4yNzktMi40IDIuMzQzLTYuMSAwLTguNS0yLjM0My0yLjMtNi4xNTctMi4zLTguNSAwbC0wLjc1IDAuOC0wLjc1LTAuOGMtMS4xNzItMS4yLTIuNzE0NS0xLjctNC4yNS0xLjd6IiBmaWxsPSIjYzAzOTJiIi8+PC9nPjwvc3ZnPg==)](https://github.com/semaphoreio/semaphore/graphs/contributors)
[![GitHub Good First Issue](https://img.shields.io/github/issues/semaphoreio/semaphore/good%20first%20issue?label=Good%20First%20Issues&logo=github)](https://github.com/semaphoreio/semaphore/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)
[![GitHub help wanted](https://img.shields.io/github/issues/semaphoreio/semaphore/help%20wanted?label=Help%20Wanted&logo=github)](https://github.com/semaphoreio/semaphore/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)

There are many ways to contribute to Semaphore:

- Join the conversation in [GitHub Discussions](https://github.com/semaphoreio/semaphore/discussions)
- Submit and review [RFCs](rfcs/README.md)
- Participate in [Special Interest Groups (SIGs)](sigs/README.md)
- Report bugs or request features through [GitHub Issues]([https://github.com/semaphoreio/semaphore/issues](https://github.com/semaphoreio/semaphore/issues/new/choose))

🚧 As we open up the project, our documentation and processes are still evolving.  
For now, please refer to:

- [Contributing Guide](CONTRIBUTING.md) - Development setup and workflow
- [Release Process](RELEASE.md) - How we version and release Semaphore
- Developer Guidelines - Coding standards and best practices **WIP** 🚧

---

## ❤️ Support & Stay Connected  

💬 **Join our community on Discord** → [Semaphore Discord](https://discord.com/invite/FBuUrV24NH)

🐦 **Follow us on Twitter** → [@semaphoreci](https://twitter.com/semaphoreci)  

▶️ **Watch our development meetings** → [Semaphore Backstage](https://www.youtube.com/@SemaphoreBackstage)",FAUX
sergueik/springboot_study,Documentations,Documentations,2025-03-16T23:37:23Z,2024-11-28T08:24:54Z,360,0,0,0,0,0,0,0,2017-08-25T13:38:05Z,2025-03-16T23:37:42Z,89785,10,Java,VRAI,7,FAUX,38,"docker,spring-boot",38,basic project collection exploring spring boot and docker,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,2,,FAUX
ShanguUncle/UnityChatHololens,Application System,Documentations,2024-06-15T05:42:01Z,2021-04-14T02:46:39Z,4,0,0,0,0,0,0,0,2020-03-31T12:41:18Z,2024-11-20T00:20:44Z,356247,48,C#,VRAI,23,FAUX,0,"chat,hololens,vidoechat",0,Unity Video Chat Hololens,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,3,"# UnityChatHololens v8.2
UnityChatHololens(**unity2019.4LTS,MixedRealityToolkit-Unity-2.7.3**)

HoloLens video call example.

This project comes from

**UnityChatSDK**

https://github.com/ShanguUncle/UnityChatSDK

Please update vs2019 to the latest version！

![Image text](https://github.com/ShanguUncle/UnityChatHololens/blob/master/Screenshot/h5.png)

QQ group：211031265

https://www.bilibili.com/video/BV1VK411V74p

https://www.bilibili.com/video/BV1Jg4y1B7Ts/


**microsoft official resource**：

https://github.com/Microsoft/MixedRealityToolkit-Unity/releases

https://docs.microsoft.com/en-us/windows/mixed-reality/unity-development-overview

https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/GettingStartedWithTheMRTK.html


【**Hololens1 setting**】

![Image text](https://github.com/ShanguUncle/UnityChatHololens/blob/master/Screenshot/h2.png)
![Image text](https://github.com/ShanguUncle/UnityChatHololens/blob/master/Screenshot/h3.png)

【**Hololens2 setting**】

![Image text](https://github.com/ShanguUncle/UnityChatHololens/blob/master/Screenshot/v2.1.png)
![Image text](https://github.com/ShanguUncle/UnityChatHololens/blob/master/Screenshot/v2.2.png)
![Image text](https://github.com/ShanguUncle/UnityChatHololens/blob/master/Screenshot/untitled5.jpg)
![Image text](https://github.com/ShanguUncle/UnityChatHololens/blob/master/Screenshot/untitled6.jpg)

## RemoteMark Project

https://www.bilibili.com/video/BV1rp4y1h7kc

https://www.bilibili.com/video/BV19v4y1U7xz

![Image text](https://github.com/ShanguUncle/UnityChatHololens/blob/master/Screenshot/rm01.jpg)

## Pay for RemoteMark
<a href=""https://www.paypal.com/cgi-bin/webscr?&cmd=_xclick&business=1786570525@qq.com&currency_code=USD&amount=1760&item_name=RemoteMarkProject"" target=""_blank""><img src=""https://github.com/ShanguUncle/UnityChatSDK/blob/master/Readme/Images/SDK/pay.gif"" border=""0"" name=""submit"" alt=""Click to pay with PayPal!""></a>",VRAI
shortlink-org/shortlink,Application System,Documentations,2025-04-21T07:27:48Z,2025-04-21T07:11:13Z,0,0,0,0,88,0,0,0,2019-08-27T16:16:58Z,2025-03-20T19:00:13Z,713324,761,Go,VRAI,39,FAUX,12,"architecture,argocd,best-practices,codegen,csi-driver,ddd,ddd-sample,event-sourcing,example,gitops,go,golang,helm-chart,k8s,kafka,kubernetes,layered-architecture,microservices,mq,shortlink",12,Shortlink service (Microservice example) ⭐️ Star the repo if you like it!,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,22,"<div align=""center"">

# shortlink 

ShortLink is an open-source educational project that provides a pretty user interface and respects GDPR. 

The goal of the project is to demonstrate the practical application of microservices architecture.

[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/shortlink)](https://artifacthub.io/packages/search?repo=shortlink)
[![PkgGoDev](https://pkg.go.dev/badge/mod/github.com/shortlink-org/shortlink)](https://pkg.go.dev/mod/github.com/shortlink-org/shortlink)
[![codecov](https://codecov.io/gh/shortlink-org/shortlink/branch/main/graph/badge.svg?token=Wxz5bI4QzF)](https://codecov.io/gh/shortlink-org/shortlink)
[![Go Report Card](https://goreportcard.com/badge/github.com/shortlink-org/shortlink)](https://goreportcard.com/report/github.com/shortlink-org/shortlink)
[![Releases](https://img.shields.io/github/release-pre/shortlink-org/shortlink.svg)](https://github.com/shortlink-org/shortlink/releases)
[![LICENSE](https://img.shields.io/github/license/shortlink-org/shortlink.svg)](https://github.com/shortlink-org/shortlink/blob/main/LICENSE)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3510/badge)](https://bestpractices.coreinfrastructure.org/projects/3510)
[![StackShare](http://img.shields.io/badge/tech-stack-0690fa.svg?style=flat)](https://stackshare.io/shortlink-org/shortlink)
[![FOSSA Status](https://app.fossa.com/api/projects/custom%2B396%2Fgithub.com%2Fshortlink-org%2Fshortlink.svg?type=shield)](https://app.fossa.com/projects/custom%2B396%2Fgithub.com%2Fshortlink-org%2Fshortlink?ref=badge_shield)
[![DeepSource](https://app.deepsource.com/gh/shortlink-org/shortlink.svg/?label=active+issues&show_trend=true&token=DL-zlqtnyx6CvlHCroG0Jdx5)](https://app.deepsource.com/gh/shortlink-org/shortlink/)

<hr />

<div style=""align-items: center; display: flex;"">
  <a href=""https://www.producthunt.com/posts/shortlink-2?utm_source=badge-featured&utm_medium=badge&utm_souce=badge-shortlink&#0045;2"" target=""_blank""><img src=""https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=374140&theme=light"" alt=""ShortLink - Get&#0032;ready&#0032;to&#0032;share&#0032;your&#0032;links&#0032;with&#0032;ease&#0033; | Product Hunt"" style=""width: 250px; height: 54px;"" width=""250"" height=""54"" /></a>
  <img height=""100px"" src=""https://slsa.dev/images/SLSA-Badge-full-level1.svg"" alt=""SLSA"">
</div>

</div>
<hr />

### High Level Architecture 🚀

The project covers the entire process—from identifying Bounded Contexts to implementing microservices using
cutting-edge technologies and best practices.  

![shortlink-architecture](./docs/shortlink-architecture.png)
_Please [star ⭐](https://github.com/shortlink-org/shortlink/stargazers) the repo if you want us to continue developing and improving ShortLink! 😀_

- [Boundaries](./boundaries/README.md) - read more about boundaries.

> #### Contributing
>
> - [Getting Started](./CONTRIBUTING.md#getting-started)

### Architecture decision records (ADR)

> [!IMPORTANT]
> An architecture decision record (ADR) is a document that captures an important architecture decision 
made along with its context and consequences.
>
>+ [Docs ADR](https://github.com/joelparkerhenderson/architecture-decision-record)
>
> **Decisions:**
>  + [main decisions](./docs/ADR/README.md)
>  + [ops decisions](./ops/docs/ADR/README.md)
>
> Also, each boundary context and service has its own ADR. You can find them in the relevant sections.

### License

> [!WARNING]
> 
> This project includes dependencies licensed under the GNU Lesser General Public License (LGPL). 
> Users must comply with LGPL terms when using or modifying these dependencies. 
> For detailed information on each LGPL library used in this project, please refer to the respective license documentation 
> included with each library. For comprehensive license compliance information, including dependencies and their licenses, 
> you can read more details in our FOSSA report.

[![FOSSA Status](https://app.fossa.com/api/projects/custom%2B396%2Fgithub.com%2Fshortlink-org%2Fshortlink.svg?type=large)](https://app.fossa.com/projects/custom%2B396%2Fgithub.com%2Fshortlink-org%2Fshortlink?ref=badge_large)

[mergify]: https://mergify.io

[mergify-status]: https://img.shields.io/endpoint.svg?url=https://dashboard.mergify.io/badges/shortlink-org/shortlink&style=flat",VRAI
Shuffle/python-apps,Application System,Documentations,2025-04-02T22:39:25Z,2025-01-31T14:57:36Z,0,0,0,0,0,1,0,0,2020-05-09T06:57:26Z,2025-04-02T22:39:30Z,42333,109,Python,VRAI,111,FAUX,129,"api,generated,python",129,"Apps to be used for Shuffle automation. Most of Shuffle's apps (2500+) are generated from APIs, and available in the search engine below:",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,53,"# Shuffle Apps
All public apps are available in the search, engine either in your local instance or on [https://shuffler.io/search?tab=apps](https://shuffler.io/search?tab=apps). This is a repository for apps to be used in [Shuffle](https://github.com/shuffle/shuffle)

**PS:** These apps should be valid with WALKOFF (from NSA), but the SDK is different, meaning you have to change the FIRST line in each Dockerfile (FROM shuffle/shuffle:app_sdk) to make it compatible with Shuffle.

## App Creation 
App creation can be done with the Shuffle App Creator (exports as OpenAPI) or Python, which makes it possible to connect _literally_ any tool. Always prioritize using the App Creator when applicable. 

![Shuffle-workflow-categories](https://github.com/shuffle/shuffle-workflows/blob/master/images/categories_circle_dark.png)

### References 
* [App Development Process](https://github.com/shuffle/shuffle-docs/blob/master/handbook/engineering/app_development.md)
* [Python app documentation](https://shuffler.io/docs/app_creation)
* [Apps in progress](https://github.com/shuffle/shuffle-apps/projects/1)

### Categories 
We have defined eight (8) ""major"" categories of tools that are necessary to any cybersecurity threat. Most security-related tools can fit into one of these eight.
1. [Communication](https://github.com/shuffle/shuffle-apps/issues/26) 		- Any way to chat; WhatsApp, SMS, Email etc. 
2. [Case Management](https://github.com/shuffle/shuffle-apps/issues/22)	- The central hub for operation teams.
3. [SIEM](https://github.com/shuffle/shuffle-apps/issues/21)							- Search engine for logs in an enterprise. Used to find evil.
4. [Assets](https://github.com/shuffle/shuffle-apps/issues/25) 					- Discover endpoint information. Vulnerabilities, owners, departments etc.
5. [IAM](https://github.com/shuffle/shuffle-apps/issues/86)  						- Access Management. Active Directory, Google Workspaces, Single Sign-on etc.
6. [Intelligence](https://github.com/shuffle/shuffle-apps/issues/24) 		- Typically a vendor explaining what you should be looking for.
7. [Network](https://github.com/shuffle/shuffle-apps/issues/27)					- Anything BETWEEN your connected devices. Firewalls, WAF, Switches, Bluetooth...
8. [Eradication](https://github.com/shuffle/shuffle-apps/issues/23) 			- Control machines directly to eradicate evil. Hard and undefined (EDR & AV)

## OpenAPI
Apps in this repository are mostly manually made. Shuffle is striving for standardization and accessability, and our effort is focused on OpenAPI rather than manual work. With this in mind, most app creation that supports REST API's will be continued here.

[Shuffle OpenAPI](https://github.com/frikky/security-openapis)

## Support
* [Discord](https://discord.gg/B2CBzUm)
* [Twitter](https://twitter.com/shuffleio)
* [Email](mailto:support@shuffler.io)
* [Open issue](https://github.com/shuffle/shuffle/issues/new)
* [Shuffler.io](https://shuffler.io/contact)

## External contributions
[**App magicians**](https://github.com/shuffle/shuffle-apps)
<a href=""https://github.com/shuffle/shuffle-apps/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=shuffle/shuffle-apps"" />
</a>

[**OpenAPI creators**](https://github.com/frikky/security-openapis)
<a href=""https://github.com/shuffle/shuffle-apps/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=frikky/security-openapis"" />
</a>

# License
All apps, workflows and modular parts of Shuffle including our App SDK is under licensed under MIT, meaning you can freely use it anywhere in any way you want.

# Contributing
Contributing guidelines for outlined [here](https://github.com/shuffle/shuffle/blob/master/.github/CONTRIBUTING.md).",VRAI
sixeyed/kiamol,Documentations,Documentations,2025-02-19T22:06:58Z,2021-12-15T21:18:24Z,0,0,0,0,0,0,0,7,2020-03-04T20:32:12Z,2025-04-01T01:39:55Z,10591,316,C#,VRAI,246,FAUX,59,kubernetes,59,Learn Kubernetes in a Month of Lunches,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,2,"# [Learn Kubernetes in a Month of Lunches](https://www.manning.com/books/learn-kubernetes-in-a-month-of-lunches)

This is the source code for my Manning book _Learn Kubernetes in a Month of Lunches_ (codename `kiamol`).

> It's a work in progress. You can get all the completed chapters with [Manning's Early Access Program](https://www.manning.com/books/learn-kubernetes-in-a-month-of-lunches).

## Elevator Pitch

In Learn Kubernetes in a Month of Lunches you’ll go from ""what’s a Pod?"" to automatically scaling clusters of containers and components in just 22 hands-on lessons, each short enough to fit into a lunch break. 

Every lesson is task-focused and covers an essential skill on the road to Kubernetes mastery. You'll learn how to smooth container management with Kubernetes, including securing your clusters, and upgrades and rollbacks with zero downtime. No development stack, platform, or background is assumed. 

All patterns are described generically, so you can easily apply them to your applications and port them to other projects!

## Cover Art

![Cover of the book, Learn Kubernetes in a Month of Lunches](docs/img/cover_meap.jpg)

### Builds

![Ch02](https://github.com/sixeyed/kiamol/workflows/Ch02%20Image%20Builds/badge.svg)
![Ch03](https://github.com/sixeyed/kiamol/workflows/Ch03%20Image%20Builds/badge.svg)
![Ch04](https://github.com/sixeyed/kiamol/workflows/Ch04%20Image%20Builds/badge.svg)
![Ch05](https://github.com/sixeyed/kiamol/workflows/Ch05%20Image%20Builds/badge.svg)
![Ch07](https://github.com/sixeyed/kiamol/workflows/Ch07%20Image%20Builds/badge.svg)
![Ch09](https://github.com/sixeyed/kiamol/workflows/Ch09%20Image%20Builds/badge.svg)
![Ch10](https://github.com/sixeyed/kiamol/workflows/Ch10%20Image%20Builds/badge.svg)
![Ch11](https://github.com/sixeyed/kiamol/workflows/Ch11%20Image%20Builds/badge.svg)
![Ch12](https://github.com/sixeyed/kiamol/workflows/Ch12%20Image%20Builds/badge.svg)
![Ch13](https://github.com/sixeyed/kiamol/workflows/Ch13%20Image%20Builds/badge.svg)
![Ch14](https://github.com/sixeyed/kiamol/workflows/Ch14%20Image%20Builds/badge.svg)
![Ch15](https://github.com/sixeyed/kiamol/workflows/Ch15%20Image%20Builds/badge.svg)
![Ch16](https://github.com/sixeyed/kiamol/workflows/Ch16%20Image%20Builds/badge.svg)
![Ch17](https://github.com/sixeyed/kiamol/workflows/Ch17%20Image%20Builds/badge.svg)
![Ch19](https://github.com/sixeyed/kiamol/workflows/Ch19%20Image%20Builds/badge.svg)",FAUX
snyk/kubernetes-monitor,Toolkit,Application System,2025-05-12T09:19:16Z,2024-11-26T07:26:59Z,0,1,0,0,0,0,0,0,2019-05-05T08:22:38Z,2025-04-04T04:40:14Z,22185,90,TypeScript,VRAI,77,FAUX,22,,22,Use Snyk to find and fix vulnerabilities in your Kubernetes workloads,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,60,"# Testing the Kubernetes-Monitor

- [Testing the Kubernetes-Monitor](#testing-the-kubernetes-monitor)
  - [Unit Tests](#unit-tests)
  - [System Tests](#system-tests)
  - [Debugging with Tilt](#debugging-with-tilt)
    - [Start a debugging session](#start-a-debugging-session)
    - [Errors with read-only file system](#errors-with-read-only-file-system)
    - [Cleaning up](#cleaning-up)

The Kubernetes-Monitor has different testing suites, each with different purposes and requirements.
All our tests prefer a blackbox approach whenever possible.

Different tests have different requirements in terms of software and environment variables. Requirements specific to one test suite will be described in each section, but the requirements shared by all of them are:

1. npm
2. Node (v10 or higher)

In order to run the Kubernetes-Monitor's tests, please run
`npm test`.

## Unit Tests

These tests aim to check a single function, class or module.
Our unit tests aren't thoroughly mocked, resulting in some tests' code reaching the Kubernetes client library we're using, adding noise and/or failures to some unit tests.
Until this is fixed, one workaround is setting one's KUBECONFIG environment variable to a valid kubeconfig file.

Run with `npm run test:unit`.

## System Tests

System tests are supposed to test the Kubernetes-Monitor as a stand-alone component with as little external dependencies as possible. They are also supposed to completely cover the core functionality, so mocking or ignoring the Kubernetes API is out of the question.
The resulting infrastructure is comprised of a local KinD cluster (like our integration tests) but does not install the Kubernetes-Monitor inside it. Rather, it runs the Kubernetes-Monitor's code as part of the test, and configures it against the KinD cluster.
This means we're not running in the real runtime environment we expect to run (a Kubernetes cluster), but it's much easier to test the Monitor's outgoing requests or even internal state if we choose to, instead of relying on the Upstream service's state and API.

This test requires Skopeo for MacOS machines, but will install it for Linux machines that don't have it.

Run with `npm run test:system`.

## Debugging with Tilt

Tilt allows you to run and debug the snyk-monitor while it is running in a container. Tilt deploys the snyk-monitor using the same Helm chart that we publish to users.

You can download Tilt from the [Tilt GitHub repository](https://github.com/tilt-dev/tilt#install-tilt).

### Start a debugging session

First, ensure you have the snyk-monitor namespace set up and the snyk-monitor Secret with your integration ID and dockercfg (as per the prerequisites for installing snyk-monitor).

Finally, put breakpoints in the code and run `tilt up`.

### Errors with read-only file system

If you see an error like the following...

```shell
Error: EROFS: read-only file system, mkdir '/srv/app/.npm/_npx'
```

... it means that the `readOnlyRootFilesystem` protection on the snyk-monitor Helm Deployment causes issues with Tilt. This can be fixed by removing the `readOnlyRootFilesystem: true` value from the Helm chart located in `snyk-monitor/templates/deployment.yaml`.

### Cleaning up

Run `tilt down` to tear down the debugging session.",VRAI
snyk/policy-engine,Toolkit,Toolkit,2025-03-31T16:06:49Z,2024-03-22T11:03:24Z,0,40,0,0,0,0,0,0,2022-04-05T17:17:52Z,2025-04-07T07:24:34Z,3705,53,Go,FAUX,15,FAUX,3,,3,Unified Policy Engine,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,22,"# Policy Engine

This repository contains a Go library that performs two main functions:

* Parses IaC configurations
* Evaluates resources using Open Policy Agent

It also provides a small CLI that can be used to author and test policies.

```sh
go build
./policy-engine help
```

## Additional documentation

Additional documentation can be found in the [`docs`](./docs) directory. The current
set of additional documents are:

* [Policies specification](docs/policy_spec.md)
  * Describes the structure and API for writing policies
* [Policy authoring guide](docs/policy_authoring.md)
  * Contains a tutorial for authoring policies and instructions for writing policy tests
* [Use as a library](docs/library_usage.md)
  * Describes how to use `policy-engine` as a Go library
* [Notes for policy engine developers](docs/development.md)
  * Describes processes and conventions for working on this repository
* [Security](docs/security.md)
  * Describes measures to take when policy-engine on untrusted inputs or code

## Contributing

Should you wish to make a contribution please open a pull request against this
repository with a clear description of the change with tests demonstrating
the functionality. You will also need to agree to the [Contributor
Agreement](./Contributor-Agreement.md) before the code can be accepted and
merged.",FAUX
snyk/snyk-iac-rules,Toolkit,Toolkit,2025-04-01T14:48:41Z,2023-02-16T11:20:01Z,0,19,0,0,0,0,0,0,2021-08-26T14:30:47Z,2025-04-04T04:40:21Z,9641,11,Go,FAUX,10,FAUX,0,infrastructure-as-code,0,,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,21,"# `snyk-iac-rules` SDK
---

[![CircleCI](https://dl.circleci.com/status-badge/img/gh/snyk/snyk-iac-rules/tree/main.svg?style=svg)](https://dl.circleci.com/status-badge/redirect/gh/snyk/snyk-iac-rules/tree/main)
[![Shellspec Tests](https://github.com/snyk/snyk-iac-rules/actions/workflows/main.yml/badge.svg)](https://github.com/snyk/snyk-iac-rules/actions/workflows/main.yml)
[![Contract Tests](https://github.com/snyk/snyk-iac-rules/actions/workflows/contract.yml/badge.svg)](https://github.com/snyk/snyk-iac-rules/actions/workflows/contract.yml)
[![Registries Tests](https://github.com/snyk/snyk-iac-rules/actions/workflows/registries.yml/badge.svg)](https://github.com/snyk/snyk-iac-rules/actions/workflows/registries.yml)
[![Golang Version](https://img.shields.io/github/go-mod/go-version/snyk/snyk-iac-rules)](https://github.com/snyk/snyk-iac-rules)

[![Latest release version](https://img.shields.io/github/v/release/snyk/snyk-iac-rules)](https://github.com/snyk/snyk-iac-rules)
[![Latest release date](https://img.shields.io/github/release-date/snyk/snyk-iac-rules)](https://github.com/snyk/snyk-iac-rules)

`snyk-iac-rules` is a Golang SDK that provides flags for writing, debugging, testing, bundling, and distributing custom rules for the [Snyk IaC CLI](https://github.com/snyk/snyk/).

---

# About
The SDK is a tool for writing, debugging, testing, and bundling custom rules for [Snyk Infrastructure as Code](https://snyk.io/product/infrastructure-as-code-security/). See our [Custom Rules documentation](https://docs.snyk.io/products/snyk-infrastructure-as-code/custom-rules) to learn more.

<!---
This should be generated automatically from the UML code. We need to specify the branch name though, and this can not happen while we are in main. We need to get the branch name first if we continue using two branches. For now, we can use the rendered image instead.

![system overview](http://www.plantuml.com/plantuml/proxy?cache=no&src=https://raw.github.com/snyk/snyk-iac-rules/main/assets/overview-activity-swimlanes.puml)
-->
![image](https://user-images.githubusercontent.com/6989529/139833924-da0f79c7-997b-4510-a6e9-f40f39b28482.png)


# Install
The SDK can be installed through multiple channels.

## Install with npm or Yarn

[snyk-iac-rules available as an npm package](https://www.npmjs.com/package/snyk-iac-rules). If you have Node.js installed locally, you can install it by running:

```bash
npm install snyk-iac-rules@latest -g
```

or if you are using Yarn:

```bash
yarn global add snyk-iac-rules
```

## Install with Docker

[snyk-iac-rules available as a docker image](https://hub.docker.com/r/snyk/snyk-iac-rules). If you have Docker installed locally, you can install it by running:

```bash
docker pull snyk/snyk-iac-rules:latest
```

You can then run the container like so:
```bash
docker run --rm -v $(pwd):/app snyk/snyk-iac-rules --help
```

## More installation methods

<details>
  <summary>Standalone executables (macOS, Linux, Windows)</summary>

### Standalone executables

Use [GitHub Releases](https://github.com/snyk/snyk-iac-rules/releases) to download a standalone executable of Snyk CLI for your platform.

For example, to download and run the latest SDK on macOS, you could run:

```bash
wget https://github.com/snyk/snyk-iac-rules/releases/download/v0.1.0/snyk-iac-rules_0.1.0_Darwin_x86_64.tar.gz
chmod +x ./snyk-iac-rules
mv ./snyk-iac-rules /usr/local/bin/
```

Drawback of this method is, that you will have to manually keep the SDK up to date.

</details>

<details>
  <summary>Install with Homebrew (macOS, Linux)</summary>

### Homebrew

Install the SDK from [Snyk tap](https://github.com/snyk/homebrew-tap) with [Homebrew](https://brew.sh) by running:

```bash
brew tap snyk/tap
brew install snyk-iac-rules
```

</details>

<details>
  <summary>Scoop (Windows)</summary>

### Scoop

Install the SDK from our [Snyk bucket](https://github.com/snyk/scoop-snyk) with [Scoop](https://scoop.sh) on Windows:

```
scoop bucket add snyk https://github.com/snyk/scoop-snyk
scoop install snyk-iac-rules
```

</details>

---

# Getting started with snyk-iac-rules

Once you installed the `snyk-iac-rules` SDK, you can verify it's working by running

```bash
snyk-iac-rules --help
```

For more help, read the documentation about [Snyk Infrastructure as Code](https://docs.snyk.io/snyk-infrastructure-as-code).

# Getting support

We recommend reaching out via the [support@snyk.io](mailto:support@snyk.io) email whenever you need help with the SDK or Snyk in general.


* See [DEVELOPMENT.md](DEVELOPMENT.md) for how to setup the environment, add a new command, run the code locally, and run the tests.
* See [RELEASE.md](RELEASE.md) for how to release a new version of the SDK.

---

# Contributing

This project is open source but we don't encourage outside contributors.",FAUX
solo-io/hoot,Documentations,Documentations,2024-02-22T00:42:55Z,2023-02-07T19:41:24Z,0,6,0,0,0,0,0,0,2020-10-25T19:16:10Z,2025-03-06T15:40:10Z,26520,111,Go,VRAI,40,FAUX,11,,11,code from hoot episodes,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,19,"![Hoot Image](images/hoot-background.png)
# Hoot - Learn Kubernetes, Envoy, Istio, eBPF and GraphQL
We understand it is important for you to learn Envoy, Istio, Kubernetes, eBPF and GraphQL as part of your journey to cloud native so you can make sense of any technology or architecture decision. Hoot is designed to help you learn these technologies so you can be well prepared at your job!

## Upcoming episodes

[View Episode Calendar](https://calendar.google.com/calendar/embed?src=c_lnbos22onj2mi70gjf3r6211l0%40group.calendar.google.com) | [Add Google Calendar](https://calendar.google.com/calendar/u/0?cid=Y19sbmJvczIyb25qMm1pNzBnamYzcjYyMTFsMEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t)

## Suggest a topic
Please [open an issue](https://github.com/solo-io/hoot/issues) if you have an idea for a topic we should cover or a guest we should invite.
## Previous episodes
This repo contains the code, slides and show notes for our Hoot series:

The full playlist:
- [Youtube Playlist](https://www.youtube.com/playlist?list=PLBOtlFtGznBjESk9G7wHd9HWzprbJuoRS).

Videos:
- Episode 1: Intro to envoy - https://www.youtube.com/watch?v=KsO4pw4tEGA
- Episode 2: Observe envoy - https://www.youtube.com/watch?v=ZthWg-_Bg_c
- Episode 3: Securing enovy - https://www.youtube.com/watch?v=TwsT7oJpEas
- Episode 4: Envoy, XDS - https://www.youtube.com/watch?v=S5Fm1Yhomc4
- Episode 5: Envoy filters - https://www.youtube.com/watch?v=Po0Z9jTECfQ
- Episode 6: Envoy WASM filters - https://www.youtube.com/watch?v=8fty-sqFyoY
- Episode 7: Into to OPA - https://www.youtube.com/watch?v=iaDuJIZj6Yw
- Episode 8: OPA + Envoy - https://www.youtube.com/watch?v=X1sdpMkHW9U
- Episode 9: GitOps + Flux - https://www.youtube.com/watch?v=KKOARkFcllU
- Episode 10: Waypoint - https://www.youtube.com/watch?v=VSr1hXPP2vQ
- Episode 11: Advanced Istio Configuration with Envoy CRDs https://www.youtube.com/watch?v=sUkeFAERvE8
- Episode 12: Hitless Deploys - https://www.youtube.com/watch?v=xYFx0a0W9_E
- Episode 13: Approaches to Multi-Cluster Networking in Service Mesh (and Istio) - https://www.youtube.com/watch?v=9kl7CR8MH-w
- Episode 14: Istio Debugging - https://www.youtube.com/watch?v=QLuQB_JdzvU
- Episode 15: Envoy + External Services - https://www.youtube.com/watch?v=Veib7duO0vw
- Episode 16: Adding real-time updates to OPA with OPAL - https://www.youtube.com/watch?v=n2TdVmY8CLI
- Episode 17: Envoy + Rate Limiting - https://www.youtube.com/watch?v=1Y4kOu2TJ1k
- Episode 18: Envoy Filters - https://www.youtube.com/watch?v=yOtEG1luTwU
- Episode 19: eBPF: A Top-Down view - https://www.youtube.com/watch?v=PqghsyBF7ug
- Episode 20: 1-Click Upgrade to Istio 1.13 using Helm - https://www.youtube.com/watch?v=Q3G5TEmXq7o
- Episode 21: Istio In Action Book Highlight - https://www.youtube.com/watch?v=gpWuVnOyWnE
- Episode 22: Accelerate your mesh with eBPF and Merbridge - https://www.youtube.com/watch?v=r2wgInmsqsU
- Episode 23: HOW to increase application resiliency for Kubernetes services running on Spot VMs - https://www.youtube.com/watch?v=WIcWekCQTJU
- Episode 24: Debug Envoy Configs and Analyze Access Logs - https://www.youtube.com/watch?v=OQFFIXFeZns
- Episode 25: Istio + Spire + VM - https://www.youtube.com/watch?v=WOPoNqfrhb4
- Episode 26: Declarative Kubernetes Lifecycle Management Across Multi-Clusters/Clouds 
with Cluster API - https://www.youtube.com/watch?v=fKeqbkGiHog
- Episode 27: Gloo Cilium and Istio Seamlessly Together - https://www.youtube.com/watch?v=bAjAJtQioPU
- Episode 28: What is new in Istio 1.14? -  https://www.youtube.com/watch?v=XxMAjo8yrLI
- Episode 29: Porting eBPF applications to BumbleBee - https://www.youtube.com/watch?v=NQcOQ1-sJII
- Episode 30: HTTP/3 With Envoy Explained - https://www.youtube.com/watch?v=TjaJ5oMxNpc
- Episode 31: Cilium L7 Policies vs Istio’s -  https://youtu.be/d4mEnVZKum8
- Episode 32: GraphQL for Developers or Platform Team? - https://youtu.be/cFcIb1mh998
- Episode 33: Speed your Istio development environment with vcluster - https://youtu.be/b7OkYjvLf4Y
- Episode 34: Optional Sidecar? A Tour of Cilium Service Mesh - https://t.co/2ZtJ40wFdw
- Episode 35: Successes? Lessons? Istio @ Quizlet - https://www.youtube.com/watch?v=ESIG-ZQ4j-I
- Episode 36: Istio ambient, what does it mean to you? - https://www.youtube.com/watch?v=fCX6hkj_xeQ
- EPisode 37: Common Questions for Istio Ambient - https://www.youtube.com/watch?v=aXGwaSV1K1s
- Episode 38: What Istio ambient means for your wallet - https://www.youtube.com/watch?v=nQtN0BJZ8lw
- Episode 39: Ambient and Gloo Mesh - https://www.youtube.com/watch?v=vxy-yqarirY
- Episode 40: Ask Us Anything about Istio Ambient Explained Book - https://www.youtube.com/watch?v=fyX1PrRaTf4
- Episode 41: What is NEW in Istio 1.16? - https://www.youtube.com/watch?v=hO8PlznWbmI
- Episode 42: Rust-based ztunnel for Istio ambient mesh? - https://www.youtube.com/watch?v=nyd_hxCWnds
- Episode 43: Successes? Lessons? Istio @ Virtru - https://www.youtube.com/watch?v=NP7JTXlQkI8
- Episode 44: Overview of SPIRE - https://www.youtube.com/watch?v=MuYmhc4mJHI
- Episode 45: Solve Large Volumes of Metrics Challenges with Istio Service Mesh and Open Telemetry - https://www.youtube.com/watch?v=TGZrwk3L-mo
- Episode 46: Istio Ambient Service Mesh & Rust-Based Ztunnel Update - https://www.youtube.com/watch?v=fCYAvsk_vlw
- Episode 47: Rotating certificates in Istio - https://www.youtube.com/watch?v=hD7L-haWJew
- Episode 48: GitOps and Service Mesh for Resilient and Secure Operations - https://www.youtube.com/watch?v=hYHgUPO13pw
- Episode 49: Configuring external services with Istio's ServiceEntry - https://www.youtube.com/watch?v=FOFzetag9d4
- Episode 50: Kubernetes Networking and Cilium - Part 1 - https://www.youtube.com/watch?v=TxPEpArfjwY
- Episode 51: Kubernetes Networking and Cilium - Part 2 - https://www.youtube.com/watch?v=he2sLeJsMqU
- Episode 52: Istio and Open Policy Agent (OPA) - https://www.youtube.com/watch?v=EnckV6lyre8
- Episode 53: Cut Service Mesh Overhead by 90% or More with Istio Ambient Mesh - https://www.youtube.com/watch?v=AHujRw0H8F4",FAUX
spacelift-io/spacelift-policies-example-library,Documentations,Documentations,2024-12-06T08:16:34Z,2024-02-26T10:15:15Z,0,134,0,0,0,0,0,0,2022-03-18T16:48:55Z,2025-03-24T22:42:53Z,161,46,Go,VRAI,14,FAUX,2,"examples,opa,policies,spacelift",2,A library of example Spacelift policies,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,22,"# Spacelift Policies Example Library

This repository contains a collection of Spacelift Policy examples that can be re-purposed (if needed), and used with [Spacelift](https://spacelift.io/). Spacelift Policies use the Open Policy Agent, which are written in the rego language. As you'll find in this repository, there are various types of Spacelift Policies - which allow for a lot of flexibility and customization. For more information on Spacelift Policies please refer to the [documentation](https://docs.spacelift.io/concepts/policy).

## Useful resources

- [Spacelift Policies](https://docs.spacelift.io/concepts/policy): You can find information about all available Spacelift Policy types here.
- [Open Policy Agent](https://www.openpolicyagent.org/docs/latest/policy-language/): Spacelift Policies utilize the Open Policy Agent, which uses the Rego language.
- [Spacelift Policy Workbench](https://docs.spacelift.io/concepts/policy#policy-workbench): Use the Spacelift Policy Workbench to debug your policies using sample policy inputs.
- [Testing Policies](https://docs.spacelift.io/concepts/policy#testing-policies): Learn about creating test cases for your Spacelift Policies.

## Policy Examples by Type

Policy Types Currently In This Library are below. Feel free to click on a given policy type to be taken to examples for that policy type.

| Policy Type                      | Description                                                                       |
| -------------------------------- | --------------------------------------------------------------------------------- |
| [ACCESS](examples/access/) (Deprecated) | Define who gets to access individual Stacks and with what level of access.        |
| [APPROVAL](examples/approval)           | Define who can approve or reject a run/task and how a run/task can be approved.   |
| [LOGIN](examples/login)                 | Define who gets to login to your Spacelift account and with what level of access. |
| [PLAN](examples/plan)                   | Define which changes can be applied.                                              |
| [PUSH](examples/push/)                  | Define how git push events are interpreted.                                       |
| [TRIGGER](examples/trigger)             | Define what happens when blocking runs terminate.                                 |

## All Policy Examples

### Access Policy

_Access policies have been deprecated. Please [read this](examples/access/README.md) for details._

- [Engineering Team Access](examples/access/engineering-team-access.rego)
- [Downgrade access after hours](examples/access/downgrade-access-after-hours.rego)
- [Label Based Team Access](examples/access/label-based-team-access.rego)
- [Protect Administrative Stacks](examples/access/protect-administrative-stacks.rego)
- [Slack Channel Access](examples/access/slack-channel-access.rego)
- [Who When Where Access Restrictions](examples/access/who-when-where-access-restrictions.rego)

### Approval Policy

- [Allowlist Task Commands](examples/approval/allowlist-task-commands.rego)
- [Approval needed outside working hours](examples/approval/approval-outside-working-hours.rego)
- [Require private worker](examples/approval/require-private-worker.rego)
- [Role-based Approval](examples/approval/role-based-approval.rego)
- [Require Approval from Security Team for certain resources](examples/approval/require-approval-from-security-team.rego)
- [Task and Run Approvals](examples/approval/task-and-run-approvals.rego)
- [Two Approvals Two Rejections](examples/approval/two-approvals-two-rejections.rego)
- [Two Approvals Two Rejections](examples/approval/two-approvals-two-rejections.rego)

### Login Policy

- External Contributor Access:
  - [GitHub](examples/login/external-contributor-access-github.rego)
  - [Google](examples/login/external-contributor-access-google.rego)
- [Managing access levels within an organization](examples/login/access-levels-within-an-organization.rego)
- [Readers Writers Admins Teams](examples/login/readers-writers-admins-teams.rego)
- [Rewriting User Teams](examples/login/rewriting-user-teams.rego)
- [Who When Where Login Restrictions](examples/login/who-when-where-login-restrictions.rego)

### Notification Policy

- [Drift Detection with changes](examples/notification/drift-detection-with-changes.rego)
- [Slack Channels set with labels](examples/notification/slack-channels-with-labels.rego)
- [Notification for link to failure logs](examples/notification/notification-failure.rego)
- [Notification for Origins of Failed Stacks](examples/notification/notification-stack-failure-origins.rego)

### Plan Policy

- [Check blast radius](examples/plan/check-blast-radius.rego)
- [Check sanitized value](examples/plan/check-sanitized-value.rego)
- [Checkov failed checks](examples/plan/checkov-failed-checks.rego)
- [Deny on proposed runs but warn on tracked runs](examples/plan/deny-proposed-runs-warn-track-runs.rego)
- [Do not delete stateful resources](examples/plan/do-not-delete-stateful-resources.rego)
- [Don't Allow Resource Type](examples/plan/dont-allow-resource-type.rego)
- [Enforce cloud provider](examples/plan/enforce-cloud-provider.rego)
- [Enforce Instance Type List](examples/plan/enforce-instance-type-list.rego)
- [Enforce module use policy](examples/plan/enforce-module-use-policy.rego)
- [Enforce Password Length](examples/plan/enforce-password-length.rego)
- [Enforce Google Cloud SQL Instance Networks](examples/plan/enforce-sqlinstance-network.rego)
- [Enforce Tags on Resources](examples/plan/enforce-tags-on-resources.rego)
- [Enforce Terraform version list](examples/plan/enforce-terraform-version-list.rego)
- [Ensure resource creation before deletion](examples/plan/ensure-resource-creation-before-deletion.rego)
- [Infracost Monthly Cost Restriction](examples/plan/infracost-monthly-cost-restriction.rego)
- [Kics severity counter](examples/plan/kics-severity-counter.rego)
- [Mandatory and Acceptable Labels for GCP resources](examples/plan/mandatory-and-acceptable-labels-gcp.rego)
- [Mandatory and required labels for stacks](examples/plan/mandatory-and-acceptable-labels-stack.rego)
- [Require human review for drift detection reconciliation](examples/plan/require-human-review-for-drift-detection-reconciliation.rego)
- [Require human review for unreachable Ansible hosts](examples/plan/require-human-review-for-unreachable-ansible-hosts.rego)
- [Require human review for resource update and deletion](examples/plan/require-human-review-for-update-deletion.rego)
- [Require reasonable commit size](examples/plan/require-reasonable-commit-size.rego)
- [Trusted engineers bypass review](examples/plan/trusted-engineers-bypass-review.rego)
- [Terrascan violated policies](examples/plan/terrascan-violated-policies.rego)
- [Tfsec high severity issues](examples/plan/tfsec-high-severity-issues.rego)
- [Warn On Change To Sensitive Resources](examples/plan/warn-on-change-senstitive-resources.rego)

### Push Policy

- [Allow forks](examples/push/allow-forks.rego)
- [Cancel In Progress Runs](examples/push/cancel-in-progress-runs.rego)
- [Create Proposed Run From PR Label](examples/push/create-proposed-run-from-env-pr-labels.rego)
- [Deploy with Git tag](examples/push/deploy-with-git-tag.rego)
- [Deploy with PR label](examples/push/deploy-with-pr-label.rego)
- [Ignore Changes Outside Root](examples/push/ignore-changes-outside-root.rego)
- [Terragrunt Monorepo Ignore Changes Outside Root](examples/push/terragrunt-monorepo-ignore-changes-outside-root.rego)
- [Lock button pauses runs by push events](/examples/push/lock-button-pauses-runs-by-pushes.rego)
- [PR comment-driven actions](examples/push/pr-comment-driven-actions.rego)
- [PR comment-driven user specific actions](examples/push/pr-comment-driven-user.rego)
- [PRs Only](examples/push/prs-only.rego)
- [Set head commit but don't trigger run](examples/push/set-head-commit-no-trigger.rego)
- [Tag-driven Terraform module release flow](examples/push/tag-driven-tf-module-release-flow.rego)
- [Track using labels](examples/push/track-using-labels.rego)

### Trigger Policy

- [Automated Retries](examples/trigger/automated-retries.rego)
- [Trigger Dependencies via Labels](examples/trigger/trigger-dependencies-via-labels.rego)
- [Trigger Dependencies via Labels, with state](examples/trigger/trigger-dependencies-via-labels-with-state.rego)
- [Trigger hardcoded dependencies](examples/trigger/trigger-harcoded-dependencies.rego)

## Policy Tests

Tests can be added for policies using the convention `<policy_filename>_test.rego`. For example
if you have a policy called `plan.rego`, you can create a test file called `plan_test.rego`.

You can use the following command to run all policy tests:

```shell
./run_policy_tests.sh
```",FAUX
sphenlee/waterwheel,Application System,Application System,2025-03-11T12:59:58Z,2024-10-03T06:57:10Z,0,1,0,0,0,0,0,0,2020-08-17T12:01:46Z,2025-03-19T03:34:27Z,1715,74,Rust,VRAI,10,FAUX,8,,8,A workflow scheduler based on petri-nets,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,2,"Waterwheel
==========

Waterwheel is a job scheduler similar to Airflow but with a very different design.

For more information see [here](./docs/intro.md).

To get started with a basic deployment see [here](./docs/getting-started.md).

Developing Locally
--------------------

Waterwheel is built using Rust.
Command automation uses [Just](https://github.com/casey/just#packages).
Build system is [Bazel](https://bazel.build/)

Create a `.env` file with the mandatory config settings:

```
# docker compose will use this when creating the database:
POSTGRES_PASSWORD=password

WATERWHEEL_DB_URL=postgres://postgres:${POSTGRES_PASSWORD}@localhost/
WATERWHEEL_SERVER_ADDR=http://localhost:8080/
WATERWHEEL_HMAC_SECRET=shared secret
WATERWHEEL_NO_AUTHZ=true
```

Now launch the services (Postgres and RabbitMQ):

```
just up
```

In two separate terminals build and launch Waterwheel server and worker:

```
# run the server
bazel run //:waterwheel server

# in a separate terminal
bazel run //:waterwheel worker
```

Building a Release binary
--------------------------

Build a release binary using bazel:

```
bazel build -c opt //:waterwheel
```

Build a docker image and load into the local Docker service:

```
bazel run //:waterwheel_load
```

> Other commands are available in the Justfile. Run `just help` for a list.",FAUX
spidernet-io/egressgateway,DevOPs,Toolkit,2025-04-30T07:49:43Z,2024-12-25T03:53:56Z,0,0,0,0,2,0,0,0,2022-10-24T07:14:58Z,2025-04-03T05:52:32Z,29200,233,Go,VRAI,22,FAUX,42,"egress,egress-gateway,kubernetes,networking",42,Layer4 egress gateway for Kubernetes,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,17,"# egressgateway

[![Auto Nightly CI](https://github.com/spidernet-io/egressgateway/actions/workflows/auto-nightly-ci.yaml/badge.svg)](https://github.com/spidernet-io/egressgateway/actions/workflows/auto-nightly-ci.yaml)
[![Auto Release Version](https://github.com/spidernet-io/egressgateway/actions/workflows/auto-release.yaml/badge.svg)](https://github.com/spidernet-io/egressgateway/actions/workflows/auto-release.yaml)
[![codecov](https://codecov.io/gh/spidernet-io/egressgateway/branch/main/graph/badge.svg?token=8CCT4CIIPx)](https://codecov.io/gh/spidernet-io/egressgateway)
[![Go Report Card](https://goreportcard.com/badge/github.com/spidernet-io/egressgateway)](https://goreportcard.com/report/github.com/spidernet-io/egressgateway)
![badge](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/bzsuni/cc6d42eb27d8ee4c3d19c936eff2c478/raw/egressgatewaye2e.json)
[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7410/badge)](https://bestpractices.coreinfrastructure.org/projects/7410)

English | [简体中文](docs/README.zh.md)

## About

In a Kubernetes (k8s) cluster, when Pods access external services, their Egress IP addresses are not fixed. In the Overlay network, the Egress IP address is determined by the node where the Pod resides. While in the Underlay network, Pods directly use their own IP addresses for external communication. Consequently, when Pods are rescheduled, regardless of the network mode, their IP addresses for external communication change. This instability poses a challenge for system administrators in managing IP addresses, especially as the cluster scales and during network fault diagnostics. Controlling egress traffic based on a Pod's original egress IP outside the cluster becomes difficult.

To solve this problem, EgressGateway has been introduced into the k8s cluster. It is an open-source EgressGateway designed to resolve egress egress IP address issues across various CNI network modes, such as Calico, Flannel, Weave, and Spiderpool. Through flexible configuration and management of egress policies, EgressGateway allows setting egress IP addresses for tenant-level or cluster-level workloads. When Pods need to access the external network, the system consistently uses the configured Egress IP as the egress address, providing a stable solution for egress traffic management.

## Architecture

![Architecture](docs/images/architecture02.png)

## Why EgressGateway

### Support a range of features and advantages

* Solve IPv4 IPv6 dual-stack connectivity,ensuring seamless communication across different protocol stacks.
* Solve the high availability of Egress Nodes, ensuring network connectivity remains unaffected by single-point failures.
* Support finer-grained policy control, allowing flexible filtering of Pods' Egress policies, including Destination CIDR.
* Support application-level control, allowing EgressGateway to filter Egress applications (Pods) for precise management of specific application outbound traffic.
* Support multiple egress gateways instance,capable of handling communication between multiple network partitions or clusters.
* Support namespaced egress IP.
* Supports automatic detection of cluster traffic for egress gateways policies.
* Support namespace default egress instances.
* Can be used in low kernel version, making EgressGateway suitable for various Kubernetes deployment environments.
  
### Compatible with the following network solutions

* [Calico](https://github.com/projectcalico/calico)
* [Flannel](https://github.com/flannel-io/flannel)
* [Weave](https://github.com/weaveworks/weave)
* [Spiderpool](https://github.com/spidernet-io/spiderpool)
* [Cilium](https://cilium.io/) native mode

## Getting started using EgressGateway

Please refer to the [installation guide](docs/usage/Install.en.md).

## Join the EgressGateway Community

We welcome contributions in any kind. If you have any questions about contributions, please consult the [contribution documentation](docs/develop/Contribute.en.md).

## License

EgressGateway is licensed under the Apache License, Version 2.0. See [LICENSE](https://github.com/spidernet-io/spiderpool/blob/main/LICENSE) for the full license text.",VRAI
spinnaker/spinnaker.io,Application System,Documentations,2025-05-12T20:19:35Z,2025-02-04T19:45:04Z,0,0,0,0,1,0,0,0,2020-06-03T22:53:58Z,2025-04-01T00:16:27Z,81063,20,HTML,VRAI,138,FAUX,18,,18,spinnaker.io website content,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,102,"# Spinnaker.io Documentation and Community Site

This site is built using [Hugo](https://gohugo.io) and the [Docsy Theme](https://www.docsy.dev/).

## Contributing

1. Start new development branches off of the `master` branch.
2. Create a pull request from your branch onto `master`.
3. Netlify will spawn a preview branch which will verify build success.
4. Branches merged back into `master` will deploy to the current active version-branch of the site.

## Using this repository

You can run the website locally using Hugo (Extended version).

## Prerequisites

- [Hugo (Extended version)](https://gohugo.io/); check the Hugo version specified in `netlify.toml`.

Before you start, install the dependencies. Clone the repository and navigate to the directory:

```
git clone https://github.com/spinnaker/spinnaker.io.git
cd spinnaker.io
```

The Spinnaker website uses the [Docsy Hugo theme](https://github.com/google/docsy#readme). Pull in the submodule and other development dependencies by running the following:

```
# pull in the Docsy submodule
git submodule update --init --recursive --depth 1
```

## Running the website locally using Hugo

Make sure to install the Hugo extended version specified by the `HUGO_VERSION` environment variable in the [`netlify.toml`](netlify.toml#L10) file.

To build and test the site locally, run:

```bash
hugo server
```

This will start the local Hugo server on port 1313. Open up your browser to http://localhost:1313 to view the website. As you make changes to the source files, Hugo updates the website and forces a browser refresh.

## Documentation Versioning

## Translation

At present, there is only one language in the `./content` directory. Docsy assumes lang-en and uses this language automatically, but you can add additional directories with different contents. There is also a langauge switcher in the navbar that can be enabled by adding that language to the `[languages]` map in `.config.toml`.

## Theme Customization

The Docsy theme is installed as a git submodule to this site. To update the theme, follow the Docsy documentation for git submodules. Make theme upgrades with care. Any changes to markup in the theme may render existing SCSS modifications ineffective.

Overrides to the theme are in `./layouts`, `./assets`, and `./static`. In order to continue to use Docsy's color variables, the **entire** theme SCSS collection is has been copied to `./assets`. Some of these SCSS files have been further modified to alter the appearance of various site components. If something ""breaks"" on upgrade, a good first step is to compare the previous markup for that component and make sure old SCSS selectors are still valid.

Dependencies are loaded into `./assets/scss` from `./themes/docsy`. If subsequent theme upgrades fail to load Bootstrap or Font Awesome assets, verify that the paths to these vendor dependencies within `./themes/docsy` are still valid.

## Docs Frontmatter Variables

`title`: Displayed on the content page  
`linkTitle`: displayed where a link to the page appears (in the docs menu)  
`weight`: Determines the order of appearance in lists of content in the same directory, lowest first. To let all titles appear in alphabetical order, remove all weights.  
`description`: Short description, appears in lists of directory contents and on content page.  
`mermaid`: Boolean `true` indicates that MermaidJS should be loaded on the page.

## Homepage Frontmatter Page Params

Edit the file `./content/en/_index.md` to change the homepage frontmatter variables.

### Changing the News Link

`news_banner`: Boolean `true` enables the news link in the hero info panel.  
`news_text`: Sets the news link text.  
`news_link`: Sets the news link target.

Example news link:

```
---
title: 'Spinnaker'
subtitle: 'Cloud Native Continuous Delivery'
subtitle_1: 'Fast, safe, repeatable deployments for every enterprise'
date: '2020-06-04'
type: 'en'
is_index: true
layout: 'index'
has_carousel: true
news_banner: false
news_text: 'Testing news banner'
news_link: 'https://google.com'
...
---
```

![desktop homepage hero](https://user-images.githubusercontent.com/70309473/125411287-9e818f80-e372-11eb-99eb-d24404e387e1.png)
![tablet homepage hero](https://user-images.githubusercontent.com/70309473/125411010-582c3080-e372-11eb-83e4-7564097b3f2d.png)
![mobile homepage hero](https://user-images.githubusercontent.com/70309473/125411499-d4bf0f00-e372-11eb-9ab0-1ecd6497c1ab.png)

## Mermaid

Mermaid is loaded into content pages only when the boolean frontmatter variable `mermaid` is set to `true`.

1. Use the `mermaid` shortcode to make sure your graph isn't processed as markdown:

```
{{< mermaid >}}
graph TB

clouddriver(Clouddriver) --> clouddriver-caching(Clouddriver-Caching);
clouddriver --> clouddriver-rw(Clouddriver-RW);
clouddriver --> clouddriver-ro(Clouddriver-RO);
clouddriver --> clouddriver-ro-deck(Clouddriver-RO-Deck)

classDef default fill:#d8e8ec,stroke:#39546a;
linkStyle default stroke:#39546a,stroke-width:1px,fill:none;

classDef split fill:#42f4c2,stroke:#39546a;
class clouddriver-caching,clouddriver-ro,clouddriver-ro-deck,clouddriver-rw,echo-scheduler,echo-worker split
{{< /mermaid >}}
```

2. Add the frontmatter variable to the page: `mermaid: true`.

## Custom YouTube Shortcode

The internal YouTube embed template provided by Hugo does not allow for the setting if height and width. A custom YouTube shortcode has been added to the repository to allow for the setting of height and width of YouTube videos embedded in Markdown content. Width and height should always include percent or unit of measure.

```
{{< customyoutube id=""b7BmMY1kR10"" width=""320px"" height=""240px"" >}}
```

## Promo Banner on homepage

The promo banner across the top of the home page is displayed depending on a parameter in config.toml and also configured there:

[params.promoBanner]
show = true
text = ""Spinnaker Summit is co-located with KubeCon this year! Join us on Oct 23-24 in Detroit.""
ctaLink = ""http://go.armory.io/ss22""
ctaText = ""Register""
label = ""UPCOMING EVENT""",VRAI
sst/sst,Toolkit,DevOPs,2025-05-16T00:45:33Z,2025-05-12T18:05:16Z,0,0,137,0,0,0,0,0,2020-08-04T20:28:14Z,2025-04-08T09:38:14Z,158317,23390,TypeScript,VRAI,1798,FAUX,560,,560,Build full-stack apps on your own infrastructure.,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,192,"<p align=""center"">
  <a href=""https://sst.dev/"">
    <img alt=""SST"" src=""https://raw.githubusercontent.com/sst/identity/main/variants/sst-full.svg"" width=""300"" />
  </a>
</p>

<p align=""center"">
  <a href=""https://sst.dev/discord""><img alt=""Discord"" src=""https://img.shields.io/discord/983865673656705025?style=flat-square&label=Discord"" /></a>
  <a href=""https://www.npmjs.com/package/sst""><img alt=""npm"" src=""https://img.shields.io/npm/v/sst.svg?style=flat-square"" /></a>
  <a href=""https://github.com/sst/sst/actions/workflows/build.yml""><img alt=""Build status"" src=""https://img.shields.io/github/actions/workflow/status/sst/sst/build.yml?style=flat-square&branch=dev"" /></a>
</p>

---

Build full-stack apps on your own infrastructure.

SST v3 uses a new engine for deploying SST apps. It uses Pulumi and Terraform, as opposed to CDK and CloudFormation. [Read the full announcement here](https://sst.dev/blog/sst-v3).

## Installation

If you are using SST as a part of your Node project, we recommend installing it locally.

```bash
npm install sst
```

If you are not using Node, you can install the CLI globally.

```bash
curl -fsSL https://sst.dev/install | bash
```

To install a specific version.

```bash
curl -fsSL https://sst.dev/install | VERSION=0.0.403 bash
```

To use a package manager, [check out our docs](https://sst.dev/docs/reference/cli/).

#### Manually

Download the pre-compiled binaries from the [releases](https://github.com/sst/sst/releases/latest) page and copy to the desired location.

## Get Started

Get started with your favorite framework:

- [Next.js](https://sst.dev/docs/start/aws/nextjs)
- [Remix](https://sst.dev/docs/start/aws/remix)
- [Astro](https://sst.dev/docs/start/aws/astro)
- [API](https://sst.dev/docs/start/aws/api)

## Learn More

Learn more about some of the key concepts:

- [Live](https://sst.dev/docs/live)
- [Linking](https://sst.dev/docs/linking)
- [Console](https://sst.dev/docs/console)
- [Components](https://sst.dev/docs/components)

## Contributing

Here's how you can contribute:

- Help us improve our docs
- Find a bug? Open an issue
- Feature request? Submit a PR 

## Running Locally

1. Clone the repo
2. `bun install`
3. `go mod tidy`
4. `cd platform && bun run build`

Now you can run the CLI locally on any of the `examples/` apps.

```bash
cd examples/aws-api
go run ../../cmd/sst <command>
```

If you want to build the CLI, you can run `go build ./cmd/sst` from the root. This will create a
`sst` binary that you can use.

---

**Join our community** [Discord](https://sst.dev/discord) | [YouTube](https://www.youtube.com/c/sst-dev) | [X.com](https://x.com/SST_dev)",VRAI
star3am/hashiqube,DevOPs,DevOPs,2025-04-11T18:55:54Z,2025-02-27T02:05:15Z,14,0,0,0,0,0,0,0,2023-07-06T21:44:54Z,2025-04-04T23:35:41Z,180931,113,JavaScript,VRAI,56,FAUX,0,"airflow,ansible,ansible-project,boundary,consul,dbt,devops,gitlab,hashicorp,hashiqube,jenkins,minikube,nomad,packer,terraform,vagrant,vault,waypoint",0,HashiQube - The Ultimate Hands on DevOps Lab running All the HashiCorp Products in a Github Codespace or a Docker Container using Vagrant or Docker Compose,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,10,"# HashiQube - A DevOps Development Lab Using All the HashiCorp Products

![HashiQube](images/hashiqube-banner.png?raw=true ""HashiQube "")

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/star3am/hashiqube?quickstart=1)

## Overview

Hashiqube is the Ultimate Hands-on DevOps Lab in a Docker Container.
It can run in a Github Codespace, locally using Vagrant or Docker Compose, or as a Virtual Machine VM on AWS, GCP and Azure.

Hashiqube has a Docker daemon inside, meaning we can run containers inside Hashiqube using Kubernetes (Minikube), Nomad, or Docker run. It runs all HashiCorp products: [Vault](/vault/?id=vault), [Terraform](/terraform/?id=terraform), [Nomad](/nomad/?id=nomad), [Consul](/consul/?id=consul), [Waypoint](/waypoint/?id=waypoint), [Boundary](/boundary/?id=boundary), [Vagrant](/vagrant/?id=vagrant), [Packer](/packer/?id=packer) and [Sentinel](/sentinel/?id=sentinel).

It also runs other popular Open Source DevOps/DevSecOps applications (Minikube, Ansible AWX Tower, Traefik, etc.) showcasing how simple integration with HashiCorp products can result in tangible learnings and benefits for all users.

Once Hashiqube is up, an internet connection is no longer needed, meaning sales pitches and demos for potential and existing customers are greatly aided.

Hashiqube has been created to help Engineers, Developers and anyone who wants to practice, learn or demo HashiCorp products to get started quickly with a local lab.

Please connect with me on [LinkedIn (Riaan Nolan)](https://www.linkedin.com/in/riaannolan/) or check out [my Credly profile](https://www.credly.com/users/riaan-nolan.e657145c)

---

## Quik Start

<!-- tabs:start -->

### **Github Codespace**

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/star3am/hashiqube?quickstart=1)

```bash
bash docker/docker.sh
bash consul/consul.sh
bash nomad/nomad.sh
bash vault/vault.sh
bash boundary/boundary.sh
```

### **Vagrant**

```bash
vagrant up --provision-with basetools,docker,docsify,consul,nomad,vault,boundary
```

### **Docker Compose**

```bash
docker compose exec hashiqube /bin/bash
bash hashiqube/basetools.sh
bash docker/docker.sh
bash consul/consul.sh
bash nomad/nomad.sh
bash vault/vault.sh
bash boundary/boundary.sh
```

<!-- tabs:end -->

---

## Table of Contents

- [HashiQube - A DevOps Development Lab Using All the HashiCorp Products](#hashiqube---a-devops-development-lab-using-all-the-hashicorp-products)
  - [Overview](#overview)
  - [Table of Contents](#table-of-contents)
  - [Running Hashiqube](#running-hashiqube)
    - [Hashiqube Github Codespaces](#hashiqube-github-codespaces)
    - [Hashiqube Local Vagrant](#hashiqube-local-vagrant)
      - [Pre-requisites](#pre-requisites)
      - [Installation Instructions](#installation-instructions)
    - [Docker Compose](#docker-compose)
  - [Documentation and Status](#documentation-and-status)
  - [HashiQube Overview](#hashiqube-overview)
  - [HashiQube Diagrams](#hashiqube-diagrams)
    - [Draw.io Diagram](#drawio-diagram)
  - [Links](#links)
    - [Official Resources](#official-resources)
    - [Articles and Media](#articles-and-media)
  - [Hashiqube Integrations](#hashiqube-integrations)
    - [HashiCorp Products](#hashicorp-products)
    - [Infrastructure \& Orchestration](#infrastructure--orchestration)
    - [Databases](#databases)
    - [Monitoring \& Visualization](#monitoring--visualization)
    - [DevOps Tools](#devops-tools)
  - [HashiQube's Purpose](#hashiqubes-purpose)
  - [Supported Architectures](#supported-architectures)
  - [HashiCorp product Versions](#hashicorp-product-versions)
  - [Components](#components)
    - [Running Individual Components](#running-individual-components)
    - [Running Multiple Components Together](#running-multiple-components-together)
  - [Docker Desktop](#docker-desktop)
    - [Configuration Requirements](#configuration-requirements)
    - [Resource Allocation](#resource-allocation)
  - [Consul DNS](#consul-dns)
  - [The HashiStack](#the-hashistack)
  - [Other](#other)
    - [Service Ports](#service-ports)
  - [Vagrant Basic Usage](#vagrant-basic-usage)
  - [Docker Basic Usage](#docker-basic-usage)
  - [Errors you might encounter](#errors-you-might-encounter)
  - [Support \& Feedback](#support--feedback)
    - [About Hashiqube](#about-hashiqube)
    - [About Me](#about-me)
  - [Contributors](#contributors)
  - [Videos](#videos)

---

## Running Hashiqube

There are several ways to run Hashiqube depending on your needs. The easiest is using Github Codespaces:

| Method | Description | Requirements |
|:-------|:------------|:------------|
| [**Github Codespaces**](#hashiqube-github-codespaces) | Quick start in the cloud (1-minute setup) | Github account only |
| [**Local Vagrant/Docker**](#hashiqube-local-vagrant) | Run locally on your machine | Docker, Vagrant (optional), VirtualBox (optional) |
| [**Hyperscaler VMs**](/multi-cloud/README) | Run on AWS, GCP or Azure | Cloud account |

### Hashiqube Github Codespaces

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/star3am/hashiqube?quickstart=1)

1. Head over to <https://github.com/star3am/hashiqube>
2. Fork the Hashiqube repository
3. In the forked Hashiqube repository in your Github account, launch a new Codespace
4. Watch the video below for follow-along instructions

[![HashiQube on Github's Codespaces](images/hashiqube-on-github-codespace.png)](https://www.youtube.com/watch?v=8uDfDnQZZoA)

### Hashiqube Local Vagrant

Follow the instructions below to run Hashiqube locally on your laptop or PC.

#### Pre-requisites

- **Hardware Requirements:**
  - 10GB of disk space
  - 4G RAM (Minimum) - 8G RAM Recommended
- **Software Requirements:**
  - Docker
  - Vagrant

> :bulb: If you want to run Minikube and a workload like AWX Ansible Tower or Airflow, you need at least 8G RAM. For Gitlab, allocate at least 12G RAM to Docker.

#### Installation Instructions

> :clock3: Duration: 15-30 minutes  
> :bulb: Docker is the default and preferred way to run Hashiqube

1. **Install Docker**: Download from the [Docker desktop installer](https://www.docker.com/products/docker-desktop)
2. **Install Vagrant**: Download from the [Vagrant installer](https://www.vagrantup.com/downloads.html)
3. **Clone the repo**: `git clone https://github.com/star3am/hashiqube.git` - [What is Git?](git/#git)
4. **Start Hashiqube**: Inside the local repo folder, run:

   ```bash
   vagrant up --provision
   ```

   This will set up Vault, Nomad, Consul, Terraform, Localstack and Docker
5. **Access documentation**: Visit <http://localhost:3333>

### Docker Compose

Docker compose is also supported! To bring up Hashiqube with Docker Compose:

1. **Install Docker**: Download from the [Docker desktop installer](https://www.docker.com/products/docker-desktop)
2. **Clone the repo**: `git clone https://github.com/star3am/hashiqube.git`
3. **Start Hashiqube**: Inside the local repo folder, run:

   ```bash
   docker-compose up -d
   ```

4. **Initialize services**:

   ```bash
   # Access the container shell
   docker compose exec hashiqube /bin/bash
   
   # Install dependencies
   bash hashiqube/basetools.sh
   
   # Install Docker daemon
   bash docker/docker.sh
   
   # Start HashiCorp Vault
   bash vault/vault.sh
   ```

To check status:

```bash
docker compose ls
```

Example output:

```bash
NAME                STATUS              CONFIG FILES
hashiqube           running(1)          /Users/riaan/workspace/personal/hashiqube/docker-compose.yml
```

---

## Documentation and Status

After completing the installation steps:

- :book: **Documentation**: <http://localhost:3333>
- :vertical_traffic_light: **Status of Integrations**: <http://localhost:3001>

---

## HashiQube Overview

[![HashiQube: A Jedi DevOps Lab Using All the HashiCorp Products](images/youtube-hashiqube-the-jedi-devops-lab.png)](https://www.youtube.com/watch?v=sFiWzKbpEpU)

---

## HashiQube Diagrams

In essence, Hashiqube is a Docker Container (by default) or a Virtual Machine with a Docker Daemon inside, meaning we can run containers inside Hashiqube using Kubernetes, Nomad, or Docker run.

Hashiqube runs on your local machine or cloud instance and provides:

- All HashiCorp products
- Popular open-source DevOps tools (Minikube, Ansible AWX Tower, etc.)
- Multi-cloud capabilities on AWS, GCP and Azure ([Multi-Cloud](multi-cloud/README))

> Hashiqube is a Training/Development Lab for practice, learning, and demos. It is not designed for production use.

![Diagram](images/hashiqube.drawio.svg)

---

## Links

### Official Resources

- [HashiQube Website](https://hashiqube.com)
- [GitHub Repository](https://github.com/star3am/hashiqube)
- [Terraform Registry Module](https://registry.terraform.io/modules/star3am/hashiqube/hashicorp/latest)

### Articles and Media

- [HashiCorp Blog Post](https://www.hashicorp.com/resources/hashiqube-a-development-lab-using-all-the-hashicorp-products)
- [YouTube Channel](https://www.youtube.com/watch?v=6jGDAGWaFiw)
- [Medium Articles](https://medium.com/search?q=hashiqube)
- [Terraform Development Environment Guide](https://medium.com/@riaan.nolan/top-gun-terraform-development-environment-60ac00d49577)

---

## Hashiqube Integrations

![HashiQube](images/thestack.png?raw=true ""HashiQube"")

### HashiCorp Products

- [Vault](/vault/#vault) - Secure, store and control access to tokens, passwords, and sensitive data
- [Terraform](/terraform/#terraform) - Infrastructure as Code to provision and manage any cloud or service
- [Consul](/consul/#consul) - Service Mesh connecting applications across environments
- [Nomad](/nomad/#nomad) - Scheduler and orchestrator for containers and applications
- [Boundary](/boundary/#boundary) - Secure remote access to any system from anywhere
- [Waypoint](/waypoint/#waypoint) - Modern workflow for build, deploy, and release across platforms
- [Vagrant](/vagrant/#vagrant) - Build and maintain portable development environments
- [Packer](/packer/#packer) - Create identical machine images for multiple platforms
- [Sentinel](/sentinel/#sentinel) - Embedded policy-as-code framework

### Infrastructure & Orchestration

- [Docker](/docker/#docker) - Container platform
- [Minikube](/minikube/#minikube) - Local Kubernetes cluster
- [Traefik](/nomad/#traefik-load-balancer-for-nomad) - Modern HTTP reverse proxy and load balancer
- [Fabio](/nomad/#fabio-load-balancer-for-nomad) - HTTP/TCP reverse proxy with Consul integration
- [Localstack](/localstack/#localstack) - Local AWS cloud stack

### Databases

- [Oracle MySQL](/database/#oracle-mysql) - Open-source RDBMS
- [Microsoft MSSQL](/database/#microsoft-sql-mssql-express) - Microsoft's RDBMS
- [PostgreSQL](/database/#postgresql) - Advanced open-source RDBMS

### Monitoring & Visualization

- [Prometheus](/prometheus-grafana/#prometheus-and-grafana) - Monitoring system with time series database
- [Grafana](/prometheus-grafana/#prometheus-and-grafana) - Analytics & monitoring solution
- [Elasticsearch](/elasticsearch-kibana-cerebro/#elasticsearch-kibana-and-cerebro) - Search engine
- [Kibana](/elasticsearch-kibana-cerebro/#elasticsearch-kibana-and-cerebro) - Data visualization for Elasticsearch
- [Cerebro](/elasticsearch-kibana-cerebro/#elasticsearch-kibana-and-cerebro) - Elasticsearch admin interface

### DevOps Tools

- [Ansible](/ansible/#ansible) - Infrastructure as code and automation
- [Ansible-Tower](/ansible-tower/#ansible-tower) - Web UI for Ansible
- [Jenkins](/jenkins/#jenkins) - Automation server for CI/CD
- [LDAP](/ldap/#ldap) - Lightweight Directory Access Protocol
- [Dbt](/dbt/#dbt) - Data transformation tool
- [Apache Airflow](/apache-airflow/#apache-airflow) - Workflow management for data pipelines
- [Visual-Studio-Code](/visual-studio-code/#visual-studio-code) - Code editor
- [Portainer](/portainer/#portainer) - Container management UI
- [Gitlab](/gitlab/#gitlab) - DevOps platform
- [Argocd](/argocd/#argocd) - GitOps continuous delivery for Kubernetes
- [Docsify](/docsify/#docsify) - Documentation site generator
- [Mermaid](/mermaid/#mermaid) - Diagram generation from text
- [Newrelic Kubernetes Monitoring](/newrelic-kubernetes-monitoring/#newrelic-kubernetes-monitoring) - Monitor Kubernetes

Once the stack is up, you will have many services running and available on `localhost`.  
For documentation, open <http://localhost:3333> in your browser.

![Hashiqube Integrations](images/logo-qube.png?raw=true ""Hashiqube Integrations"")

---

## HashiQube's Purpose

Hashiqube enables anyone interested in secure automation pipelines to run a suite of 'best in class' tools locally with minimal system resources.

It empowers users to deploy these tools in a way that covers multiple use cases, providing a 'concept to completion' test bed using open-source HashiCorp products.

The original use case was to demystify DevSecOps using Terraform, Vault, Consul, Sentinel, and Nomad along with other open-source CI/CD tools, demonstrating the value of secret and credential management in software development pipelines.

Thanks to HashiCorp's flexibility, there's no need to wonder how to achieve secure and timely software delivery - just Vagrant up!

---

## Supported Architectures

| Name      | Docker | Virtualbox | Hyper-V |
|:--------- |:------:|:----------:|:-------:|
| amd64     | ✓      | ✓          | ✘       |
| arm64     | ✓      | ✘          | ✘       |
| linux     | ✓      | ✓          | ✘       |
| windows   | ✓      | ✘          | ✘       |
| mac intel | ✓      | ✓          | ✘       |
| mac apple | ✓      | ✘          | ✘       |

---

## HashiCorp product Versions

By default, Hashiqube installs community editions of HashiCorp products, but you can also test and demo Enterprise versions. You can request a trial license from [HashiCorp's website](https://developer.hashicorp.com/vault/tutorials/enterprise/hashicorp-enterprise-license#request-a-trial-license).

To use Enterprise editions, copy the license.hclic into the corresponding product folder and run:

```bash
vagrant up --provision-with basetools,vault,consul,nomad,boundary
```

| Name      | Community | Enterprise |
|:--------- |:---------:|:----------:|
| Vault     | ✓         | ✓          |
| Consul    | ✓         | ✓          |
| Nomad     | ✓         | ✓          |
| Boundary  | ✓         | ✓          |
| Terraform | ✓         | ✘          |

Directory structure example:

```bash
tree -L 1 boundary consul nomad vault

boundary
├── README.md
├── boundary.sh
├── images
└── license.hclic
consul
├── README.md
├── consul.sh
├── images
└── license.hclic
nomad
├── README.md
├── images
├── license.hclic
├── nomad
└── nomad.sh
vault
├── README.md
├── images
├── license.hclic
└── vault.sh
```

---

## Components

Hashiqube is modular - components can be run separately or together.

### Running Individual Components

```bash
vagrant up --provision-with basetools
vagrant up --provision-with docker
vagrant up --provision-with docsify
vagrant up --provision-with vault
vagrant up --provision-with consul
vagrant up --provision-with nomad
vagrant up --provision-with minikube
```

### Running Multiple Components Together

```bash
vagrant up --provision-with basetools,docker,minikube,postgresql,dbt,apache-airflow
```

---

## Docker Desktop

Docker Desktop is an application for Mac/Windows that enables building and sharing containerized applications. It provides a graphical interface for the Docker service.

1. [Download Docker Desktop](https://www.docker.com/products/docker-desktop) and install it
2. Verify installation by opening the Docker Desktop application

![Docker Desktop](images/docker_desktop_installed.png?raw=true ""Docker Desktop"")

### Configuration Requirements

- Ensure you have the latest version installed
- Keep your operating system updated ([Docker Desktop performance improvements](https://www.docker.com/blog/speed-boost-achievement-unlocked-on-docker-desktop-4-6-for-mac/))

### Resource Allocation

> :bulb: While Hashiqube runs with 4GB (minimum) or 8GB (recommended) RAM, running multiple services simultaneously (Vault, Nomad, Consul, Waypoint, Boundary, Minikube, AWX) requires more resources to avoid contention errors.

![Docker Desktop Resources](images/docker_installed_resources.png?raw=true ""Docker Desktop Resources"")

- Allocate at least **8GB RAM** to your Docker daemon
- Ensure sufficient disk space is available

---

## Consul DNS

To enable local DNS resolution via Consul, create a file at `/etc/resolver/consul` with:

```bash
nameserver 10.9.99.10
port 8600
```

This enables DNS names like `nomad.service.consul:9999` and `vault.service.consul:9999` via Fabio Load Balancer.

---

## The HashiStack

| Dimension          | Products |                  |                  |
|:------------------ |:-------- |:---------------- |:---------------- |
| **Applications**   | ![Nomad](https://www.datocms-assets.com/2885/1620155098-brandhcnomadprimaryattributedcolor.svg) <br> [**Nomad**](/nomad/#nomad) <br> Scheduler and workload orchestrator | ![Waypoint](https://www.datocms-assets.com/2885/1620155130-brandhcwaypointprimaryattributedcolor.svg) <br> [**Waypoint**](/waypoint/#waypoint) <br> Build, deploy, release workflow | |
| **Networking**     | ![Consul](https://www.datocms-assets.com/2885/1620155090-brandhcconsulprimaryattributedcolor.svg) <br> [**Consul**](/consul/#consul) <br> Service Mesh across any cloud | | |
| **Security**       | ![Boundary](https://www.datocms-assets.com/2885/1620155080-brandhcboundaryprimaryattributedcolor.svg) <br> [**Boundary**](/boundary/#boundary) <br> Secure remote access | ![Vault](https://www.datocms-assets.com/2885/1620159869-brandvaultprimaryattributedcolor.svg) <br> [**Vault**](/vault/#vault) <br> Secrets management | |
| **Infrastructure** | ![Packer](https://www.datocms-assets.com/2885/1620155103-brandhcpackerprimaryattributedcolor.svg) <br> [**Packer**](/packer/#packer) <br> Machine image automation | ![Vagrant](https://www.datocms-assets.com/2885/1620155118-brandhcvagrantprimaryattributedcolor.svg) <br> [**Vagrant**](/vagrant/#vagrant) <br> Development environments | ![Terraform](https://www.datocms-assets.com/2885/1620155113-brandhcterraformprimaryattributedcolor.svg) <br> [**Terraform**](/terraform/#terraform) <br> Infrastructure automation |

---

## Other

### Service Ports

| Service | URL/Port |
|:--------|:---------|
| LDAP | ldap://localhost:389 |
| Localstack web | <http://localhost:8080> |
| DBT web | <http://localhost:28080> |
| Apache Airflow | <http://localhost:18889> |
| Ansible provisioning Apache2 | <http://localhost:8888> |
| Ansible AWX Tower | <http://localhost:8043> |
| Jenkins | <http://localhost:8088> |
| Oracle MySQL | localhost:3306 |
| Microsoft SQL | localhost:1433 |
| Minikube | <http://localhost:10888> |
| Traefik | <http://localhost:8181> |
| Fabio | <http://localhost:9999> |

---

## Vagrant Basic Usage

```bash
# Initial setup with provisioning
vagrant up --provision

# Selective provisioning
vagrant up --provision-with bootstrap|nomad|consul|vault|docker|ldap

# Check VM status
vagrant global-status
vagrant global-status --prune  # Remove stale VMs from cache
vagrant status

# Other common commands
vagrant reload
vagrant up
vagrant destroy
vagrant provision
vagrant plugin list
```

## Docker Basic Usage

```bash
docker image ls
docker ps
docker stop
```

---

## Errors you might encounter

<details>
<summary><strong>Error: Shell provisioner path does not exist</strong></summary>

If you see this error when running `vagrant destroy`:

```bash
hashiqube0: Are you sure you want to destroy the 'hashiqube0' VM? [y/N] y
There are errors in the configuration of this machine. Please fix
the following errors and try again:

shell provisioner:
* `path` for shell provisioner does not exist on the host system: /users/username/workspace/personal/hashiqube/vault/vault.sh
```

**Command to check:** `docker ps`

```bash
CONTAINER ID IMAGE COMMAND CREATED STATUS  PORTS NAMES
1d835d757279   15f77507dce7   ""/usr/sbin/init""   38 hours ago   Up 38 hours   0.0.0.0:1433->1433/tcp, 0.0.0.0:3000->3000/tcp, 0.0.0.0:3306->3306/tcp, 0.0.0.0:3333->3333/tcp, 0.0.0.0:4566->4566/tcp, 0.0.0.0:4646-4648->4646-4648/tcp, 0.0.0.0:5001-5002->5001-5002/tcp, 0.0.0.0:5432->5432/tcp, 0.0.0.0:5580->5580/tcp, 0.0.0.0:5601-5602->5601-5602/tcp, 0.0.0.0:7777->7777/tcp, 0.0.0.0:8000->8000/tcp, 0.0.0.0:8043->8043/tcp, 0.0.0.0:8080->8080/tcp, 0.0.0.0:8088->8088/tcp, 0.0.0.0:8181->8181/tcp, 0.0.0.0:8200-8201->8200-8201/tcp, 0.0.0.0:8300-8302->8300-8302/tcp, 0.0.0.0:8500-8502->8500-8502/tcp, 0.0.0.0:8888-8889->8888-8889/tcp, 0.0.0.0:9001-9002->9001-9002/tcp, 0.0.0.0:9011->9011/tcp, 0.0.0.0:9022->9022/tcp, 0.0.0.0:9090->9090/tcp, 0.0.0.0:9093->9093/tcp, 0.0.0.0:9200->9200/tcp, 0.0.0.0:9333->9333/tcp, 0.0.0.0:9701-9702->9701-9702/tcp, 0.0.0.0:9998-9999->9998-9999/tcp, 0.0.0.0:10888->10888/tcp, 0.0.0.0:11888->11888/tcp, 0.0.0.0:18080->18080/tcp, 0.0.0.0:18181->18181/tcp, 0.0.0.0:18888-18889->18888-18889/tcp, 0.0.0.0:19200->19200/tcp, 0.0.0.0:19701-19702->19701-19702/tcp, 0.0.0.0:28080->28080/tcp, 0.0.0.0:31506->31506/tcp, 0.0.0.0:32022->32022/tcp, 0.0.0.0:8600->8600/udp, 0.0.0.0:2255->22/tcp, 0.0.0.0:33389->389/tcp   hashiqube_hashiqube0_1689246032
```

**Solution:** Run `docker stop 1d835d757279` (using your container ID)
</details>

<details>
<summary><strong>Error: IP address not within allowed ranges</strong></summary>

```bash
The IP address configured for the host-only network is not within the
allowed ranges. Please update the address used to be within the allowed
ranges and run the command again.

Address: 10.9.99.10
Ranges: 192.168.56.0/21

Valid ranges can be modified in the /etc/vbox/networks.conf file. For
more information including valid format see:

https://www.virtualbox.org/manual/ch06.html#network_hostonly
```

**Solution:** Create file `/etc/vbox/networks.conf` with:

```bash
* 10.0.0.0/8 192.168.0.0/16
* 2001::/64
```

Then re-run `vagrant up --provision`
</details>

<details>
<summary><strong>Error: Cannot stop container</strong></summary>

When running `vagrant destroy`:

```bash
hashiqube0.service.consul: Are you sure you want to destroy the 'hashiqube0.service.consul' VM? [y/N] y
==> hashiqube0.service.consul: Stopping container...
A Docker command executed by Vagrant didn't complete successfully!
The command run along with the output from the command is shown
below.

Command: [""docker"", ""stop"", ""-t"", ""1"", ""6c0c8135620ff47efe12df417a0df0e57d7a81a7f7ca06d011323fbb52e573db"", {:notify=>[:stdout, :stderr]}]

Stderr: Error response from daemon: cannot stop container: 6c0c8135620ff47efe12df417a0df0e57d7a81a7f7ca06d011323fbb52e573db: tried to kill container, but did not receive an exit event
```

**Solution:** Run `vagrant destroy` again
</details>

<details>
<summary><strong>Error: Port collision</strong></summary>

```bash
Vagrant cannot forward the specified ports on this VM, since they would collide with some other application that is already listening on these ports. The forwarded port to `9200` is already in use on the host machine.

To fix this, modify your current project's Vagrantfile to use another port. For example, where '1234' would be replaced by a unique host port:

config.vm.network :forwarded_port, guest: 9200, host: 1234

Sometimes, Vagrant will attempt to auto-correct this for you. In this case, Vagrant was unable to. This is usually because the guest machine is in a state which doesn't allow modifying port forwarding. You could try 'vagrant reload' (the equivalent of running a halt followed by an up) so vagrant can attempt to auto-correct this upon booting. Be warned that any unsaved work might be lost.
```

**Solution:** Either stop the conflicting service (e.g., Elasticsearch) or modify the Vagrantfile to use a different port:

```bash
# config.vm.network ""forwarded_port"", guest: 9200, host: 9200 # elasticsearch
```

</details>

---

## Support & Feedback

For suggestions, feedback, and queries, please submit a Pull Request or contact **Riaan Nolan**, creator of HashiQube via [GitHub](https://github.com/star3am/hashiqube).

### About Hashiqube

Hashiqube runs all HashiCorp products and many popular open-source tools used in the industry today.

After running `vagrant up --provision`, you'll have access to:

| Service | URL | Setup Command |
|:--------|:----|:--------------|
| Vault | <http://localhost:8200> | `vagrant up --provision-with basetools,vault` |
| Nomad | <http://localhost:4646> | `vagrant up --provision-with basetools,docker,nomad` |
| Consul | <http://localhost:8500> | `vagrant up --provision-with basetools,consul` |
| Waypoint on Nomad | <https://localhost:9702> | `vagrant up --provision-with basetools,docker,waypoint` |
| Waypoint on Minikube | <https://localhost:19702> | `vagrant up --provision-with basetools,docker,waypoint-kubernetes-minikube` |
| Boundary | <http://localhost:9200> | `vagrant up --provision-with basetools,boundary` |
| Docsify | <http://localhost:3333> | `vagrant up --provision-with basetools,docsify` |

![Hashiqube Integrations](images/logo-qube.png?raw=true ""Hashiqube Integrations"")

---

### About Me

My name is **Riaan Nolan** and I was born in South Africa. I started as a Web Developer in 2000 and progressed into Systems Administration with a strong focus on Automation, Infrastructure and Configuration as Code.

I have worked for multinational companies in Portugal, Germany, China, South Africa, United States, and Australia.

Please connect with me on [LinkedIn](https://www.linkedin.com/in/riaannolan/) or check out [my Credly profile](https://www.credly.com/users/riaan-nolan.e657145c)

![My Hashicorp Badges](images/hashicorp-badges.png?raw=true ""My Hashicorp Badges"")

---

## Contributors

A very special mention to HashiQube's contributors. Thank you all for your help, suggestions, and contributions, no matter how small! ❤️

- [Thomas Cockin](https://www.aslantechnology.com.au/)
- [Konstantin Vanyushov](https://www.linkedin.com/in/konstantin-vanyushov/)
- [Tristan Morgan](https://www.linkedin.com/in/tristanmorgan/)
- [Ringo Chan](https://www.linkedin.com/in/ringochan/)
- [Ehsan Mirzaei](https://www.linkedin.com/in/ehsan-mirzaei-54305181/)
- [Greg Luxford](https://www.linkedin.com/in/greg-luxford/)
- [Byron Tuckett](https://www.linkedin.com/in/byrontuckett/)
- [Lane Birmingham](https://www.linkedin.com/in/lane-birmingham-93261b151/)
- [Devang Dhameliya](https://www.linkedin.com/in/devangdhameliya/)
- [Rajesh Cholleti](https://www.linkedin.com/in/rajesh-cholleti-2b25a1116/)
- [Adriana Villela](https://www.linkedin.com/in/adrianavillela/)
- [Charle Van Der Walt](https://linkedin.com/in/charle-van-der-walt-b9ba2628/)
- [JJ Badenhorst](https://github.com/badj)
- [Jan Laubscher](https://howdypress.com/)
- [Nesar Uddin Rahid](https://dribbble.com/rahiddesigner)
- [Ruaan Deysel](https://www.linkedin.com/in/ruaan-deysel/)

---

## Videos

Videos were made with [asciinema](https://asciinema.org/)

```bash
asciinema rec -i 1
asciicast2gif -S 1 -s 2 tmp
```

[google ads](googleads.html ':include :type=iframe width=100% height=300px')",FAUX
startxfr/helm-repository,Application System,Documentations,2025-05-14T20:29:01Z,2025-05-14T20:20:36Z,0,0,0,0,4,0,0,0,2020-09-26T14:18:29Z,2025-03-31T13:14:33Z,1801574,12,HTML,VRAI,10,FAUX,3,,3,helm charts for various infrastructure configuration and services running under an Openshift Container Platform (or OKD),FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,8,"# helm-repository [![release](https://img.shields.io/badge/release-v17.14.19-blue.svg)](https://github.com/startxfr/helm-repository/releases/tag/17.14.19) [![last commit](https://img.shields.io/github/last-commit/startxfr/helm-repository.svg)](https://github.com/startxfr/helm-repository) [![Doc](https://readthedocs.org/projects/helm-repository/badge)](https://helm-repository.readthedocs.io) [![Artifacthub](https://img.shields.io/badge/ArtifactHub-STARTX-blue.svg)](https://artifacthub.io/packages/search?repo=startx)

helm charts for various infrastructure configuration and services running under an Openshift Container Platform (or OKD). 

More informations are available on the [STARTX Helm repository homepage](https://helm-repository.readthedocs.io).",VRAI
statnett/image-scanner-operator,Toolkit,Application System,2025-05-16T05:54:42Z,2025-04-25T06:14:54Z,0,0,0,0,2,0,0,0,2023-01-06T14:54:59Z,2025-04-07T06:36:29Z,2176,26,Go,VRAI,9,FAUX,9,,9,Kubernetes Operator supporting detection of vulnerabilities in running container images,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,10,"# Image Scanner Operator

[![Conventional Commits][conventional-commits-img]][conventional-commits]
[![Go Report Card][report-card-img]][report-card]
[![CodeQL][CodeQL-img]][CodeQL]

Image Scanner is a Kubernetes Operator supporting detection of vulnerabilities
in running container images.

Kubernetes clusters run containers created from a great diversity of container
images with several origins.
Some images are custom for an in-house application, while others are pulled
from internal/external registries.
It is considered best practice to scan images in container image build
pipelines, to provide developers with early feedback on potential
vulnerabilities.

While this is good, it is not enough:

Software vulnerabilities are typically detected after the software is available
for use, and some applications will typically ""just run"" in their runtime
environment - with little/no maintenance.
And some users might not use pipelines at all or use pipelines without image
scanning enabled.

Running applications on container images with vulnerabilities _might_ represent
an unacceptable threat.
To mend this, we want **a mechanism to identify vulnerabilities
in running container images**.
This could then be used by developers, system administrators, platform
administrators and security officers
to take action when vulnerabilities are detected.

Actions could for instance be:

- upgrade to a patched version of the vulnerable software component
- conclude that the vulnerability is not relevant
- conclude that the vulnerability represents an acceptable risk
- shut down the application

There seem to be quite a lot of companies/communities providing similar
software. Some key features of this operator are:

- **Can be configured to scan container images for any
  [Kubernetes Workload](https://kubernetes.io/docs/concepts/workloads/)**,
  including custom resources. The only requirement is that the resource
  MUST
  [own](https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/)
  Pods - either directly or indirectly.
- **Container images are scanned based on immutable image sha256 digests**
  obtained from
  [Container Runtime Interface (CRI)](https://kubernetes.io/docs/concepts/architecture/cri/),
  not from potentially mutable image tags.
- To avoid unnecessary image pulls, the image scan workload is preferred
  scheduled on the same node(s) as the workload to be scanned.
- Since the image to scan should already be present in the node container
  registry, **the image scan workload does not have to bother with image pull secrets for
  private images**, exploiting a
  [Kubernetes ""bug""](https://github.com/kubernetes/kubernetes/issues/18787).

## Description

// TODO(user): An in-depth paragraph about your project and overview of use

### Custom resources

The Image Scanner operator currently defines a single user-facing Custom
Resource Definition (CRD), [ContainerImageScan][CIS-CRD] (CIS), that represents the
Kubernetes API for runtime image scanning of workload container images.
See [stas_v1alpha1_containerimagescan.yaml][CIS-example] for a (simplified)
example of a CIS resource.

The CIS resource `.spec` specifies the container image to scan and some
additional workload metadata, and the image scan result is added/updated
in `.status` by the `ContainerImageScan` controller.

CIS resources should not be edited by standard users, as the `Workload`
controller will create CIS resources from running pods. And the standard
Kubernetes garbage collector deletes the obsolete CIS resources when the
owning pods are gone.

A user can influence the image scanning process by adding annotations to pods.
The set of annotations is currently limited, but more might be added in the
future:

| Pod annotation key                         | Default value | Description                                                                                                           |
|--------------------------------------------|:--------------|:----------------------------------------------------------------------------------------------------------------------|
| `image-scanner.statnett.no/ignore-unfixed` | `""false""`     | If set to `""true""`, the Image Scanner will ignore any detected vulnerability that can't be fix by updating package(s) |

### Supported features

- Namespaced container image scan API (custom resource): `ContainerImageScan` (CIS)
- A CIS resource contains details and a summary of detected vulnerabilities
- Users can identify the owning/controlling resource (workload) of the scanned container image
- All container images are rescanned regularly with configurable interval
- Provides vulnerability summary metrics from CIS to enable dashboards/alerts
- Any user with access to a namespace is allowed to _view_ CIS
- Cluster-scoped operator that operates in configured namespaces with an
  include/block list of namespaces that should be scanned
- Supports any type of workload (also CRDs) by configuration
- Scanning workload images from private/authenticated image registries

### Future improvements

- Push-based feedback to stakeholders
- Enable users to ignore/suppress vulnerabilities
- Perform actions from detected vulnerabilities

### Eschewed Features

- Produce metrics for vulnerability details
- Historical container image scans
- Helm chart installation

## Getting Started

You’ll need a Kubernetes cluster to run the Image Scanner.
You can use [KIND](https://sigs.k8s.io/kind) or [k3s](https://k3s.io/)
to get a local cluster for testing.

We currently only support installation using
[Kustomize](https://kustomize.io/), either as a standalone tool,
using the kustomize (`-k`) feature in recent versions of `kubectl`
or any GitOps tool with support for Kustomize.

### Install

Since you probably want to adjust the default configuration (and/or have
multiple clusters), we suggest you start by creating a kustomize overlay using
the Image Scanner default kustomization as a
[remote directory](https://github.com/kubernetes-sigs/kustomize/blob/master/examples/remoteBuild.md#remote-directories)
base. Your initial kustomization.yaml could be as simple as:

<!-- x-release-please-start-version -->
```yaml
resources:
  - https://github.com/statnett/image-scanner-operator?ref=v0.8.45
```
<!-- x-release-please-end -->

If you have multiple clusters, you should create one
[variant](https://kubectl.docs.kubernetes.io/references/kustomize/glossary/#variant)
overlay per cluster.

To install (or update) the operator into your cluster run:

```sh
kubectl apply --server-side -k <overlay-directory>
```

### Configure

The Image Scanner operator is highly configurable and supports numerous
flags with corresponding environment variables. To get an overview over
all that can be configured, the easiest is to use the CLI:

```sh
docker run ghcr.io/statnett/image-scanner-operator --help
```

You can also use environment variables for configuration, but a flag takes
precedence over the corresponding environment variable.
Environment variable names can be deduced from flags by upper-casing and
replacing the `-` delimiter with `_`.

Since we use kustomize to install the operator, the easiest is
to customize the environment variables provided
from the ConfigMap in the default Image Scanner configuration using a
[configMapGenerator](https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/configmapgenerator/).

```yaml
configMapGenerator:
  - name: image-scanner-config
    behavior: merge
    literals:
      - CIS_METRICS_LABELS=app.kubernetes.io/name
      - SCAN_INTERVAL=24h
```

This example will override the default configuration to:

- add metric labels with values obtained from `app.kubernetes.io/name` Pod labels
- rescan workload images with an interval of 24 hours

#### Configure Trivy scan jobs

A workload container image is scanned by scheduling a Kubernetes _Job_ running
on the scan target container image. The image is scanned using the
[`trivy filesystem`](https://aquasecurity.github.io/trivy/latest/docs/references/configuration/cli/trivy_filesystem/)
command inside the job's container.

The `trivy filesystem` scan command can be customized by modifying the
`trivy-job-config` _ConfigMap_. All entries in the _ConfigMap_ are mounted
as environment variables with the `TRIVY_` prefix - which will allow them
to be picked up by Trivy. Example:

```yaml
  - name: trivy-job-config
    namespace: image-scanner
    behavior: merge
    literals:
      - DB_REPOSITORY=<company-ghcr-registry-proxy>/aquasecurity/trivy-db
      - JAVA_DB_REPOSITORY=<company-ghcr-registry-proxy>/aquasecurity/trivy-java-db
      - OFFLINE_SCAN=true # enabling offline mode for air-gapped environments
```

### Upgrade

At this early stage, we might introduce breaking changes. But when we do,
the breaking changes should be highlighted in the changelog and release notes.

We might also do breaking changes in the CRD(s), without adding the complexity
of conversion webhooks. So if you experience any issue when upgrading to a
newer version, please try to reinstall the operator as a first step.

### Uninstall

To uninstall the operator, just use `kubectl` to delete all resources produced
by the overlay used when [installing](#install) the operator:

```sh
kubectl delete -k --ignore-not-found=true <overlay-directory>
```

### How it works

This project aims to follow the Kubernetes
[Operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/).

It uses
[Controllers](https://kubernetes.io/docs/concepts/architecture/controller/)
which provides a reconcile function responsible for synchronizing resources
until the desired state is reached.

Image Scanner consists of three controllers that coordinates
scanning of running container images as illustrated in the diagrams below.

The container image scan Kubernetes API is materialized
by the `ContainerImageScan` custom resources providing
an eventually consistent image scanning result in its
status.

The actual vulnerability scan of a container image and the vulnerability
database is provided by an external service to the operator.

Using a simple `Pod`, with a single container, as an example:

1. Create a `ContainerImageScan` when the immutable image reference is available in the `Pod` status.
2. Create a scan `Job` from immutable image reference in the `ContainerImageScan` spec.
3. When a scan `Job` is completed, read the scan result from pod log of the scan `Job`,
   and update the `ContainerImageScan` status.
4. When the `Pod` is deleted, the `ContainerImageScan` is garbage collected.

![Image scanner component diagram](http://www.plantuml.com/plantuml/proxy?cache=no&src=https://raw.githubusercontent.com/statnett/image-scanner-operator/main/docs/operator-component.puml))
![Scan image sequence diagram](http://www.plantuml.com/plantuml/proxy?cache=no&src=https://raw.githubusercontent.com/statnett/image-scanner-operator/main/docs/scan-sequence.puml)

## Contributing

We would love your feedback on any aspect of the Image Scanner!
Feel free to open issues for things you think can be improved.
Or/and open a PR (better) to show how we can improve.

See [Contributing](CONTRIBUTING.md) for information about setting up
your local development environment, and the contribution workflow expected.

Please ensure that you are following our [Code Of Conduct](CODE_OF_CONDUCT.md)
during any interaction with the community.

## License

Licensed under the [MIT License](LICENSE).

[CodeQL-img]: https://github.com/statnett/image-scanner-operator/actions/workflows/codeql.yml/badge.svg?branch=main
[CodeQL]: https://github.com/statnett/image-scanner-operator/actions/workflows/codeql.yml?query=branch%3Amain
[conventional-commits-img]: https://img.shields.io/badge/Conventional%20Commits-1.0.0-%23FE5196?logo=conventionalcommits&logoColor=white
[conventional-commits]: https://conventionalcommits.org
[report-card-img]: https://goreportcard.com/badge/github.com/statnett/image-scanner-operator
[report-card]: https://goreportcard.com/report/github.com/statnett/image-scanner-operator
[CIS-CRD]: https://doc.crds.dev/github.com/statnett/image-scanner-operator/stas.statnett.no/ContainerImageScan/v1alpha1
[CIS-example]: config/samples/stas_v1alpha1_containerimagescan.yaml",VRAI
stellar-expert/stellar-expert-explorer,Toolkit,Application System,2025-05-05T15:42:04Z,2024-12-23T16:38:11Z,0,0,0,0,0,1,0,0,2017-12-20T21:45:18Z,2025-03-31T18:41:35Z,1200,64,JavaScript,FAUX,34,FAUX,250,,250,StellarExpert Explorer – block explorer and analytics platform for Stellar Network.,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,2,"# Stellar📡Expert

StellarExpert – block explorer and analytics platform
for [Stellar Network](https://stellar.org).

#### Links

- 📡 [StellarExpert explorer](https://stellar.expert)
- 📖 [Open API documetation](https://stellar.expert/openapi.html) for
  developers (the data from Open Directory API is publicly available for
  developers and users, free of charge)
- 📩 [Request](https://github.com/stellar-expert/stellar-expert-explorer/issues)
  new features, submit bug reports, and vote for issues
- 🏷️Request [Directory listing](https://stellar.expert/directory/add) for a
  service account or asset issuer address
- ⚠️[Report](https://stellar.expert/directory/blocked-domains/add) a fraudulent
  website related to Stellar ecosystem

## API Server

#### Install dependencies

```
cd api
npm i
```

#### Configuration

All configuration parameters stored in `app.config.json` file.

Copy the template config file and edit parameters:

```
cp example.app.config.json app.config.json
```

- `port` - API server port
- `apiCacheDisabled` - set to `true` to disable response caching (recommended
  for development environment)
- `networks` - supported Stellar networks configuration
    - `db` - connection string Mongodb database with ingested ledger data
    - `horizon` - URL of the public Horizon server,
    - `network` - network identifier
- `directory` - public Directory configuration
    - `repository` - Github repository identifier in the
      format `{owner}/{repository}`
    - `accessToken` - Github access token for the bot with repository access
    - `admins` - handles of the users with write permissions to the repository
- `oauth` - OAuth providers configuration
    - `clientId` - application ClientId obtained from OAuth provider
    - `secret` - corresponding secret
- `corsWhitelist` - array containing all origins that will have CORS enabled for
  all requests

#### Start

```
node api.js
```

(for verbose HTTP requests logging pass `MODE=development` environment variable)

## Frontend

#### Install dependencies

```
cd ui
pnpm i
```

(requires PNPM package manager to be installed)

#### Configuration

All configuration parameters stored in `app.config.json` file.

- `apiEndpoint` - URL of the API sever
- `networks` - supported Stellar networks configuration
    - `passphrase` - network passphrase
    - `horizon` - URL of the public Horizon server,
    - `title` - friendly name
- `directoryAdmins` - handles of the users with write permissions to the
  Directory repository
- `oauth` - OAuth providers configuration
    - `clientId` - application ClientId obtained from OAuth provider

Additional build options are located in `webpack-config.js`

#### Start the application in the development mode

```
pnpm dev-server
```

*(check webpack dev-server output for the hot-reload browser link)*

#### Build production bundle

```
pnpm build
```

*(check for the generated files in the `./public` repository)*

#### Re-generate Open API docs

```
pnpm build-api-docs 
```

---

### TBD

- Provide access credentials for the test database
- Review all existing tests and docs, move everything to this repository
- Gradually transfer issues from the team bugtracker to Github Issues",VRAI
StyraInc/opa-aws-cloudformation-hook,Toolkit,Documentations,2024-03-06T10:34:23Z,2022-03-30T00:05:51Z,0,29,0,0,0,0,0,0,2022-03-08T13:41:40Z,2024-09-02T11:58:20Z,210,36,Python,VRAI,5,FAUX,6,"authorization,aws,aws-cloudformation,aws-cloudformation-hooks,cloudformation,opa,open-policy-agent,policy-as-code,rego",6,AWS Cloudformation Hook for OPA-powered infrastructure policy enforcement,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,6,"# OPA AWS CloudFormation Hook

<p align=""center"">
    <img alt=""OPA AWS CloudFormation Hook Diagram"" src=""docs/assets/opa-aws-cloudformation.svg"">
</p>

This repository integrates AWS CloudFormation (CFN) with OPA using
[AWS Cloud Formation Hooks](https://aws.amazon.com/about-aws/whats-new/2022/02/aws-announces-general-availability-aws-cloudformation-hooks/).
Use this integration if you want to enforce policies over AWS resources (e.g., EC2 instances, S3 buckets, etc.)
provisioned with CloudFormation. For example, using this integration you can enforce policy across resources like:

* [EC2 Security Groups](https://github.com/StyraInc/opa-aws-cloudformation-hook/blob/main/examples/policy/ec2/security_group/security_group.rego)
* [IAM Admin Rules](https://github.com/StyraInc/opa-aws-cloudformation-hook/blob/main/examples/policy/iam/user/no_admin_test.rego)
* [S3 Public Access](https://github.com/StyraInc/opa-aws-cloudformation-hook/blob/main/examples/policy/s3/bucket/public_access_test.rego)

> AWS Cloud Formation Hooks were added in February 2022. The feature is still relatively new for AWS Cloud Formation.
> If you run into any issues please report them [here](https://github.com/StyraInc/opa-aws-cloudformation-hook/issues).

## How it Works

The OPA hook works by installing an
[AWS CloudFormation Hook](https://docs.aws.amazon.com/cloudformation-cli/latest/userguide/hooks-structure.html)
to your environment.

When creating, updating, or deleting a CloudFormation Stack, the hook is triggered to validate the configuration.
When used in conjunction with OPA, the hook will send the property information from each resource in a Stack to your
OPA server. When this information is received, OPA will validate the request against your defined policies and send
back any violations it may have found, which will stop the stack creation and log the violations to AWS CloudWatch.
If no violations are reported, the resources contained in the stack are created, updated or deleted accordingly.

**NOTE:** Installing OPA into your AWS environment is currently out of scope for this documentation. For local
development, a tool like [ngrok](https://ngrok.com/) could be used to point at an OPA running on your machine.

Want to try out this integration yourself? See the AWS Cloud Formation Hooks tutorial in the
[OPA documentation](https://www.openpolicyagent.org/docs/latest/aws-cloudformation-hooks/).

## Repository Contents

Provided in this repository, you'll find the code for the hook you'll deploy in your AWS account to enable OPA policy
enforcement for your CloudFormation resources under the `hooks` directory. See the
[OPA tutorial](https://www.openpolicyagent.org/docs/latest/aws-cloudformation-hooks/) on the topic for instructions on
how to quickly get started, or the
[development guide](https://docs.aws.amazon.com/cloudformation-cli/latest/userguide/hooks.html)
in AWS the documentation if you'd like to learn more about how it works.

To give you an idea about what policy for AWS CloudFormation Hooks might look like, this repository provides a number
of example resources and policies:

* The `examples/templates` directory contains example templates used for testing
* The `examples/policy` directory contains example policies

### Policy Development

In order to quickly iterate on changes in your Rego policies, you may use the `validate.py` tool provided under the
`test` directory. The tool allows you to test your policies against provided CloudFormation template files, without
actually submitting them to a hook installed in your environment. With an OPA server started with your policy
files loaded (e.g. `opa run --server --watch examples/policy`), you may use the tool like:

```shell
test/validate.py my-cloudformation-template.yaml
```

The tool will extract all resources found in the template and submit them to OPA one by one, in the same manner
the hook operates once installed. Should any violation be encountered, the tool will print them to the console.

### Deregistering the Hook

Deregistering a hook requires removal of not just the hook type, but also any versions of the hook deployed. In order
to help with that, you may use the `deregister-hook.sh` script provided in this repo, with the ARN of the hook provided
as the only argument:

```script
./deregister-hook.sh <ARN of your hook here>
```

## Community

For questions, discussions and announcements related to Styra products, services and open source projects, please join the Styra community on [Slack](https://communityinviter.com/apps/styracommunity/signup)!",VRAI
StyraInc/regal,Toolkit,Application System,2025-05-14T12:23:50Z,2025-04-22T17:06:48Z,0,277,0,0,0,0,0,0,2023-01-23T15:23:06Z,2025-04-04T13:11:43Z,7546,291,Go,VRAI,40,FAUX,95,"code-quality,language-server,linter,lsp,magnificent,opa,open-policy-agent,policy-as-code,rego,static-analysis",95,"Regal is a linter and language server for Rego, bringing your policy development experience to the next level!",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,33,"# Regal

[![Build Status](https://github.com/styrainc/regal/workflows/Build/badge.svg?branch=main)](https://github.com/styrainc/regal/actions)
![OPA v1.3.0](https://openpolicyagent.org/badge/v1.3.0)
[![codecov](https://codecov.io/github/StyraInc/regal/graph/badge.svg?token=EQK01YF3X3)](https://codecov.io/github/StyraInc/regal)
[![Downloads](https://img.shields.io/github/downloads/styrainc/regal/total.svg)](https://github.com/StyraInc/regal/releases)

Regal is a linter and language server for [Rego](https://www.openpolicyagent.org/docs/latest/policy-language/), making
your Rego magnificent, and you the ruler of rules!

With its extensive set of linter rules, documentation and editor integrations, Regal is the perfect companion for policy
development, whether you're an experienced Rego developer or just starting out.

<img
  src=""/docs/assets/regal-banner.png""
  alt=""illustration of a viking representing the Regal logo""
  width=""150px"" />

> regal
>
> adj : of notable excellence or magnificence : splendid

\- [Merriam Webster](https://www.merriam-webster.com/dictionary/regal)

## **New!** Regal and OPA 1.0+

OPA 1.0 was [recently released](https://blog.openpolicyagent.org/announcing-opa-1-0-a-new-standard-for-policy-as-code-a6d8427ee828),
and starting from version v0.30.0, Regal supports working with both OPA 1.0+ policies and Rego from earlier versions
of OPA. While everything should work without additional configuration, we recommend checking out our documentation on
using Regal with [OPA 1.0](https://docs.styra.com/regal/opa-one-dot-zero) and later for the best possible experience
managing projects of any given Rego version, or even a mix of them.

## Goals

- Deliver an outstanding policy development experience by providing the best possible tools for that purpose
- Identify common mistakes, bugs and inefficiencies in Rego policies, and suggest better approaches
- Provide advice on [best practices](https://github.com/StyraInc/rego-style-guide), coding style, and tooling
- Allow users, teams and organizations to enforce custom rules on their policy code

## What People Say About Regal

> I really like that at each release of Regal I learn something new!
> Of all the linters I'm exposed to, Regal is probably the most instructive one.

— Leonardo Taccari, [NetBSD](https://www.netbsd.org/)

> Reviewing the Regal rules documentation. Pure gold.

— Dima Korolev, [Miro](https://miro.com/)

> Such an awesome project!

— Shawn McGuire, [Atlassian](https://www.atlassian.com/)

> I am really impressed with Regal. It has helped me write more expressive and deterministic Rego.

— Jimmy Ray, [Boeing](https://www.boeing.com/)

See the [adopters](/docs/adopters.md) file for more Regal users.

## Getting Started

### Download Regal

**MacOS and Linux**
```shell
brew install styrainc/packages/regal
```

<details>
  <summary><strong>Other Installation Options</strong></summary>

Please see [Packages](https://docs.styra.com/regal/adopters#packaging)
for a list of package repositories which distribute Regal.

Manual installation commands:

**MacOS (Apple Silicon)**
```shell
curl -L -o regal ""https://github.com/StyraInc/regal/releases/latest/download/regal_Darwin_arm64""
```

**MacOS (x86_64)**
```shell
curl -L -o regal ""https://github.com/StyraInc/regal/releases/latest/download/regal_Darwin_x86_64""
```

**Linux (x86_64)**
```shell
curl -L -o regal ""https://github.com/StyraInc/regal/releases/latest/download/regal_Linux_x86_64""
chmod +x regal
```

**Windows**
```shell
curl.exe -L -o regal.exe ""https://github.com/StyraInc/regal/releases/latest/download/regal_Windows_x86_64.exe""
```

**Docker**
```shell
docker pull ghcr.io/styrainc/regal:latest
```

See all versions, and checksum files, at the Regal [releases](https://github.com/StyraInc/regal/releases/) page, and
published Docker images at the [packages](https://github.com/StyraInc/regal/pkgs/container/regal) page.
</details>

### Try it out!

First, author some Rego!

**policy/authz.rego**
```rego
package authz

default allow = false

allow if {
    isEmployee
    ""developer"" in input.user.roles
}

isEmployee if regex.match(""@acmecorp\\.com$"", input.user.email)
```

Next, run `regal lint` pointed at one or more files or directories to have them linted.

```shell
regal lint policy/
```
<!-- markdownlint-capture -->
<!-- markdownlint-disable MD010 -->
```text
Rule:         	non-raw-regex-pattern
Description:  	Use raw strings for regex patterns
Category:     	idiomatic
Location:     	policy/authz.rego:12:27
Text:         	isEmployee if regex.match(""@acmecorp\\.com$"", input.user.email)
Documentation:	https://docs.styra.com/regal/rules/idiomatic/non-raw-regex-pattern

Rule:         	use-assignment-operator
Description:  	Prefer := over = for assignment
Category:     	style
Location:     	policy/authz.rego:5:1
Text:         	default allow = false
Documentation:	https://docs.styra.com/regal/rules/style/use-assignment-operator

Rule:         	prefer-snake-case
Description:  	Prefer snake_case for names
Category:     	style
Location:     	policy/authz.rego:12:1
Text:         	isEmployee if regex.match(""@acmecorp\\.com$"", input.user.email)
Documentation:	https://docs.styra.com/regal/rules/style/prefer-snake-case

1 file linted. 3 violations found.
```
<!-- markdownlint-restore -->
<br />

> **Note**
> If you're running Regal on an existing policy library, you may want to disable the `style` category initially, as it
> will likely generate a lot of violations. You can do this by passing the `--disable-category style` flag to
> `regal lint`.

### Using Regal in Your Editor

Linting from the command line is a great way to get started with Regal, and even for some experienced developers
the preferred way to work with the linter. However, not only is Regal a linter, but a full-fledged development
companion for Rego development!

Integrating Regal in your favorite editor means you'll get immediate feedback from the linter as you work on your
policies. More than that, it'll unlock a whole new set of features that leverage Regal's
[language server](#regal-language-server), like context-aware completion suggestions, informative tooltips on hover,
or go-to-definition.

Elevate your policy development experience with Regal in VS Code, Neovim, Zed, Helix
and more on our [Editor Support page](https://docs.styra.com/regal/editor-support)!

To learn more about the features provided by the Regal language server, see the
[Language Server](https://docs.styra.com/regal/language-server) page.

### Using Regal in Your Build Pipeline

To ensure Regal's rules are enforced consistently in your project or organization,
we've made it easy to run Regal as part of your builds.
See the docs on [Using Regal in your build pipeline](./docs/cicd.md) to learn more
about how to set up Regal to lint your policies on every commit or pull request.

## Rules

Regal comes with a set of built-in rules, grouped by category.

- **bugs**: Common mistakes, potential bugs and inefficiencies in Rego policies.
- **custom**: Custom, rules where enforcement can be adjusted to match your preferences.
- **idiomatic**: Suggestions for more idiomatic constructs.
- **imports**: Best practices for imports.
- **style**: [Rego Style Guide](https://github.com/StyraInc/rego-style-guide) rules.
- **testing**: Rules for testing and development.

The following rules are currently available:

<!-- RULES_TABLE_START -->

|  Category   |                                                  Title                                                  |                        Description                        |
|-------------|---------------------------------------------------------------------------------------------------------|-----------------------------------------------------------|
| bugs        | [annotation-without-metadata](https://docs.styra.com/regal/rules/bugs/annotation-without-metadata)      | Annotation without metadata                               |
| bugs        | [argument-always-wildcard](https://docs.styra.com/regal/rules/bugs/argument-always-wildcard)            | Argument is always a wildcard                             |
| bugs        | [constant-condition](https://docs.styra.com/regal/rules/bugs/constant-condition)                        | Constant condition                                        |
| bugs        | [deprecated-builtin](https://docs.styra.com/regal/rules/bugs/deprecated-builtin)                        | Avoid using deprecated built-in functions                 |
| bugs        | [duplicate-rule](https://docs.styra.com/regal/rules/bugs/duplicate-rule)                                | Duplicate rule                                            |
| bugs        | [if-empty-object](https://docs.styra.com/regal/rules/bugs/if-empty-object)                              | Empty object following `if`                               |
| bugs        | [if-object-literal](https://docs.styra.com/regal/rules/bugs/if-object-literal)                          | Object literal following `if`                             |
| bugs        | [import-shadows-rule](https://docs.styra.com/regal/rules/bugs/import-shadows-rule)                      | Import shadows rule                                       |
| bugs        | [impossible-not](https://docs.styra.com/regal/rules/bugs/impossible-not)                                | Impossible `not` condition                                |
| bugs        | [inconsistent-args](https://docs.styra.com/regal/rules/bugs/inconsistent-args)                          | Inconsistently named function arguments                   |
| bugs        | [internal-entrypoint](https://docs.styra.com/regal/rules/bugs/internal-entrypoint)                      | Entrypoint can't be marked internal                       |
| bugs        | [invalid-metadata-attribute](https://docs.styra.com/regal/rules/bugs/invalid-metadata-attribute)        | Invalid attribute in metadata annotation                  |
| bugs        | [leaked-internal-reference](https://docs.styra.com/regal/rules/bugs/leaked-internal-reference)          | Outside reference to internal rule or function            |
| bugs        | [not-equals-in-loop](https://docs.styra.com/regal/rules/bugs/not-equals-in-loop)                        | Use of != in loop                                         |
| bugs        | [redundant-existence-check](https://docs.styra.com/regal/rules/bugs/redundant-existence-check)          | Redundant existence check                                 |
| bugs        | [redundant-loop-count](https://docs.styra.com/regal/rules/bugs/redundant-loop-count)                    | Redundant count before loop                               |
| bugs        | [rule-assigns-default](https://docs.styra.com/regal/rules/bugs/rule-assigns-default)                    | Rule assigned its default value                           |
| bugs        | [rule-named-if](https://docs.styra.com/regal/rules/bugs/rule-named-if)                                  | Rule named ""if""                                           |
| bugs        | [rule-shadows-builtin](https://docs.styra.com/regal/rules/bugs/rule-shadows-builtin)                    | Rule name shadows built-in                                |
| bugs        | [sprintf-arguments-mismatch](https://docs.styra.com/regal/rules/bugs/sprintf-arguments-mismatch)        | Mismatch in `sprintf` arguments count                     |
| bugs        | [time-now-ns-twice](https://docs.styra.com/regal/rules/bugs/time-now-ns-twice)                          | Repeated calls to `time.now_ns`                           |
| bugs        | [top-level-iteration](https://docs.styra.com/regal/rules/bugs/top-level-iteration)                      | Iteration in top-level assignment                         |
| bugs        | [unassigned-return-value](https://docs.styra.com/regal/rules/bugs/unassigned-return-value)              | Non-boolean return value unassigned                       |
| bugs        | [unused-output-variable](https://docs.styra.com/regal/rules/bugs/unused-output-variable)                | Unused output variable                                    |
| bugs        | [var-shadows-builtin](https://docs.styra.com/regal/rules/bugs/var-shadows-builtin)                      | Variable name shadows built-in                            |
| bugs        | [zero-arity-function](https://docs.styra.com/regal/rules/bugs/zero-arity-function)                      | Avoid functions without args                              |
| custom      | [forbidden-function-call](https://docs.styra.com/regal/rules/custom/forbidden-function-call)            | Forbidden function call                                   |
| custom      | [missing-metadata](https://docs.styra.com/regal/rules/custom/missing-metadata)                          | Package or rule missing metadata                          |
| custom      | [naming-convention](https://docs.styra.com/regal/rules/custom/naming-convention)                        | Naming convention violation                               |
| custom      | [narrow-argument](https://docs.styra.com/regal/rules/custom/narrow-argument)                            | Function argument can be narrowed                         |
| custom      | [one-liner-rule](https://docs.styra.com/regal/rules/custom/one-liner-rule)                              | Rule body could be made a one-liner                       |
| custom      | [prefer-value-in-head](https://docs.styra.com/regal/rules/custom/prefer-value-in-head)                  | Prefer value in rule head                                 |
| idiomatic   | [ambiguous-scope](https://docs.styra.com/regal/rules/idiomatic/ambiguous-scope)                         | Ambiguous metadata scope                                  |
| idiomatic   | [boolean-assignment](https://docs.styra.com/regal/rules/idiomatic/boolean-assignment)                   | Prefer `if` over boolean assignment                       |
| idiomatic   | [custom-has-key-construct](https://docs.styra.com/regal/rules/idiomatic/custom-has-key-construct)       | Custom function may be replaced by `in` and `object.keys` |
| idiomatic   | [custom-in-construct](https://docs.styra.com/regal/rules/idiomatic/custom-in-construct)                 | Custom function may be replaced by `in` keyword           |
| idiomatic   | [directory-package-mismatch](https://docs.styra.com/regal/rules/idiomatic/directory-package-mismatch)   | Directory structure should mirror package                 |
| idiomatic   | [equals-pattern-matching](https://docs.styra.com/regal/rules/idiomatic/equals-pattern-matching)         | Prefer pattern matching in function arguments             |
| idiomatic   | [in-wildcard-key](https://docs.styra.com/regal/rules/idiomatic/in-wildcard-key)                         | Unnecessary wildcard key                                  |
| idiomatic   | [no-defined-entrypoint](https://docs.styra.com/regal/rules/idiomatic/no-defined-entrypoint)             | Missing entrypoint annotation                             |
| idiomatic   | [non-raw-regex-pattern](https://docs.styra.com/regal/rules/idiomatic/non-raw-regex-pattern)             | Use raw strings for regex patterns                        |
| idiomatic   | [prefer-set-or-object-rule](https://docs.styra.com/regal/rules/idiomatic/prefer-set-or-object-rule)     | Prefer set or object rule over comprehension              |
| idiomatic   | [use-contains](https://docs.styra.com/regal/rules/idiomatic/use-contains)                               | Use the `contains` keyword                                |
| idiomatic   | [use-if](https://docs.styra.com/regal/rules/idiomatic/use-if)                                           | Use the `if` keyword                                      |
| idiomatic   | [use-in-operator](https://docs.styra.com/regal/rules/idiomatic/use-in-operator)                         | Use in to check for membership                            |
| idiomatic   | [use-object-keys](https://docs.styra.com/regal/rules/idiomatic/use-object-keys)                         | Prefer to use `object.keys`                               |
| idiomatic   | [use-some-for-output-vars](https://docs.styra.com/regal/rules/idiomatic/use-some-for-output-vars)       | Use `some` to declare output variables                    |
| idiomatic   | [use-strings-count](https://docs.styra.com/regal/rules/idiomatic/use-strings-count)                     | Use `strings.count` where possible                        |
| imports     | [avoid-importing-input](https://docs.styra.com/regal/rules/imports/avoid-importing-input)               | Avoid importing input                                     |
| imports     | [circular-import](https://docs.styra.com/regal/rules/imports/circular-import)                           | Circular import                                           |
| imports     | [confusing-alias](https://docs.styra.com/regal/rules/imports/confusing-alias)                           | Confusing alias of existing import                        |
| imports     | [ignored-import](https://docs.styra.com/regal/rules/imports/ignored-import)                             | Reference ignores import                                  |
| imports     | [implicit-future-keywords](https://docs.styra.com/regal/rules/imports/implicit-future-keywords)         | Use explicit future keyword imports                       |
| imports     | [import-after-rule](https://docs.styra.com/regal/rules/imports/import-after-rule)                       | Import declared after rule                                |
| imports     | [import-shadows-builtin](https://docs.styra.com/regal/rules/imports/import-shadows-builtin)             | Import shadows built-in namespace                         |
| imports     | [import-shadows-import](https://docs.styra.com/regal/rules/imports/import-shadows-import)               | Import shadows another import                             |
| imports     | [prefer-package-imports](https://docs.styra.com/regal/rules/imports/prefer-package-imports)             | Prefer importing packages over rules                      |
| imports     | [redundant-alias](https://docs.styra.com/regal/rules/imports/redundant-alias)                           | Redundant alias                                           |
| imports     | [redundant-data-import](https://docs.styra.com/regal/rules/imports/redundant-data-import)               | Redundant import of data                                  |
| imports     | [unresolved-import](https://docs.styra.com/regal/rules/imports/unresolved-import)                       | Unresolved import                                         |
| imports     | [use-rego-v1](https://docs.styra.com/regal/rules/imports/use-rego-v1)                                   | Use `import rego.v1`                                      |
| performance | [defer-assignment](https://docs.styra.com/regal/rules/performance/defer-assignment)                     | Assignment can be deferred                                |
| performance | [non-loop-expression](https://docs.styra.com/regal/rules/performance/non-loop-expression)               | Non-loop expression                                       |
| performance | [walk-no-path](https://docs.styra.com/regal/rules/performance/walk-no-path)                             | Call to `walk` can be optimized                           |
| performance | [with-outside-test-context](https://docs.styra.com/regal/rules/performance/with-outside-test-context)   | `with` used outside test context                          |
| style       | [avoid-get-and-list-prefix](https://docs.styra.com/regal/rules/style/avoid-get-and-list-prefix)         | Avoid `get_` and `list_` prefix for rules and functions   |
| style       | [chained-rule-body](https://docs.styra.com/regal/rules/style/chained-rule-body)                         | Avoid chaining rule bodies                                |
| style       | [comprehension-term-assignment](https://docs.styra.com/regal/rules/style/comprehension-term-assignment) | Assignment can be moved to comprehension term             |
| style       | [default-over-else](https://docs.styra.com/regal/rules/style/default-over-else)                         | Prefer default assignment over fallback else              |
| style       | [default-over-not](https://docs.styra.com/regal/rules/style/default-over-not)                           | Prefer default assignment over negated condition          |
| style       | [detached-metadata](https://docs.styra.com/regal/rules/style/detached-metadata)                         | Detached metadata annotation                              |
| style       | [double-negative](https://docs.styra.com/regal/rules/style/double-negative)                             | Avoid double negatives                                    |
| style       | [external-reference](https://docs.styra.com/regal/rules/style/external-reference)                       | External reference in function                            |
| style       | [file-length](https://docs.styra.com/regal/rules/style/file-length)                                     | Max file length exceeded                                  |
| style       | [function-arg-return](https://docs.styra.com/regal/rules/style/function-arg-return)                     | Function argument used for return value                   |
| style       | [line-length](https://docs.styra.com/regal/rules/style/line-length)                                     | Line too long                                             |
| style       | [messy-rule](https://docs.styra.com/regal/rules/style/messy-rule)                                       | Messy incremental rule                                    |
| style       | [mixed-iteration](https://docs.styra.com/regal/rules/style/mixed-iteration)                             | Mixed iteration style                                     |
| style       | [no-whitespace-comment](https://docs.styra.com/regal/rules/style/no-whitespace-comment)                 | Comment should start with whitespace                      |
| style       | [opa-fmt](https://docs.styra.com/regal/rules/style/opa-fmt)                                             | File should be formatted with `opa fmt`                   |
| style       | [pointless-reassignment](https://docs.styra.com/regal/rules/style/pointless-reassignment)               | Pointless reassignment of variable                        |
| style       | [prefer-snake-case](https://docs.styra.com/regal/rules/style/prefer-snake-case)                         | Prefer snake_case for names                               |
| style       | [prefer-some-in-iteration](https://docs.styra.com/regal/rules/style/prefer-some-in-iteration)           | Prefer `some .. in` for iteration                         |
| style       | [rule-length](https://docs.styra.com/regal/rules/style/rule-length)                                     | Max rule length exceeded                                  |
| style       | [rule-name-repeats-package](https://docs.styra.com/regal/rules/style/rule-name-repeats-package)         | Rule name repeats package                                 |
| style       | [todo-comment](https://docs.styra.com/regal/rules/style/todo-comment)                                   | Avoid TODO comments                                       |
| style       | [trailing-default-rule](https://docs.styra.com/regal/rules/style/trailing-default-rule)                 | Default rule should be declared first                     |
| style       | [unconditional-assignment](https://docs.styra.com/regal/rules/style/unconditional-assignment)           | Unconditional assignment in rule body                     |
| style       | [unnecessary-some](https://docs.styra.com/regal/rules/style/unnecessary-some)                           | Unnecessary use of `some`                                 |
| style       | [use-assignment-operator](https://docs.styra.com/regal/rules/style/use-assignment-operator)             | Prefer := over = for assignment                           |
| style       | [yoda-condition](https://docs.styra.com/regal/rules/style/yoda-condition)                               | Yoda condition                                            |
| testing     | [dubious-print-sprintf](https://docs.styra.com/regal/rules/testing/dubious-print-sprintf)               | Dubious use of print and sprintf                          |
| testing     | [file-missing-test-suffix](https://docs.styra.com/regal/rules/testing/file-missing-test-suffix)         | Files containing tests should have a _test.rego suffix    |
| testing     | [identically-named-tests](https://docs.styra.com/regal/rules/testing/identically-named-tests)           | Multiple tests with same name                             |
| testing     | [metasyntactic-variable](https://docs.styra.com/regal/rules/testing/metasyntactic-variable)             | Metasyntactic variable name                               |
| testing     | [print-or-trace-call](https://docs.styra.com/regal/rules/testing/print-or-trace-call)                   | Call to print or trace function                           |
| testing     | [test-outside-test-package](https://docs.styra.com/regal/rules/testing/test-outside-test-package)       | Test outside of test package                              |
| testing     | [todo-test](https://docs.styra.com/regal/rules/testing/todo-test)                                       | TODO test encountered                                     |

<!-- RULES_TABLE_END -->

Rules in all categories except for those in `custom` are **enabled** by default. Some rules however — like
`use-contains` and `use-if` — are conditionally enabled only when a version of OPA/Rego before 1.0 is targeted. See the
configuration options below if you want to use Regal to lint ""legacy"" policies.

**Aggregate Rules**

Most Regal rules will use data only from a single file at a time, with no consideration for other files. A few rules
however require data from multiple files, and will therefore collect, or aggregate, data from all files provided for
linting. These rules are called *aggregate rules*, and will only be run when there is more than one file to lint, such
as when linting a directory or a whole policy repository. One example of such a rule is the `prefer-package-imports`
rule, which will aggregate package names and imports from all provided policies in order to determine if any imports
are pointing to rules or functions rather than packages. You normally won't need to care about this distinction other
than being aware of the fact that some linter rules won't be run when linting a single file.

If you'd like to see more rules, please [open an issue](https://github.com/StyraInc/regal/issues) for your feature
request, or better yet, submit a PR! See the [custom rules](/docs/custom-rules.md) page for more information on how to
develop your own rules, for yourself or for inclusion in Regal.

### Custom Rules

The `custom` category is a special one, as the rules in this category allow you to enforce rules that are specific to
your project, team or organization. This typically includes things like naming conventions, where you might want to
ensure that, for example, all package names adhere to an organizational standard, like having a prefix matching the
organization name.

Since these rules require configuration provided by the user, or are more opinionated than other rules, they are
disabled by default. In order to enable them, see the configuration options available for each rule for how to configure
them according to your requirements.

For more advanced requirements, see the guide on writing [custom rules](/docs/custom-rules.md) in Rego.

## Configuration

A custom configuration file may be used to override the [default configuration](https://github.com/StyraInc/regal/blob/main/bundle/regal/config/provided/data.yaml)
options provided by Regal. The most common use case for this is to change the severity level of a rule. These three
levels are available:

- `ignore`  — disable the rule entirely
- `warning` — report the violation without changing the exit code of the lint command
- `error`   — report the violation and have the lint command exit with a non-zero exit code (default)

Additionally, some rules may have configuration options of their own. See the documentation page for a rule to learn
more about it.

**.regal/config.yaml** or **.regal.yaml**
```yaml
rules:
  style:
    todo-comment:
      # don't report on todo comments
      level: ignore
    line-length:
      # custom rule configuration
      max-line-length: 100
      # warn on too long lines, but don't fail
      level: warning
    opa-fmt:
      # not needed as error is the default, but
      # being explicit won't hurt
      level: error
      # files can be ignored for any individual rule
      # in this example, test files are ignored
      ignore:
        files:
          - ""*_test.rego""
  custom:
    # custom rule configuration
    naming-convention:
      level: error
      conventions:
        # ensure all package names start with ""acmecorp"" or ""system""
        - pattern: '^acmecorp\.[a-z_\.]+$|^system\.[a-z_\.]+$'
          targets:
            - package

capabilities:
  from:
    # optionally configure Regal to target a specific version of OPA
    # this will disable rules that has dependencies to e.g. built-in
    # functions or features not supported by the given version
    #
    # if not provided, Regal will use the capabilities of the latest
    # version of OPA available at the time of the Regal release
    engine: opa
    version: v0.58.0

ignore:
  # files can be excluded from all lint rules according to glob-patterns
  files:
    - file1.rego
    - ""*_tmp.rego""

project:
  roots:
    # declares the 'main' and 'lib/jwt' directories as project roots
    - main
    - lib/jwt
    # may also be provided as an object with additional options
    - path: lib/legacy
      rego-version: 0
```

Regal will automatically search for a configuration file (`.regal/config.yaml`
or `.regal.yaml`) in the current directory, and if not found, traverse the
parent directories either until either one is found, or the top of the directory
hierarchy is reached. If no configuration file is found, and no file is found at
`~/.config/regal/config.yaml` either, Regal will use the default configuration.

A custom configuration may be also be provided using the `--config-file`/`-c`
option for `regal lint`, which when provided will be used to override the
default configuration.

### User-level Configuration

Generally, users will want to commit their Regal configuration file to the repo
containing their Rego source code. This allows configurations to be shared
among team members and makes the configuration options available to Regal when
running as a [CI linter](https://docs.styra.com/regal/cicd) too.

Sometimes however it can be handy to have some user defaults when a project
configuration file is not found, hasn't been created yet or is not applicable.

In such cases Regal will check for a configuration file at
`~/.config/regal/config.yaml` instead.

## Ignoring Rules

If one of Regal's rules doesn't align with your team's preferences, don't worry! Regal is not meant to be the law,
and some rules may not make sense for your project, or parts of it.
Regal provides several different methods to ignore rules with varying precedence.
The available methods are (ranked highest to lowest precedence):

- [Inline Ignore Directives](#inline-ignore-directives) cannot be overridden by any other method.
- Enabling or Disabling Rules with CLI flags.
  - Enabling or Disabling Rules with `--enable` and `--disable` CLI flags.
  - Enabling or Disabling Rules with `--enable-category` and `--disable-category` CLI flags.
  - Enabling or Disabling All Rules with `--enable-all` and `--disable-all` CLI flags.
  - See [Ignoring Rules via CLI Flags](#ignoring-rules-via-cli-flags) for more details.
- [Ignoring a Rule In Config](#ignoring-a-rule-in-config)
- [Ignoring a Category In Config](#ignoring-a-category-in-config)
- [Ignoring All Rules In Config](#ignoring-all-rules-in-config)

In summary, the CLI flags will override any configuration provided in the file, and inline ignore directives for a
specific line will override any other method.

It's also possible to ignore messages on a per-file basis. The available methods are (ranked High to Lowest precedence):

- Using the `--ignore-files` CLI flag.
  See [Ignoring Rules via CLI Flags](#ignoring-rules-via-cli-flags).
- [Ignoring Files Globally](#ignoring-files-globally) or
  [Ignoring a Rule in Some",VRAI
Sunbird-Lern/userorg-service,Toolkit,Documentations,2024-07-01T07:17:05Z,2022-02-07T13:25:54Z,0,2,0,0,0,4,0,0,2017-07-03T14:51:28Z,2025-03-13T11:11:28Z,23969,32,Java,VRAI,145,FAUX,21,,21,API services for Learning management system of sunbird,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,50,"# Sunbird User Org Service

This repository contains the code for the User Org micro-service, providing the APIs for User and Org functionality of Sunbird. The code in this repository is licensed under the MIT License unless otherwise noted. Please see the [LICENSE](https://github.com/project-sunbird/sunbird-lms-service/blob/master/LICENSE) file for details.

## User org development environment setup

This readme file provides instructions for installing and starting the User Org Service and setting up the default organization & user creation in a local machine.

### System Requirements

### Prerequisites

- Java 11
- Latest Docker
- Latest Maven
  (Only For Mac m1 users use 3.8.8 Maven version)

### Prepare folders for database data and logs

To prepare folders for database data and logs, run the following command:

```shell
mkdir -p ~/sunbird-dbs/cassandra ~/sunbird-dbs/es 
export sunbird_dbs_path=~/sunbird-dbs
```

To verify the creation of folders, run:

```shell
echo $sunbird_dbs_path
```

### Cassandra database setup in Docker

1. To get the Cassandra image, use the following command:

```shell
docker pull cassandra:3.11.6 
```
For Mac M1 users follow the bellow command:
```shell
docker pull --platform=linux/amd64 cassandra:3.11.6 
```

For the network, you can either use an existing network or create a new one by executing the following command:
```shell
docker network create sunbird_db_network
```

2. To create the Cassandra instance, run the following command:

```shell
docker run -p 9042:9042 --name sunbird_cassandra \
 -v $sunbird_dbs_path/cassandra/data:/var/lib/cassandra \
 -v $sunbird_dbs_path/cassandra/logs:/opt/cassandra/logs \
 -v $sunbird_dbs_path/cassandra/backups:/mnt/backups \
 --network sunbird_db_network -d cassandra:3.11.6 
```
For Mac M1 users follow the below command:
```shell
docker run --platform=linux/amd64 -p 9042:9042 --name sunbird_cassandra \
 -v $sunbird_dbs_path/cassandra/data:/var/lib/cassandra \
 -v $sunbird_dbs_path/cassandra/logs:/opt/cassandra/logs \
 -v $sunbird_dbs_path/cassandra/backups:/mnt/backups \
 --network sunbird_db_network -d cassandra:3.11.6 
```

3. To verify the setup, run the following command, which will show the status of Cassandra as up and running:

```shell
docker ps -a | grep cassandra
```

## To create/load keyspaces and tables to Cassandra

Click the link [sunbird-utils-cassandra-setup](https://github.com/Sunbird-Lern/sunbird-utils/tree/release-5.3.0#readme) and follow the steps for creating/loading the Cassandra keyspaces and tables to your development environment.

Note: It is mandatory to follow the instructions provided in the link.

4. To verify the creation of keyspaces and tables, connect to the Cassandra Docker container using SSH and run the following command:

```shell
docker exec -it sunbird_cassandra /bin/bash
```

## Setting up Elastic Search in Docker

To set up Elastic Search in Docker, follow the below steps:

1. Obtain the Elastic Search image by executing the following command:

```shell
docker pull elasticsearch:6.8.11
```

For Mac M1 users follow the bellow command:
```shell
docker pull --platform=linux/amd64 elasticsearch:6.8.11
```

2. Create an Elastic Search instance by executing the following command to run it in a container:

```shell
docker run -p 9200:9200 --name sunbird_es -v 
$sunbird_dbs_path/es/data:/usr/share/elasticsearch/data -v 
$sunbird_dbs_path/es/logs://usr/share/elasticsearch/logs -v 
$sunbird_dbs_path/es/backups:/opt/elasticsearch/backup 
-e ""discovery.type=single-node"" --network sunbird_db_network 
-d docker.elastic.co/elasticsearch/elasticsearch:6.8.11
```

For Mac M1 users follow the bellow command::
```shell
docker run --platform=linux/amd64 -p 9200:9200 --name sunbird_es -v 
$sunbird_dbs_path/es/data:/usr/share/elasticsearch/data -v 
$sunbird_dbs_path/es/logs://usr/share/elasticsearch/logs -v 
$sunbird_dbs_path/es/backups:/opt/elasticsearch/backup 
-e ""discovery.type=single-node"" --network sunbird_db_network 
-d docker.elastic.co/elasticsearch/elasticsearch:6.8.11
```

The above command performs the following actions:
- ""-p 9200:9200"" maps the host's port 9200 to the container's port 9200, allowing access to the Elasticsearch API.
- ""--name <container_name>"" assigns a name to the container, which can be used to reference it in other Docker commands.
- ""-v <host_directory_path>/es/data:/usr/share/elasticsearch/data"" mounts the host's directory ""<host_directory_path>/es/data"" as the Elasticsearch data directory inside the container.
- ""-v <host_directory_path>/es/logs://usr/share/elasticsearch/logs"" mounts the host's directory ""<host_directory_path>/es/logs"" as the Elasticsearch logs directory inside the container.
- ""-v <host_directory_path>/es/backups:/opt/elasticsearch/backup"" mounts the host's directory ""<host_directory_path>/es/backups"" as the Elasticsearch backups directory inside the container.
- ""-e ""discovery.type=single-node"""" sets an environment variable ""discovery.type"" with the value ""single-node"", which tells Elasticsearch to start as a single-node cluster.
- ""--network <network_name>"" assigns the container to a Docker network, which is used to connect the container to other containers in the same network.
- ""-d"" runs the container in detached mode, which allows it to run in the background.

To verify the setup, execute the following command. It will display the elastic search status as up and running.
```shell
docker ps -a | grep es
```

If you are using an Ubuntu system, perform the following step to ensure that the necessary permissions are created for the folder:
```shell
chmod -R 777 sunbird-dbs/es
```

### Elastic Search Indices and Mappings Setup

To create indices, follow these steps:

1. Copy the JSON content of the index from the provided link below for each index.
2. Replace `<indices_name>` with the name of the index for which you want to create the mapping.
3. Replace `<respective_index_json_content>` with the JSON content you copied in step 1.

Use the following api to create each index:

```
PUT {{es_host}}/<indices_name>
Body : <respective_index_json_content>
```

Here's an example curl command for creating the `location` index:

```
curl --location --request PUT 'localhost:9200/location' \
--header 'Content-Type: application/json' \
--data '<location_json_content>'
```

Make sure to replace `location.json` with the name of the index JSON file for the corresponding index.

Here's the list of indices to create and their corresponding links:

- [user](https://github.com/project-sunbird/sunbird-devops/blob/release-5.3.0-lern/ansible/roles/es-mapping/files/indices/userv3.json)
- [userfeed](https://github.com/project-sunbird/sunbird-devops/blob/release-5.3.0-lern/ansible/roles/es-mapping/files/indices/userfeed.json)
- [usernotes](https://github.com/project-sunbird/sunbird-devops/blob/release-5.3.0-lern/ansible/roles/es-mapping/files/indices/usernotes.json)
- [org](https://github.com/project-sunbird/sunbird-devops/blob/release-5.3.0-lern/ansible/roles/es-mapping/files/indices/orgv3.json)
- [location](https://github.com/project-sunbird/sunbird-devops/blob/release-5.3.0-lern/ansible/roles/es-mapping/files/indices/location.json)

To create mappings for the listed indices, follow these steps:

1. Copy the JSON content of the mapping from the provided link for each index.
2. Replace `<indices_name>` with the name of the index for which you want to create the mapping.
3. Replace `<respective_mapping_json_content>` with the JSON content you copied in step 1.

Use the following api to create each mapping:

```
PUT {{es_host}}/<indices_name>/_mapping/_doc 
Body: <respective_mapping_json_content>
```

Here's an example curl command for creating the mapping for the `location` index:

```
curl --location --request PUT 'localhost:9200/location/_mapping/_doc' \
--header 'Content-Type: application/json' \
--data '@location-mapping.json'
```

Make sure to replace `location-mapping.json` with the name of the mapping JSON file for the corresponding index.

Here's the list of mappings to create and their corresponding links:

- [user](https://github.com/project-sunbird/sunbird-devops/blob/release-5.3.0-lern/ansible/roles/es-mapping/files/mappings/userv3-mapping.json)
- [userfeed](https://github.com/project-sunbird/sunbird-devops/blob/release-5.3.0-lern/ansible/roles/es-mapping/files/mappings/userfeed-mapping.json)
- [usernotes](https://github.com/project-sunbird/sunbird-devops/blob/release-5.3.0-lern/ansible/roles/es-mapping/files/mappings/usernotes-mapping.json)
- [org](https://github.com/project-sunbird/sunbird-devops/blob/release-5.3.0-lern/ansible/roles/es-mapping/files/mappings/orgv3-mapping.json)
- [location](https://github.com/project-sunbird/sunbird-devops/blob/release-5.3.0-lern/ansible/roles/es-mapping/files/mappings/location-mapping.json)

## User Org Service Setup

To set up the User Org service, follow the steps below:

1. Clone the latest branch of the user-org service using the following command:
```shell
git clone https://github.com/Sunbird-Lern/sunbird-lms-service.git
```

2. Set up the necessary environment variables by running the following script in the path `<project-base-path>/sunbird-lms-service`:
```shell
./scripts/userorg-config.sh
```

3. Build the application using the following maven command in the path `<project-base-path>/sunbird-lms-service`:
```shell
mvn clean install -DskipTests -DCLOUD_STORE_GROUP_ID=org.sunbird -DCLOUD_STORE_ARTIFACT_ID=cloud-store-sdk -DCLOUD_STORE_VERSION=1.4.6
```
Make sure the build is successful before proceeding to the next step. If the build is not successful,
fix any configuration issues and rebuild the application.

4. Run the netty server using the following maven command in the path `<project-base-path>/sunbird-lms-service/controller`:
```shell
mvn play2:run
```

5. Verify the database connections by running the following command:
```shell
curl --location --request GET 'http://localhost:9000/health’
```
If all connections are established successfully, the health status will be shown as 'true', otherwise it will be 'false'.

To make the User/Org service completely working, some pre-required configuration setup is mandatory.
Follow the steps given in the link [pre-required configuration setup](https://github.com/Sunbird-Lern/sunbird-lms-service/blob/release-5.3.0/lernsetup.md) to complete the setup.",VRAI
suzuki-shunsuke/tfaction,Toolkit,Toolkit,2025-05-15T21:31:53Z,2025-05-08T14:15:43Z,0,1,0,0,0,0,0,0,2022-01-17T12:40:25Z,2025-04-07T01:18:34Z,27045,317,TypeScript,VRAI,43,FAUX,50,"github-actions,oss,terraform",50,Framework for Monorepo to build high level Terraform Workflows by GitHub Actions,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,28,"# tfaction

Framework for Monorepo to build high level Terraform Workflows by GitHub Actions

## Document

https://suzuki-shunsuke.github.io/tfaction/docs/

[Source code](https://github.com/suzuki-shunsuke/tfaction-docs)

## Who uses tfaction?

> [!NOTE]
> If you want to add your company or organization to the list, please send a pull request or send a comment to the [GitHub Discussion](https://github.com/suzuki-shunsuke/tfaction/discussions/1280)!

- [Topotal, Inc.](https://topotal.com/)
- Recruit Co., Ltd. - [StudySapuri](https://brand.studysapuri.jp/) and [Quipper](https://www.quipper.com/) product team
  - [Terraform の CI を AWS CodeBuild から GitHub Actions + tfaction に移行しました](https://blog.studysapuri.jp/entry/2022/02/04/080000)
  - [Migrate Terraform CI from AWS CodeBuild to GitHub Actions](https://devs.quipper.com/2022/02/25/terraform-github-actions.html)
- [Henry, Inc.](https://corp.henry-app.jp/)
  - Contributed to make [tfaction](https://github.com/suzuki-shunsuke/tfaction/releases/tag/v0.5.16) and [tfmigrate](https://github.com/minamijoyo/tfmigrate/releases/tag/v0.3.7) GoogleCloud-ready
- [Macbee Planet, Inc.](https://macbee-planet.com)
- [Gunosy Inc.](https://gunosy.co.jp/)
  - [2023-12-14 tfaction を導入したら便利だった話](https://tech.gunosy.io/entry/tfaction_ci_cd)
- [Luup, Inc.](https://luup.sc/)
  - [2022-12-18 tfaction導入のお話](https://zenn.dev/luup_developers/articles/sre-nakanishi-20221218)
- [Wantedly, Inc.](https://wantedlyinc.com/)
- [newmo, Inc.](https://newmo.me/)
  - [tfactionを使ったGitHub Actions Workflowの構築](https://tech.newmo.me/entry/tfaction-github-terraform-workflow)
- [tacoms Inc.](https://www.tacoms-inc.com/)
  - [tfactionを使ってTerraformの実行を自動化した！！](https://zenn.dev/tacoms/articles/988e4c7efc8cf3)

## LICENSE

[MIT](LICENSE)",FAUX
swgriffith/azure-guides,Documentations,Documentations,2025-04-10T15:07:12Z,2024-06-26T14:23:23Z,0,0,0,0,0,0,0,3,2019-10-09T20:54:13Z,2025-04-04T18:06:00Z,85347,49,Shell,VRAI,31,FAUX,1,,1,,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,5,,FAUX
synmetrix/synmetrix,Toolkit,Toolkit,2025-02-07T17:56:57Z,2024-04-07T23:28:20Z,0,0,7,0,0,0,0,0,2021-06-18T17:57:03Z,2025-04-04T12:10:58Z,4591,547,JavaScript,VRAI,30,FAUX,8,"big-data,bigquery,business-intelligence,clickhouse,cube,cubejs,data-engineering,databricks,dremio,druid,firebolt,llm,prestodb,redshift,semantic-layer,snowflake,vertica",8,Synmetrix – production-ready open source semantic layer on Cube,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,5,"<p align=""center""><a href=""https://synmetrix.org""><img src=""https://raw.githubusercontent.com/mlcraft-io/client-v2/master/src/assets/logo_with_text.png"" alt=""Synmetrix"" width=""300px""></a></p>

<p align=""center"">
<a href=""https://synmetrix.org"">Website</a> • <a href=""https://docs.synmetrix.org"">Docs</a> • <a href=""https://cube.dev/docs/schema/getting-started"">Cube.js Models docs</a> • <a href=""https://hub.docker.com/u/synmetrix"">Docker Hub</a> • <a href=""https://join.slack.com/t/mlcraft/shared_invite/zt-1x2gxwn37-J3tTvCR5xSFVfxwUU_YKtg"">Slack community</a>
</p>

<div align=""center"">
  <a href=""README.md"">Readme in English</a> • 
  <a href=""README_CN.md"">Readme in Chinese</a> • 
  <a href=""README_RU.md"">Readme in Russian</a>
</div>

# Synmetrix

Synmetrix (prev. MLCraft) is an open source data engineering platform and semantic layer for centralized metrics management. It provides a complete framework for modeling, integrating, transforming, aggregating, and distributing metrics data at scale.

### Key Features

- **Data modeling and transformations**: Flexibly define metrics and dimensions using SQL and [Cube](https://github.com/cube-js/cube) data models. Apply transformations and aggregations.
- **Semantic layer**: Consolidate metrics from across sources into a unified, governed data model. Eliminate metric definition differences.
- **Scheduled reports and alerts**: Monitor metrics and get notified of changes via configurable reports and alerts.
- **Versioning**: Track schema changes over time for transparency and auditability.
- **Role-based access control**: Manage permissions for data models and metrics access.
- **Data exploration**: Analyze metrics through the UI, or integrate with any BI tool via the SQL API.
- **Caching**: Optimize performance using pre-aggregations and caching from [Cube](https://github.com/cube-js/cube).
- **Teams**: Collaborate on metrics modeling across your organization.

![Synmetrix – Open Source Semantic Layer](https://synmetrix.org/assets/6542558ce0ae954c7fb97894_Open%20Graph-min.webp)

## Overview

Synmetrix leverages [Cube (Cube.js)](https://github.com/cube-js/cube) to implement flexible data models that can consolidate metrics from across warehouses, databases, APIs and more. This unified semantic layer eliminates differences in definitions and calculations, providing a single source of truth.

The metrics data model can then be distributed downstream to any consumer via a SQL API, allowing integration into BI tools, reporting, dashboards, data science, and more.

By combining best practices from data engineering, like caching, orchestration, and transformation, with self-service analytics capabilities, Synmetrix speeds up data-driven workflows from metrics definition to consumption.

### Use cases

1. **Data Democratization**: Synmetrix makes data accessible to non-experts, enabling everyone in an organization to make data-driven decisions easily.

2. **Business Intelligence (BI) and Reporting**: Integrate Synmetrix with any BI tool for advanced reporting and analytics, enhancing data visualization and insights.
  - [Integrating Synmetrix with Apache Superset](https://github.com/mlcraft-io/examples/tree/main/superset) ([Video](https://www.youtube.com/watch?v=TzLy88IAYZo))

3. **Embedded Analytics**: Use the Synmetrix API to embed analytics directly into applications, providing users with real-time data insights within their workflows.
  - [Integrating Synmetrix with Observable: A Quick Guide](https://github.com/mlcraft-io/examples/tree/main/observable) ([Video](https://www.youtube.com/watch?v=VcAP4vrL8cY))
  - [Guide to Connecting Synmetrix with DBeaver Using SQL API](https://github.com/mlcraft-io/examples/tree/main/dbeaver) ([Video](https://www.youtube.com/watch?v=8l_Ud3IM0OQ))

4. **Semantic Layer for LLM**: Enhance LLM's accuracy in data handling and queries with Synmetrix's semantic layer, improving data interaction and precision.
  - [Synmetrix with Large Language Model (LLM) example](https://github.com/mlcraft-io/examples/tree/main/langchain) ([Video](https://www.youtube.com/watch?v=TtH-pFGDK84))

# Getting Started

### Prerequisite Software

Ensure the following software is installed before proceeding:

- [Docker](https://docs.docker.com/install)
- [Docker Compose](https://docs.docker.com/compose/install)

### Step 1: Download the docker-compose file

The repository [mlcraft-io/mlcraft/install-manifests](https://github.com/mlcraft-io/mlcraft/tree/main/install-manifests) houses all the necessary installation manifests for deploying Synmetrix anywhere. You can download the docker compose file from this repository:

Execute this in a new directory
```
wget https://raw.githubusercontent.com/mlcraft-io/mlcraft/main/install-manifests/docker-compose/docker-compose.yml
```

Alternatively, you can use `curl`

```
curl https://raw.githubusercontent.com/mlcraft-io/mlcraft/main/install-manifests/docker-compose/docker-compose.yml -o docker-compose.yml
```

NOTE: Ensure to review the [environment variables](docs/environments.md) in the docker-compose.yml file. Modify them as necessary.

### Step 2: Launch Synmetrix

Execute the following command to start Synmetrix along with a Postgres database for data storage.

```
docker-compose pull stack && docker-compose up -d
```

Verify if the containers are operational:

```
docker ps
```

Output:
```
CONTAINER ID IMAGE                 ... CREATED STATUS PORTS          ...
c8f342d086f3 synmetrix/stack       ... 1m ago  Up 1m  80->8888/tcp ...
30ea14ddaa5e postgres:12           ... 1m ago  Up 1m  5432/tcp  
```

The installation of all dependencies will take approximately 5-7 minutes. Wait until you see the `Synmetrix Stack is ready` message. You can view the logs using `docker-compose logs -f` to confirm if the process has completed.

#### Running Synmetrix on ARM64v8 Architecture

First, it's recommended to install [Rosetta 2](https://support.apple.com/en-gb/102527) on your Mac. This will allow Docker to run ARM64v8 containers. Since Docker [version 4.25](https://www.docker.com/blog/docker-desktop-4-25/) it allows to run ARM64v8 containers natively, but some users still encounter issues without Rosetta installed.

For ARM64v8, Cubestore requires a specific version. Update the Cubestore version in the docker-compose file to include the `-arm64v8` suffix. For instance, use `v0.35.33-arm64v8` (refer to the [Cubestore tags on Docker Hub](https://hub.docker.com/r/cubejs/cubestore/tags) for the latest version).

To run the docker-compose file for ARM64v8, use the following command:

```
docker-compose pull stack && CUBESTORE_VERSION=v0.35.33-arm64v8 docker-compose up -d
```

Video guide (MacOS, M3 Max processor):

[![Video guide](https://img.youtube.com/vi/nLorFq-WpGw/0.jpg)](https://youtu.be/nLorFq-WpGw)

### Step 3: Explore Synmetrix

* You can access Synmetrix at [http://localhost/](http://localhost/)
* The GraphQL endpoint is located at [http://localhost/v1/graphql](http://localhost/v1/graphql)
* The Admin Console (Hasura Console) can be found at [http://localhost/console](http://localhost/console)
* The Cube Swagger API can be found at [http://localhost:4000/docs](http://localhost:4000/docs)

#### Important Notes

1. **Admin Console Access**: Ensure to check `HASURA_GRAPHQL_ADMIN_SECRET` in the docker-compose file. This is mandatory for accessing the Admin Console. The default value is `adminsecret`. Remember to modify this in a production environment.

2. **Environment Variables**: Set up all necessary environment variables. Synmetrix will function with the default values, but certain features might not perform as anticipated.

3. **Preloaded Seed Data**: The project is equipped with preloaded seed data. Use the credentials below to sign in:
    - Email:  `demo@synmetrix.org`
    - Password:  `demodemo`

    This account is pre-configured with two demo datasources and their respective SQL API access. For SQL operations, you can use the following credentials with any PostgreSQL client tool such as DBeaver or TablePlus:

    | Host      | Port  | Database | User                 | Password              |
    |-----------|-------|----------|----------------------|-----------------------|
    | localhost | 15432 | db       | demo_pg_user         | demo_pg_pass          |
    | localhost | 15432 | db       | demo_clickhouse_user | demo_clickhouse_pass  |

## Documentation

- [Official Documentation](https://docs.synmetrix.org/)
- [Cube Documentation](https://cube.dev/docs)

---

## Demo online

Demo: [app.synmetrix.org](https://app.synmetrix.org)

* Login: `demo@synmetrix.org`
* Password: `demodemo`

### Database demo credentials

| Database type | Host                        | Port | Database | User | Password    | SSL   |
| ------------- | --------------------------- | ---- | -------- | ---- | ----------- | ----- |
| ClickHouse    | gh-api.clickhouse.tech      | 443  | default  | play | no password | true  |
| PostgreSQL    | demo-db-examples.cube.dev   | 5432 | ecom     | cube | 12345       | false |

---

## Data Modeling

Synmetrix leverages Cube for flexible data modeling and transformations.

Cube implements a multi-stage SQL data modeling architecture:

- Raw data sits in a source database such as Postgres, MySQL, etc.
- The raw data is modeled into reusable data marts using Cube Data Models files. These models files allow defining metrics, dimensions, granularities and relationships.
- The models act as an abstraction layer between the raw data and application code.
- Cube then generates optimized analytical SQL queries against the raw data based on the model.
- The Cube Store distributed cache optimizes query performance by caching query results.

This modeling architecture makes it simple to create fast and complex analytical queries with Cube that are optimized to run against large datasets.

The unified data model can consolidate metrics from across different databases and systems, providing a consistent semantic layer for end users.

## Cube Store

For production workloads, Synmetrix uses Cube Store as the caching and query execution layer.

Cube Store is a purpose-built database for operational analytics, optimized for fast aggregations and time series data. It provides:

- Distributed querying for scalability
- Advanced caching for fast queries
- columnar storage for analytics performance
- Integration with Cube for modeling

By leveraging Cube Store and Cube together, Synmetrix benefits from excellent analytics performance and flexibility in modeling metrics.

#### Benchmarks

- [Synmetrix with Cube: Caching and Highload](https://github.com/mlcraft-io/examples/tree/main/benchmarks)

---

## Ecosystem

| Repository                                                   | Description        |
| ------------------------------------------------------------ | ------------------ |
| [mlcraft-io/mlcraft](https://github.com/mlcraft-io/mlcraft)     | Synmetrix Monorepo |
| [mlcraft-io/client-v2](https://github.com/mlcraft-io/client-v2) | Synmetrix Client   |
| [mlcraft-io/docs](https://github.com/mlcraft-io/docs)           | Synmetrix Docs     |
| [mlcraft-io/examples](https://github.com/mlcraft-io/examples)           | Synmetrix Examples     |

## Community support

For general help using Synmetrix, please refer to the official Synmetrix documentation. For additional help, you can use one of these channels to ask a question:

* [Slack](https://join.slack.com/t/mlcraft/shared_invite/zt-1x2gxwn37-J3tTvCR5xSFVfxwUU_YKtg) / For live discussion with the Community and Synmetrix team
* [GitHub](https://github.com/mlcraft-io/mlcraft) / Bug reports, Contributions
* [Twitter](https://twitter.com/trySynmetrix) / Updates and news
* [Youtube](https://www.youtube.com/channel/UCEPlxaWYrdOaf9IXjD2IRTg) / Video tutorials and demos

## Roadmap

Check out our [roadmap](https://github.com/mlcraft-io/mlcraft/projects) to get informed on what we are currently working on, and what we have in mind for the next weeks, months and years.

## License

The core Synmetrix is available under the [Apache License 2.0](https://github.com/mlcraft-io/mlcraft/blob/main/LICENSE) (Apache-2.0).

All **other contents** are available under the [MIT License](LICENSE-community).

## Hardware requirements

| Component       | Requirement                                                                          |
| --------------- | ------------------------------------------------------------------------------------ |
| Processor (CPU) | 3.2 GHz or higher, modern processor with multi-threading and virtualization support. |
| RAM             | 8 GB or more to handle computational tasks and data processing.                      |
| Disk Space      | At least 30 GB of free space for software installation and storing working data.     |
| Network         | Internet connectivity is required for cloud services and software updates.           |

## Authors

[@ifokeev](https://github.com/ifokeev), [@Libertonius](https://github.com/Libertonius), [@ilyozzz](https://github.com/ilyozzz)",FAUX
sysnet4admin/IaC,Documentations,Documentations,2025-03-14T10:16:00Z,2024-11-17T07:29:38Z,0,0,0,0,0,0,0,0,2019-11-22T04:47:06Z,2025-03-14T10:16:07Z,19135,47,Shell,VRAI,39,FAUX,0,,0,Infrastructure as Code,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,3,,FAUX
tenable/terrascan,Toolkit,Application System,2025-05-06T11:08:08Z,2023-08-02T13:48:21Z,0,350,0,0,1,0,0,0,2017-09-11T03:11:10Z,2025-04-08T04:20:16Z,16073,4891,Go,VRAI,517,FAUX,282,"architecture,aws,aws-security,azure-security,cloud-security,cloudsecurity,devops,devsecops,gcp-security,iac,infrastructure,infrastructure-as-code,kubernetes,sast,scans,security,security-tools,security-violations,terraform,terrascan",282,Detect compliance and security violations across Infrastructure as Code to mitigate risk before provisioning cloud native infrastructure.,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,91,"![Terrascan](https://raw.githubusercontent.com/tenable/runterrascan.io/main/static/images/TerrascanTM_BY_Logo.png)

[![GitHub release](https://img.shields.io/github/release/tenable/terrascan)](https://github.com/tenable/terrascan/releases/latest)
[![License: Apache 2.0](https://img.shields.io/badge/license-Apache%202-blue)](https://github.com/tenable/terrascan/blob/master/LICENSE)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/tenable/terrascan/pulls)
![CI](https://github.com/tenable/terrascan/workflows/build/badge.svg)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=tenable_terrascan&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=tenable_terrascan)
[![AUR package](https://repology.org/badge/version-for-repo/aur/terrascan.svg)](https://repology.org/project/terrascan/versions)
[![codecov](https://codecov.io/gh/tenable/terrascan/branch/master/graph/badge.svg)](https://codecov.io/gh/tenable/terrascan)
[![Documentation Status](https://readthedocs.com/projects/tenable-terrascan/badge/?version=latest)](https://runterrascan.io/)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](code_of_conduct.md)
![GitHub all releases](https://img.shields.io/github/downloads/tenable/terrascan/total)

## Introduction

Terrascan is a static code analyzer for Infrastructure as Code. Terrascan allows you to:

- Seamlessly scan infrastructure as code for misconfigurations.
- Monitor provisioned cloud infrastructure for configuration changes that introduce posture drift, and enables reverting to a secure posture.
- Detect security vulnerabilities and compliance violations.
- Mitigate risks before provisioning cloud native infrastructure.
- Offers flexibility to run locally or integrate with your CI\CD.



### Resources
* To try Terrascan in your browser, see the Terrascan Sandbox https://www.tenable.com/terrascan

* To learn more about Terrascan's features and capabilities, see the documentation portal: https://runterrascan.io

<p align=""center"">
    Join Tenable community 👇
<br/>
<a href=""https://discord.gg/ScUPMzyG3n"">
    <img src=""http://fig.io/icons/discord-logo-square.png"" width=""80px"" height=""80px"" />
</a>
</p>


## Key features
* 500+ Policies for security best practices
* Scanning of [Terraform](https://runterrascan.io/docs/usage/command_line_mode/#scanning-current-directory-containing-terraform-files-for-aws-resources) (HCL2)
* Scanning of AWS CloudFormation Templates (CFT)
* * Scanning of Azure Resource Manager (ARM)
* Scanning of [Kubernetes](https://runterrascan.io/docs/usage/command_line_mode/#scanning-for-a-specific-iac-provider) (JSON/YAML), [Helm](https://runterrascan.io/docs/usage/command_line_mode/#scanning-a-helm-chart) v3, and [Kustomize](https://runterrascan.io/docs/usage/command_line_mode/#scanning-a-kustomize-chart)
* Scanning of [Dockerfiles](https://runterrascan.io/docs/usage/command_line_mode/#scanning-a-dockerfile)
* Support for [AWS](https://runterrascan.io/docs/policies/aws/), [Azure](https://runterrascan.io/docs/policies/azure/), [GCP](https://runterrascan.io/docs/policies/gcp/), [Kubernetes](https://runterrascan.io/docs/policies/k8s/), [Dockerfile](https://runterrascan.io/docs/policies/docker/), and [GitHub](https://runterrascan.io/docs/policies/github/)
* Integrates with docker image vulnerability scanning for AWS, Azure, GCP, Harbor container registries.

## Quick Start

1. [Install](#install)
2. [Scan](#scan)
3. [Integrate](#integrate)

### Step 1: Install
Terrascan supports multiple ways to install and is also available as a Docker image.
See Terrascan's [releases](https://github.com/tenable/terrascan/releases) page for the latest version of builds in all supported platforms. Select the correct binary for your platform.

#### Install as a native executable

```sh
$ curl -L ""$(curl -s https://api.github.com/repos/tenable/terrascan/releases/latest | grep -o -E ""https://.+?_Darwin_x86_64.tar.gz"")"" > terrascan.tar.gz
$ tar -xf terrascan.tar.gz terrascan && rm terrascan.tar.gz
$ install terrascan /usr/local/bin && rm terrascan
$ terrascan
```

#### Install on ArchLinux / Manjaro via `AUR`

ArchLinux and Manjaro users can install by:

```
yay -S terrascan
```

#### Install via `brew`

[Homebrew](https://brew.sh/) users can install by:

```sh
$ brew install terrascan
```

#### Docker image

Terrascan is also available as a Docker image and can be used as follows

```sh
$ docker run tenable/terrascan
```
Refer to [documentation](https://runterrascan.io/docs/getting-started/) for information.

### Step 2: Scan
To scan your code for security issues you can run the following (defaults to scanning Terraform).

```sh
$ terrascan scan
```
**Note**: Terrascan will exit with an error code if any errors or violations are found during a scan.

#### List of possible Exit Codes
| Scenario      | Exit Code |
| ----------- | ----------- |
| scan summary has errors and violations | 5 |
| scan summary has errors but no violations | 4 |
| scan summary has violations but no errors | 3 |
| scan summary has no violations or errors | 0 |
| scan command errors out due to invalid inputs | 1 |
### Step 3: Integrate with CI\CD

Terrascan can be integrated into CI/CD pipelines to enforce security best practices.
Please refer to our [documentation to integrate with your pipeline](https://runterrascan.io/docs/integrations/).

## Terrascan Commands
You can use the `terrascan` command with the following options:

```sh
$ terrascan
Terrascan

Usage:
  terrascan [command]

Available Commands:
  help        Help about any command
  init        Initialize Terrascan
  scan        Detect compliance and security violations across Infrastructure as Code.
  server      Run Terrascan as an API server
  version     Terrascan version

Flags:
  -c, --config-path string   config file path
  -h, --help                 help for terrascan
  -l, --log-level string     log level (debug, info, warn, error, panic, fatal) (default ""info"")
  -x, --log-type string      log output type (console, json) (default ""console"")
  -o, --output string        output type (human, json, yaml, xml) (default ""human"")

Use ""terrascan [command] --help"" for more information about a command.
```

## Policies
Terrascan policies are written using the [Rego policy language](https://www.openpolicyagent.org/docs/latest/policy-language/). Every rego includes a JSON ""rule"" file which defines metadata for the policy.
By default, Terrascan downloads policies from Terrascan repositories while scanning for the first time. However, if you want to download the latest policies, you need to run the Initialization process. See [Usage](https://runterrascan.io/docs/usage/command_line_mode/) for information about the Initialization process.

Note: The scan command will implicitly run the initialization process if there are no policies found.

## Docker Image Vulnerabilities
You can use the `--find-vuln` flag to collect vulnerabilities as reported in its registry as part of Terrascan's output. Currently Terrascan supports Elastic Container Registry (ECR), Azure Container Registry, Google Container Registry, and Google Artifact Registry.

The `--find-vuln` flag can be used when scanning IaC files as follows:

```
$ terrascan scan -i <IaC provider> --find-vuln
```

For more information and explanation of how to setup your environment to authenticate with the registry's APIs see the [usage](https://runterrascan.io/docs/usage/command_line_mode/) documentation.

## Customizing scans

By default, Terrascan scans your entire configuration against all policies. However, Terrascan supports granular configuration of policies and resources.

Read more about [in-file instrumentation](https://runterrascan.io/docs/usage/in-file_instrumentation/) and [the config file](https://runterrascan.io/docs/usage/config_options/) on our documentation site.

For now, some quick tips:

- [Exclude a particular policy for a specific resource.](#How_to_exclude_a_policy_while_scanning_a_resource)
- [Manually configure policies to be suppressed or applied globally from a scan across all resources or, for just a particular resource.](#_How_to_include_or_exclude_specific_policies_or_resources_from_being_scanned)

### How to exclude a policy while scanning a resource

You can configure Terrascan to skip a particular policy (rule) while scanning a resource. Follow these steps depending on your platform:

#### Terraform
Use Terraform scripts to configure Terrascan to skip rules by inserting a comment with the phrase `""ts:skip=<RULENAME><SKIP_REASON>""`. The comment should be included inside the resource as shown in the example below.

![tf](docs/img/tf_skip_rule.png)

#### Kubernetes
In Kubernetes yamls, you can configure Terrascan to skip policies by adding an annotation as seen in the snippet below.

![k8s](docs/img/skiprules.png)

### How to include or exclude specific policies or resources from being scanned

Use the Terrascan config file to manually select the policies which should be included or excluded from the entire scan. This is suitable for edge use cases.
Use the ""in-file"" suppression option to specify resources that should be excluded from being tested against selected policies. This ensures that the policies are skipped only for particular resources, rather than all of the resources.

![config](https://user-images.githubusercontent.com/74685902/105115887-83e2f380-5a7e-11eb-82b8-a1d18c83a405.png)

### Sample scan output

Terrascan's default output is a list of violations present in the scanned IaC. A sample output:

![Screenshot 2021-01-19 at 10 52 47 PM](https://user-images.githubusercontent.com/74685902/105115731-32d2ff80-5a7e-11eb-93b0-2f0620eb1295.png)

## Building Terrascan
Terrascan can be built locally. This is helpful if you want to be on the latest version or when developing Terrascan. [gcc](https://gcc.gnu.org/install/) and [Go](https://go.dev/doc/install) 1.19 or above are required.

```sh
$ git clone git@github.com:tenable/terrascan.git
$ cd terrascan
$ make build
$ ./bin/terrascan
```

### To build your own docker, refer to this example (Alpine Linux):
```
FROM golang:alpine AS build-env

RUN apk add --update git

RUN git clone https://github.com/tenable/terrascan && cd terrascan \
  && CGO_ENABLED=0 GO111MODULE=on go build -o /go/bin/terrascan cmd/terrascan/main.go

```

## Developing Terrascan
To learn more about developing and contributing to Terrascan, refer to the [contributing guide](CONTRIBUTING.md).

## Code of Conduct
We believe having an open and inclusive community benefits all of us. Please note that this project is released with a [Contributor Code of Conduct](code_of_conduct.md). By participating in this project you agree to abide by its terms.

## License

Terrascan is licensed under the [Apache 2.0 License](LICENSE).

### Stargazers

[![Stargazers @tenable/terrascan](https://reporoster.com/stars/tenable/terrascan)](https://github.com/tenable/terrascan/stargazers)

### Forkers

[![Forkers @tenable/terrascan](https://reporoster.com/forks/tenable/terrascan)](https://github.com/tenable/terrascan/network/members)

####",VRAI
terraform-google-modules/terraform-example-foundation,Documentations,Documentations,2025-05-08T18:51:33Z,2024-11-12T19:00:22Z,0,3,0,0,0,0,0,81,2019-12-02T23:54:32Z,2025-04-08T07:06:16Z,3310,1303,HCL,VRAI,730,FAUX,20,"cft-terraform,end-to-end,operations",20,Shows how the CFT modules can be composed to build a secure cloud foundation,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,73,"# terraform-example-foundation

This example repository shows how the CFT Terraform modules can build a secure Google Cloud foundation, following the [Google Cloud Enterprise Foundations Blueprint](https://cloud.google.com/architecture/security-foundations) (previously called the _Security Foundations Guide_).
The supplied structure and code is intended to form a starting point for building your own foundation with pragmatic defaults that you can customize to meet your own requirements.

The intended audience of this blueprint is large enterprise organizations with a dedicated platform team responsible for deploying and maintaining their GCP environment, who is commited to separation of duties across multiple teams and managing their environment solely through version-controlled Infrastructure as Code. Smaller organizations looking for a turnkey solution might prefer other options such as [Google Cloud Setup](https://console.cloud.google.com/cloud-setup/overview)

## Intended usage and support

This repository is intended as an example to be forked, tweaked, and maintained in the user's own version-control system; the modules within this repository are not intended for use as remote references.
Though this blueprint can help accelerate your foundation design and build, we assume that you have the engineering skills and teams to deploy and customize your own foundation based on your own requirements.

We will support:
 - Code is semantically valid, pinned to known good versions, and passes terraform validate and lint checks
 - All PR to this repo must pass integration tests to deploy all resources into a test environment before being merged
 - Feature requests about ease of use of the code, or feature requests that generally apply to all users, are welcome

We will not support:
 - In-place upgrades from a foundation deployed with an earlier version to a more recent version, even for minor version changes, might not be feasible. Repository maintainers do not have visibility to what resources a user deploys on top of their foundation or how the foundation was customized in deployment, so we make no guarantee about avoiding breaking changes.
 - Feature requests that are specific to a single user's requirement and not representative of general best practices

## Overview

This repo contains several distinct Terraform projects, each within their own directory that must be applied separately, but in sequence.
Stage `0-bootstrap` is manually executed, and subsequent stages are executed using your preferred CI/CD tool.

Each of these Terraform projects are to be layered on top of each other, and run in the following order.

### [0. bootstrap](./0-bootstrap/)

This stage executes the [CFT Bootstrap module](https://github.com/terraform-google-modules/terraform-google-bootstrap) which bootstraps an existing Google Cloud organization, creating all the required Google Cloud resources and permissions to start using the Cloud Foundation Toolkit (CFT).
For [CI/CD Pipelines](/docs/GLOSSARY.md#foundation-cicd-pipeline), you can use either Cloud Build (by default) or Jenkins. If you want to use Jenkins instead of Cloud Build, see [README-Jenkins](./0-bootstrap/README-Jenkins.md) on how to use the Jenkins sub-module.

The bootstrap step includes:

- The `prj-b-seed` project that contains the following:
  - Terraform state bucket
  - Custom service accounts used by Terraform to create new resources in Google Cloud
- The `prj-b-cicd` project that contains the following:
  - A [CI/CD Pipeline](/docs/GLOSSARY.md#foundation-cicd-pipeline) implemented with either Cloud Build or Jenkins
  - If using Cloud Build, the following items:
    - Cloud Source Repository
    - Artifact Registry
  - If using Jenkins, the following items:
    - A Compute Engine instance configured as a Jenkins Agent
    - Custom service account to run Compute Engine instances for Jenkins Agents
    - VPN connection with on-prem (or wherever your Jenkins Controller is located)

It is a best practice to separate concerns by having two projects here: one for the Terraform state and one for the CI/CD tool.
  - The `prj-b-seed` project stores Terraform state and has the service accounts that can create or modify infrastructure.
  - The `prj-b-cicd` project holds the CI/CD tool (either Cloud Build or Jenkins) that coordinates the infrastructure deployment.

To further separate the concerns at the IAM level as well, a distinct service account is created for each stage. The Terraform custom service accounts are granted the IAM permissions required to build the foundation.
If using Cloud Build as the CI/CD tool, these service accounts are used directly in the pipeline to execute the pipeline steps (`plan` or `apply`).
In this configuration, the baseline permissions of the CI/CD tool are unchanged.

If using Jenkins as the CI/CD tool, the service account of the Jenkins Agent (`sa-jenkins-agent-gce@prj-b-cicd-xxxx.iam.gserviceaccount.com`) is granted [impersonation](https://cloud.google.com/iam/docs/create-short-lived-credentials-direct) access so it can generate tokens over the Terraform custom Service Accounts.
In this configuration, the baseline permissions of the CI/CD tool are limited.

After executing this step, you will have the following structure:

```
example-organization/
└── fldr-bootstrap
    ├── prj-b-cicd
    └── prj-b-seed
```

When this step uses the Cloud Build submodule, it sets up the cicd project (`prj-b-cicd`) with Cloud Build and Cloud Source Repositories for each of the stages below.
Triggers are configured to run a `terraform plan` for any non-environment branch and `terraform apply` when changes are merged to an environment branch (`development`, `nonproduction` or `production`).
Usage instructions are available in the 0-bootstrap [README](./0-bootstrap/README.md).

### [1. org](./1-org/)

The purpose of this stage is to set up the common folder used to house projects that contain shared resources such as Security Command Center notification, Cloud Key Management Service (KMS), org level secrets, and org level logging.
This stage also sets up the network folder used to house network related projects such as DNS Hub, Interconnect, network hub and projects for each environment  (`development`, `nonproduction` or `production`).
This will create the following folder and project structure:

```
example-organization
└── fldr-common
    ├── prj-c-logging
    ├── prj-c-billing-export
    ├── prj-c-scc
    ├── prj-c-kms
    └── prj-c-secrets
└── fldr-network
    ├── prj-net-hub-svpc
    ├── prj-net-dns
    ├── prj-net-interconnect
    ├── prj-d-svpc
    ├── prj-n-svpc
    └── prj-p-svpc
```

#### Logs

Under the common folder, a project `prj-c-logging` is used as the destination for organization wide sinks. This includes admin activity audit logs from all projects in your organization and the billing account.

Logs are collected into a logging bucket with a linked BigQuery dataset, which can be used for ad-hoc log investigations, querying, or reporting. Log sinks can also be configured to export to Pub/Sub for exporting to external systems or Cloud Storage for long-term storage.

**Notes**:

- Log export to Cloud Storage bucket has optional object versioning support via `log_export_storage_versioning`.
- The various audit log types being captured in BigQuery are retained for 30 days.
- For billing data, a BigQuery dataset is created with permissions attached, however you will need to configure a billing export [manually](https://cloud.google.com/billing/docs/how-to/export-data-bigquery), as there is no easy way to automate this at the moment.

#### Security Command Center notification

Another project created under the common folder. This project will host the Security Command Center notification resources at the organization level.
This project will contain a Pub/Sub topic, a Pub/Sub subscription, and a [Security Command Center notification](https://cloud.google.com/security-command-center/docs/how-to-notifications) configured to send all new findings to the created topic.
You can adjust the filter when deploying this step.

#### KMS

Another project created under the common folder. This project is allocated for [Cloud Key Management](https://cloud.google.com/security-key-management) for KMS resources shared by the organization.

Usage instructions are available for the org step in the [README](./1-org/README.md).

#### Secrets

Another project created under the common folder. This project is allocated for [Secret Manager](https://cloud.google.com/secret-manager) for secrets shared by the organization.

Usage instructions are available for the org step in the [README](./1-org/README.md).

#### DNS hub

This project is created under the network folder. This project will host the DNS hub for the organization.

#### Interconnect

Another project created under the network folder. This project will host the Dedicated Interconnect [Interconnect connection](https://cloud.google.com/network-connectivity/docs/interconnect/concepts/terminology#elements) for the organization. In case of Partner Interconnect, this project is unused and the [VLAN attachments](https://cloud.google.com/network-connectivity/docs/interconnect/concepts/terminology#for-partner-interconnect) will be placed directly into the corresponding hub projects.

#### Networking

Under the network folder, one project for shared vpc network, are created per environment (`development`, `nonproduction`, and `production`) which is intended to be used as a [Shared VPC host project](https://cloud.google.com/vpc/docs/shared-vpc) for all projects in that environment.
This stage only creates the projects and enables the correct APIs, the following networks stages, [3-networks-svpc](./3-networks-svpc/) and [3-networks-hub-and-spoke](./3-networks-hub-and-spoke/), create the actual Shared VPC networks.

### [2. environments](./2-environments/)

The purpose of this stage is to set up the environments folders that contain shared projects for each environemnt.
This will create the following folder and project structure:

```
example-organization
└── fldr-development
    ├── prj-d-kms
    └── prj-d-secrets
└── fldr-nonproduction
    ├── prj-n-kms
    └── prj-n-secrets
└── fldr-production
    ├── prj-p-kms
    └── prj-p-secrets
```

#### KMS

Under the environment folder, a project is created per environment (`development`, `nonproduction`, and `production`), which is intended to be used by [Cloud Key Management](https://cloud.google.com/security-key-management) for KMS resources shared by the environment.

Usage instructions are available for the environments step in the [README](./2-environments/README.md).

#### Secrets

Under the environment folder, a project is created per environment (`development`, `nonproduction`, and `production`), which is intended to be used by [Secret Manager](https://cloud.google.com/secret-manager) for secrets shared by the environment.

Usage instructions are available for the environments step in the [README](./2-environments/README.md).

### [3. networks-svpc](./3-networks-svpc/)

This step focuses on creating a [Shared VPC](https://cloud.google.com/architecture/security-foundations/networking#vpcsharedvpc-id7-1-shared-vpc-) per environment (`development`, `nonproduction`, and `production`) in a standard configuration with a reasonable security baseline. Currently, this includes:

- (Optional) Example subnets for `development`, `nonproduction`, and `production` inclusive of secondary ranges for those that want to use Google Kubernetes Engine.
- Hierarchical firewall policy created to allow remote access to [VMs through IAP](https://cloud.google.com/iap/docs/using-tcp-forwarding), without needing public IPs.
- Hierarchical firewall policy created to allow for [load balancing health checks](https://cloud.google.com/load-balancing/docs/health-checks#firewall_rules).
- Hierarchical firewall policy created to allow [Windows KMS activation](https://cloud.google.com/compute/docs/instances/windows/creating-managing-windows-instances#kms-server).
- [Private service networking](https://cloud.google.com/vpc/docs/configure-private-services-access) configured to enable workload dependant resources like Cloud SQL.
- Shared VPC with [restricted.googleapis.com](https://cloud.google.com/vpc-service-controls/docs/supported-products) configured for restricted access to googleapis.com and gcr.io. Route added for VIP so no internet access is required to access APIs.
- Default routes to internet removed, with tag based route `egress-internet` required on VMs in order to reach the internet.
- (Optional) Cloud NAT configured for all subnets with logging and static outbound IPs.
- Default Cloud DNS policy applied, with DNS logging and [inbound query forwarding](https://cloud.google.com/dns/docs/overview#dns-server-policy-in) turned on.

Usage instructions are available for the networks step in the [README](./3-networks-svpc/README.md).

### [3. networks-hub-and-spoke](./3-networks-hub-and-spoke/)

This step configures the same network resources that the step 3-networks-svpc does, but this time it makes use of the architecture based on the [hub-and-spoke](https://cloud.google.com/architecture/security-foundations/networking#hub-and-spoke) reference network model.

Usage instructions are available for the networks step in the [README](./3-networks-hub-and-spoke/README.md).

### [4. projects](./4-projects/)

This step is focused on creating service projects with a standard configuration that are attached to the Shared VPC created in the previous step and application infrastructure pipelines.
Running this code as-is should generate a structure as shown below:

```
example-organization/
└── fldr-development
    └── fldr-development-bu1
        ├── prj-d-bu1-sample-floating
        ├── prj-d-bu1-sample-svpc
        ├── prj-d-bu1-sample-peering
    └── fldr-development-bu2
        ├── prj-d-bu2-sample-floating
        ├── prj-d-bu2-sample-svpc
        └── prj-d-bu2-sample-peering
└── fldr-nonproduction
    └── fldr-nonproduction-bu1
        ├── prj-n-bu1-sample-floating
        ├── prj-n-bu1-sample-svpc
        ├── prj-n-bu1-sample-peering
    └── fldr-nonproduction-bu2
        ├── prj-n-bu2-sample-floating
        ├── prj-n-bu2-sample-svpc
        └── prj-n-bu2-sample-peering
└── fldr-production
    └── fldr-production-bu1
        ├── prj-p-bu1-sample-floating
        ├── prj-p-bu1-sample-svpc
        ├── prj-p-bu1-sample-peering
    └── fldr-production-bu2
        ├── prj-p-bu2-sample-floating
        ├── prj-p-bu2-sample-svpc
        └── prj-p-bu2-sample-peering
└── fldr-common
    ├── prj-c-bu1-infra-pipeline
    └── prj-c-bu2-infra-pipeline
```

The code in this step includes two options for creating projects.
The first is the standard projects module which creates a project per environment, and the second creates a standalone project for one environment.
If relevant for your use case, there are also two optional submodules which can be used to create a subnet per project, and a dedicated private DNS zone per project.

Usage instructions are available for the projects step in the [README](./4-projects/README.md).

### [5. app-infra](./5-app-infra/)

The purpose of this step is to deploy a simple [Compute Engine](https://cloud.google.com/compute/) instance in one of the business unit projects using the infra pipeline set up in 4-projects.

Usage instructions are available for the app-infra step in the [README](./5-app-infra/README.md).

### Final view

After all steps above have been executed, your Google Cloud organization should represent the structure shown below, with projects being the lowest nodes in the tree.

```
example-organization
└── fldr-common
    ├── prj-c-logging
    ├── prj-c-billing-export
    ├── prj-c-scc
    ├── prj-c-kms
    ├── prj-c-secrets
    ├── prj-c-bu1-infra-pipeline
    └── prj-c-bu2-infra-pipeline
└── fldr-network
    ├── prj-net-hub-svpc
    ├── prj-net-dns
    ├── prj-net-interconnect
    ├── prj-d-svpc
    ├── prj-n-svpc
    └── prj-p-svpc
└── fldr-development
    ├── prj-d-kms
    └── prj-d-secrets
    └── fldr-development-bu1
        ├── prj-d-bu1-sample-floating
        ├── prj-d-bu1-sample-svpc
        ├── prj-d-bu1-sample-peering
    └── fldr-development-bu2
        ├── prj-d-bu2-sample-floating
        ├── prj-d-bu2-sample-svpc
        └── prj-d-bu2-sample-peering
└── fldr-nonproduction
    ├── prj-n-kms
    └── prj-n-secrets
    └── fldr-nonproduction-bu1
        ├── prj-n-bu1-sample-floating
        ├── prj-n-bu1-sample-svpc
        ├── prj-n-bu1-sample-peering
    └── fldr-nonproduction-bu2
        ├── prj-n-bu2-sample-floating
        ├── prj-n-bu2-sample-svpc
        └── prj-n-bu2-sample-peering
└── fldr-production
    ├── prj-p-kms
    └── prj-p-secrets
    └── fldr-production-bu1
        ├── prj-p-bu1-sample-floating
        ├── prj-p-bu1-sample-svpc
        ├── prj-p-bu1-sample-peering
    └── fldr-production-bu2
        ├── prj-p-bu2-sample-floating
        ├── prj-p-bu2-sample-svpc
        └── prj-p-bu2-sample-peering
└── fldr-bootstrap
    ├── prj-b-cicd
    └── prj-b-seed
```

### Branching strategy

There are three main named branches: `development`, `nonproduction`, and `production` that reflect the corresponding environments. These branches should be [protected](https://docs.github.com/en/github/administering-a-repository/about-protected-branches). When the [CI/CD Pipeline](/docs/GLOSSARY.md#foundation-cicd-pipeline) (Jenkins or Cloud Build) runs on a particular named branch (say for instance `development`), only the corresponding environment (`development`) is applied. An exception is the `shared` environment, which is only applied when triggered on the `production` branch. This is because any changes in the `shared` environment may affect resources in other environments and can have adverse effects if not validated correctly.

Development happens on feature and bug fix branches (which can be named `feature/new-foo`, `bugfix/fix-bar`, etc.) and when complete, a [pull request (PR)](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) or [merge request (MR)](https://docs.gitlab.com/ee/user/project/merge_requests/) can be opened targeting the `development` branch. This will trigger the [CI/CD Pipeline](/docs/GLOSSARY.md#foundation-cicd-pipeline) to perform a plan and validate against all environments (`development`, `nonproduction`, `shared`, and `production`). After the code review is complete and changes are validated, this branch can be merged into `development`. This will trigger a [CI/CD Pipeline](/docs/GLOSSARY.md#foundation-cicd-pipeline) that applies the latest changes in the `development` branch on the `development` environment.

After validated in `development`, changes can be promoted to `nonproduction` by opening a PR or MR targeting the `nonproduction` branch and merging them. Similarly, changes can be promoted from `nonproduction` to `production`.

### Policy validation

This repo uses the [terraform-tools](https://cloud.google.com/docs/terraform/policy-validation/validate-policies) component of the `gcloud` CLI to validate the Terraform plans against a [library of Google Cloud policies](https://github.com/GoogleCloudPlatform/policy-library).

The [Scorecard bundle](https://github.com/GoogleCloudPlatform/policy-library/blob/master/docs/bundles/scorecard-v1.md) was used to create the [policy-library folder](./policy-library) with [one extra constraint](https://github.com/GoogleCloudPlatform/policy-library/blob/master/samples/serviceusage_allow_basic_apis.yaml) added.

See the [policy-library documentation](https://github.com/GoogleCloudPlatform/policy-library/blob/master/docs/index.md) if you need to add more constraints from the [samples folder](https://github.com/GoogleCloudPlatform/policy-library/tree/master/samples) in your configuration based in your type of workload.

Step 1-org has [instructions](./1-org/README.md#deploying-with-cloud-build) on the creation of the shared repository to host these policies.

### Optional Variables

Some variables used to deploy the steps have default values, check those **before deployment** to ensure they match your requirements. For more information, there are tables of inputs and outputs for the Terraform modules, each with a detailed description of their variables. Look for variables marked as **not required** in the section **Inputs** of these READMEs:

- Step 0-bootstrap: If you are using Cloud Build in the [CI/CD Pipeline](/docs/GLOSSARY.md#foundation-cicd-pipeline), check the main [README](./0-bootstrap/README.md#Inputs) of the step. If you are using Jenkins, check the [README](./0-bootstrap/modules/jenkins-agent/README.md#Inputs) of the module `jenkins-agent`.
- Step 1-org: The [README](./1-org/envs/shared/README.md#Inputs) of the environment `shared`.
- Step 2-environments: The READMEs of the environments [development](./2-environments/envs/development/README.md#Inputs), [nonproduction](./2-environments/envs/nonproduction/README.md#Inputs), and [production](./2-environments/envs/production/README.md#Inputs)
- Step 3-networks-svpc: The READMEs of the environments [shared](./3-networks-svpc/envs/shared/README.md#inputs), [development](./3-networks-svpc/envs/development/README.md#Inputs), [nonproduction](./3-networks/envs/nonproduction/README.md#Inputs), and [production](./3-networks/envs/production/README.md#Inputs)
- Step 3-networks-hub-and-spoke: The READMEs of the environments [shared](./3-networks-hub-and-spoke/envs/shared/README.md#inputs), [development](./3-networks-hub-and-spoke/envs/development/README.md#Inputs), [nonproduction](./3-networks/envs/nonproduction/README.md#Inputs), and [production](./3-networks/envs/production/README.md#Inputs)
- Step 4-projects: The READMEs of the environments [shared](./4-projects/business_unit_1/shared/README.md#inputs), [development](./4-projects/business_unit_1/development/README.md#Inputs), [nonproduction](./4-projects/business_unit_1/nonproduction/README.md#Inputs), and [production](./4-projects/business_unit_1/production/README.md#Inputs)

## Errata summary

Refer to the [errata summary](./ERRATA.md) for an overview of the delta between the example foundation repository and the [Google Cloud security foundations guide](https://cloud.google.com/architecture/security-foundations).

## Contributing

Refer to the [contribution guidelines](./CONTRIBUTING.md) for information on contributing to this module.",FAUX
tigera-solutions/calicocloud-kaas-workshop-demo,Documentations,Documentations,2024-01-02T17:29:22Z,2022-09-01T13:57:25Z,0,0,0,0,0,0,0,2,2021-07-19T22:07:13Z,2025-03-30T02:58:29Z,69024,16,,VRAI,12,FAUX,0,"aks,aws,azure,cc,eks,gcp,gke,regismartins,rke,training,vanilla,workshop",0,"Master repo for Calico cloud workshop with managed cluster(EKS & AKS and GKE), RKE and self-managed cluster  ",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,4,"# Tigera Calico Cloud Workshop

## Join the Slack Channel

[Calico User Group Slack](https://slack.projectcalico.org/) is a great resource to ask any questions about Calico. If you are not a part of this Slack group yet, we highly recommend [joining it](https://slack.projectcalico.org/) to participate in discussions or ask questions. For example, you can ask questions specific to EKS and other managed Kubernetes services in the `#eks-aks-gke-iks` channel.

## Workshop objectives

The intent of this workshop is to guide users on connecting their K8s cluster to Calico Cloud and to test out its security and observability features.

## STEP 1 - Create a compatible k8s cluster 

  - [Calico Cloud system requirements](https://docs.calicocloud.io/get-started/connect/system-requirements)

  - [EKS: Use Cloud9 IDE to create a compatible EKS cluster](modules/creating-eks-cluster.md)
  - [AKS: Use Azure SDK to create a compatible AKS cluster](modules/creating-aks-cluster.md)
  - [GKE: Use gcloud SDK to create a compatible GKE cluster](modules/creating-gke-cluster.md)

  - [Kubeadm: Use kops to create a compatible self-managed cluster in GCP](modules/creating-kubeadm-cluster.md)
  - [RKE: Use Rancher server to create a compatible RKE cluster in GCP](modules/creating-rke-cluster.md)


## STEP 2 - Sign up for Calico Cloud  

  - Use this [link to register](https://www.calicocloud.io/) for a Calico Trial account

## STEP 3 - Connect your cluster to Calico Cloud

  - [Connect your cluster to Calico Cloud](modules/joining-calico-cloud.md)

## STEP 4 - Configure demo applications

  - [Configure your cluster and install demo applications](modules/configuring-demo-apps.md)

## STEP 5 - Test Calico Cloud features

Use cases:

### Network Policy for segmentation and access control

- [Segmentation and Workload Access Control](modules/app-service-control.md)
- [Egress Traffic Policy Based on FQDN](modules/dns-egress-controls.md)
- [Calico Cloud UI and Policy Management](modules/manager-ui.md)

### Observability

- [L7 logging](modules/enable-l7-visibility.md)
- [Service Graph](modules/manager-ui.md)
- [Kibana Dashboard](modules/kibana-dashboard.md)
- [Packet Capture](modules/dynamic-packet-capture.md) 

### Intrusion and Breach Detection

- [Global ThreatFeeds](modules/global-threatfeed.md)
- [Honeypods and Anomaly Detection](modules/intrusion-detection-protection.md)
- [Deep Packet Inspection](modules/deep-packet-inspection.md) 

### Compliance

- [Compliance reports](modules/compliance-reports.md) 

### Network Encryption with WireGuard

- [WireGuard Encryption](modules/encryption.md) 

## STEP 6 - Clean up your test environment

- [Clean Up](modules/clean-up.md)",FAUX
tilt-dev/tilt-extensions,Toolkit,Database,2025-04-14T13:59:05Z,2024-04-24T15:29:31Z,0,0,1,0,0,0,0,0,2020-02-10T21:53:32Z,2025-04-04T17:55:48Z,1925,214,Starlark,VRAI,176,FAUX,51,"developer-tools,development-environment,hacktoberfest,kubernetes",51,Extensions for Tilt,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,140,"# Tilt Extensions

[![Build Status](https://circleci.com/gh/tilt-dev/tilt-extensions/tree/master.svg?style=shield)](https://circleci.com/gh/tilt-dev/tilt-extensions)

This is the official Tilt Extensions Repository. Read more in [docs](https://docs.tilt.dev/extensions.html).

All extensions have been vetted and approved by the Tilt team.

- [`api_server_logs`](/api_server_logs): Print API server logs. Example from [Contribute an Extension](https://docs.tilt.dev/contribute_extension.html).
- [`cancel`](/cancel): Adds a cancel button to the UI.
- [`cert_manager`](/cert_manager): Deploys cert-manager.
- [`color`](/color): Allows colorful log prints.
- [`configmap`](/configmap): Create configmaps from files and auto-deploy them.
- [`conftest`](/conftest): Use [Conftest](https://www.conftest.dev/) to test your configuration files.
- [`coreos_prometheus`](/coreos_prometheus): Deploys Prometheus to a monitoring namespace, managed by the CoreOS Prometheus Operator and CRDs
- [`current_namespace`](/current_namespace): Reads the default namespace from your kubectl config.
- [`custom_build_with_restart`](/restart_process): Wrap a `custom_build` to restart the given entrypoint after a Live Update
- [`deployment`](/deployment): Create K8s deployments, jobs, and services without manifest YAML files.
- [`docker_build_sub`](/docker_build_sub): Specify extra Dockerfile directives in your Tiltfile beyond [`docker_build`](https://docs.tilt.dev/api.html#api.docker_build).
- [`docker_build_with_restart`](/restart_process): Wrap a `docker_build` to restart the given entrypoint after a Live Update
- [`dotenv`](/dotenv): Load environment variables from `.env` or another file.
- [`earthly`](/earthly): Build container images using [earthly](https://earthly.dev)
- [`execute_in_pod`](/execute_in_pod): Execute a command on a pod container.
- [`file_sync_only`](/file_sync_only): No-build, no-push, file sync-only development. Useful when you want to live-reload a single config file into an existing public image, like nginx.
- [`get_obj`](/get_obj): Get object yaml and the container's registry and image from an existing k8s resource such as deployment or statefulset
- [`git_resource`](/git_resource): Deploy a dockerfile from a remote repository -- or specify the path to a local checkout for local development.
- [`hasura`](/hasura): Deploys [Hasura GraphQL Engine](https://hasura.io/) and monitors metadata/migrations changes locally.
- [`hello_world`](/hello_world): Print ""Hello world!"". Used in [Extensions](https://docs.tilt.dev/extensions.html).
- [`helm_remote`](/helm_remote): Install a remote Helm chart (in a way that gets properly uninstalled when running `tilt down`)
- [`helm_resource`](/helm_resource): Deploy with the Helm CLI. New Tilt users should prefer this approach over `helm_remote`.
- [`honeycomb`](/honeycomb): Report dev env performance to [Honeycomb](https://honeycomb.io).
- [`jest_test_runner`](/jest_test_runner): Jest JavaScript test runner. Example from [Contribute an Extension](https://docs.tilt.dev/contribute_extension.html).
- [`k8s_attach`](/k8s_attach): Attach to an existing Kubernetes resource that's already in your cluster. View their health and live-update them in-place.
- [`k8s_yaml_glob`](/k8s_yaml_glob): Load kubernetes manifests by glob patterns.
- [`kim`](/kim): Use [kim](https://github.com/rancher/kim) to build images for Tilt
- [`knative`](/knative): Use [knative serving](https://knative.dev/docs/serving/) to iterate on scale-to-zero servers.
- [`ko`](/ko): Use [Ko](https://github.com/google/ko) to build Go-based container images
- [`kubebuilder`](/kubebuilder): Enable live-update for developing [Kubebuilder](https://github.com/kubernetes-sigs/kubebuilder) projects.
- [`kubectl_build`](/kubectl_build): Get faster build cycles and smaller disk usage by building docker images directly in the k8s cluster with [BuildKit CLI for kubectl](https://github.com/vmware-tanzu/buildkit-cli-for-kubectl).
- [`kubefwd`](/kubefwd): Use [Kubefwd](https://kubefwd.com/) to bulk-forward Kubernetes services.
- [`local_output`](/local_output): Run a `local` command and get the output as string
- [`min_k8s_version`](/min_k8s_version): Require a minimum Kubernetes version to run this Tiltfile.
- [`min_tilt_version`](/min_tilt_version): Require a minimum Tilt version to run this Tiltfile.
- [`namespace`](/namespace): Functions for interacting with namespaces.
- [`nix`](/nix): Use [nix](https://nixos.org/guides/install-nix.html) to build nix-based container images.
- [`nix_flake`](/nix_flake): Use [nix flake](https://nixos.org/manual/nix/stable/command-ref/new-cli/nix3-flake.html) to build images for Tilt from one or more flakes.
- [`ngrok`](/ngrok): Expose public URLs for your services with [`ngrok`](https://ngrok.com/).
- [`pack`](/pack): Build container images using [pack](https://buildpacks.io/docs/install-pack/) and [buildpacks](https://buildpacks.io/).
- [`podman`](/podman): Build container images using [podman](https://podman.io)
- [`print_tiltfile_dir`](/print_tiltfile_dir): Print all files in the Tiltfile directory. If recursive is set to True, also prints files in all recursive subdirectories.
- [`procfile`](/procfile): Create Tilt resources from a foreman Procfile.
- [`pulumi`](/pulumi): Install Kubernetes resources with [Pulumi](https://www.pulumi.com/).
- [`pypiserver`](/pypiserver): Run [pypiserver](https://pypi.org/project/pypiserver/) local container.
- [`restart_process`](/restart_process): Wrap a `docker_build` or `custom_build` to restart the given entrypoint after a Live Update (replaces `restart_container()`)
- [`secret`](/secret): Functions for creating secrets.
- [`snyk`](/snyk): Use [Snyk](https://snyk.io) to test your containers, configuration files, and open source dependencies.
- [`syncback`](/syncback): Sync files/directories from your container back to your local FS.
- [`tarfetch`](/tarfetch): Fetch new and updated files from a container to your local FS.
- [`tests`](/tests): Some common configurations for running your tests in Tilt.
- [`tilt_inspector`](/tilt_inspector): Debugging server for exploring internal Tilt state.
- [`uibutton`](/uibutton): Customize your Tilt dashboard with [buttons to run a command](https://blog.tilt.dev/2021/06/21/uibutton.html).
- [`vault_client`](/vault_client): Reach secrets from a Vault instance.
- [`wait_for_it`](/wait_for_it): Wait until command output is equal to given output.
- [`base64`](/base64): Base64 encode or decode a string.
- [`yarn`](/yarn): Create Tilt resources from package.json scripts in a yarn workspace.

## Contribute an Extension

See [Contribute an Extension](https://docs.tilt.dev/contribute_extension.html).

We want everyone to feel at home in this repo and its environs; please see our [Code of Conduct](CODE_OF_CONDUCT.md) for some rules that govern everyone's participation.",VRAI
toboshii/home-ops,DevOPs,Documentations,2025-02-25T01:21:34Z,2023-04-17T23:15:21Z,0,0,0,0,4,0,0,0,2021-05-12T04:31:00Z,2025-04-05T18:20:37Z,2852,337,Shell,VRAI,20,FAUX,88,"ansible,bgp,external-dns,flux,gitops,k8s,k8s-at-home,k8s-gateway,kubernetes,kubernetes-cluster,sops,talos,terraform",88,"My home Kubernetes cluster managed by GitOps (Flux), deployed on Talos Linux.",FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,3,"<img src=""https://camo.githubusercontent.com/5b298bf6b0596795602bd771c5bddbb963e83e0f/68747470733a2f2f692e696d6775722e636f6d2f7031527a586a512e706e67"" align=""left"" width=""144px"" height=""144px""/>

# My home operations repository 🎛🔨
_... managed by Flux Renovate, and GitHub Actions_ 🤖

<br />

<div align=""center"">

[![Discord](https://img.shields.io/discord/673534664354430999?style=for-the-badge&label=discord&logo=discord&logoColor=white)](https://discord.gg/k8s-at-home)
[![talos](https://img.shields.io/badge/talos-v1.1.2-brightgreen?style=for-the-badge&logo=linux&logoColor=white)](https://www.talos.dev/)
[![kubernetes](https://img.shields.io/badge/kubernetes-v1.24.3-brightgreen?style=for-the-badge&logo=kubernetes&logoColor=white)](https://kubernetes.io/)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white&style=for-the-badge)](https://github.com/pre-commit/pre-commit)
[![GitHub Workflow Status](https://img.shields.io/github/workflow/status/toboshii/home-ops/Schedule%20-%20Renovate?label=renovate&logo=renovatebot&style=for-the-badge)](https://github.com/toboshii/home-ops/actions/workflows/schedule-renovate.yaml)
[![Lines of code](https://img.shields.io/tokei/lines/github/toboshii/home-ops?style=for-the-badge&color=brightgreen&label=lines&logo=codefactor&logoColor=white)](https://github.com/toboshii/home-ops/graphs/contributors)

</div>

---

## 📖 Overview

This is a mono repository for my home infrastructure and Kubernetes cluster implementing Infrastructure as Code (IaC) and GitOps practices using tools like [Kubernetes](https://kubernetes.io/), [Flux](https://github.com/fluxcd/flux2), [Renovate](https://github.com/renovatebot/renovate) and [GitHub Actions](https://github.com/features/actions).

Feel free to open a [Github issue](https://github.com/toboshii/home-ops/issues/new/choose) or join the [k8s@home Discord](https://discord.gg/sTMX7Vh) if you have any questions.

---

## ⛵ Kubernetes

This repo generally attempts to follow the structure and practices of the excellent [k8s-at-home/template-cluster-k3](https://github.com/k8s-at-home/template-cluster-k3s), check it out if you're uncomfortable starting out with an immutable operating system.

### Installation

The cluster is running on [Talos Linux](https://talos.dev/), an immutable and ephemeral Linux distribution built around Kubernetes, deployed on bare-metal. [Rook Ceph](https://rook.io/) running hyper-converged with workloads provides persistent block and object storage, while a seperate server provides bulk (NFS) file storage.

### Core components

- [cilium/cilium](https://github.com/cilium/cilium): Internal Kubernetes networking plugin.
- [rook/rook](https://github.com/rook/rook): Distributed block storage for peristent storage.
- [mozilla/sops](https://toolkit.fluxcd.io/guides/mozilla-sops/): Manages secrets for Kubernetes, Ansible and Terraform.
- [kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns): Automatically manages DNS records from my cluster in a cloud DNS provider.
- [jetstack/cert-manager](https://cert-manager.io/docs/): Creates SSL certificates for services in my Kubernetes cluster.
- [kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx/): Ingress controller to expose HTTP traffic to pods over DNS.

### GitOps

[Flux](https://github.com/fluxcd/flux2) watches my [cluster](./cluster/) folder (see Directories below) and makes the changes to my cluster based on the YAML manifests.

[Renovate](https://github.com/renovatebot/renovate) watches my **entire** repository looking for dependency updates, when they are found a PR is automatically created. When PRs are merged, [Flux](https://github.com/fluxcd/flux2) applies the changes to my cluster.

### Directories

This Git repository contains the following directories (_kustomizatons_) under [cluster](./cluster/).

```sh
📁 cluster      # k8s cluster defined as code
├─📁 bootstrap  # contains the initial kustomization used to install flux
├─📁 flux       # flux, gitops operator, loaded before everything
├─📁 crds       # custom resources, loaded before 📁 core and 📁 apps
├─📁 charts     # helm repos, loaded before 📁 core and 📁 apps
├─📁 config     # cluster config, loaded before 📁 core and 📁 apps
├─📁 core       # crucial apps, namespaced dir tree, loaded before 📁 apps
└─📁 apps       # regular apps, namespaced dir tree, loaded last
```

### Networking

| Name                                         | CIDR            |
|----------------------------------------------|-----------------|
| Kubernetes Nodes                             | `10.75.40.0/24` |
| Kubernetes external services (Cilium w/ BGP) | `10.75.45.0/24` |
| Kubernetes pods                              | `172.22.0.0/16` |
| Kubernetes services                          | `172.24.0.0/16` |

## 🌐 DNS

### Ingress Controller

Over WAN, I have port forwarded ports `80` and `443` to the load balancer IP of my ingress controller that's running in my Kubernetes cluster.

[Cloudflare](https://www.cloudflare.com/) works as a proxy to hide my homes WAN IP and also as a firewall. When not on my home network, all the traffic coming into my ingress controller on port `80` and `443` comes from Cloudflare. In `VyOS` I block all IPs not originating from [Cloudflares list of IP ranges](https://www.cloudflare.com/ips/).

🔸 _Cloudflare is also configured to GeoIP block all countries except a few I have whitelisted_

### Internal DNS

[k8s_gateway](https://github.com/ori-edge/k8s_gateway) is deployed on my router running [VyOS](https://vyos.io/). With this setup, `k8s_gateway` has direct access to my clusters ingress records and serves DNS for them in my internal network.

Without much engineering of DNS @home, these options have made my `VyOS` router a single point of failure for DNS. I believe this is ok though because my router _should_ have the most uptime of all my systems.

### External DNS

[external-dns](https://github.com/kubernetes-sigs/external-dns) is deployed in my cluster and configured to sync DNS records to [Cloudflare](https://www.cloudflare.com/). The only ingresses `external-dns` looks at to gather DNS records to put in `Cloudflare` are ones where I explicitly set an annotation of `external-dns.home.arpa/enabled: ""true""`

---

## 🔧 Hardware

| Device                    | Count | OS Disk Size | Data Disk Size             | Ram   | Operating System | Purpose                        |
|---------------------------|-------|--------------|----------------------------|-------|------------------|--------------------------------|
| Dell R220                 | 1     | 120GB SSD    | N/A                        | 16GB  | VyOS 1.4         | Router                         |
| HP S01-pf1000             | 3     | 120GB SSD    | N/A                        | 8GB   | Talos Linux      | Kubernetes Control Nodes       |
| HP S01-pf1000             | 3     | 120GB SSD    | 1TB NVMe (rook-ceph)       | 32GB  | Talos Linux      | Kubernetes Workers             |
| SuperMicro SC836          | 1     | 120GB SSD    | 16x8TB + 16x3TB ZFS RAIDZ2 | 192GB | Ubuntu 20.04     | NFS                            |
| Brocade ICX 6610          | 1     | N/A          | N/A                        | N/A   | N/A              | Core Switch                    |
| Raspberry Pi 4B           | 1     | 32GB SD Card | N/A                        | 4GB   | PiKVM            | Network KVM                    |
| TESmart 8 Port KVM Switch | 1     | N/A          | N/A                        | N/A   | N/A              | Network KVM switch for PiKVM   |
| APC SUA3000RMXL3U w/ NIC  | 1     | N/A          | N/A                        | N/A   | N/A              | UPS                            |
| APC AP7930                | 1     | N/A          | N/A                        | N/A   | N/A              | PDU                            |

---

## 🤝 Thanks

Thanks to all folks who donate their time to the [Kubernetes @Home](https://github.com/k8s-at-home/) community. A lot of inspiration for my cluster came from those that have shared their clusters over at [awesome-home-kubernetes](https://github.com/k8s-at-home/awesome-home-kubernetes).

---

## 📜 Changelog

See [commit history](https://github.com/onedr0p/home-ops/commits/main)

---

## 🔏 License

See [LICENSE](./LICENSE)",VRAI
topmonks/hlidac-shopu,Application System,Application System,2025-05-16T01:29:05Z,2025-05-06T00:34:33Z,0,0,5,0,0,0,0,0,2018-11-28T12:08:05Z,2025-04-08T08:48:00Z,318393,73,JavaScript,VRAI,39,FAUX,42,"chrome-extension,esm,firefox-extension,ios-app,pulumi-aws,pwa-app,safari-extension",42,Hlídač shopů,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,45,"# Hlídač Shopů

[![CircleCI](https://circleci.com/gh/topmonks/hlidac-shopu.svg?style=shield)](https://circleci.com/gh/topmonks/hlidac-shopu)
[![codecov](https://codecov.io/gh/topmonks/hlidac-shopu/branch/trunk/graph/badge.svg?token=nlCFOKXCHx)](https://codecov.io/gh/topmonks/hlidac-shopu)
[![CodeFactor](https://www.codefactor.io/repository/github/topmonks/hlidac-shopu/badge)](https://www.codefactor.io/repository/github/topmonks/hlidac-shopu)
[![CodeScene Code Health](https://codescene.io/projects/10253/status-badges/code-health)](https://codescene.io/projects/10253)
[![CodeScene System Mastery](https://codescene.io/projects/10253/status-badges/system-mastery)](https://codescene.io/projects/10253)

[PWA](https://www.hlidacshopu.cz/app/) a rozšíření do nejrošířenějších prohlížečů, které zobrazuje historická data cen na největších
českých a slovenských e-shopech vč. [Reálné slevy](https://www.hlidacshopu.cz/metodika/). 

---

PWA and browser extension shows historical prices for biggest czech and slovak e-commerce websites.

## Install

* [Chrome extension](https://chrome.google.com/webstore/detail/hl%C3%ADda%C4%8D-shop%C5%AF/plmlonggbfebcjelncogcnclagkmkikk?hl=cs) - also works in Edge, Brave and Opera
* [Firefox extension](https://addons.mozilla.org/en-US/firefox/addon/hl%C3%ADda%C4%8D-shop%C5%AF/)
* [Safari extension](https://apps.apple.com/us/app/hl%C3%ADda%C4%8D-shop%C5%AF/id1488295734?mt=12)
* [Progressive Web Application](https://www.hlidacshopu.cz/app/) - app installable on most platforms
* [iOS app](https://apps.apple.com/us/app/hl%C3%ADda%C4%8D-shop%C5%AF/id1488295734#?platform=iphone) - also works on iPadOS

## Development

We are using `package.json` `scripts` (run `yarn run` for a list) for project automation.

### Prerequisites

*If you only want to build the Firefox extension, it is enough to install Docker and follow the guide in the next step.*

You will need:

* Node.js 22 (we use `nvm` for Node.js version management)
* `yarn` (we use Workspaces. You can't use `npm`. Sorry)
* Firefox
* Chrome
* XCode Command Line Tools (for Safari and iOS development)
* Pulumi (for Infrastructure and backend development)
* `jq` (for Extension distribution)

We have installation scripts for Debian and macOS. See `scripts` folder for installation scripts for your system.

On debian run `bash ./scripts/install-debian-tools.sh` - this will use apt-get to install `jq`.
On macOS run `bash ./scripts/install-macos-tools.sh` - this will use homebrew to install `jq`, `nvm` and `pulumi`.

## Step by step build of Firefox extension for reviewers

Install Docker for your OS (all OSes are supported) and then execute the following command from the root of the project:

```bash
./scripts/build-firefox-extension-dockerized.sh
```

Execution of this command might take a while. Built extension will be placed in the `dist` folder.

## Building extensions

All extensions (except Safari version) will be built to `./dist` folder by calling the `npm build` script.

Firefox supports Dark and Light themes for action icons, and we are optimizing action icons for these.
Chrome doesn't support action icons theming via `manifest.json` so we use `background.js` script to
add support for themes programmatically. We are removing `background.js` script, and
its entry in manifest, in the build step with other unnecessary files.

Content script `content.js` is written in ESM, but ESM is not widely supported in content scripts.
So we use simple bundle script `yarn build:extension` to convert ESM to IIFE bundle.

### Firefox extension

To build Firefox extension run `yarn build:firefox`. It will create `extension-dist` folder
for development time and packaged extension in `./dist` folder.

### Chrome extension

To build Chrome extension run `yarn build:chrome`. It will create package in `./dist` folder.

### Safari extension

1. Run `yarn build:extension` to get latest bundle script, domains (eshops) permissions and current version for Safari
2. Distribute app by XCode: `yarn start:safari` > Product > Archive > Distribute App\*
3. Manually send new app version to Review on [Itunes Connect](https://itunesconnect.apple.com/) - you will need to be logged in as TopMonks developer (credentials in 1Password)

\* Use autosigning feature and use the TopMonks s.r.o Apple developer team account. 
If this fails with missing private key, download one named ""itunes Mac App Distribution mac_app.cer""
from TopMonks 1Password.

## Updating extension version

To check the current version in `package.json`, `manifest.json` and `about.html` run

```bash
./version.sh
```

Update to new version run

```bash
./version.sh x.y.z
```

## Extension development

For seamless development experience we have `yarn watch:extension` script with incremental builds
on source files changes.

We also have convenient script `yarn start:chrome` and `yarn start:firefox` to start browsers with
an already registered extension and automatic reloading on changes.

For visual testing at scale, there is `./scripts/screenshotter.mjs`. This will run Chrome with an installed extension
and take a screenshot of embedded widget on every supported e-shop. You can find resulting pictures in `./screenshots`
folder.

## Extension release

The Release process of the extension is fully automated. To start the release process, you have to:
1. run `./version.sh` to see the current version and then bump it with `./version.sh x.y.z`
2. Commit & create tag in GitHub repository in the following format: `extension-x.y.z`, 
   where `x.y.z` is the version you set in the previous step. 


## Web www.hlidacshopu.cz development

Website has its own [Blendid](https://github.com/topmonks/blendid) configuration.
Start `www.hlidacshopu.cz` development with the following command:

```bash
op run --env-file=.env --no-masking -- yarn start:www.hlidacshopu.cz
```

### Cloudinary

Sites can automatically upload images to Cloudinary and generate Cloudinary URLs.
Cloudinary needs to be properly configured. Our Cloudinary credentials are stored in 1password team vault `Hlidac shopu`.
Use [1password CLI](https://1password.com/downloads/command-line/) to get credentials 
and set them as environment variables:

```bash
op run --env-file=.env --no-masking -- yarn build:www.hlidacshopu.cz
```

If this step is skipped, you will get the following error:

```
cloudinaryUrl Unknown cloud_name
```

You can enable Cloudinary aut-upload by setting `cloudinary: true` in `task-config.json` file. You can also configure
source and destination paths in `path-config.json` file. By default, will be uploaded everything in `cloudinary` directory.
Auto-uploader will generate `images.json` data file, that will be loaded into Nunjucks context via `collections: [""images""]`
setting in `task-config.json` file.

We have implementation of helpers to generate Cloudinary URLs. One `cloudinaryUrl` filter for Nunjucks templates
that should work in conjunction with generated `images.json`. Usage should be as follows:

```twig
<img src=""{{ images[""picture.png""][""public_id""] | cloudinaryUrl(width=300, height=240) }}"" alt="""">
```

You can use all supported transformations in JS SDK, for more details see [Cloudinary JS SDK](https://cloudinary.com/documentation/image_transformations).

## Other sources

* [Figma design sources](https://www.figma.com/file/hKLyCOXXN6LtS0NtVAbJzk/Hlidacshopu.cz?node-id=869%3A3)
* [Apify Actors sources](https://gitlab.com/apify-private-actors/hlidac-shopu/)
* [Keboola Connect](https://connection.eu-central-1.keboola.com/admin/projects/395/dashboard)

---

## Update @hlidac-shopu/lib version for actors
1. Update version @hlidac-shopu/lib in ./lib/package.json
2. Publish package to npm. Login credentials are in TopMonks 1password. 
    ```bash
    cd lib
    npm login
    npm publish --access public --tag latest
    ```
3. Update version @hlidac-shopu/lib across the project
    ```bash
    yarn up @hlidac-shopu/lib -i
    ```

© 2018-2024 TopMonks s.r.o., Apify Technologies s.r.o., Keboola Czech s.r.o. & contributors; Licensed under [EUPL-1.2](LICENSE.txt)",FAUX
trendmicro/cloudone-container-security-helm,DevOPs,Toolkit,2025-05-06T18:13:28Z,2025-02-11T19:07:57Z,0,0,0,0,2,0,0,0,2020-08-14T01:31:17Z,2025-04-01T13:27:48Z,2086,19,Mustache,VRAI,18,FAUX,6,,6,Helm chart for Trend Micro Cloud One Container Security,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,30,"# Trend Micro Cloud One Container Security Helm Chart

## Getting started

### Installing Helm

Trend Micro Cloud One Container Security components use the `helm` package manager for Kubernetes.

Helm 3 or later is supported when installing Trend Micro Cloud One - Container Security components.
To get started, see the [Helm installation guide](https://helm.sh/docs/intro/install/).

> [!NOTE]
> - Clusters deployed using Helm chart versions older than **2.3.25** will no longer receive new rule updates.
>  - Clusters that use unsupported Helm chart versions retain protection from their last applied policy but may create error logs in Scout due to failures downloading newer rules.
> - To ensure continued rule updates, upgrade Helm chart to the latest version. See [Upgrade a Trend Micro Cloud One Container Security deployment](#upgrade-a-trend-micro-cloud-one-container-security-deployment).

### Kubernetes Network Policies with Container Security Continuous Compliance

Container Security Continuous Compliance enforces policies by leveraging [Kubernetes network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) to perform isolation mitigation. Network policies are implemented by the [network plugin](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/).

To install Container Security, a network plugin with NetworkPolicy support is required to allow for network isolation mitigation.

- In Amazon Elastic Kubernetes Service (Amazon EKS), the [Calico network plugin](https://docs.aws.amazon.com/eks/latest/userguide/calico.html) can be used as network policy engine.
- In OpenShift 4.x, [OpenShift SDN](https://docs.openshift.com/container-platform/4.7/networking/network_policy/about-network-policy.html) supports using network policy in its default network isolation mode.
- In Azure Kubernetes Service (AKS), network policy are supported by [Azure Network Policies or Calico](https://docs.microsoft.com/en-us/azure/aks/use-network-policies).
- In Google Kubernetes Engine (GKE), you could enable [network policy enforcement](https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy) for a cluster.

**Note**: If you are running Container Security in a **Red Hat OpenShift** environment, network isolation mitigation is only supported for pods whose security context is acceptable by oversight controller's SecurityContextConstraint.  If you want to let Container Security isolate pods that are not allowed by default, you can use overrides.yaml to override the default setting.

By default, Container Security Continuous Compliance will create a Kubernetes network policy on your behalf. If you want to create it manually, follow the steps below:
1. Change the value of `cloudOne.oversight.enableNetworkPolicyCreation` to `false`, as seen below:

```
  cloudOne:
    oversight:
      enableNetworkPolicyCreation: false
```

2. Create a network policy with `matchLabels` set to `trendmicro-cloud-one: isolate` in your desired namespaces.

```
  apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    labels:
      app.kubernetes.io/instance: trendmicro
    name: trendmicro-oversight-isolate-policy
  spec:
    podSelector:
      matchLabels:
        trendmicro-cloud-one: isolate
    policyTypes:
    - Ingress
    - Egress
```

**Warning**: The network policy with matchLabels `trendmicro-cloud-one: isolate` must exist in each application namespaces in order to perform proper isolation mitigation.

### Registering Cluster with Trend Micro Container Security

There are two methods to register a cluster with Trend Micro Container Security:

1. [**Manual Registration**](#getting-a-container-security-api-key): Create a cluster in the Trend Micro Container Security console and obtain an API key or use the Public API to create a cluster and obtain an API key.
2. [**Automated Registration**](#using-automated-cluster-registration): Use the Vision One API key to automatically register all clusters with Trend Micro Container Security.

### Getting a Container Security API Key

To use the Trend Micro Cloud One Container Security components with your Kubernetes cluster an API key is required to be able to communicate with _Trend Micro Cloud One Container Security_.

To obtain an API key:
1. Navigate to the _Trend Micro Cloud One Container Security_ console using https://cloudone.trendmicro.com.

2. Go to [Add a cluster](https://cloudone.trendmicro.com/docs/container-security/cluster-add/).

3. Give your Kubernetes cluster a unique name.

4. Copy your API key, as it will be used during the installation process.

5. It is recommended to create a api key secret as outlined in the [Use Existing Secrets for API Key](#use-existing-secrets-for-api-key) section.

### Use Existing Secrets for API Key and Proxy Credentials

#### Use Existing Secrets for API Key

By default, the helm chart expects the api key to be provided through the `cloudOne.APIKey` helm value in the `overrides.yaml` file. This method creates an API key secret in the same namespace where the Container Security components are installed but can expose the API key in helm values. 

It is recommended to use the `useExistingSecrets.containerSecurityAuth: true` option and create a secret in the same namespace where the Container Security components will be installed. The secret should be named `trendmicro-container-security-auth` with the key `api.key` set to the API key value. This will also allow automation of the api key secret creation and management.

```sh
kubectl create secret generic trendmicro-container-security-auth --from-literal
api.key=<container-security-api-key> --namespace <trendmicro-namespace>
```

Then, set the `useExistingSecrets.containerSecurityAuth: true` in the `overrides.yaml` file.
```yaml
useExistingSecrets:
  containerSecurityAuth: true
```

#### Use Existing Secrets for Proxy Credentials

For more details on configuring a proxy, see the [Configure Container Security to use a proxy](#configure-container-security-to-use-a-proxy) section.

If you are outbound proxy for the Container Security components, you can also set the `useExistingSecrets.outboundProxy: true` in the `overrides.yaml` file and create a secret in the same namespace as chart installation. The secret should be named `trendmicro-container-security-outbound-proxy-credentials`. For secret format, see the [Proxy Credentials Secret Template](./templates/outbound-proxy.yaml).

### Using automated cluster registration

Trend Vision One Container Security users can configure automated cluster registration
to automate the management of cluster lifecycle. This feature enables clusters to be automatically registered clusters with Trend Vision One Container Security when the Container Security is installed and unregistered when the Container Security is uninstalled. This also allows users to register multiple clusters with a single Vision One API key instead of manually registering each cluster.

To use automated cluster registration:
1. Navigate to the _Trend Micro Vision One_ console using https://portal.xdr.trendmicro.com/

2. Create a Vision One API Key with a role that contains only the ""Automatically register cluster"" permission

3. Put the Vision One API Key into a secret called `trendmicro-container-security-registration-key` with the key `registration.key` in the same namespace where the Container Security components are installed.

4. Install the Container Security Helm chart using the values `cloudOne.clusterRegistrationKey: true` and `cloudOne.groupId=<your cluster group ID>`. You can optionally define the cluster name by setting either `cloudOne.clusterName` or `cloudOne.clusterNamePrefix`, if these are not specified the name will be a random string. An existing policy can also be assigned to the cluster by setting `cloudOne.policyId=<your policy ID>`.

### Override configuration defaults

Helm uses a file called `values.yaml` to set configuration defaults. You can find detailed documentation for each of the configuration options in this file.

You can override the defaults in this file by creating an `overrides.yaml` file and providing the location of this file as input during installation. The `cloudOne.APIKey` should be overridden in the `overrides.yaml` file.

**Note**: If you create a file to override the values, make sure to copy the structure from the chart's `values.yaml` file. You only need to provide the values that you are overriding.

### Installing the Container Security Helm chart

1. Create a file called overrides.yaml that will contain your cluster-specific settings. You can find these values in the Container Security console or Container Security API when creating a cluster. The [Values.yaml](values.yaml) file can be used as a reference when creating your overrides file.

2. Use `helm` to install Container Security components with your cluster-specific settings. We recommend that you run Container Security in its own namespace.

To install Container Security chart into an existing Kubernetes namespace, use the `--namespace` flag with the `helm install` command:

```sh
  helm install \
    --values overrides.yaml \
    --namespace ${namespace} \
    trendmicro \
    https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz
```

In the example below, we create a new namespace by using `helm`'s `--create-namespace` option:

```sh
  helm install \
    --values overrides.yaml \
    --namespace trendmicro-system \
    --create-namespace \
    trendmicro \
    https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz
```

For more information about `helm install`, see the [Helm installation documentation](https://helm.sh/docs/helm/helm_install/).

**Note**: If you are running Container Security in a pure **AWS EKS Fargate** environment, you may need to adjust your Fargate profile to allow pods in a non-default namespace (ex: `trendmicro-system`) to be scheduled. See [AWS documentation](https://docs.aws.amazon.com/eks/latest/userguide/fargate-profile.html) for more information on Fargate profiles.

**Note**: If you are running Container Security in a **Red Hat OpenShift** environment, the Helm Chart creates a [Security Context Constraint](https://docs.openshift.com/container-platform/4.7/authentication/managing-security-context-constraints.html) to allow Container Security components to have the minimum security context requirements to run.

**Note**: If you are running Container Security in a cluster where Pod Security Admission is available and you have runtime security enabled, ensure the namespace where Container Security is installed is using the [privileged Pod Security Standards policy](https://kubernetes.io/docs/concepts/security/pod-security-standards/#privileged).

### Upgrade a Trend Micro Cloud One Container Security deployment

To upgrade an existing installation in the default Kubernetes namespace to the latest version:

```sh
  helm upgrade \
    --values overrides.yaml \
    --namespace ${namespace} \
    trendmicro \
    https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz
```

**Note**: Helm will override or reset values in `overrides.yaml`. If you want to use the values you had previously, use the [--reuse-values](https://helm.sh/docs/helm/helm_upgrade/) option during a Helm upgrade:

```sh
  helm upgrade \
    --namespace ${namespace} \
    --reuse-values \
    trendmicro \
    https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz
```

### Uninstall the Container Security Helm chart

You can delete all of the resources created by a helm chart using Helm's `uninstall` command:

**Warning**: `helm uninstall` and `kubectl delete namespace` are destructive commands, and will delete all of the associated resources.

```sh
  helm uninstall trendmicro --namespace ${namespace}
```

Use the `helm list --all-namespaces` command to list installed releases in all namespaces.

If you created a `trendmicro-system` namespace during install, and don't have any other components in the `trendmicro-system` namespace, you can delete the namespace by running `kubectl delete namespace trendmicro-system`.

By default, Container Security Continuous Compliance will create a Kubernetes network policy for you. The created network policies will be cleaned up, even if the chart is uninstalled. To clean them up, run:

```sh
  kubectl delete networkpolicy -l app.kubernetes.io/instance=trendmicro --all-namespaces
```

**Warning**: If you have running Pods that are isolated by a network policy, removing the network policy will give them network access again.

## Documentation

- [Trend Micro Cloud One Container Security Documentation](https://cloudone.trendmicro.com/docs/container-security)
- [Trend Micro Vision One Container Security Documentation](https://docs.trendmicro.com/en-us/documentation/article/trend-vision-one-container-security)

## Advanced topics

### Install a specific version of the Container Security helm chart

If you want to install a specific version you can use the archive link for the tagged release. For example, to install Trend Micro Cloud One Container Security helm chart version 2.6.5, run the following command:

```sh
  helm install \
    --values overrides.yaml \
    --namespace ${namespace} \
    --create-namespace \
    trendmicro \
    https://github.com/trendmicro/cloudone-container-security-helm/archive/2.6.5.tar.gz
```

### Enabling or disabling a specific component

If desired, specifics components of the Container Security helm chart can be enabled or disabled individually using an overrides file.
For example, you can choose to enable the runtime security component by including the below in your `overrides.yaml` file:
```yaml
  cloudOne:
    runtimeSecurity:
      enabled: true
```

### Managing Container Security policies with Policy as Code

To learn more about managing Container Security policies with custom resources and policy operator, see the [Policy as Code documentation](./docs/policy-as-code.md).

### Configure Container Security to use a proxy

You can configure Container Security to use either a socks5 proxy or http proxy by setting the `httpsProxy` value.
For example, you can configure a socks5 proxy with authentication in your `overrides.yaml` file this way:
```
proxy:
  httpsProxy: socks5://10.10.10.10:1080
  username: user
  password: password
```

For http proxy, you can configure it this way:
```
proxy:
  httpsProxy: http://10.10.10.10:3128
  username: user
  password: password
```

### Runtime vulnerability scanning for OpenShift

On OpenShift, new namespaces created after installing container security need to be configured by upgrading container security to create RBAC resources and provide scanners in the new namespaces the required privileges.

When uninstalling the Trend Micro Helm chart, the associated **ServiceAccounts** and **ClusterRoleBindings** used to assign security context constraints to scanner pods will also be removed. To uninstall, run:

```sh
helm uninstall trendmicro -n trendmicro-system
```

***Troubleshooting Installation Issues***: If you encounter an error preventing the uninstallation of the Helm chart, run the following cleanup script to ensure all remaining resources are removed, run the following script with admin privileges:

```sh
./scripts/openshift-cleanup.sh
```
After running the script, proceed with the Helm uninstall as usual.

### Enable runtime security on AWS bottlerocket

You can run runtime security on AWS bottlerocket nodes by adding these configurations in your `overrides.yaml` file:
```yaml
securityContext:
  scout:
    scout:
      allowPrivilegeEscalation: true
      privileged: true
```

### Configure Runtime Container Interface

You can configure Container Security to customize container runtime interface.
For example, you can specify a custom path:

```
scout:
  falco:
    cri:
      socket: ""/run/cri/containerd.sock""
```

You can also configure a custom path for k0s or k3s. For example:

```
scout:
  falco:
    k0s:
      socket: ""/run/k0s/containerd.sock""
```

### Add capabilities to runtime vulnerability scanner

Runtime vulnerability scanner needs the privilege to access all directories and files in an image. `DAC_READ_SEARCH` is needed when the file permissions do not allow scanner to access the files or directories in an image. In this case, you can add `DAC_READ_SEARCH` to the `scanner`'s capabilities

```
securityContext:
  scanner:
    target:
      capabilities:
        add: [""DAC_READ_SEARCH""]
```

### Configure node selectors and tolerations for the Container Security components

To configure the scheduling of the Container Security components, you can set the `nodeSelector` and `tolerations` values in your `overrides.yaml` file:
```yaml
nodeSelector:
  defaults: # Node selector applied to all components except scanner pods (see below)
    kubernetes.io/arch: arm64

  admissionController: # Node selector applied to specific component
    kubernetes.io/arch: amd64

tolerations:
  defaults: # Tolerations applied to all components except scanner pods (see below)
  - key: kubernetes.io/arch
    operator: Equal
    value: amd64
    effect: NoSchedule

  admissionController: # Tolerations applied to specific component
  - key: kubernetes.io/os
    operator: Exists
    effect: NoSchedule
```

For scanner pods, since they run images from the pods being scanned, you can configure the scanner to inherit the node selectors and tolerations from the owner resource (ie. deployment, daemonset, pod, etc.):
```yaml
nodeSelector:
  inheritNodeSelectorScanner: true # Inherit node selector from the owner resource (default: false)

  filterNodeSelectorScanner: # Only inherit node selector specified in the filter (default: all node selectors are inherited)
    kubernetes.io/arch: amd64

tolerations:
  inheritTolerationsScanner: true # Inherit tolerations from the owner resource (default: false)

  filterTolerationsScanner: # Only inherit tolerations specified in the filter (default: all tolerations are inherited)
  - key: kubernetes.io/arch
    operator: Equal
    value: amd64
    effect: NoSchedule
```


### Configuring logging for the Container Security components

You can configure the logging for all components by setting the `logConfig` value in your `overrides.yaml` file:

```yaml
logConfig:
  logLevel: info # Sets the log verbosity level. Supported values are debug, info, and error. Overrides the logLevel set for each component
  logFormat: json # Sets the log encoder. Supported values are json and console
  stackTraceLevel: error # Sets the level above which stacktraces are captured. Supported values are info, error or panic
  timeEncoding: rfc3339 # Sets the time encoding format. Supported values are epoch, millis, nano, iso8601, rfc3339 or rfc3339nano
```

You can also configure the log level for each component individually by setting the `logLevel` value for the component in your `overrides.yaml` file:

```yaml
cloudone:
  admissionController:
    logLevel: debug
```

### Configuring Falco event outputs

You can enable Falco event outputs to stdout or syslog by setting values under `scout.falco` in your `overrides.yaml` file:

```yaml
scout:
  falco:
    stdout_enabled: true # Enable stdout output for Falco events.
    syslog_enabled: true # Enable syslog output for Falco events
```

Note: Enabling stdout output will cause large amounts of logs to be generated. Enable these if the events are being consumed from the respective channel. Container security will only consume the events from the grpc channel.


### Configuring Splunk HEC token for Falco Custom Rules

To learn more about configuring the Splunk HEC token for falco custom rules, see the Splunk HEC Secret docs [here](./docs/falco-splunk-hec-secret.md).

### Configuring OCI repository artifacts for Falco Custom Rules

To learn more about configuring OCI repository artifacts for falco custom rules, see the OCI repo docs [here](./docs/oci-artifact-with-falcoctl.md).
Note that this feature is subject to breaking changes in the future.

### Least Privileged mode

Falco runs in full privileged mode by default. For the sake of security, you can enable `least_privileged` to make Falco to run in the least privileged mode. In this case, Falco will be non-privileged container with minimum capabilities added.

```yaml
scout:
  falco:
    least_privileged: true
```

## Falco Version Matrix 

The following matrix shows the Falco version that is bundled with each Helm chart version:

| **Helm Chart Version** | **Falco Version** |
|------------------------|-------------------|
| 2.6.x                  | 0.39.2            |
| 2.5.x                  | 0.37.1            |
| 2.4.x                  | 0.37.1            |
| 2.3.24 - 2.3.47        | 0.36.1            |
| 2.3.14 - 2.3.23        | 0.34.1            |

## Troubleshooting

### Access logs
Most issues can be investigated using the application logs. The logs can be accessed using `kubectl`.

* Access the logs for the admission controller using the following command:
```sh
  kubectl logs deployment/trendmicro-admission-controller --namespace ${namespace}
```

* Access the logs for the runtime security component using the following command, where container can be one of `scout`, or `falco`:
```sh
  kubectl logs daemonset/trendmicro-scout --namespace ${namespace} -c ${container}
```

* Access the logs for Oversight controller (Continuous Compliance policy enforcement) using the following command:
```sh
  kubectl logs deployment/trendmicro-oversight-controller [controller-manager | rbac-proxy] --namespace ${namespace}
```

* Access the logs for Usage controller using the following command:
```sh
  kubectl logs deployment/trendmicro-usage-controller [controller-manager | rbac-proxy] --namespace ${namespace}
```

### Collect support logs
To help debug issues reported in support cases, a log collection script is provided for customer use.

To enable debug logging, set the `logConfig.logLevel` to `debug` in the `overrides.yaml` file and upgrade the helm chart.
```yaml
logConfig:
  logLevel: debug
```

Gather logs using the following command:

```sh
./scripts/collect-logs.sh
```

The following environment variables are supported for log collection:

| Environment variable      | Description                             | Default                                                                                        |
| ------------------------- |:----------------------------------------|:-----------------------------------------------------------------------------------------------|
| RELEASE                   | Helm release name                       | `trendmicro`                                                                         |
| NAMESPACE                 | The namespace that the helm chart is deployed in | Current namespace declared in `kubeconfig`. If no namespace setting exists in `kubeconfig`, then `trendmicro-system` will be used. |

### Known Limitations

1. Malware scanning is not supported in air-gapped environments.
2. Malware scanning is not supported in ARM64 environments.",VRAI
trevorbox/service-mesh-patterns,Toolkit,Documentations,2024-06-05T23:34:08Z,2024-03-15T00:08:01Z,0,0,0,0,0,0,0,3,2020-06-15T21:26:16Z,2024-06-05T23:34:17Z,1885,30,Smarty,VRAI,16,FAUX,0,,0,Design Considerations at the Edge of the ServiceMesh,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,2,"# Service Mesh Patterns

This project provides examples for widely practiced Service Mesh configurations.

> To update git submodules to latest commit use:
>
> `git submodule update --init --recursive`
>
> `git submodule foreach git pull origin master`

## Setup

### Install service mesh operators

> Note: you need to manually approve the InstallPlans as the script describes.

```sh
./install-service-mesh-operators.sh
```

### Export Default vars

```sh
source default-vars.txt && export $(cut -d= -f1 default-vars.txt)
```

*Or*, Export Custom vars

```sh
export bookinfo_namespace=bookinfo
export control_plane_namespace=istio-system
export control_plane_name=basic-install
export control_plane_route_name=api
```

## Basic Gateway Configuration

This example demonstrates a basic confiuration using:

- A single Gateway deployed in the *""<control_plane_namespace>""*.
- A VirtualService deployed in the member namespace referencing the Gateway in *""<control_plane_namespace>/<gateway_name>""*.

### Install control plane

```sh
./install-service-mesh-control-plane.sh
```

### Install basic gateway configuration

```sh
./install-basic-gateway-configuration.sh
```

### Test the bookinfo application

Open the following url in a web browser.

```sh
echo ""https://$(oc get route ${control_plane_route_name} -n ${control_plane_namespace} -o jsonpath={'.spec.host'})/productpage""
```

### Cleanup basic gateway configuration

```sh
./cleanup-basic-gateway-configuration.sh
```

### Cleanup control plane

```sh
./cleanup-service-mesh-control-plane.sh
```

## Multiple Ingress Gateways with MongoDB

This example is based on the blog post [Consuming External MongoDB Services](https://istio.io/latest/blog/2018/egress-mongo/) but takes it a step further to show how to deploy a MongoDB instance behind the same Service Mesh on Openshift and expose it via an External Load Balancer on the mongo ingress gateway for external communication. With this configuration we can present a certificate in the mongo-ingressgateway proxy and test TLS connections from outside the mesh to MongoDB.

The bookinfo application is also deployed with an additional ratings-v2 service that connects to MongoDB via a ServiceEntry to the NodePort with TLS.

The updated architecture of the bookinfo app appears below:

![Updated Bookinfo architecture with mongodb in mesh](./documentation/pictures/bookinfo-mongo.png)

### Install control plane mongodb

```sh
./install-service-mesh-control-plane-mongodb.sh
```

### Install mongo gateway configuration

```sh
./install-mongo-gateway-configuration.sh
```

### Setup mongodb

Wait for the mongodb-v1 pod to run before running the setup script.

This will create the test database bookinfo rating-v2 service will connect to.

```sh
./ingress-mongodb-setup-tls.sh
```

### Test the bookinfo application connectivity to mongodb

Open the following url in a web browser.

```sh
echo ""https://$(oc get route ${control_plane_route_name} -n ${control_plane_namespace} -o jsonpath={'.spec.host'})/productpage""
```

Refresh the product info page multiple times. If all was successful, you should see Reviewer 1 with a one star rating under Book Reviews.

Within Kiali, all reviews requests should be directed to the rating-v2 service and then to the mongodb ServiceEntry.

You won't see traffic in kiali for mongodb requests since it is not using http or grcp, just tcp.

### Cleanup mongo gateway configuration

```sh
./cleanup-mongo-gateway-configuration.sh
```

### Cleanup control plane mongodb

```sh
./cleanup-service-mesh-control-plane-mongodb.sh
```

## Multiple Ingress Gateways and Egress Gateway to MongoDB

This example is also based on the blog post [Consuming External MongoDB Services](https://istio.io/latest/blog/2018/egress-mongo/#configure-tcp-traffic-from-sidecars-to-the-egress-gateway)

The mongo service still exists in the same location as the previous example's architecture describes.

Direct mongo traffic through an egress gateway:

![Kiali mongodb traffic through an egressgateway](./documentation/pictures/bookinfo-mongo-egressgateway.png)

### Install control plane mongodb via egressgateway

```sh
./install-service-mesh-control-plane-mongodb-egressgateway.sh
```

### Install mongo egressgateway configuration

```sh
./install-mongo-egressgateway-configuration.sh
```

### Configure mongodb

Wait for the mongodb-v1 pod to run before running the setup script.

This will create the test database bookinfo rating-v2 service will connect to.

```sh
./ingress-mongodb-setup-tls.sh
```

### Verify traffic flows through the egressgateway

Open the following url in a web browser.

```sh
echo ""https://$(oc get route ${control_plane_route_name} -n ${control_plane_namespace} -o jsonpath={'.spec.host'})/productpage""
```

Refresh the product info page multiple times. If all was successful, you should see Reviewer 1 with a one star rating under Book Reviews.

> Note: the External Load Balancer host's IP can change to the mongo-ingressgateway kubernetes service on AWS. If that happens you will need to rerun install-mongo-egressgateway-configuration.sh to update the IP address of the direct-mongo-through-egress-gateway VirtualService and mongodb ServiceEntry.

The istio-proxy access logs within the istio-egrassgateway pod should show outbound traffic from it. This logging was enabled by the servicemeshcontrolplane's `global.proxy.accessLogFile` configuration.

```sh
[2020-07-22T00:38:16.510Z] ""- - -"" 0 - ""-"" ""-"" 1536 3960 24 - ""-"" ""-"" ""-"" ""-"" ""13.58.124.191:27018"" outbound|27018||my-mongo.tcp.svc 10.130.0.166:43250 10.130.0.166:15666 10.130.0.170:54548 - -
```

### Cleanup mongo egressgateway configuration

```sh
./cleanup-mongo-egressgateway-configuration.sh
```

### Cleanup control plane mongodb egressgateway

```sh
./cleanup-service-mesh-control-plane-mongodb-egressgateway.sh
```

## Egress Traffic Control

This example demonstrates controlling outgoing traffic from the service mesh to external services.  Priorities are applied based on the header that is provided with the request.  In a real scenario this will most likely be injected based on some form of authentication and authorization.  The example also provides samples to demonstrate the limits that are applied to the different service levels based on Istio destination rules using subsets for the external service.

This [guide](https://github.com/cloudfirst-dev/istio-egress-traffic-control) will walk through running the examples.

## Originate TLS through an Egress Gateway with a trusted CA Certificate

- See http TLS origination example [README.md](egressgateway/http-trusted-ca/).
- See mongo TLS origination example [README.md](egressgateway/mongodb-trusted-ca/).",VRAI
TrueTechLabs/fabric-trace,Application System,Application System,2025-04-17T07:10:15Z,2024-08-08T13:30:04Z,0,2,0,0,0,0,0,0,2024-02-28T01:26:29Z,2025-04-05T16:25:44Z,180387,137,Shell,VRAI,19,FAUX,0,,0,基于区块链Hyperledger Fabric V2.5的农产品溯源/商品/通用溯源应用模板，部署简单，附压测工具、区块链浏览器，文档详细。可以快速使用本系统搭建自己的溯源系统，帮助想法快速落地。,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,3,"基于区块链Hyperledger Fabric V2.5的农产品溯源/商品/通用溯源应用模板，部署简单，附压测工具tape/caliper、区块链浏览器，文档详细并有配套课程。可以快速使用本系统搭建自己的溯源系统，帮助想法快速落地。经统计，目前已部署1100次以上。

**项目演示站点:** http://t.realcool.top/

与B站官方合作，本项目代码讲解与二次开发课程已发布，限时特价219元，支持试看: [B站：Fabric V2.5通用溯源项目讲解与二次开发课程](https://www.bilibili.com/cheese/play/ss15923?bsource=link_copy) ，购买课程有配套社群，方便交流与答疑。

Fabric V2.5通用溯源课程活动：[【腾讯文档】：提交源码仓库地址/PPT/视频获得返现50-219元](https://docs.qq.com/form/page/DQ3ZJUERPVXJCdUtX)

二次开发优秀案例（参加活动可返现）：
1.  [木白/基于区块链的电子证照存储与溯源系统](https://gitee.com/surgar2022/fabric-eCert-trace-ipfs) 【全额返现219元】


#### 如果项目有帮助请给项目点上Star，我们将十分感谢！ 欢迎加入TrueTechLabs Fabric学习交流QQ群：776873343
群内可免费阅读此项目的区块链部分、应用后端部分的代码说明文档。

![TrueTechLabs Fabric学习交流QQ群：776873343](https://truetechlabs-1259203851.cos.ap-shanghai.myqcloud.com/picgoimg202403062054644.jpg)
#### 一、项目介绍
基于区块链Hyperledger Fabric V2.5的农产品溯源/商品/通用溯源应用模板，部署简单，附压测工具、区块链浏览器，文档详细并有配套课程。可以快速使用本系统搭建自己的溯源系统，帮助想法快速落地。采用的技术栈：Fabric V2.5、Gin、Vue.js、Mysql。

![项目系统架构图](https://truetechlabs-1259203851.cos.ap-shanghai.myqcloud.com/picgo319183738-60337eb8-1799-435f-b0a5-8ac61761aa28.png)
![多类型用户注册](https://truetechlabs-1259203851.cos.ap-shanghai.myqcloud.com/picgo202404151236356.jpg)
![农产品上链](https://truetechlabs-1259203851.cos.ap-shanghai.myqcloud.com/picgo202404151238978.png)
![农产品溯源](https://truetechlabs-1259203851.cos.ap-shanghai.myqcloud.com/picgo202404151238196.png)
![区块链浏览器可视化](https://truetechlabs-1259203851.cos.ap-shanghai.myqcloud.com/picgo202404151239574.png)

##### 项目地址：
**Github**:
[https://github.com/TrueTechLabs/fabric-trace](https://github.com/TrueTechLabs/fabric-trace)

**Gitee**（与Github同步）:
[https://gitee.com/real__cool/fabric-trace](https://gitee.com/real__cool/fabric-trace)

##### 搭建视频地址：
[https://www.bilibili.com/video/BV1Ar421H7TK](https://www.bilibili.com/video/BV1Ar421H7TK)

##### 附完整一刀未剪的搭建视频:
演示使用临时云服务器，建议使按年付费轻量应用服务器价格划算一些，具体购买方式参见下方搭建步骤。
[https://www.bilibili.com/video/BV1mF4m1P7Go](https://www.bilibili.com/video/BV1mF4m1P7Go)。

##### 项目文档地址(部分内容需要订阅专栏）：
[https://blog.csdn.net/qq_41575489/category_12075943.html](https://blog.csdn.net/qq_41575489/category_12075943.html)

##### 项目代码讲解与修改前端课程（包含全部项目资料）：
[B站：Fabric V2.5通用溯源项目讲解与二次开发课程](https://www.bilibili.com/cheese/play/ss15923?bsource=link_copy)


#### 二、版权声明
本项目基于Apache License 2.0开源协议，在【**个人**】的科研、学习范围内可以自由使用，请附上项目链接。如有商业需求（包括教学案例授权）或合作需求，请填写收集表：[【腾讯文档】本项目搭建服务或商务合作意向收集](https://docs.qq.com/form/page/DQ1hIck5OQkNGQXF2)，禁止未授权情况下将本项目用于商业收费项目，如发现有相关情况请在B站私信举报，谢谢支持。若您不认可本声明内容，请勿使用本项目。

#### 三、项目特点
本项目采用Hyperledger Fabric V2.5，属于目前最新的Fabric版本，具有更好的性能和稳定性，调用链码使用fabric-gateway模式，是当前版本的推荐方式。内置了tape压测工具（课程包含caliper测试代码），可以方便的对区块链网络进行压测；内置了区块链浏览器，可以方便地查询交易信息。项目结构清晰，代码注释详细，方便二次开发。结合了mysql实现账户注册登录功能，更符合真实业务场景。

#### 四、项目背景
区块链技术的出现，为溯源系统的建设提供了新的思路。区块链技术的不可篡改性、去中心化、可追溯等特点，使得区块链技术成为溯源系统的理想选择。本项目基于Hyperledger Fabric V2.5，实现了基于区块链的农产品溯源系统。 在本系统中，有5个内置的角色：种植户、工厂、驾驶员、商店、消费者。其中种植户、工厂、驾驶员、商店可以将信息上链，消费者有信息溯源权限。上述可以上链信息的角色各可以输入5个农产品的属性，方便自定义项目的二次开发。本项目的目标是作为基于Fabric V2.5版本的一个通用区块链溯源模板。

#### 五、搭建步骤
> 由于相关资源的失效，如果部分内容与视频不一致请以本文档为准

强烈推荐按照步骤使用全新的腾讯云轻量云服务器搭建本系统，虚拟机的问题较多。点击此链接直达新人活动页面：[https://curl.qcloud.com/Sjy0zKjy](https://curl.qcloud.com/Sjy0zKjy) ，**购买轻量2核4G或以上的服务器**，199/年（价格经常会调整），如果后续准备做程序开发可以用新用户优惠买三年的，安装**Ubuntu20.04**系统。系统版本不一致极有可能部署不成功。

注：不建议使用阿里云等其他云服务器，与演示步骤会不一致，且会有额外问题。

**严格按照以下步骤操作，以下步骤已经经过上千人次的验证，如果遇到报错请仔细检查是否遗漏某个步骤。已做好换源处理，不要使用任何代理工具。
如果对Linux命令不熟悉，请一定先学习下：[快速入门Linux及使用VSCode远程连接Linux服务器](https://blog.csdn.net/qq_41575489/article/details/139434933)，遇到报错后一定不能跳过，解决完再往下一步！**

[**特别推荐：使用Docker部署项目**](./Docker_Setup_Guide.md)
1. 安装docker 

	```bash
	#下载docker 
	# 官方脚本当前已无法下载，使用gitee备份的脚本:
	curl -fsSL https://gitee.com/real__cool/fabric_install/raw/main/docker_install.sh | bash -s docker --mirror Aliyun
	#添加当前用户到docker用户组 
	sudo usermod -aG docker $USER 
	newgrp docker 
	sudo mkdir -p /etc/docker
	#配置docker镜像加速，近期非常不稳定，如果以下源不好用可以再找下其他源
	#下边的源2024.12.26日测试可用
	sudo tee /etc/docker/daemon.json <<-'EOF'
	{
	    ""registry-mirrors"": [
	        ""https://docker.m.daocloud.io"",
	        ""https://docker.1panel.live"",
	        ""https://hub.rat.dev""
	    ]
	}
	EOF

	#重启docker 
	sudo systemctl daemon-reload
	sudo systemctl restart docker
	```

2. 安装开发使用的go、node、jq

	```bash
	#下载二进制包
	wget https://golang.google.cn/dl/go1.19.linux-amd64.tar.gz
	#将下载的二进制包解压至 /usr/local目录
	sudo tar -C /usr/local -xzf go1.19.linux-amd64.tar.gz
	mkdir $HOME/go
	#将以下内容添加至环境变量 ~/.bashrc
	export GOPATH=$HOME/go
	export GOROOT=/usr/local/go
	export PATH=$GOROOT/bin:$PATH
	export PATH=$GOPATH/bin:$PATH
	#更新环境变量
	source  ~/.bashrc 
	#设置代理
	go env -w GO111MODULE=on
	go env -w GOPROXY=https://goproxy.cn,direct
	
	#下载nvm安装脚本
	wget https://gitee.com/real__cool/fabric_install/raw/main/nvminstall.sh
	#安装nvm；屏幕输出内容添加环境变量
	chmod +x nvminstall.sh
	./nvminstall.sh
	# 将环境变量写入.bashrc
	export NVM_DIR=""$HOME/.nvm""
	[ -s ""$NVM_DIR/nvm.sh"" ] && \. ""$NVM_DIR/nvm.sh""  # This loads nvm
	[ -s ""$NVM_DIR/bash_completion"" ] && \. ""$NVM_DIR/bash_completion""  # This loads nvm bash_completion
	export NVM_NODEJS_ORG_MIRROR=http://npmmirror.com/mirrors/node/ #更换阿里云nvm node源
 
	# 更新环境变量
	source  ~/.bashrc
	# 安装node16
	nvm install 16
	#换源
	npm config set registry https://registry.npmmirror.com
	
	#安装jq 
	sudo apt install jq
	```



3. 克隆本项目 

	```bash
	git clone https://gitee.com/real__cool/fabric-trace
	```

4. 启动区块链部分。在fabric-trace/blockchain/network目录下:

	```bash
	# 仅在首次使用执行：下载Fabric Docker镜像。如果拉取速度过慢或失败请检查是否完成docker换源并执行了重启docker命令。
	./install-fabric.sh -f 2.5.6 d 
	```
	```bash
	# 启动区块链网络
	./start.sh
	```	
 	 **如果在启动区块链网络时遇到报错可以尝试:**
	```bash
	# 执行清理所有的容器指令：
	docker rm -f $(docker ps -aq)
	```
	**然后再重新启动区块链网络**

5. 启动后端 在fabric-trace/application/backend目录下： 执行： `go run main.go`

6. 修改后端IP，将以下文件中的IP：`119.45.247.29`，换成自己云服务器的IP。
	```bash
	fabric-trace/application/web/.env.production
	fabric-trace/application/web/.env.development
	fabric-trace/application/web/src/router/index.js
	```
	或使用application/replaceip.sh脚本根据指引修改IP，在fabric-trace/application目录下
	```bash
	./replaceip.sh
	```

7. 新开一个窗口，启动前端 在fabric-trace/application/web目录下： 执行： 

	```bash
	# 仅在首次运行执行：安装依赖
	npm install 
	```

	```bash
	# 启动前端
	npm run dev
	```
8. 在腾讯云轻量应用服务器防火墙页面，放行TCP端口`8080,9090,9528`。如果是云服务器，则修改[安全组](https://cloud.tencent.com.cn/document/product/215/39790)。
![防火墙配置](https://truetechlabs-1259203851.cos.ap-shanghai.myqcloud.com/picgo202404151240899.png)
9. 在浏览器中打开：http://云服务器IP:9528 即可看到前端页面。如果出现network error等网络报错，请按步骤6更换IP后重启项目。
10. 使用tape对项目进行压力测试（仅做测试demo，信息上链时有bug，建议使用课程里的caliper）
根据blockchain/chaincode/chaincode/smartcontract.go中的合约函数的签名，编写压测的参数，需要修改的内容是tape目录下的yaml文件中的args。args第一个参数是函数名，后面的参数是函数的参数。例如：
	```yaml
	args:
	# 函数名
	- RegisterUser
	# userID string
	- 1
	#userType string
	- randomString8
	# realInfoHash string
	- randomString8
	```
执行`./tape --config config_register.yaml -n 1`即可完成用户1的注册，然后可以对农产品上链操作与获取用户信息函数进行压测。更多的压测案例可以根据合约函数的签名进行修改。
附农产品上链操作与获取用户信息函数进行压测操作指令：

```	bash
./tape --config config_invoke.yaml -n 100
./tape --config config_query.yaml -n 100
```

#### 六、关闭项目与重新运行步骤
##### 关闭项目：
1. 前端（`npm run dev`界面）与后端（`go run main.go`界面：

	使用键盘组合键：`ctrl+c`

2. 区块链部分：

	在`fabric-trace/blockchain/network`目录`./stop.sh`，此步骤会清理所有的区块链、Mysql中的数据。

##### 开发模式启动项目：
1. 在`fabric-trace/blockchain/network`目录
`./start.sh` 如果遇到报错可以执行以下命令后再试：
执行清理所有的容器指令：
`docker rm -f $(docker ps -aq)`
2. 在`fabric-trace/application/backend`目录下： 执行： `go run main.go`
3. 在`fabric-trace/application/web`目录下： 执行：
`npm run dev`
4. 在http://服务器IP:9528打开

##### 生产环境部署项目(后台运行，访问速度更快)
1. 在`fabric-trace/blockchain/network`目录
`./start.sh` 如果遇到报错可以执行以下命令后再试：
执行清理所有的容器指令：
`docker rm -f $(docker ps -aq)`
2. 在`fabric-trace/application`目录下： 执行： `./start_prod.sh`
3. 在http://服务器IP:9090打开

	注意：此方式部署项目会在后台运行，如果后续遇到端口号占用可以尝试关闭占用9090端口号的进程，可以参考：
	[解决端口占用 bind:address already in use](https://blog.csdn.net/qq_41575489/article/details/137434008?spm=1001.2014.3001.5501)

#### 七、本项目相关的后续计划：

 本项目目前不够完美，将持续维护，欢迎给项目点亮Star与B站三连，非常感谢！与B站官方合作，本项目代码讲解与二次开发课程已发布，限时特价219元，支持试看: [B站：Fabric V2.5通用溯源项目讲解与二次开发课程](https://www.bilibili.com/cheese/play/ss15923?bsource=link_copy),购买课程将赠送[《Fabric项目学习笔记》](https://blog.csdn.net/qq_41575489/article/details/128637560)中所有与本项目相关的资料。购买课程与订阅专栏均配套社群，方便交流与答疑。


#### 八、特别提示：
1. 使用虚拟机时区块链浏览器有时候会出现无法访问的情况，可以尝试重启浏览器容器。
2. 为了减少用户运行本项目时的难度，区块链目录的start.sh脚本在启动区块链时同时会清理掉所有的历史数据！如果重启机器后不希望清理原来的数据启动区块链，可以使用指令：`docker start $(docker ps -aq)`启动所有节点

#### 常见问题总结（检查第一个报错的位置）
上述部署步骤已经上百人次验证并顺利完成，如果您通过上述步骤未能运行项目，请检查环境是否与本项目要求的一致，任何修改或遗漏步骤都可能引起项目不能正常运行，请严格按照视频与文章步骤再次尝试或查看以下常见问题列表。若还是有问题请在[B站项目搭建视频](https://www.bilibili.com/video/BV1Ar421H7TK)评论区查看其他人的留言是否有相同的问题，如果还是没有解决请在视频下评论问题并附上与【安装步骤不一致的地方】或【第一个遇到的报错】，如果问题不够明确，我们也很难帮助到您。购买[B站：Fabric V2.5通用溯源项目讲解与二次开发课程](https://www.bilibili.com/cheese/play/ss15923?bsource=link_copy)可以加入配套社群，方便本项目交流与答疑。如果需要远程搭建服务或商业合作请填写收集表，对于政府等机构公益项目可以提供免费技术支持：[【腾讯文档】本项目搭建服务或商务合作意向收集](https://docs.qq.com/form/page/DQ1hIck5OQkNGQXF2)

1. 需要给机器安装mysql吗？
按照项目搭建过程即可部署好mysql，mysql容器与区块链节点一起启动，因此不需要单独安装mysql。
2. docker镜像拉取过慢或提示timeout
按照步骤docker换源并重启再试。
3. 安装链码时报错：exec: ""go"": executable file not found in $PATH
可能是go环境未安装好；可能使用了sudo，不要与安装步骤不一致！
4. jq:未找到命令
漏掉安装步骤中的安装jq，使用sudo apt install jq即可解决
5. docker权限报错：docker: permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock:
重新执行安装步骤中的：

	```bash
	#添加当前用户到docker用户组 
	sudo usermod -aG docker $USER 
	newgrp docker 
	```

6. 提示mysql或3337端口错误
mysql未启动，重启区块链网络部分可以一并重启mysql。
7. 前端提示network error、timeout
修改好IP后重启整个项目；检查防火墙是否放行相关端口。
8. 前端登录页面提示404
检查是否修改好IP并重启前端服务器，除了IP不要修改任何字符。
9. npm run dev 不能启动前端
检查npm install是否完整把所有包装上了。
10. Chaincode installation on peerg.orgl has failed
Deploying chaincode failed
此问题并非第一个报错，仔细检查之前的步骤中第一个报错，然后与上述问题对照。
11. eslint相关报错 此报错为代码规范性报错，不影响运行，可以根据提示自行修复（可不修复）
12. 其他问题
使用全新Ubuntu20.04系统，重新严格按照步骤再试，出现第一个报错后在视频下留言。



## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=TrueTechLabs/fabric-trace&type=Date)](https://star-history.com/#TrueTechLabs/fabric-trace&Date)",FAUX
TwitchPlaysPokemon/pbr-movesets,Application System,Documentations,2025-05-06T22:13:59Z,2024-12-22T23:15:50Z,0,0,0,0,0,0,0,0,2016-09-27T03:41:05Z,2025-03-29T14:43:51Z,13717,5,Python,VRAI,22,FAUX,3,,3,,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,46,"# pbr-movesets

## What are the .YAML Files?
The .YAML files within this repository are all to help inject Pokemon within Twitch Plays Pokemon, Battle Revolution.
## Making the .YAML Files
A blank Pokemon .YAML file will generally look like this (may vary depending on meta/tags):
```
species:
setname:
tags: []
item: []
ability: []
moves:
    - []
    - []
    - []
    - []
combinations:
    - []
separations:
    - []
gender: [m, f]
happiness: 255
biddable: True
rarity: 1.0
ball: []
nature:
ivs: {hp: , atk: , def: , spA: , spD: , spe: }
evs: {hp: , atk: , def: , spA: , spD: , spe: }
```

### Things NOT necessary to include:
- __Shiny__ 
  - _Hidden goes with Shiny._
- __Ingamename and displayname__ 
  - _Both only added if a nickname exceeds the maximum of 10 Characters for ingamename._)
- __Additions + suppressions__ 
  - _If there aren't any._
- __Levels__ 
  - _Unless if they differ from the standard Level 100._

### Things that ARE necessary to include:
- __Species__
   - _All of one species for a meta would be included within the same file._
- __Setname__
  - _There can't be any of the same setnames._
- __Tags__
  - _Usually just the meta tag._
- __Items__
  - _Put [null] if there is none._
- __Ability__ 
  - _Put [null] if there is none._
- __Moves__
  - _If there are less than 4 moves, don't include any after the ones it has._
- __Gender__
  - _If there is no gender, do not include._
- __Happiness__
  - _Default 255, max_
- __Biddable__
  - _Yes or No._
- __Rarity__
  - _0.0 means it does not appear within matchmaking._
- __Ball__
  - _No apricot balls._
- __Nature__
  - _Put [null] if there is none_.
- __IVs + EVs__
  - _If there's all of one (for example, something has all 31 IVs), you can put the number itself instead of the full format._

## Editing .YAMLS Efficiently
Download the [Github desktop app](https://desktop.github.com/) and [Atom](https://atom.io/). Using the Github app, simply copy the repository for pbr-movesets. You can then open in Atom to mass edit / upload / etc. multiple files. Once you update something, save your changes on Atom and it will show on the Github app. Only thing left is to remember to title your changes and to actually commit them to the repository (under the master branch).

---
__If there are any questions regarding anything, feel free to ask.__",VRAI
uhhhci/immersive-ngp,Toolkit,Toolkit,2024-06-24T15:13:44Z,2022-11-08T14:58:56Z,2,0,0,0,0,0,0,0,2022-11-08T14:35:18Z,2025-03-21T00:53:11Z,250566,307,C,VRAI,25,FAUX,13,,13,"We present the first open-source VR NERF Unity package that brings high resolution, low-latency, 6-DOF NERF rendering to VR. This work is based on Nvidia's ground breaking instant-ngp technique.",FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,3,"# Immersive Neural Graphics Primitives

In this project, we present immersive NGP, the first open-source VR NERF Unity package that brings high resolution, low-latency, 6-DOF NERF rendering to VR. This work is based on Nvidia's ground breaking [instant-ngp](https://github.com/NVlabs/instant-ngp) technique. Current version uses [this commit](https://github.com/NVlabs/instant-ngp/commit/54aba7cfbeaf6a60f29469a9938485bebeba24c3) of instant-ngp.

## Features

* Stereoscopic, 6-DOF, real-time, low-latency NERF VR rendering in Unity. 
        <img src="".\images\stereo-nerf-demo.gif""
        alt=""plugin-files.PNG""
        style=""float: center; margin-right: 10px; height:300px;"" />

* DLSS Support for rendering at higher framerate.

* 6-DOF continuous locomotion in VR.

* Offline volume image slices rendering via [Unity Volume Rendering Toolkit](https://github.com/mlavik1/UnityVolumeRendering).

* Integration with [MRTK 2.8](https://github.com/microsoft/MixedRealityToolkit-Unity) for building mixed reality applications with NERF. 

## Magic NeRF Lens

* Now avaliable on the [magic-nerf-lens](https://github.com/uhhhci/immersive-ngp/tree/magic-nerf-lens) feature branch. More update && tutorials to come .. 

* Checkout our [Magic NeRF Lens paper]( https://doi.org/10.3389/frvir.2024.1377245 ) to see how we merge a NeRF with a CAD model in VR for viewing large-scale scene with one-to-one real-world scale.

    <img src="".\images\basic-magic-nerf-lens.gif""
    alt=""plugin-files.PNG""
    style=""float: center; margin-right: 10px; height:250px;"" />

* NeRF model manipulation, crop-box adjustment, FoV adjustment.
    <p float=""left"">
    <img src="".\images\manipulate.gif""
    alt=""plugin-files.PNG""
    style=""margin-right: 10px; height:200px;"" />
    <img src="".\images\crop_box.gif""
    alt=""plugin-files.PNG""
    style=""margin-right: 10px; height:200px;"" />
    <img src="".\images\FoV.gif""
    alt=""plugin-files.PNG""
    style=""margin-right: 10px; height:200px;"" />
    </p>

* NeRF model editing and model saving.
    <p float=""left"">
    <img src="".\images\editing.gif""
    alt=""plugin-files.PNG""
    style=""margin-right: 10px; height:200px;"" />
    <img src="".\images\3DNeRFDrawing.gif""
    alt=""plugin-files.PNG""
    style=""margin-right: 10px; height:200px;"" />
    </p>
* Depth textures in Unity and depth occlusion effects; AR NeRF. 
    <p float=""left"">
    <img src="".\images\occlusion.gif""
    alt=""plugin-files.PNG""
    style=""margin-right: 10px; height:200px;"" />
    <img src="".\images\AR-NeRF.gif""
    alt=""plugin-files.PNG""
    style=""margin-right: 10px; height:200px;"" />
    </p>

## Dependencies

* Unity 2019.4.29 ( Use the legacy XR manager for compatibility with OpenVR)
* [instant-ngp](https://github.com/NVlabs/instant-ngp)
* Unity OpenVR desktop plugin && SteamVR
* Microsoft Mixed Reality Toolkit MRTK 2.8 (already included in the Unity project)
* OpenGL Graphics API
* Current version of the repository was tested on Windows 10, Windows 11, using a Oculus Quest 2. 

## Installation

1. Clone this repository: ```git clone --recursive https://github.com/uhhhci/immersive-ngp```

2. Make sure you have all the dependencies for [instant-ngp](https://github.com/NVlabs/instant-ngp) installed before proceed.

3. Update dependencies for submodules

    ```
    git submodule sync --recursive
    git submodule update --init --recursive
    ```

4. Build the instant-ngp project, similar to the build process for the original instant-ngp project.

    ```
    cmake . -B build
    cmake --build build --config RelWithDebInfo -j
    ```

5. After succesful build, copy the following plugin files from ```\instant-ngp\build\``` folder to the ```\stereo-nerf-unity\Assets\Plugins\x86_64``` folder.

    <img src="".\images\plugin-files.PNG""
    alt=""plugin-files.PNG""
    style=""float: center; margin-right: 10px; height:150px;"" />

5. Now instant-ngp can be loaded as native plugins via Unity.

## Usage for Immersive NERF Rendering
1. For Oculus Quest 2, Lunch Oculus Rift, and connect the headset to the PC via Link Cable, or Air Link. 
2. Launch SteamVR, make sure that SteamVR detects your headset and controllers. 
3. For a quick demo train a model using the fox scene via:
   ```
   build\testbed.exe --scene ..\data\nerf\fox
   ```
   and safe a snapshot of the instant-ngp model through Instant-ngp > Snapshot > Save 
5. Open the stereo-nerf-unity Unity project with Unity 2019.4.29. 
6. For a quick VR test of your own NERF scene, go to the ```Assets\NERF_NativeRendering\Scenes\XRTest``` scene.
7. Copy the path to your nerf model, images folder, and transform.json file to the ``` Stereo Nerf Renderer``` in the ```Nerf path``` parameters, as ilustrated below.
   
    <img src="".\images\stereo-nerf-gameobj.PNG""
    alt="".\images\stereo-nerf-gameobj.PNG""
    style=""float: center; margin-right: 10px; height:300px;"" />

    (Note: please generate the nerf model using [this instant-ngp commit](https://github.com/NVlabs/instant-ngp/commit/54aba7cfbeaf6a60f29469a9938485bebeba24c3) and above, or just use the instant-ngp instance included in this repo).

6. Adjust DLSS settings, and image resolution as you like. 
7. Now you can run the scene in Editor :)
8. Use the joystick of the VR controllers for locomotion. 

## Common Questions & Troubleshoot

1. **How to reach good framerate and lower latency**

    Beside having a good GPU, it is highly recommended to turn on DLSS support in Unity, also when building the native plugin. 

    [The instant-ngp commit](https://github.com/NVlabs/instant-ngp/commit/54aba7cfbeaf6a60f29469a9938485bebeba24c3) we use also allows saving aabb cropping in the pre-trained model snapshot. If you adjust the aabb cropping when training the model, it will saved and be loaded in Unity as well. Reducing aabb cropping could reduce the render volume, thus save some computational power. 

2. **Locomotion doesn't work.**

    Make sure that SteamVR detects both of your controllers before starting the scenes in the Editors. 

3. **Is VR headset required to run the demo?**
    
    Yes, the main focus of this repository target immersive applications with a VR headset + steamVR. Please feel free to fork this repo and build immersive desktop applications. 


## Contributions

We welcome community contributions to this repository. 

## Thanks

Many thanks to the authors of these open-source repositories:

1. [instant-ngp](https://github.com/NVlabs/instant-ngp)
2. [Unity Volume Rendering](https://github.com/mlavik1/UnityVolumeRendering)
3. [Mixed Reality Toolkit](https://github.com/microsoft/MixedRealityToolkit-Unity)
4. [Unity Native Tool](https://github.com/mcpiroman/UnityNativeTool)


## Citations

```bibtex
@ARTICLE{magic-nerf-lens,
        AUTHOR={Li, Ke  and Schmidt, Susanne  and Rolff, Tim  and Bacher, Reinhard  and Leemans, Wim  and Steinicke, Frank },
        TITLE={Magic NeRF lens: interactive fusion of neural radiance fields for virtual facility inspection},
        JOURNAL={Frontiers in Virtual Reality},
        VOLUME={5},
        YEAR={2024},
        URL={https://www.frontiersin.org/journals/virtual-reality/articles/10.3389/frvir.2024.1377245},
        DOI={10.3389/frvir.2024.1377245},
        ISSN={2673-4192}}

@misc{immersive-ngp,
      doi = {10.48550/ARXIV.2211.13494},
      url = {https://arxiv.org/abs/2211.13494},
      author = {*Li, Ke and *Rolff, Tim and Schmidt, Susanne and Bacher, Reinhard and Frintrop, Simone and Leemans, Wim and Steinicke, Frank},
      title = {Immersive Neural Graphics Primitives},
      publisher = {arXiv},
      year = {2022}} (*These authors contributed equally to the work.)

```
Contact: ke.li1@desy.de, tim.rolff@uni-hamburg.de
 
## Acknowledgment

This work was supported by DASHH (Data Science in Hamburg - HELMHOLTZ Graduate School for the Structure of Matter) with the Grant-No. HIDSS-0002, and the German Federal Ministry of Education and Research (BMBF).

## License

Please check [here](LICENSE.txt) to view a copy of Nvidia's license for instant-ngp and for this repository.",FAUX
UNINETTSigma2/helm-charts,Application System,Documentations,2025-02-26T08:24:39Z,2024-09-13T09:24:58Z,0,4,0,0,0,0,0,0,2017-06-08T10:52:29Z,2025-02-26T08:24:42Z,84050,14,Mustache,VRAI,11,FAUX,1,,1,Helm charts curated by Sigma2 AS,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,18,"[![Build Status](https://travis-ci.org/UNINETTSIgma2/helm-charts.svg?branch=master)](https://travis-ci.org/UNINETTSigma2/helm-charts)

# Helm repository
Github pages is used to automatically host the Helm repos in this git repo.
Currently, two Helm repos are in use.
- Stable: all charts in this repo should be usable in production
- Testing: charts that are currently being testing, and are thus not ready for production

To add the stable repo to a local helm client, run:

```
helm repo add researchlab https://uninettsigma2.github.io/helm-charts/repos/stable
```

The primary Docker files used in this repo is hosted in the following [repo](https://github.com/UNINETTSigma2/helm-charts-dockerfiles), which uses [quay.io/uninett](https://quay.io/organization/uninett) to host the actual images.

### Documentation
- [Wiki](https://github.com/Uninett/helm-charts/wiki/)
- [Adding a new package](https://github.com/Uninett/helm-charts/wiki/Creating-a-new-package)",VRAI
veraison/services,Toolkit,Documentations,2025-05-02T21:41:40Z,2025-03-05T12:53:51Z,0,0,0,0,0,0,0,0,2022-07-21T13:50:53Z,2025-04-04T11:17:59Z,132837,27,HTML,VRAI,21,FAUX,51,,51,Attestation verification services based on Veraison components,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,12,,VRAI
VergeOps/k8s-rvstore,Documentations,Documentations,2024-02-04T16:38:55Z,2021-10-06T03:51:54Z,0,0,0,0,0,0,0,2,2018-11-30T22:47:59Z,2025-01-05T00:10:24Z,197730,48,Go,VRAI,109,FAUX,49,,49,RV Store demo application for the Docker/Kubernetes course,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,2,"# VergeOps Docker and Kubernetes courseware

This is the courseware for Tim Solley's Docker and Kubernetes courses. It includes exercise files for lectures as well as a Hackthon application for students to practice the concepts they learn in class.

# Setup Instructions
You'll need to install a container runtime for this course. You have a couple of good options:
## Docker Desktop
<img src=""https://www.docker.com/wp-content/uploads/2022/03/horizontal-logo-monochromatic-white.png"" style=""padding:10px;"" width=""100""/>

With Docker Desktop, you'll get a managed Kubernetes cluster after some initial setup.
**IMPORTANT**: This has a potential license issue, as Docker charges larger companies a per-seat license fee. It is free for your personal/educational use. It is free for use during class, but a license requirement applies once the class is over. If you're unsure, check with your company management or choose the Rancher Desktop option below. For more details on Docker's license and whether it applies to you at work, visit their [FAQ](https://www.docker.com/pricing/faq/#subscriptionandlicensing).
1. Visit [docker.com](https://www.docker.com)
1. Click the install link in the center of the page. Be careful if you're on a Mac to choose the correct chip. It defaults to Intel Macs. If you're unsure, look at the Apple menu on your computer, then `About This Mac`.
1. Once installed, open the Docker application.
1. You'll have Docker menu on your computer (whale logo). Click it and choose the `Signup/Signin to Docker Hub` option. This will open a browser to Docker Hub where you can login or sign up for a free account.
1. (For Kubernetes courses) Install Kubernetes
    1. Once you're signed in, open the Docker menu again and choose `Preferences`. Then go to the `Kubernetes` section. Check the box for `Enable Kubernetes` then click the `Apply and Restart` button. This will begin the Kubernetes setup process which usually takes 10-15 minutes.
    1. Once finished, you'll be able to open a terminal and run `kubectl get all` and get some output that is clearly not a connection error.

## Rancher Desktop
<img src=""https://www.rancher.com/assets/img/brand-guidelines/assets/logos/png/color/rancher-logo-stacked-color.png"" style=""padding:10px;"" width=""100"">

A great open-source option with no license issues. Rancher Desktop comes with automatic Kubernetes support.
1. Visit [rancherdesktop.io](https://www.rancherdesktop.io)
1. Scroll down to the installer section and pick the correct installer. Be careful if you're on a Mac to choose the correct chip. It defaults to Intel Macs. If you're unsure, look at the Apple menu on your computer, then `About This Mac`.
1. Follow the installation instructions. When prompted for for container engine, choose `dockerd (moby)`. When prompted for a Kubernetes version, choose the latest stable release.

## Optional Multi-Node Kubernetes Cluster
k3d offers a great way to start up a multi-node cluster on your local workstation using Docker containers as Kubernetes nodes. This allows you to work in a simulated large cluster environment to practice cluster management. I've tested this using Rancher Desktop, but it should work fine for Docker Desktop.
1. Visit [k3d.io](https://www.k3d.io) for more detailed instructions.
1. In a terminal, run `wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash`. Alternatively, you can use Homebrew with `brew install k3d`
1. Set up your cluster. In this repository, navigate to the `k3d` directory. Run `make create-cluster` or open the `Makefile` and grab the command and run it.
1. To switch your Kubernetes context to the k3d cluster, run `kubectl config use-context k3d-k3d`


# Repository Tour
This repository is used for all of Tim Solley's Docker and Kubernetes courses, beginning through expert. As such, not all areas will be relevant to all students.

## Docker Classes
* docker-compose.yml - used for Compose running of the RV Store application
* Microservices - The full source code for the RV Store example application. Includes all code, Dockerfiles, Makefiles with useful commands, etc.

## Kubernetes Classes
* Advanced Kubernetes - contains a variety of sub directories with files on advanced topics such as RBAC, initContainers, high availability, etc.
* argocd - scripts for installing ArgoCD into a cluster
* elk - scripts for installing the EFK (Elasticsearch, FluentD, Kibana) stack into a cluster
* exercises/day 4 - labs for various topics
* extras - helpful kubectl commands
* helm - the RV Store application built using the Helm package manager
* ingress - an ingress and ingress controller configured for the RV Store application
* Microservices - The full source code for the RV Store example application. Includes all code, Dockerfiles, Makefiles with useful commands, etc.
* minikube - Kubernetes manifest files for the RV Store example application.
* rvstore_hackathon - contains information relevant to the RV Store Hackathon. For instructions, view the `Kubernetes-RV Store Hackathon.pdf` file.

---

### All content is copyright VergeOps, LLC
For more information, visit [VergeOps](https://www.vergeops.com). Questions about this repository can be directed to [Tim Solley](mailto:tsolley@vergeops.com)",FAUX
vfarcic/cncf-demo,Documentations,Documentations,2025-05-15T12:28:05Z,2025-01-30T18:26:31Z,0,0,0,0,4,1,0,3,2022-11-11T19:47:35Z,2025-04-06T19:55:51Z,3084,222,Go,VRAI,56,FAUX,1,,1,,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,6,"# Choose Your Own Adventure: The Treacherous Trek to Production

From the moment of their inception as source code on the developer's laptop, our hero knows that they are destined for great things. They long to be a real, running application, living in production, serving end users! But the epic journey to production is an arduous one, filled with cascading choices - choices concerning app design, testing, security, container image building, deployment strategy, and observability, to name a few. And who knows what other unseen forces lurk in the shadows! One wrong step could be catastrophic.

## Pitch

It is up to us, the audience, to guide our hero; and to help them grow from source code to container image, to their final form as a running application in production. In this ""Choose Your Own Adventure""-style journey, Whitney and Viktor will present a linear view of all of the choices that an anthropomorphized application must make as they try to find their way to the fabled land of production. Throughout the trek, the audience will vote to choose which path our hero application will take. Can we navigate CNCF projects and avoid pitfalls and dead-ends to get our application to production?

Join us if you dare!  This is not for the faint of heart!

## Rules

* Non-CNCF projects lead to a dead end ☠

## The Adventure!

The best place to start the adventure is the beginning. Perform the [setup](manuscript/setup/dev.md) steps and, from there, start the first chapter by going to [Build Container Image In Dev Environments](manuscript/build-container-image/README.md). As an alternative, you can use graph below to navigate through the adventure.

Almost all items in the graphs contain a link if you prefer to jump straight into a specific part of the adventure. If you do so, please note that the steps work only if one start from the beginning of any of the chapters (e.g., Development, Production, etc.).

*For reasons I cannot explain, the links do not work if they are not opened in a separate tab.*

```mermaid
flowchart TD

    subgraph ""Legend""

        red(Not yet implemented)
        style red fill:red
        blue{{Make a choice}}
        style blue fill:blue
        green(Chosen by viewers)
        style green fill:green

    end
```

```mermaid
flowchart TD

    subgraph Development

        %% -----------
        %% -- Setup --
        %% -----------
        setup-dev((Setup))
        click setup-dev ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/setup/dev.md""

        %% -- Setup Connections --
        setup-dev-->bci

        %% ---------------------------
        %% -- Build Container Image --
        %% ---------------------------
        bci{{Build Container Image}}
        click bci ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/build-container-image/README.md""
        style bci fill:blue
        bci-kbld(Carvel kbld)
        click bci-kbld ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/build-container-image/kbld.md""
        bci-lima(Lima)
        click bci-lima ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/build-container-image/lima.md""
        bci-buildpacks(Cloud Native Buildpacks / CNB)
        style bci-buildpacks fill:green
        click bci-buildpacks ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/build-container-image/buildpacks.md""

        %% -- Build Container Image Connections --
        bci --> bci-kbld & bci-lima & bci-buildpacks --> registry

        %% -----------------------------------------
        %% -- Store Container Image in a Registry --
        %% -----------------------------------------
        registry{{Store Container Image In A Registry}}
        click registry ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/registry/README.md""
        style registry fill:blue
        registry-docker-hub(Docker Hub)
        click registry-docker-hub ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/registry/docker-hub.md""
        registry-harbor(Harbor)
        click registry-harbor ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/registry/harbor.md""
        style registry-harbor fill:green
        registry-dragonfly(Dragonfly)
        click registry-dragonfly ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/registry/dragonfly.md""
        registry --> registry-docker-hub & registry-harbor & registry-dragonfly --> ddd

        %% --------------------------------------
        %% -- Define And Deploy The App To Dev --
        %% --------------------------------------
        ddd{{Define And Deploy The App To Dev}}
        click ddd ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/define-deploy-dev/README.md""
        style ddd fill:blue
        ddd-helm(Helm)
        click ddd-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/define-deploy-dev/helm.md""
        ddd-kustomize(Kustomize)
        click ddd-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/define-deploy-dev/kustomize.md""
        ddd-carvel(Carvel ytt)
        click ddd-carvel ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/define-deploy-dev/carvel-ytt.md""
        style ddd-carvel fill:green
        ddd-cdk8s(""CDK For Kubernetes (cdk8s)"")
        click ddd-cdk8s ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/define-deploy-dev/cdk8s.md""
        ddd --> ddd-helm & ddd-kustomize & ddd-carvel & ddd-cdk8s --> https

        %% ---------------
        %% -- Use HTTPS --
        %% ---------------
        https{{Use HTTPS}}
        click https ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/https/README.md""
        style https fill:blue
        https-cert-manager(cert-manager)
        click https-cert-manager ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/https/cert-manager.md""
        style https-cert-manager fill:green
        https-cert-manager-helm(App as Helm)
        click https-cert-manager-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/https/cert-manager-helm.md""
        https-cert-manager-kustomize(App as Kustomize)
        click https-cert-manager-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/https/cert-manager-kustomize.md""
        https-cert-manager-carvel(App as Carvel ytt)
        click https-cert-manager-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/https/cert-manager-carvel.md""
        style https-cert-manager-carvel fill:green
        https-cert-manager-cdk8s(App as cdk8s)
        click https-cert-manager-cdk8s ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/https/cert-manager-cdk8s.md""
        https--> https-cert-manager--> https-cert-manager-helm & https-cert-manager-kustomize & https-cert-manager-carvel & https-cert-manager-cdk8s --> db

        %% --------------------------------
        %% -- Setup PostgreSQL DB In Dev --
        %% --------------------------------
        db{{Setup PostgreSQL DB In Dev}}
        click db ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/README.md""
        style db fill:blue
        db-helm(Helm Chart)
        click db-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/helm.md""
        db-helm-helm(App as Helm)
        click db-helm-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/helm-helm.md""
        db-helm-kustomize(App as Kustomize)
        click db-helm-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/helm-kustomize.md""
        db-helm-cdk8s(App as cdk8s)
        click db-helm-cdk8s ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/helm-cdk8s.md""
        db-helm-carvel(App as Carvel ytt)
        click db-helm-carvel ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/helm-carvel.md""
        db-crossplane-local(Crossplane Composition In Kubernetes)
        click db-crossplane-local ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/crossplane-local.md""
        db-crossplane-cloud(Crossplane Composition In Cloud)
        click db-crossplane-cloud ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/crossplane-cloud.md""
        style db-crossplane-cloud fill:green
        db-crossplane-google(Google Cloud)
        click db-crossplane-google ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/crossplane-google.md""
        db-crossplane-aws(AWS)
        click db-crossplane-aws ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/crossplane-aws.md""
        style db-crossplane-aws fill:green
        db-crossplane-azure(Azure)
        click db-crossplane-azure ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/crossplane-azure.md""
        db-crossplane-helm(App as Helm)
        click db-crossplane-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/crossplane-helm.md""
        db-crossplane-carvel(App as Carvel ytt)
        click db-crossplane-carvel ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/crossplane-carvel.md""
        style db-crossplane-carvel fill:green
        db-crossplane-kustomize(App as Kustomize)
        click db-crossplane-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/crossplane-kustomize.md""
        db-crossplane-cdk8s(App as cdk8s)
        click db-crossplane-cdk8s ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db/crossplane-cdk8s.md""
        db --> db-helm & db-crossplane-local & db-crossplane-cloud
        db-helm --> db-helm-helm & db-helm-kustomize & db-helm-cdk8s & db-helm-carvel --> db-schema
        db-crossplane-local --> db-crossplane-helm & db-crossplane-kustomize & db-crossplane-cdk8s & db-crossplane-carvel
        db-crossplane-cloud --> db-crossplane-google & db-crossplane-aws & db-crossplane-azure --> db-crossplane-helm & db-crossplane-kustomize & db-crossplane-cdk8s & db-crossplane-carvel --> db-schema

        %% ----------------------
        %% -- Manage DB Schema --
        %% ----------------------
        db-schema{{Manage DB Schema}}
        click db-schema ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-schema/README.md""
        style db-schema fill:blue
        db-schema-schemahero(SchemaHero)
        click db-schema-schemahero ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-schema/schemahero.md""
        style db-schema-schemahero fill:green
        db-schema-liquibase(Liquibase)
        click db-schema-liquibase ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-schema/liquibase.md""
        db-schema-schemahero-helm(App as Helm)
        click db-schema-schemahero-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-schema/schemahero-helm.md""
        db-schema-schemahero-kustomize(App as Kustomize)
        click db-schema-schemahero-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-schema/schemahero-kustomize.md""
        db-schema-schemahero-cdk8s(App as cdk8s)
        click db-schema-schemahero-cdk8s ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-schema/schemahero-cdk8s.md""
        db-schema-schemahero-carvel(App as Carvel ytt)
        click db-schema-schemahero-carvel ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-schema/schemahero-carvel.md""
        style db-schema-schemahero-carvel fill:green
        db-schema --> db-schema-liquibase & db-schema-schemahero
        db-schema-schemahero --> db-schema-schemahero-helm & db-schema-schemahero-kustomize & db-schema-schemahero-cdk8s & db-schema-schemahero-carvel --> develop

        %% ---------------------
        %% -- Develop The App --
        %% ---------------------
        develop{{Develop The App}}
        click develop ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/develop/README.md""
        style develop fill:blue
        develop-telepresence(Telepresence)
        click develop-telepresence ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/develop/telepresence.md""
        develop-devspace(DevSpace)
        click develop-devspace ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/develop/devspace.md""
        style develop-devspace fill:green
        develop-nocalhost(Nocalhost)
        click develop-nocalhost ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/develop/nocalhost.md""
        develop-devspace-kustomize(App as Kustomize)
        click develop-devspace-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/develop/devspace-kustomize.md""
        develop-devspace-cdk8s(App as cdk8s)
        click develop-devspace-cdk8s ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/develop/devspace-cdk8s.md""
        develop-devspace-helm(App as Helm)
        click develop-devspace-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/develop/devspace-helm.md""
        develop-devspace-carvel(App as Carvel ytt)
        click develop-devspace-carvel ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/develop/devspace-carvel.md""
        style develop-devspace-carvel fill:green
        develop-devfile(Devfile)
        style develop-devfile fill:red
        develop --> develop-telepresence & develop-devspace & develop-nocalhost & develop-devfile
        develop-telepresence & develop-nocalhost & develop-devfile --> dev-done
        develop-devspace --> develop-devspace-kustomize & develop-devspace-cdk8s & develop-devspace-helm & develop-devspace-carvel --> dev-done

        dev-done((Chapter End))

    end
```

```mermaid
flowchart TD

    subgraph Production

        %% -----------
        %% -- Setup --
        %% -----------
        setup-prod((Setup))
        click setup-prod ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/setup/prod.md""

        %% -- Setup Connections --
        setup-prod-->cluster

        %% -------------
        %% -- Cluster --
        %% -------------
        cluster{{Create a Cluster}}
        click cluster ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/cluster/README.md""
        style cluster fill:blue
        cluster-crossplane(Crossplane)
        click cluster-crossplane ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/cluster/crossplane.md""
        style cluster-crossplane fill:green
        cluster-crossplane-google(Google Cloud)
        click cluster-crossplane-google ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/cluster/cluster-crossplane-google.md""
        cluster-crossplane-aws(AWS)
        click cluster-crossplane-aws ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/cluster/cluster-crossplane-aws.md""
        style cluster-crossplane-aws fill:green
        cluster-crossplane-azure(Azure)
        click cluster-crossplane-azure ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/cluster/cluster-crossplane-azure.md""
        cluster-cluster-api(Cluster API)
        click cluster-cluster-api ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/cluster/cluster-api.md""
        capi-google(Google Cloud)
        click capi-google ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/cluster/capi-google.md""
        capi-aws(AWS)
        click capi-aws ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/cluster/capi-aws.md""
        capi-azure(Azure)
        click capi-azure ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/cluster/capi-azure.md""
        cluster --> cluster-crossplane --> cluster-crossplane-google & cluster-crossplane-aws & cluster-crossplane-azure --> gitops
        cluster --> cluster-cluster-api --> capi-google & capi-aws & capi-azure --> gitops

        %% ------------
        %% -- GitOps --
        %% ------------
        gitops{{GitOps}}
        click gitops ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/gitops/README.md""
        style gitops fill:blue
        gitops-flux(Flux)
        click gitops-flux ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/gitops/flux.md""
        gitops-argocd(Argo CD)
        click gitops-argocd ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/gitops/argocd.md""
        style gitops-argocd fill:green
        gitops-kapp(Carvel kapp-controller)
        click gitops-kapp ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/gitops/kapp.md""
        gitops --> gitops-flux & gitops-argocd & gitops-kapp --> ingress

        %% -------------
        %% -- Ingress --
        %% -------------
        ingress{{Ingress}}
        click ingress ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/ingress/README.md""
        style ingress fill:blue
        ingress-contour(Contour With Envoy)
        click ingress-contour ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/ingress/contour.md""
        style ingress-contour fill:green
        ingress-nginx(NGINX)
        click ingress-nginx ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/ingress/nginx.md""
        emissary-ingress(Emissary-ingress With Envoy)
        click ingress-nginx ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/ingress/emissary-ingress.md""
        ingress-argocd(GitOps With Argo CD)
        click ingress-argocd ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/ingress/gitops-argocd.md""
        style ingress-argocd fill:green
        ingress-flux(GitOps Flux)
        click ingress-flux ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/ingress/gitops-flux.md""
        ingress-kapp(GitOps Carvel kapp-controller)
        click ingress-kapp ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/ingress/gitops-kapp.md""
        ingress-->ingress-contour & ingress-nginx & emissary-ingress --> ingress-argocd & ingress-flux & ingress-kapp --> app

        %% ----------------------------------
        %% -- Deploy The App To Production --
        %% ----------------------------------
        app{{Deploy The App To Production}}
        click app ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/app/README.md""
        style app fill:blue
        app-helm(App As Helm)
        click app-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/app/helm.md""
        app-kustomize(App As Kustomize)
        click app-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/app/kustomize.md""
        app-cdk8s(App As cdk8s)
        click app-cdk8s ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/app/cdk8s.md""
        app-carvel(App As Carvel ytt)
        click app-carvel ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/app/carvel.md""
        style app-carvel fill:green
        app --> app-helm & app-kustomize & app-cdk8s & app-carvel --> db-production

        %% --------------
        %% -- Database --
        %% --------------
        db-production{{Database}}
        click db-production ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-production/README.md""
        style db-production fill:blue
        db-production-crossplane(Crossplane)
        click db-production-crossplane ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-production/crossplane.md""
        style db-production-crossplane fill:green
        db-production-helm(App As Helm)
        click db-production-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-production/helm.md""
        db-production-kustomize(App As Kustomize)
        click db-production-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-production/kustomize.md""
        db-production-cdk8s(App As cdk8s)
        click db-production-cdk8s ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-production/cdk8s.md""
        db-production-carvel(App As Carvel ytt)
        click db-production-carvel ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/db-production/carvel.md""
        style db-production-carvel fill:green
        db-production --> db-production-crossplane --> db-production-helm & db-production-kustomize & db-production-cdk8s & db-production-carvel --> prod-done

        prod-done((Chapter End))
        
    end
```

```mermaid
flowchart TD

    subgraph Security

        %% -----------
        %% -- Setup --
        %% -----------
        setup-security((Setup))
        click setup-security ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/setup/security.md""

        %% -- Setup Connections --
        setup-security-->policies

        %% -----------------------------------
        %% -- Admission Controller Policies --
        %% -----------------------------------
        policies{{Admission Controller Policies}}
        style policies fill:blue
        click policies ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/README.md""
        kyverno(Kyverno)
        click kyverno ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/kyverno.md""
        style kyverno fill:green
        policies-opa(""Open Policy Agent (OPA) With Gatekeeper"")
        click policies-opa ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/gatekeeper.md""
        cloud-custodian(Cloud Custodian)
        click cloud-custodian ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/cloud-custodian.md""
        cloud-custodian-helm(App as Helm)
        click cloud-custodian-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/cloud-custodian-helm.md""
        cloud-custodian-kustomize(App as Kustomize)
        click cloud-custodian-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/cloud-custodian-kustomize.md""
        cloud-custodian-cdk8s(App as cdk8s)
        click cloud-custodian-cdk8s ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/cloud-custodian-cdk8s.md""
        cloud-custodian-carvel(App as Carvel ytt)
        click cloud-custodian-carvel ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/cloud-custodian-carvel.md""
        kubewarden(Kubewarden)
        click kubewarden ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/kubewarden.md""
        %% Wait with VAP until it's GA
        vac(Kubernetes Validating Admission Policy)
        style vac fill:red
        policies-helm(App as Helm)
        click policies-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/helm.md""
        policies-kustomize(App as Kustomize)
        click policies-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/kustomize.md""
        policies-cdk8s(App as cdk8s)
        click policies-cdk8s ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/cdk8s.md""
        policies-carvel(App as Carvel ytt)
        style policies-carvel fill:green
        click policies-carvel ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/policies/carvel.md""
        policies --> kyverno & policies-opa & kubewarden & vac --> policies-helm & policies-kustomize & policies-cdk8s & policies-carvel --> runtime-policies
        policies --> cloud-custodian --> cloud-custodian-helm & cloud-custodian-kustomize & cloud-custodian-cdk8s & cloud-custodian-carvel --> runtime-policies

        %% ----------------------
        %% -- Runtime Policies --
        %% ----------------------
        runtime-policies{{Runtime Policies}}
        style runtime-policies fill:blue
        click runtime-policies ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/runtime-policies/README.md""
        kube-armor(KubeArmor)
        click kube-armor ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/runtime-policies/kubearmor.md""
        style kube-armor fill:green
        falco(Falco)
        click falco ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/runtime-policies/falco.md""
        runtime-policies --> kube-armor & falco --> secrets

        %% ------------------------
        %% -- Secrets Management --
        %% ------------------------
        secrets{{Secrets Management In Kubernetes}}
        click secrets ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/README.md""
        style secrets fill:blue
        eso(""External Secrets Operator (ESO)"")
        click eso ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/eso.md""
        style eso fill:green
        sscsid(""Secrets Store CSI Driver (SSCSID)"")
        click sscsid ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/sscsid.md""
        secrets-google(Google Cloud)
        click secrets-google ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/google.md""
        secrets-aws(AWS)
        click secrets-aws ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/aws.md""
        style secrets-aws fill:green
        secrets-azure(Azure)
        click secrets-azure ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/azure.md""
        secrets-helm(App as Helm)
        click secrets-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/helm.md""
        secrets-kustomize(App as Kustomize)
        click secrets-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/kustomize.md""
        secrets-cdk8s(App as cdk8s)
        click secrets-cdk8s ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/cdk8s.md""
        secrets-carvel(App as Carvel ytt)
        click secrets-carvel ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/carvel.md""
        style secrets-carvel fill:green
        client-secrets{{Secrets Management Outside Kubernetes}}
        click client-secrets ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/client.md""
        style client-secrets fill:blue
        teller(Teller)
        click teller ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/teller.md""
        teller-aws(AWS)
        click teller-aws ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/teller-aws.md""
        teller-azure(Azure)
        click teller-azure ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/teller-azure.md""
        teller-google(Google Cloud)
        click teller-google ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/secrets/teller-google.md""
        sops(SOPS)
        style sops fill:red
        secrets --> eso --> secrets-google & secrets-aws & secrets-azure --> secrets-helm & secrets-kustomize & secrets-cdk8s & secrets-carvel --> client-secrets
        secrets --> sscsid
        client-secrets --> teller --> teller-aws & teller-azure & teller-google --> mtls
        client-secrets --> sops --> mtls

        %% -------------------------------------
        %% -- Mutual TLS And Network Policies --
        %% -------------------------------------
        mtls{{""Mutual TLS (mTLS) And Network Policies""}}
        click mtls ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/mtls/README.md""
        style mtls fill:blue
        mtls-istio(Istio)
        click mtls-istio ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/mtls/istio.md""
        mtls-linkerd(""LinkerD (SMI)"")
        click mtls-linkerd ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/mtls/linkerd.md""
        mtls-cilium(Cilium)
        click mtls-cilium ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/mtls/cilium.md""
        style mtls-cilium fill:green
        mtls-kuma(Kuma)
        style mtls-kuma fill:red
        mtls-network-service-mesh(Network Service Mesh)
        style mtls-network-service-mesh fill:red
        mtls --> mtls-istio & mtls-kuma & mtls-network-service-mesh & mtls-cilium & mtls-linkerd--> scanning

        %% --------------
        %% -- Scanning --
        %% --------------
        scanning{{Scanning}}
        click signing ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/signing/README.md""
        style scanning fill:blue
        kubescape(Kubescape)
        click kubescape ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/scanning/kubescape.md""
        style kubescape fill:green
        snyk(Snyk)
        click snyk ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/scanning/snyk.md""
        scanning --> kubescape & snyk --> signing

        %% -------------
        %% -- Signing --
        %% -------------
        signing{{Signing}}
        click signing ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/signing/README.md""
        style signing fill:blue
        notary(Notary)
        click notary ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/signing/notary.md""
        style notary fill:green
        sigstore(Sigstore)
        opc(Open Policy Containers)
        style opc fill:red
        signing --> notary & sigstore & opc --> identity

        %% -----------------------
        %% -- Workload Identity --
        %% -----------------------
        identity{{Workload Identity}}
        click identity ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/workload-identity/README.md""
        style identity fill:blue
        spire(SPIRE)
        style spire fill:green
        click spire ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/workload-identity/spire.md""
        spire-istio(Istio)
        style spire-istio fill:red
        spire-linkerd(Linkerd)
        style spire-linkerd fill:red
        spire-nsm(Network Service Mesh)
        style spire-nsm fill:red
        spire-cilium(Cilium)
        style spire-cilium fill:green
        click spire-cilium ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/workload-identity/spire-cilium.md""
        spire-kuma(Kuma)
        style spire-kuma fill:red
        athenz(Athenz)
        style athenz fill:red
        identity --> spire --> spire-istio & spire-linkerd & spire-nsm & spire-cilium & spire-kuma --> user-authentication
        identity --> athenz --> user-authentication

        %% ------------------------
        %% -- User Authorization --
        %% ------------------------
        user-authentication{{User Authentication}}
        click user-authentication ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/user-authentication/README.md""
        style user-authentication fill:blue
        dex(Dex)
        style dex fill:red
        keycloak(Keycloak)
        style keycloak fill:green
        click keycloak ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/user-authentication/keycloak.md""
        user-authentication --> dex & keycloak --> access-authorization

        %% --------------------------
        %% -- Access Authorization --
        %% --------------------------
        access-authorization{{Access Control}}
        click access-authorization ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/access/README.md""
        style access-authorization fill:blue
        hexa(Hexa)
        style hexa fill:red
        paralus(Paralus)
        style paralus fill:red
        openfga(OpenFGA)
        click openfga ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/access/openfga.md""
        style openfga fill:green
        openfga-helm(App as Helm)
        click openfga-helm ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/access/openfga-helm.md""
        openfga-kustomize(App as Kustomize)
        click openfga-kustomize ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/access/openfga-kustomize.md""
        openfga-carvel(App as Carvel ytt)
        click openfga-carvel ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/access/openfga-carvel.md""
        style openfga-carvel fill:green
        openfga-cdk8s(App as cdk8s)
        style openfga-cdk8s fill:red
        access-authorization --> hexa & paralus & openfga
        openfga --> openfga-helm & openfga-kustomize & openfga-carvel & openfga-cdk8s --> security_misc
        hexa & paralus --> security_misc

        %% ----------
        %% -- Misc --
        %% ----------
        security_misc{{Misc}}
        click security_misc ""https://github.com/vfarcic/cncf-demo/blob/main/manuscript/security-misc/README.md""
        style security_misc fill:blue
        %% Not maintained
        %% curiefense(Curiefense)
        %% style curiefense fill:red
        confidential-containers(Confidential Containers)
        style confidential-containers fill:red
        container-ssh(ContainerSSH)
        style container-ssh fill:green
        security_misc --> confidential-containers & container-ssh --> security-done

        security-done((Chapter End))

    end
```

```mermaid
flowchart TD

    subgraph Observability

        %% -----------
        %% -- Setup --
        %% -----------
        setup-observability((Setup))
        click setup-observability ""https://github.com/vfa",FAUX
vfiftyfive/dadjokes,Application System,ML Model,2025-04-05T16:46:55Z,2023-04-26T14:13:44Z,0,0,0,0,1,0,0,0,2023-03-29T14:21:33Z,2025-04-05T16:47:01Z,12887,6,Go,VRAI,6,FAUX,0,,0,,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,2,"# Dad Joke Generator 🤣

This project is a Dad Joke Generator that uses the OpenAI GPT-3 API to generate the funniest dad jokes you've ever heard! The application is built using Go and is composed of a joke-server, joke-worker, Redis, MongoDB, and NATS.

## Table of Contents 📚

- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Deployment](#deployment)
- [License](#license)

## Architecture 🏗️
The application consists of the following components:

1. **joke-server**: A web server that listens for incoming HTTP requests and returns a dad joke.
2. **joke-worker**: A background worker that communicates with the OpenAI GPT-3 API, generates dad jokes, caches them in Redis, and stores them in MongoDB.
3. **Redis**: A caching layer that temporarily stores generated dad jokes.
4. **MongoDB**: A NoSQL database that permanently stores generated dad jokes.
5. **NATS**: A messaging system that facilitates communication between the joke-server and joke-worker.

![image](https://user-images.githubusercontent.com/7715763/232326149-3461b3c6-346b-4cbd-95f5-774587464342.png)


## Prerequisites ⚙️

Before you can deploy the Dad Joke Generator, you'll need the following:

- Docker and Docker Compose installed on your machine
- An OpenAI API key
- Access to a Kubernetes cluster, `helm` and `kubectl` installed if you want to deploy the application on Kubernetes
- `gpg` command-line available

## Deployment 🚀

### Docker Compose (Local) 🐳 

To deploy the Dad Joke Generator, follow these steps:

1. Clone the repository:

```bash
git clone https://github.com/vfiftyfive/dadjokes.git
cd dadjokes/deploy/docker
```

2. Create a .env file in the `dadjokes/deploy/docker` directory, with your OpenAI API key:
```bash
echo ""OPENAI_API_KEY=your_api_key_here"" > .env
```

3. Run the deployment script:
```bash
cd ../..
make deploy
```

4. Generate a lot of jokes!
```bash
for i in {1..30}; do curl http://localhost:8080/joke; echo -e; done
```
The last 10 jokes should come a lot faster than the first 20, as the joke-worker will retrieve jokes from the Redis cache after that, for a time defined in `constants.RedisTTL`.

### Kubernetes ☸

1. Clone the repository and change the directory to `deploy/devspace`:

```bash
git clone https://github.com/vfiftyfive/dadjokes.git
cd dadjokes/deploy/devspace
```

2. Install DevSpace:
```bash 
# AMD64
curl -L -o devspace ""https://github.com/loft-sh/devspace/releases/latest/download/devspace-linux-amd64"" && sudo install -c -m 0755 devspace /usr/local/bin

# ARM64
curl -L -o devspace ""https://github.com/loft-sh/devspace/releases/latest/download/devspace-linux-arm64"" && sudo install -c -m 0755 devspace /usr/local/bin
```
3. Install SOPS:
```bash
ORG=""mozilla""
REPO=""sops""
latest_release=$(curl -Ls ""https://api.github.com/repos/${ORG}/${REPO}/releases/latest"" | grep '""tag_name"":' | sed -E 's/.*""v([^""]+)"".*/\1/')

# AMD64
curl -L https://github.com/mozilla/sops/releases/download/v${latest_release}/sops_${latest_release}_amd64.deb -o sops.deb && sudo apt-get install ./sops.deb && rm sops.deb

# ARM64
curl -L https://github.com/mozilla/sops/releases/download/v${latest_release}/sops_${latest_release}_arm64.deb -o sops.deb && sudo apt-get install ./sops.deb && rm sops.deb
```

4. Generate a GPG key:
```bash
gpg --gen-key
#answer the questions
```

5. Create a SOPS configuration file:
```bash
first_pgp_key=$(gpg --list-secret-keys --keyid-format LONG | grep -m1 '^sec' | awk '{print $2}' | cut -d '/' -f2)

cat <<EOF > .sops.yaml
creation_rules:
- encrypted_regex: ""^(data|stringData)$""
  pgp: >-
    ${first_pgp_key}
EOF
```

6. Create an encrypted Kubernetes ConfigMap with your OpenAI API key:

First, create an OPENAI_API_KEY environment variable and configure it with your OpenAI API key.
```bash
devspace run encrypt-openai-secret
```

4. Specify a namespace to use with DevSpace
```bash
devspace use namespace dev
```
5. Run devspace in dev mode
```bash
devspace dev
```

6. Generate a lot of jokes:

First, expose the `joke-server` pod or service using port forwarding.

```bash
kubectl -n dev port-forward svc/joke-server 8080:80
```
Then, execute the following command:

```bash 
for i in {1..10}; do curl http://localhost:8080/joke; echo -e; done
```

7. Modify the code

Your local repository is synchronized with the project files within the joke-worker pod. Modify the file `internal/joke/joke.go` and change the code so the joke generated is now a Chuck Norris joke. Replace the line:
```go
Prompt: ""Tell me a dad joke""
```
With the line:
```go
Prompt: ""Tell me a Chuck Norris joke""
```
Then save the file. The joke-worker pod will automatically recompile and restart the binary. Now, when you run the curl command again, you should see a Chuck Norris joke instead of a dad joke (provided you have generated less than 20 jokes, as the program will retrieve jokes from the Redis cache after that, for a time defined in `constants.RedisTTL`):
  
  ```bash
  for i in {1..10}; do curl http://localhost:8080/joke; echo -e; done
  ```
## License 📄
This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.",VRAI
Vikaspogu/openshift-multicluster,DevOPs,Documentations,2025-05-15T19:57:57Z,2025-04-25T12:55:22Z,0,0,0,0,9,0,0,0,2022-06-23T23:14:45Z,2025-04-07T13:10:53Z,11444,33,Smarty,VRAI,12,FAUX,1,"argocd,external-secrets-operator,kustomize,openshift-v4,proxmox,sops",1,Homelab OpenShift Cluster - Deployed on Proxmox and Operated through Kustomize & ArgoCD,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,3,"<!-- markdownlint-disable MD041 -->

<img src=""https://avatars.githubusercontent.com/u/792337?s=280&v=4"" align=""left"" width=""45"" height=""47""/><h1>&nbsp;&nbsp;Multi-Cluster OpenShift Management with ArgoCD</h1>

_... managed by ArgoCD_ <img src=""https://redhat-scholars.github.io/argocd-tutorial/argocd-tutorial/_images/argocd-logo.png"" align=""center"" width=""40px"" height=""40px""/>

---

## :wave: Overview

This repository provides the necessary files and instructions to manage multiple OpenShift clusters using ArgoCD and GitOps principles. This setup allows for consistent, repeatable, and automated configuration of your OpenShift environments. This repository follows standards from [gitops-standards-repo-template](https://github.com/redhat-cop/gitops-standards-repo-template.git)

## Table of Contents

1. [Installation of OpenShift cluster](#installing-openshift-cluster-with-agent-based-installer)
2. [Bootstrap Argocd instance](#bootstrap-argocd-instance)
3. [Repository Structure](https://github.com/redhat-cop/gitops-standards-repo-template?tab=readme-ov-file#repo-structure)
4. [ArgoCD Plugins](#argocd-plugins-and-usage)

### Installing OpenShift cluster with Agent-based Installer

[Getting started](https://docs.openshift.com/container-platform/4.12/installing/installing_with_agent_based_installer/installing-with-agent-based-installer.html) on Agent-based installer

#### Manual Steps

- Generate ISO

  ```bash
  rm -rf installer/proxmox #remove older cluster if any
  cp -r installer/cluster installer/proxmox #copy cluster config files
  ./openshift-install agent create image --dir installer/proxmox #create image
  ```

- Upload ISO to proxmox from GUI
- Create 3 VMs with CPU type as `max`
- Start VMs and wait for the cluster installation to finish

  ```bash
  export KUBECONFIG=installer/proxmox/auth/kubeconfig
  ./openshift-install agent wait-for install-complete --dir installer/proxmox --log-level=debug
  ```

#### Ansible workflow to deploy OpenShift cluster

[Workflow](https://github.com/Vikaspogu/homelab-orchestrator/blob/main/ansible/awx/workflows/openshift-cluster.yaml) to automate manual steps described above

## Bootstrap ArgoCD instance

### Manual

```bash
oc login
./.bootstrap/setup.sh
```

### Automated

[Playbook](https://github.com/Vikaspogu/homelab-orchestrator/blob/main/ansible/playbooks/openshift/acm-gitops-bootstrap.yaml) to automate manual steps described above

## ArgoCD Plugins and Usage

Below are the list of plugins used in this Repository

- [ArgoCD Lovely Plugin](https://github.com/crumbhole/argocd-lovely-plugin/tree/main)
- [Custom Plugins](./components/openshift-gitops-config/)

### ArgoCD Lovely Plugin

ArgoCD Lovely Plugin facilitates the management of Kustomize patches and environment variable substitutions within the ArgoCD application specification.

- [Patching Operator Channel in Helm values](./clusters/proxmox/cert-manager.yaml#L15)
- [Kustomize Patch](./clusters/proxmox/metallb.yaml#L34)
- [Using sed to replace variable in all yaml files](./clusters/proxmox/cert-manager.yaml#L28)
- [Using yq to replace variable in single yaml file](./clusters/proxmox/acm.yaml#13)

## 🔍 Features

- [x] ArgoCD with SOPS plugin
- [x] Secret Management using External secrets and 1Password
- [x] Cert manager integration with Cloudflare for API and Wildcard certificate
- [x] Multi cluster management using Red Hat ACM
- [x] OpenShift Pipeline as Code
- [x] Renovate bot

## Resources

- [GitOps Catalog by RedHat COP](https://github.com/redhat-cop/gitops-catalog)",VRAI
ViktorUJ/cks,Documentations,Application System,2025-05-12T08:20:52Z,2024-07-04T18:54:09Z,0,0,0,0,0,0,0,3,2023-01-22T19:32:21Z,2025-04-07T13:37:01Z,2724,1072,HCL,VRAI,201,FAUX,1,"aws,certified-kubernetes-security-specialist,cka,cka-exam-preparation,cka-exam-questions,ckad,ckad-exam-questions,cks,cks-exam-preparation,cks-exam-questions,devops,eks,k8s,kcsa,kubeadm,kubernetes,kubernetes-learning,kubernetes-security,learn-kubernetes,terraform",1,"Open-source Platform for learning kubernetes and aws eks   and preparation for  for  Certified Kubernetes exams  (CKA ,CKS , CKAD) ",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,9,"# Welcome to the SRE Learning Platform!
<p align=""center"">
    <a href=""https://github.com/ViktorUJ/cks""><img src=""https://raw.githubusercontent.com/ViktorUJ/terraform-aws-vpc/master/img/logo_192x192.png"" width=""192"" height=""192"" alt=""SRE Learning Platform""/></a>
</p>


The **SRE Learning Platform** is an open-source hub designed to help IT engineers effectively prepare for the **CKA (Certified Kubernetes Administrator)**, **CKS (Certified Kubernetes Security Specialist)**, **CKAD (Certified Kubernetes Application Developer)**, and **LFCS (Linux Foundation Certified System Administrator)** exams. Additionally, this platform offers invaluable hands-on experience with **AWS EKS (Elastic Kubernetes Service)**, equipping users with practical insights for real-world applications. Whether you're aiming to validate your skills, boost your career prospects in Kubernetes administration, security, application development, or delve into AWS EKS, this platform provides hands-on labs, practice tests, and expert guidance to ensure certification success.

- Prepare for the **CKA**: [Certified Kubernetes Administrator Exam](https://training.linuxfoundation.org/certification/certified-kubernetes-administrator-cka/)
- Enhance your skills for the **CKS**: [Certified Kubernetes Security Specialist Exam](https://training.linuxfoundation.org/certification/certified-kubernetes-security-specialist/)
- Excel in the **CKAD**: [Certified Kubernetes Application Developer Exam](https://training.linuxfoundation.org/certification/certified-kubernetes-application-developer-ckad/)
- Prepare for the **KCNA**: [Kubernetes and Cloud Native Associate](https://training.linuxfoundation.org/certification/kubernetes-cloud-native-associate/)
- Prepare for the **KCSA**: [Kubernetes and Cloud Native Security Associate](https://training.linuxfoundation.org/certification/kubernetes-and-cloud-native-security-associate-kcsa/)
- Prepare for the **LFCS**: [Linux Foundation Certified System Administrator](https://training.linuxfoundation.org/certification/linux-foundation-certified-sysadmin-lfcs/)
- Prepare for the **KCNA**: [Kubernetes and Cloud Native Associate](https://training.linuxfoundation.org/certification/kubernetes-cloud-native-associate/)


Master Kubernetes concepts, gain practical experience, and excel in the CKA, CKS, and CKAD exams with the **SRE Learning Platform**.


## Quick start
[run via docker](docs%2Frun_from_docker.MD)

[![video instruction](docs%2Fimages%2Frun_via_docker.gif)](https://youtu.be/Xh6sWzafBmw ""run via docker"")


<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">Requirements</summary>

- [GNU Make](https://www.gnu.org/software/make/) >= 4.2.1
- [terrafrom](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)  >= v1.6.6
- [terragrunt](https://terragrunt.gruntwork.io/docs/getting-started/install/) >= v0.54.8
- [jq](https://jqlang.github.io/jq/download/) >= 1.6
- [aws IAM user](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html)  + [Access key](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)  (or [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) ) with  [Admin privilege  for VPC, EC2, IAM, EKS](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html)
- [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-version.html) > 2.2.30
- [aws profile](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)

**Or you can** [run via docker](docs%2Frun_from_docker.MD)  ( [video instruction](https://youtu.be/Xh6sWzafBmw) )

</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">Repository Structure</summary>

The repository is organized into the following sections:

- **[Makefile](Makefile)** - File contains scenarios for launching hands-on labs and mock exams.
- **[tasks](tasks)** - Directory contains lab scenarios and mock exam scripts.
  - **[cka](tasks%2Fcka)** - Directory contains everything related to CKA.
    - **[labs](tasks%2Fcka%2Flabs)**  - Directory  contains hands-on labs for the CKA exam.
    - **[mock](tasks%2Fcka%2Fmock)**  - Directory contains mock scripts for the CKA exam.
  - **[cks](tasks%2Fcks)** - Directory contains everything related to CKS.
    - **[labs](tasks%2Fcks%2Flabs)**  - Directory contains hands-on labs for the CKS exam.
    - **[mock](tasks%2Fcks%2Fmock)**  - Directory contains mock scripts for the CKS exam.
- **[terraform](terraform)** - Directory contains  modules and Terraform environments.
  - **[environments](terraform%2Fenvironments)** - Directory contains terragrunt (terraform) environments.
  - **[modules](terraform%2Fmodules)** - Directory contains terraform modules.

</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">About CKA, CKS, CKAD scenarios</summary>

- the platform uses **aws**  to create following resources :  **vpc**, **subnets**, **security groups**, **ec2** (spot ), **s3**
- after you launch the scenarios the platform will create all the necessary resources   and give access to k8s clusters.
- to create clusters the platform uses **kubeadm**
- you can easily add your own scenario using the already existing terraform module [k8s_self_managment](terraform%2Fmodules%2Fk8s_self_managment)
- [k8s_self_managment](terraform%2Fmodules%2Fk8s_self_managment)  module supports versions:
````
k8s version  : [ 1.21 , 1.32 ]   https://kubernetes.io/releases/
Rintime :
    docker                   [1.21 , 1.23]
    cri-o                    [1.21 , 1.29]
    containerd               [1.21 , 1.31]   # cks, cka  default 1.30
    containerd_gvizor        [1.21 , 1.31]
OS for nodes  :
   ubuntu  :  20.04 LTS  ,  22.04 LTS   # cks default  20.04 LTS
CNI :  calico ,culium
````
</details>

## Configuration
- change  **backend_bucket** ( **region** , **backend_region**  optional ) in [terraform/environments/terragrunt.hcl](terraform%2Fenvironments%2Fterragrunt.hcl#L4) :


## Command
Every command should be run from the project's root directory.
<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">CMDB</summary>

- ``make cmdb_get_env_all`` - get a list of all resources in CMDB
- ``USER_ID='myuser' ENV_ID='01' make cmdb_get_user_env_data`` - show all created resources of user **myuser** in environment **01**
- ``USER_ID='myuser' ENV_ID='01' make cmdb_get_user_env_lock`` - show all lock resources of user **myuser** in environment **01**
- ``USER_ID='myuser' ENV_ID='01' make cmdb_get_user_env_lock`` - show all lock resources of user **myuser** in environment **01**
- ``USER_ID='myuser' make cmdb_get_user_env_lock`` - show all lock resources of user **myuser** in **all** environment
- ``USER_ID='myuser' make cmdb_get_user_env_data`` - show all data resources of user **myuser** in **all** environment
- ``CMDB_ITEM='CMDB_data_myuser_02_k8s_cluster1' make cmdb_get_item`` - getting detailed information about **CMDB_data_myuser_02_k8s_cluster1** resource.


</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">CKA</summary>

### cka task
- ``TASK=01 make run_cka_task`` - create cka [hands-on labs](docs%2Flabs.MD#cka-labs) number 01
- ``TASK=01 make delete_cka_task`` - delete cka hands-on labs
- ``TASK=01 make run_cka_task_clean`` - run cka_task with clean terragrunt cache  for  cka_task
- ``make output_cka_task `` - show **outputs** from   **cka_task**
### cka mock
- ``TASK=01 make run_cka_mock`` - create mock  CKA exam [number 01](tasks%2Fcka%2Fmock%2F01)
- ``make delete_cka_mock`` - delete mock  CKA exam
- ``TASK=01 make run_cka_mock_clean`` - create mock  CKA exam [number 01](tasks%2Fcka%2Fmock%2F01)  with clean terragrunt cache
- ``make output_cka_mock `` - show **outputs** from   **cka_mock**

</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">CKAD</summary>

### ckad mock
- ``TASK=01 make run_ckad_mock`` - create mock  CKAD exam [number 01](tasks%2Fckad%2Fmock%2F01)
- ``make delete_ckad_mock`` - delete mock  CKAD exam
- ``TASK=01 make run_ckad_mock_clean`` - create mock  CKAD exam [number 01](tasks%2Fckad%2Fmock%2F01)  with clean terragrunt cache
- ``make output_ckad_mock `` - show **outputs** from   **ckad_mock**

</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">CKS</summary>

### cks task
- ``TASK=10 make run_cks_task`` - create cks [hands-on labs](docs%2Flabs.MD#cks-labs) number 10
- ``TASK=10 make delete_cks_task`` - delete cks hands-on labs
- ``TASK=10  make run_cks_task_clean`` - run cks_task with clean terragrunt cache  for  cks_task
- ``make output_cks_task `` - show **outputs** from   **cks_task**
### cks mock
- ``TASK=01 make run_cks_mock`` - create mock  CKS exam [number 01](tasks%2Fcks%2Fmock%2F01)
- ``make delete_cks_mock`` - delete mock  CKS exam
- ``TASK=01 make run_cks_mock_clean`` - create mock  CKS exam [number 01](tasks%2Fcks%2Fmock%2F01)  with clean terragrunt cache
- ``make output_cks_mock `` - show **outputs** from   **cks_mock**
</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">LFCS</summary>

### lfcs mock

- ``TASK=01 make run_lfcs_mock`` - create mock LFCS exam [number 01](tasks/lfcs/mock/01/)
- ``make delete_lfcs_mock`` - delete mock LFCS exam
- ``TASK=01 make delete_lfcs_mock_clean`` - delete mock LFCS exam [number 01](tasks/lfcs/mock/01/) with cleaning terragrunt cache
- ``make output_lfcs_mock`` - show **outputs** from  **lfcs_mock**

</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">KCNA</summary>

### KCNA

- check examle questions in [kcna](tasks%2Fkcna) directory

</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">KCSA</summary>

- check examle questions in [kcsa](tasks%2Fkcsa) directory

</details>


<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">HR</summary>

- ``TASK=01 make run_hr_mock`` - create mock  hr exam [number 01](tasks%2Fhr%2Fmock%2F01)
- ``make delete_hr_mock`` - delete mock  hr exam
- ``TASK=01 make run_hr_mock_clean`` - create mock  CKS exam [number 01](tasks%2Fhr%2Fmock%2F01)  with clean terragrunt cache
- ``make output_hr_mock `` - show **outputs** from   **hr_mock**
</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">EKS</summary>

- ``TASK={lab_number} make run_eks_task`` create hands-on lab
- ``make delete_eks_task`` delete eks lab cluster
</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">DEV</summary>

- ``make lint`` run linter on the project

</details>

## Usage scenarios


<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">CKA hands-on lab</summary>

- choose [a hands-on lab](docs%2Flabs.MD#cka-labs) number
- create cka lab cluster ``TASK={lab_number} make run_cka_task``
- find {master_external_ip} in terraform output
- log in to master node via ssh  ``ssh ubuntu@{master_external_ip} -i {key}``
- check init logs `` tail -f /var/log/cloud-init-output.log ``
- read lab descriptions in ``{lab_number}/README.MD``
- check solution in ``{lab_number}/SOLUTION.MD``
- delete cka lab cluster ``make delete_cka_task``
- clean cka lab cluster ``.terraform`` folder  ``make clean_cka_task ``
</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">mock CKA exam</summary>

  [Video instruction for launching **CKA mock exam**](https://www.youtube.com/watch?v=P-YYX4CTWIg)

- choose [a mock exam](tasks%2Fcka%2Fmock) number
- change instance type from ``spot`` to ``ondemand`` in  ``{mock_number}/env.hcl`` if you need
- create mock  CKA exam ``TASK={mock_number} make run_cka_mock``
- find ``worker_pc_ip`` in ``terraform output``
- connect to ``worker_pc_ip``  with your ssh key and user ``ubuntu``
- open questions list ``{mock_number}/README.MD`` and do tasks
- use ``ssh  {kubernetes_nodename}`` from  work pc to connect to node
- run ``time_left`` on work pc to check time
- run ``check_result`` on work pc to check result
- delete mock  CKA exam `make delete_cka_mock`
- find exam solutions  in ``{mock_number}/worker/files/solutions)`` and * [Video](https://youtu.be/IZsqAPpbBxM)  for [mock 01](tasks%2Fcka%2Fmock%2F01) .
- find  exam tests in ``{mock_number}/worker/files/tests.bats)``
</details>


<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">CKS hands-on lab</summary>

- choose [CKS lab](docs%2Flabs.MD#cks-labs)  number
- change **ami_id** in ``{lab_number}/scripts/terragrunt.hcl`` if you changed **region**
- create cka lab cluster ``TASK={lab_number} make run_cks_task``
- find {master_external_ip} in terraform output
- log in to master node via ssh  ``ssh ubuntu@{master_external_ip} -i {key}``
- check init logs `` tail -f /var/log/cloud-init-output.log ``
- read lab descriptions in ``{lab_number}/README.MD``
- check solution in ``{lab_number}/SOLUTION.MD``
- delete cks lab cluster ``make delete_cks_task``
- clean cks lab cluster ``.terraform`` folder  ``make clean_cks_task ``
</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">mock CKS exam</summary>

  [Video instruction for launching **CKS mock exam**](https://youtu.be/_GbsBOMaJ9Q)

### mock  CKS exam
- choose [a mock exam](tasks%2Fcks%2Fmock) number
- change **ubuntu_version** in ``{mock_number}/env.hcl`` if you need
- change instance type from ``spot`` to ``ondemand`` in  ``{mock_number}/env.hcl`` if you need
- create mock  CKS exam ``TASK={mock_number} make run_cks_mock`` or ``TASK={mock_number} make run_cks_mock_clean`` if you'd like  to run with **clean** terragrunt cache
- find ``worker_pc_ip`` in ``terraform output``
- connect to ``worker_pc_ip``  with your ssh key and user ``ubuntu``
- open questions list ``{mock_number}/README.MD`` and do tasks
- use ``ssh  {kubernetes_nodename}`` from  work pc to connect to node
- run ``time_left`` on work pc to check time
- run ``check_result`` on work pc to check result
- delete mock  CKS exam `make delete_cks_mock`
- find exam solutions in ``{mock_number}/worker/files/solutions``  [mock 1 solutions](tasks%2Fcks%2Fmock%2F01%2Fworker%2Ffiles%2Fsolutions)  and [video](https://youtu.be/I8CPwcGbrG8)
- find exam tests in ``{mock_number}/worker/files/tests.bats``
</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">mock CKAD exam</summary>

  [Video instruction for launching **CKAD mock exam**](https://youtu.be/7X4Y9QhbTsk)

### mock  CKAD exam
- choose [a mock exam](tasks%2Fckad%2Fmock)  number
- change **ubuntu_version** in ``{mock_number}/env.hcl`` if you need
- change instance type from ``spot`` to ``ondemand`` in  ``{mock_number}/env.hcl`` if you need
- create mock  CKAD exam ``TASK={mock_number} make run_ckad_mock`` or ``TASK={mock_number} make run_ckad_mock_clean`` if you'd like  to run with **clean** terragrunt cache
- find ``worker_pc_ip`` in ``terraform output``
- connect to ``worker_pc_ip``  with your ssh key and user ``ubuntu``
- open questions list ``{mock_number}/README.MD`` and do tasks
- use ``ssh  {kubernetes_nodename}`` from  work pc to connect to node
- run ``time_left`` on work pc to check time
- run ``check_result`` on work pc to check result
- delete mock  CKAD exam `make delete_ckad_mock`
- find exam solutions in ``{mock_number}/worker/files/solutions``  [mock 1 solutions](tasks%2Fckad%2Fmock%2F01%2Fworker%2Ffiles%2Fsolutions)   and [video](https://youtu.be/yQK7Ca8d-yw)
- find exam tests in ``{mock_number}/worker/files/tests.bats``
</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">KCNA </summary>

### example questions for KCNA exam

- check examle questions in [kcna](tasks%2Fkcna) directory
</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">KCSA</summary>

### example questions for KCSA exam

- check examle questions in [kcsa](tasks%2Fkcsa) directory
</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">mock HR exam</summary>

  [Video instruction for launching **HR mock exam**](https://youtu.be/4CTC1jl8lxE)

### mock  HR exam
- choose [a mock exam](tasks%2Fhr%2Fmock) number
- change **ubuntu_version** in ``{mock_number}/env.hcl`` if you need
- change instance type from ``spot`` to ``ondemand`` in  ``{mock_number}/env.hcl`` if you need
- create mock  CKS exam ``TASK={mock_number} make run_hr_mock`` or ``TASK={mock_number} make run_hr_mock_clean`` if you'd like  to run with **clean** terragrunt cache
- find ``worker_pc_ip`` in ``terraform output``
- connect to ``worker_pc_ip``  with your ssh key and user ``ubuntu``
- open questions list ``{mock_number}/README.MD`` and do tasks
- use ``ssh  {kubernetes_nodename}`` from  work pc to connect to node
- run ``time_left`` on work pc to check time
- run ``check_result`` on work pc to check result
- delete mock  CKA exam `make delete_hr_mock`
- find exam solutions in ``{mock_number}/worker/files/solutions``  [mock 1 solutions](tasks%2Fhr%2Fmock%2F01%2Fworker%2Ffiles%2Fsolutions)  and [video](https://youtu.be/4CTC1jl8lxE)
- find exam tests in ``{mock_number}/worker/files/tests.bats``
</details>

<details>
  <summary style=""font-weight: bold; font-size: 1.5em;"">EKS hands-on lab</summary>

- choose [labs](docs%2Flabs.MD#eks-labs)  number
- create hands-on lab `` TASK={lab_number} make run_eks_task ``
- find ``worker_pc_ip`` in ``terraform output``
- log in to worker_pc node via ssh  ``ssh ubuntu@{worker_pc_ip} -i {key}``
- read lab descriptions in ``{lab_number}/README.MD``
- check solution in ``{lab_number}/SOLUTION.MD``
- delete eks lab cluster ``make delete_eks_task``
</details>

## [Simultaneous work with independent environments](docs%2Fmultiple_users_envs.MD)

## [Useful links](docs%2Flinks.MD)

## [Tips And Tricks](docs%2Ftips_and_tricks.MD)

## [CHANGELOG](CHANGELOG%2FCHANGELOG.MD)

## [CODE OF CONDUCT](docs%2FCODE_OF_CONDUCT.md)

## Contribution
If you want to be part of the project development team, get in touch with [us](https://github.com/ViktorUJ/cks/tree/master#contacts). We are always happy to welcome new members to our development team.


If you want to say **thank you** or/and support the active development of **SRE Learning Platform project** :
- [Star](https://github.com/ViktorUJ/cks) the **SRE Learning Platform project** on Github
- Feel free to write articles about the project on [dev.to](https://dev.to/), [medium](https://medium.com/), [hackernoon](https://hackernoon.com) or on your personal blog and share your experiences


## License and Usage Agreement
- [Apache License 2.0](LICENSE)

## Channels for cooperative preparation
 -  [cka](https://t.me/sre_platform_cka)
 -  [cks](https://t.me/sre_platform_cks)
 -  [ckad](https://t.me/sre_platform_ckad)

## Contacts

If you encounter any issues or have questions about the project, you can reach out to:

[![email](https://badgen.net/badge/icon/email?icon=email&label)](mailto:viktoruj@gmail.com) [![Telegram](https://badgen.net/badge/icon/telegram?icon=telegram&label)](https://t.me/viktor_uj) [![LinkedI](https://badgen.net/badge/icon/linkedin?icon=linkedin&label)](https://www.linkedin.com/in/viktar-mikalayeu-mns)",VRAI
vmware-tanzu/kubeapps,DevOPs,DevOPs,2025-05-08T12:48:44Z,2024-10-22T10:40:35Z,0,0,0,0,0,1,0,0,2017-10-19T14:14:34Z,2025-04-08T07:08:50Z,129116,5087,Go,VRAI,710,FAUX,153,"carvel,deployment,flux,helm,kubernetes",153,A web-based UI for deploying and managing applications in Kubernetes clusters,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,112,"# <img src=""site/content/docs/latest/img/logo.svg"" width=""40"" align=""left""/> Kubeapps

[![Main Pipeline](https://github.com/vmware-tanzu/kubeapps/actions/workflows/kubeapps-main.yaml/badge.svg)](https://github.com/vmware-tanzu/kubeapps/actions/workflows/kubeapps-main.yaml)
[![Full Integration Pipeline](https://github.com/vmware-tanzu/kubeapps/actions/workflows/kubeapps-full-integration.yaml/badge.svg)](https://github.com/vmware-tanzu/kubeapps/actions/workflows/kubeapps-full-integration.yaml)
[![CodeQL](https://github.com/vmware-tanzu/kubeapps/actions/workflows/codeql-analysis.yml/badge.svg)](https://github.com/vmware-tanzu/kubeapps/actions/workflows/codeql-scheduled.yml)
[![Netlify Status](https://api.netlify.com/api/v1/badges/7e0e2833-1d75-43f6-b006-632d359bb83b/deploy-status)](https://app.netlify.com/sites/kubeapps-dev/deploys)

## Overview

Kubeapps is an in-cluster web-based application that enables users with a one-time installation to deploy, manage, and upgrade applications on a Kubernetes cluster.

With Kubeapps you can:

- Browse and deploy different packages like [Helm](https://github.com/helm/helm) charts, [Flux](https://fluxcd.io/) or [Carvel](https://carvel.dev/) packages from public or private repositories (including [VMware Marketplace™](https://marketplace.cloud.vmware.com) and [Bitnami Application Catalog](https://bitnami.com/application-catalog))
- Customize deployments through an intuitive user interface
- Browse, upgrade and delete applications installed in the cluster
- Browse and deploy [Kubernetes Operators](https://operatorhub.io/)
- Secure authentication to Kubeapps using a [standalone OAuth2/OIDC provider](./site/content/docs/latest/tutorials/using-an-OIDC-provider.md) or [using Pinniped](./site/content/docs/latest/howto/OIDC/using-an-OIDC-provider-with-pinniped.md)
- Secure authorization based on Kubernetes [Role-Based Access Control](./site/content/docs/latest/howto/access-control.md)

**_Note:_** Kubeapps 2.0 and onwards supports Helm 3 only. While only the Helm 3 API is supported, in most cases, charts made for Helm 2 will still work.

## Getting started with Kubeapps

Installing Kubeapps is as simple as:

```bash
kubectl create namespace kubeapps
helm install kubeapps --namespace kubeapps oci://registry-1.docker.io/bitnamicharts/kubeapps
```

See the [Getting Started Guide](./site/content/docs/latest/tutorials/getting-started.md) for detailed instructions on how to install and use Kubeapps.

> Kubeapps is deployed using the official [Bitnami Kubeapps chart](https://github.com/bitnami/charts/tree/main/bitnami/kubeapps) from the separate Bitnami charts repository. Although the Kubeapps repository also defines a chart, this is intended for development purposes only.

## Documentation

Complete documentation available in Kubeapps [documentation section](./site/content/docs/latest/README.md). Including complete tutorials, how-to guides, and reference for configuration and development in Kubeapps.

For getting started into Kubeapps, please refer to:

- [Getting started guide](./site/content/docs/latest/tutorials/getting-started.md)
- [Detailed installation instructions](./chart/kubeapps/README.md)
- [Kubeapps user guide](./site/content/docs/latest/howto/dashboard.md) to easily manage your applications running in your cluster.
- [Kubeapps FAQs](./chart/kubeapps/README.md#faq).

See how to deploy and configure [Kubeapps on VMware Tanzu™ Kubernetes Grid™](./site/content/docs/latest/tutorials/kubeapps-on-tkg/README.md)

## Troubleshooting

If you encounter issues, please review the [troubleshooting docs](./chart/kubeapps/README.md#troubleshooting), review our [project board](https://github.com/orgs/vmware-tanzu/projects/38/views/2), file an [issue](https://github.com/vmware-tanzu/kubeapps/issues), or talk to Kubeapps maintainers on the [#Kubeapps channel](https://kubernetes.slack.com/messages/kubeapps) on the Kubernetes Slack server.

- [Sign up](https://slack.k8s.io) to the Kubernetes Slack org.

- Review the FAQs section on the [Kubeapps chart README](./chart/kubeapps/README.md#faq).

## Contributing

If you are ready to jump in and test, add code, or help with documentation, follow the instructions on the [start contributing](./CONTRIBUTING.md) documentation for guidance on how to setup Kubeapps for development.

## Changelog

Take a look at the list of [releases](https://github.com/vmware-tanzu/kubeapps/releases) to stay tuned for the latest features and changes.",FAUX
vmware/dod-compliance-and-automation,Application System,Application System,2025-04-22T14:31:02Z,2024-10-29T17:08:32Z,0,0,0,0,0,0,0,18,2019-12-17T17:34:20Z,2025-04-03T19:46:20Z,91276,165,Ruby,VRAI,65,FAUX,20,,20,Security hardening content for VMware solutions to US Department of Defense standards,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,VRAI,21,"![Linting](https://github.com/vmware/dod-compliance-and-automation/actions/workflows/code-linting-push.yml/badge.svg?master)
![Docs Deployment](https://github.com/vmware/dod-compliance-and-automation/actions/workflows/deploy-docs.yml/badge.svg?docs)
# dod-compliance-and-automation

> [!CAUTION]
> Prior to using the STIG automation provided here it is assumed the user has familiarity with the rules contained in the various VMware STIGs and has evaluated those for impact and implementation considerations in their environment.

## Announcements
Please visit our new documentation page at: https://vmware.github.io/dod-compliance-and-automation/  

## Overview
VMware is a trusted partner in highly secure, mission critical systems around the world, including the US Department of Defense (DoD). In the DoD, all IT systems must adhere to the rigorous Risk Management Framework (RMF) as defined in DoDI 8510.01. A critical component of RMF is the mandatory implementation of Security Technical Implementation Guides (STIGs) and Security Requirements Guidelines (SRGs) as maintained by the Defense Information Systems Agency (DISA). Where a product specific STIG is not available, the relevant SRGs must be used instead.

[DoDI 8510.01](http://acqnotes.com/wp-content/uploads/2014/09/DoD-Instruction-8510.01-Risk-Management-Framework-RMF-for-DoD-Information-Technology-IT-24-May-2016.pdf)

>STIGs are product-specific and document applicable DoD policies and security
requirements, as well as best practices and configuration guidelines. STIGs are associated with
security controls through CCIs, which are decompositions of NIST SP 800-53 security controls
into single, actionable, measurable items. SRGs are developed by DISA to provide general
security compliance guidelines and serve as source guidance documents for STIGs. When a
STIG is not available for a product, an SRG may be used.

[DoD Cybersecurity Discipline
Implementation Plan](https://dodcio.defense.gov/Portals/0/Documents/Cyber/CyberDis-ImpPlan.pdf)

>STIGs and SRGs provide
configuration for technologies such as operating systems, browsers, antivirus, web services,
databases, Active Directory, and domain name services. The combination of applicable STIGs
and SRGs will result in a secure configuration to prevent issues such as insider threats, data
exfiltration, or advanced persistent threats.

In order to better serve the needs of our DoD partners, and those who wish to meet the bar set by the DoD, VMware is providing three elements for community consumption and contribution.

* STIG Readiness Guides
  * SRG based content that is either the source material for an in process STIG, or that can be used in the absence of an official STIG.
* Auditing Automation
  * Automation to audit and report on the state of compliance for an associated set of SRG/STIG controls.
* Remediation Automation
  * Automation to remediate findings with a set of SRG/STIG controls using publicly accessible methods and APIs.

## STIG Readiness Guides
STIG development is essentially an exercise where a specific product is filtered through all applicable SRGs to produce product-specific, NIST 800-53 backed hardening guidance. That content is then vetted, tested and approved by the DISA Risk Management Executive (RME) and posted on public.cyber.mil. VMware has a number of official STIGs published and we are working on many more. While we go through the official DISA vendor process, we want to make the SRG content available for public consumption and contributions while we wait for the official posting for products that are in process or are not scheduled to be submitted.

For more information on STIG Readiness Guides please read about our [STIG program](https://www.vmware.com/docs/vmw-stig-program-overview).

__NOTE__: This project represents VMware's effort to document our compliance against the SRG requirements and nothing more. A published STIG is our eventual goal, in most cases, but this content should not be viewed to be ""as good as a STIG"". A DISA published STIG includes technical validation, review of requirement fulfillment, accuracy and style, risk acceptance and is digitally signed by the RME and posted on a .mil. This SRG content is intended to provided value to our partners while the STIGs are in process. Except for products that have published STIGs already, there is no explicit or implied DISA approval of the provided content.

## Compliance Automation
STIG documents are written to be portable, offline hardening documentation where a sysadmin can go through, step by step, and STIG a system with no external dependencies. That said, many STIGs are either too complex or need to be applied to so many instances that manual steps are just not feasible. To augment the plain language STIG content, we are providing a number of ways to script or fully automate your VMware compliance activities.

## Repo Structure
Automation provided here will be in the following structured format:  

* Product
  * Major Version
    * README
    * docs
    * STIG Content Version

For example:
* vsphere
  * 8.0
    * README
    * docs
    * v1r1-srg
    * v1r1-stig

*`srg` will denote STIG Readiness Guide content and `stig` will denote official STIG content*

## Documentation
Depending on the product, there may be a need to host DOD specific whitepapers, notes and addendums that have no other appropriate place. These items will be provided under the docs path where applicable.

## Support
More information on support for STIGs and STIG Readiness Guides is available in the [Support](SUPPORT.md) document.

## Archives
The `master` branch in this repo will contain only content for currently supported products. Access to older revisions of guidance and automation for products that are no longer supported and End of Life(EoL) will be available in the `archived_content` branch.  

## Contributing
The dod-compliance-and-automation project team welcomes contributions from the community. If you wish to contribute code and you have not signed our contributor license agreement (CLA), our bot will update the issue when you open a Pull Request. For any questions about the CLA process, please refer to our [FAQ](https://cla.vmware.com/faq).

* __STIG Readiness Guides Content__ - VMware owns the state of the SRG/STIG controls provided here, including their applicability and how the requirements are addressed. That said, we are open to ideas for further hardening, additional methods, refinements, expansion, etc.

* __Automation Content__ - VMware provides the automation content in a beta complete state. Once it is used by the broad github audience, we expect the need for refinements and we highly encourage feedback and direct contributions.

## Disclaimer
VMware accepts no liability for the consequences of applying specific configuration settings made on the basis of the SRGs/STIGs. It must be noted that the configuration settings specified should be evaluated in a local, representative test environment before implementation in a production environment, especially within large user populations. The extensive variety of environments makes it impossible to test these configuration settings for all potential software configurations.

For some production environments, failure to test before implementation may lead to a loss of required functionality. Evaluating the risks and benefits to a system’s particular circumstances and requirements is the system owner's responsibility. The evaluated risks resulting from not applying specified configuration settings must be approved by the responsible Authorizing Official.

Furthermore, VMware implies no warranty that the application of all specified configurations will make a system 100 percent secure. Security guidance is provided for the Department of Defense. While other agencies and organizations are free to use it, care must be given to ensure that all applicable security guidance is applied both at the device hardening level as well as the architectural level. Some of the controls may not be configurable in environments outside the DoDIN.

## License
The dod-compliance-and-automation project is available under the [Apache License, Version 2.0](LICENSE).",FAUX
vrk-kpa/opendata,Application System,Documentations,2025-05-14T12:06:03Z,2025-05-13T06:17:40Z,0,0,0,0,0,1,0,0,2013-10-23T10:54:53Z,2025-03-27T08:03:34Z,82846,40,Python,VRAI,12,FAUX,66,"avoindata,ckan,ckan-extension,dcat-ap,drupal-9,dvv,finland,hacktoberfest,interoperability,national,open,open-data,opendata,opensource",66,Finland national open data portal (avoindata.fi) source code.,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,104,"## Avoindata.fi [![CircleCI][circleci-image]][circleci-url] [![Cypress Dashboard](https://img.shields.io/badge/cypress-dashboard-brightgreen.svg)](https://dashboard.cypress.io/#/projects/ssb2ut/runs)

Main repository for Yhteentoimivuuspalvelut (_Interoperability services_ in Finnish). This service combines two related subservices:

- [Avoindata.fi](https://www.avoindata.fi/), a search engine and metadata catalog for Finnish open data
- A catalog of interoperability tools and guidelines

The service is publicly available at [Avoindata.fi](https://www.avoindata.fi/). Free registration is required for features such as commenting and publishing of datasets. A developer sandbox is also available at [betaavoindata.fi](http://betaavoindata.fi) or [betaopendata.fi](http://betaopendata.fi).

### Getting started

To try out the service, visit the sandbox/development environment [betaavoindata.fi](http://betaavoindata.fi) or the production environment [avoindata.fi](http://avoindata.fi), and register a user account to create new datasets.

To get started in developing the software, install a local development environment as described in the [documentation](doc/local-installation.md), and then see the [development documentation](doc/local-development.md).

### Documentation

Please refer to the [documentation directory](doc) and [API documentation](https://github.com/vrk-kpa/ytp-api).

### Contact

Please file [issues at Github](https://github.com/vrk-kpa/opendata/issues).

## List of CKAN extensions

| :sunglasses: | Name | Description |
|---|---|---|
| :bookmark_tabs: | [ckanext-orgdashboards](https://github.com/ViderumGlobal/ckanext-orgdashboards) | CKAN extension for creating organization dashboards.
| :chart_with_upwards_trend: | [ckanext-matomo](https://github.com/vrk-kpa/ckanext-matomo) | CKAN extension to integrate Matomo data into CKAN. Gives download stats on package pages, list of most popular packages, etc.
| :tractor: | [ckanext-harvest](https://github.com/ckan/ckanext-harvest) | This extension provides a common harvesting framework for ckan extensions and adds a CLI and a WUI to CKAN to manage harvesting sources and jobs.
| :milky_way: | [ckanext-spatial](https://github.com/ckan/ckanext-spatial) | This extension contains plugins that add geospatial capabilities to CKAN.
| :watch: | [ckanext-realtime](https://github.com/alexandrainst/ckanext-realtime) | CKAN plugin which makes your CKAN site into a Realtime Data Portal.
| :earth_americas: | [ckanext-dataspatial](https://github.com/NaturalHistoryMuseum/ckanext-dataspatial) | Dataspatial is a Ckan extension to provide geospatial awareness of datastore data.
| :mailbox_with_mail: | [ckanext-requestdata](https://github.com/ViderumGlobal/ckanext-requestdata) | This extension introduces a new type of dataset in which access to data is by request.
| :bookmark_tabs: | [ckanext-orgportals](https://github.com/ViderumGlobal/ckanext-orgportals) | CKAN extension for creating organization portals.
| :mag_right: | [Data Solr](https://github.com/NaturalHistoryMuseum/ckanext-datasolr) | Datasolr is a Ckan extension to use Solr for datastore queries.
| :closed_lock_with_key: | [ckanext-cas](https://github.com/keitaroinc/ckanext-cas) | CAS (Central Authentication Service) client extension for CKAN.
| :dvd: | [ckanext-s3filestore](https://github.com/keitaroinc/ckanext-s3filestore) | Use Amazon S3 as a filestore for CKAN.
| :bar_chart: | [ckanext-c3charts](https://github.com/ViderumGlobal/ckanext-c3charts) | c3js based charts for CKAN.
| :truck: | [ckanext-cloudstorage](https://github.com/TkTech/ckanext-cloudstorage) | Implements support for resource storage against multiple popular providers via apache-libcloud (S3, Azure Storage, etc...).
| :station: | [ckanext-dcat](https://github.com/ckan/ckanext-dcat) | This extension provides plugins that allow CKAN to expose and consume metadata from other catalogs using RDF documents serialized using DCAT.
| :speak_no_evil: | [ckanext-fluent](https://github.com/ckan/ckanext-fluent) | This extension provides a way to store and return multilingul fields in CKAN datasets, resources, organizations and groups.
| :japan: | [ckanext-mapviews](https://github.com/ckan/ckanext-mapviews) | CKAN Resource View to build maps and choropleth maps.
| :open_file_folder: | [ckanext-odata](https://github.com/jqnatividad/ckanext-odata) | CKAN OData support to connect to tools like Tableau.
| :notebook: | [ckanext-pages](https://github.com/ckan/ckanext-pages) | This extension gives you an easy way to add simple pages to CKAN.
| :earth_africa: | [ckanext-spatial](https://github.com/ckan/ckanext-spatial) | This extension contains plugins that add geospatial capabilities to CKAN.
| :fast_forward: | [ckanext-xloader](https://github.com/ckan/ckanext-xloader) | Designed as a replacement for DataPusher because it offers ten times the speed and more robustness.


### Copying and License

This material is copyright (c) 2013-2022 Digital and Population Data Services Agency, Finland.

CKAN extensions and Drupal components are licensed under the GNU Affero General Public License (AGPL) v3.0
whose full text may be found at: http://www.fsf.org/licensing/licenses/agpl-3.0.html

All other content in this repository is licensed under MIT License unless otherwise specified.

### External services used during development

Some of the external services used.

<img src=""/doc/images/Browserstack-logo.svg"" width=200>

Browserstack is used to test the service with different browsers and devices.

[circleci-url]: https://circleci.com/gh/vrk-kpa/opendata
[circleci-image]: https://circleci.com/gh/vrk-kpa/opendata.svg?style=svg",VRAI
walt-id/waltid-ssikit,Toolkit,Toolkit,2024-07-16T13:58:10Z,2023-08-30T12:00:07Z,0,6,0,0,0,2,0,0,2021-08-04T08:19:14Z,2025-04-02T12:17:19Z,5273,109,Kotlin,VRAI,37,FAUX,0,"blockchain,cryptography,decentralized-identifiers,decentralized-identity,gradle,java,kotlin,self-sovereign-identity,siopv2,ssi,verifiable-credentials,wallet-service",0,All-In-One SSI infrastructure toolkit,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,17,"## MOVED to the new repo here => https://github.com/walt-id/waltid-identity

<br />
<div align=""center"">
    
 <h1>SSI Kit</h1>
 <span>by </span><a href=""https://walt.id"">walt.id</a>
 <p>Use digital identity<p>


[![Security Rating](https://sonarcloud.io/api/project_badges/measure?project=walt-id_waltid-ssikit&metric=security_rating)](https://sonarcloud.io/dashboard?id=walt-id_waltid-ssikit)
[![Vulnerabilities](https://sonarcloud.io/api/project_badges/measure?project=walt-id_waltid-ssikit&metric=vulnerabilities)](https://sonarcloud.io/dashboard?id=walt-id_waltid-ssikit)
[![Reliability Rating](https://sonarcloud.io/api/project_badges/measure?project=walt-id_waltid-ssikit&metric=reliability_rating)](https://sonarcloud.io/dashboard?id=walt-id_waltid-ssikit)
[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=walt-id_waltid-ssikit&metric=sqale_rating)](https://sonarcloud.io/dashboard?id=walt-id_waltid-ssikit)
[![Lines of Code](https://sonarcloud.io/api/project_badges/measure?project=walt-id_waltid-ssikit&metric=ncloc)](https://sonarcloud.io/dashboard?id=walt-id_waltid-ssikit)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=walt-id_waltid-ssikit-examples&metric=alert_status)](https://sonarcloud.io/dashboard?id=walt-id_waltid-ssikit)

[![CI/CD Workflow for walt.id SSI Kit](https://github.com/walt-id/waltid-ssikit/actions/workflows/build.yml/badge.svg?branch=master)](https://github.com/walt-id/waltid-ssikit/actions/workflows/build.yml)
<a href=""https://walt.id/community"">
<img src=""https://img.shields.io/badge/Join-The Community-blue.svg?style=flat"" alt=""Join community!"" />
</a>
<a href=""https://twitter.com/intent/follow?screen_name=walt_id"">
<img src=""https://img.shields.io/twitter/follow/walt_id.svg?label=Follow%20@walt_id"" alt=""Follow @walt_id"" />
</a>


</div>

## Discontinuation Notice

**Important:** Please be informed that, beginning from December 2023, the SSI Kit will no longer receive new features. Furthermore, the SSI Kit is planned for discontinuation by the end of Q3 2024.
<br />
<br />
However, all functionalities offered by the SSI Kit will be integrated into our new libraries, APIs, and apps in the walt.id [identity repo](https://github.com/walt-id/waltid-identity). Giving you more modularity, flexibility and ease-of-use to build end-to-end digital identity and wallet solutions.
<br />
<br />
For any clarification or queries, feel free to [contact us](https://walt.id/discord) as we aim to make this transition as smooth as possible.

<hr />

## Getting Started

- [CLI | Command Line Interface](https://docs.walt.id/v/ssikit/getting-started/cli-command-line-interface) - Try out the functions of the SSI Kit locally.
- [REST Api](https://docs.walt.id/v/ssikit/getting-started/rest-apis) - Use the functions of the SSI Kit via an REST api. 
- [Maven/Gradle Dependency](https://docs.walt.id/v/ssikit/getting-started/dependency-jvm) - Use the functions of the SSI Kit directly in a Kotlin/Java project.
- [Example Projects](https://github.com/walt-id/waltid-ssikit-examples) - Demonstrate how to use the SSI Kit in any Kotlin/Java app

Check out the **[Official Documentation](https://docs.walt.id/v/ssikit)**, to dive deeper into the architecture and configuration options available.


## What is the SSI Kit?

A **library** written in Kotlin/Java **to manage Keys, DIDs and VCs**. Functions can be used via **Maven/Gradle** or a **REST api**.

### Features
- **Key Management** generation, import/export
- **Decentralized Identifier (DID)** operations (create, register, update, deactivate)
- **Verifiable Credential (VC)** operations (issue, present, verify)
- **EBSI/ESSIF** related Use Cases (onboarding, VC exchange, etc.)

#### For EBSI
- **Onboarding EBSI/ESSIF** onboarding a natural person/legal entity including the DID creation and registration
- **Enable Trusted Issuer** process for entitling a legal entity to become a Trusted Issuer in the ESSIF ecosystem.
- **Credential Issuance** protocols and data formats for issuing W3C credentials from a Trusted Issuer to a natural person.
- **Credential Verification** verification facilities in order to determine the validity of a W3C Verifiable Credential aligned with EBSI/ESSIF standards.


## Example

- Creating W3C Decentralized Identifiers 
- Issuing/verifying W3C Verifiable Credentials in JSON_LD and JWT format

```kotlin
fun main() {
    // Load services
    ServiceMatrix(""service-matrix.properties"")

    // Create DIDs
    val issuerDid = DidService.create(DidMethod.ebsi)
    val holderDid = DidService.create(DidMethod.key)

    // Issue VC with LD_PROOF and JWT format (for show-casing both formats)
    val vcJson = Signatory.getService().issue(
        templateIdOrFilename = ""VerifiableId"",
        config = ProofConfig(issuerDid = issuerDid, subjectDid = holderDid, proofType = ProofType.LD_PROOF)
    )
    val vcJwt = Signatory.getService().issue(
        templateIdOrFilename = ""Europass"",
        config = ProofConfig(issuerDid = issuerDid, subjectDid = holderDid, proofType = ProofType.JWT)
    )

    // Present VC in JSON-LD and JWT format (for show-casing both formats)
    val vpJson = Custodian.getService().createPresentation(listOf(vcJson), holderDid)
    val vpJwt = Custodian.getService().createPresentation(listOf(vcJwt), holderDid)

    // Verify VPs, using Signature, JsonSchema and a custom policy
    val resJson = Auditor.getService().verify(vpJson, listOf(SignaturePolicy(), JsonSchemaPolicy()))
    val resJwt = Auditor.getService().verify(vpJwt, listOf(SignaturePolicy(), JsonSchemaPolicy()))

    println(""JSON verification result: ${resJson.policyResults}"")
    println(""JWT verification result:  ${resJwt.policyResults}"")
}
```

## Join the community

* Connect and get the latest updates: <a href=""https://discord.gg/AW8AgqJthZ"">Discord</a> | <a href=""https://walt.id/newsletter"">Newsletter</a> | <a href=""https://www.youtube.com/channel/UCXfOzrv3PIvmur_CmwwmdLA"">YouTube</a> | <a href=""https://mobile.twitter.com/walt_id"" target=""_blank"">Twitter</a>
* Get help, request features and report bugs: <a href=""https://github.com/walt-id/.github/discussions"" target=""_blank"">GitHub Discussions</a>

## Standards & Specifications

- [EBSI Wallet Conformance](https://api-conformance.ebsi.eu/docs/wallet-conformance) 
- [Verifiable Credentials Data Model 1.0](https://www.w3.org/TR/vc-data-model/) 
- [OpenID for Verifiable Credentials](https://openid.net/openid4vc/)
- [Decentralized Identifiers (DIDs) v1.0](https://w3c.github.io/did-core/) 
- [DID Method Rubric](https://w3c.github.io/did-rubric/)
- [did:web Decentralized Identifier Method Specification](https://w3c-ccg.github.io/did-method-web/) 
- [The did:key Method v0.7](https://w3c-ccg.github.io/did-method-key/)

## License

**Licensed under the [Apache License, Version 2.0](https://github.com/walt-id/waltid-ssikit/blob/master/LICENSE).**

## Funded & supported by

<a href=""https://essif-lab.eu/"" target=""_blank""><img src=""assets/logos-supporter.png""></a>",FAUX
web3infra-foundation/mega,Toolkit,Application System,2025-05-15T12:04:46Z,2025-05-11T14:16:31Z,0,0,0,1,0,0,0,0,2023-03-01T23:26:30Z,2025-04-08T02:37:47Z,41720,215,Rust,VRAI,42,FAUX,17,"buck2,decentralised,git,google,hacktoberfest,monorepo,p2p,p2p-git,piper,rust",17,Mega is an unofficial open source implementation of Google Piper.,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,29,"# Mega - is an unofficial open source implementation of Google Piper.

Mega is an unofficial open source implementation of Google Piper. It is a monorepo & monolithic codebase management system that supports Git. Mega is designed to manage large-scale codebases, streamline development, and foster collaboration.

## What's the Piper?

Google Piper is a massive, centralized version control system that Google uses internally to manage their vast codebase. It is a monorepo, and a monolithic which mean is a single repository that contains all the source code for Google's software. It is designed to manage large-scale codebases, streamline development, and foster collaboration. It is built on top of Google's internal infrastructure and is designed to be highly scalable and efficient. More information on the [Why Google Stores Billions of Lines of Code in a Single Repository](https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext).

**Google Piper is not open source**

## Mega features

Mega is an unofficial open source implementation of Google Piper. And it has the following features:

### Git compatible

Mega offers the ability to utilize Git with a monorepo. This allows for easy cloning or pulling of any monorepo folder into local filesystem as a Git repository, and seamless pushing of changes back.

### Trunk-based Development

When it comes to managing large codebases in a centralized manner, trunk-based development is the way to go. More trunk-based Development information on the [Trunk-Based Development](https://trunkbaseddevelopment.com/).

### Conventional Commits

Mega will support conventional commits, which are a set of rules for creating clear and concise commit messages.  More information on the [Conventional Commits](https://www.conventionalcommits.org/).

### Code Owners

Mega will support code owners, which are a set of rules for defining who owns a particular piece of code. More information on the [Code Owners](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/about-code-owners).

### Decentralized Open Source Collaboration

For now, the entire open source community base on Git and GitHub. It's centralized model, and it's not suitable for growing speed of open source world. Mega is working on build a decentralized open source collaboration model with [ZTM](https://github.com/flomesh-io/ztm)(Zero Trust Model) and decentralized social network like [Nostr](https://nostr.com), [Matrix](https://matrix.org) and [Mastodon](https://joinmastodon.org).

## Quick Try Monorepo Engine with Docker

For now, the monorepo engine could be deployed on your host machine or insulated into containers. For deploying through docker, follow the steps below:

1. Pull docker images from Docker Hub

```bash
$ docker pull genedna/mega:mono-pg-0.1-pre-release
$ docker pull genedna/mega:mono-engine-0.1-pre-release
$ docker pull genedna/mega:mono-ui-0.1-pre-release
```

2. Initialize for mono-engine and PostgreSQL

```bash
$ git clone https://github.com/web3infra-foundation/mega.git
$ cd mega
# Linux or MacOS
$ ./docker/init-volume.sh /tmp/data ./docker/config.toml
```

3. Run the mono-engine and PostgreSQL with docker, and open the mono-ui in your browser with `http://localhost:3000`.

```bash
# create network
$ docker network create mono-network

# run postgres
$ docker run --rm -it -d --name mono-pg --network mono-network -v /tmp/data/mono/pg-data:/var/lib/postgresql/data -p 5432:5432 genedna/mega:mono-pg-0.1-pre-release
$ docker run --rm -it -d --name mono-engine --network mono-network -v /tmp/data/mono/mono-data:/opt/mega -p 8000:8000 genedna/mega:mono-engine-0.1-pre-release
$ docker run --rm -it -d --name mono-ui --network mono-network -e MEGA_INTERNAL_HOST=http://mono-engine:8000 -e MEGA_HOST=http://localhost:8000 -p 3000:3000 genedna/mega:mono-ui-0.1-pre-release
```

4. Try to upload a repository to mono-engine

```bash
$ git clone http://localhost:8000/project.git
$ cd project
$ git clone https://github.com/dagrs-dev/dagrs.git
$ sudo rm -r dagrs/.git
$ git add .
$ git commit -a -m""Initial the dagrs project""
$ git push
```

5. Check the repository in UI
Open the mono-ui in your browser with `http://localhost:3000`, and you will see the `project` folder.

## Contributing

The mega project relies on community contributions and aims to simplify getting started. To develop Mega, clone the repository, then install all dependencies and initialize the database schema, run the test suite and try it out locally. Pick an issue, make changes, and submit a pull request for community review.

More information on contributing to Mega is available in the [Contributing Guide](docs/contributing.md).

## Talk and Share

If you interested in Mega, you can make an appointment with us on [Google Calendar](https://calendar.app.google/QuBf2sdmf68wVYWL7) to discuss your ideas, questions or problems, and we will share our vision and roadmap with you.

## License

Mega is licensed under this Licensed:

- MIT LICENSE ( [LICENSE-MIT](LICENSE-MIT) or https://opensource.org/licenses/MIT)
- Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or https://www.apache.org/licenses/LICENSE-2.0)",VRAI
web3privacy/explorer-data,Documentations,Database,2025-05-16T02:06:18Z,2025-05-13T01:56:07Z,0,0,0,0,0,1,0,0,2023-10-06T14:20:29Z,2025-03-25T16:19:22Z,37749,18,JavaScript,VRAI,34,FAUX,26,"database,privacy",26,Privacy Explorer Data Repository,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,36,"# Web3Privacy Now Data Repository
You can create/edit projects by making changes in index.yaml file, which you can find in project folder inside /src/projects/
Upload project logo in root of project directory to be automatically included. Check below for a short guide on how to do this

If you'd rather use our UI, you can also apply your changes through our editor. 

How to: <p>
[Edit a project](https://mirror.xyz/0x0f1F3DAf416B74DB3DE55Eb4D7513a80F4841073/yDbRRq8FjSogK7iUWdiRKkm54wvx6DgRt99gFuineuY) <p>
[Add a new project](https://mirror.xyz/0x0f1F3DAf416B74DB3DE55Eb4D7513a80F4841073/Ri2ZMIq6Os-ZKQyT_l6a5F1-gJURySvvwNRKzBvNpWM)

Information on how our scoring mechanism works [can be found here](https://mirror.xyz/0x0f1F3DAf416B74DB3DE55Eb4D7513a80F4841073/s9flkE6tMaJ4f2tzWu-FmDy7Zx_TRPe3jdXr2iYmYH0)
# How to Add or Update Your Project's Information to the Explorer Through GitHub

To add or update your project's information to the explorer, please follow these steps. **A GitHub account is required** to complete the process.

### Steps:

1. **Go to the GitHub repository** at the following URL: [www.github.com/web3privacy/explorer-data](<placeholder>)
2. Navigate to the `src` directory, then go into the `projects` folder.
3. In the upper-right corner, click on **""Create new file""**.
4. At this point, GitHub will ask you to **fork the branch**. Confirm the fork.
5. **Enter the name of your project** in the new directory you created. Make sure there is a slash before the name. This way you create the project folder. Example: /NAME 
6. Inside your project directory, create an `index.yaml` file. Follow the template here: [sample](https://github.com/web3privacy/explorer-data/blob/main/sample-project.yaml)
7. When you're ready to save, click **""Commit changes...""** in the top-right corner.
8. Toggle the option for **""Create a new branch for this commit and start a pull request""**.
9. Click **""Propose changes""**.
10. GitHub will redirect you to the **""Open a pull request""** page.
11. In the pull request title, add your title as such: `Create index.yaml <your_project_name>`.
12. Finally, click **""Create pull request""**. Adding a description is optional.

### Adding the Logo

1. Now you will be able to add the logo (PNG format, 400x400 pixels). 
2. Go to `www.github.com/explorer-data/tree/main/src/projects/(name of the file added)`.
3. In the top-right corner, click on **""Add file""** and then **""Fork this Repository.""**
4. Navigate to your GitHub profile, find the forked repository, and open the project you're working on. The URL should look like this: `github.com/(your GitHub username)/explorer-data/tree/main/src/projects/(name of the file added)`.
5. In the top-right corner, click on **""Add file""** and then **""Upload files.""**
6. A window will open where you can upload your logo (name the file just **""logo,""** without any other text).
7. At the bottom, select **""Create a new branch for this commit and start a pull request.""** Then press **""Propose changes.""**
8. The pull request page will open again. Add the file name and click on **""Create pull request.""**

🎉 **Congratulations!** Your project, including its logo, has now been submitted.

# Docs

There's an ongoing effort to upkeep our docs, [have a look here.](https://docs.web3privacy.info/projects/privacy-explorer/)

# Database

https://data.web3privacy.info/

# Project description
List of variables used for Privacy Explorer (https://explorer.web3privacy.info)
Feel free to submit any suggestions or changes to this scheme.



| Field                  | Type                  | Required | Description |
|------------------------|-----------------------|----------|-------------|
| id                     | string                | x        | Unique project identifier            |
| name                   | string                | x        | Name of project |
| categories             | array                 | x        | Categories are defined in explorer-data/schema/category.yaml |
| logos                  | array                 |          | Links to project logo (Note: Upload logos into root folder of project) |
| ecosystem              | array               |          | What is projects native networks? (ex. Ethereum, Arbitrum, Cosmos,...) |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| project_type           | string                |          | Main usecases of project (ex. ZK Pool mixer, Privacy transactions) |
| description            | string                |          | Short description of project features and mission |
| product_launch_day     | string                |          | Date of the project launch (YYYY-MM-DD)|
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| token_link             | string url            |          | Link to the project token contract |
| tokens                 | array                 |          | Native tokens of project (ex. privUSDC, sETH,...) |
| assets_used            | array                 |          | Digital assets that you can use in project/protocol (ex. BTC, ETH, USDC,...)|
| fee                    | string                |          | Cost of usage (ex. 0.15%, $5, None) |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| team anonymous        | boolean                |          | Is project developed by anonymous team? (Yes/No) |
| team teammembers name | string                 |          | Member's name |
| team teammembers role | string                 |          | Member's role |
| team teammembers link | string url             |          | Member's social link |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| funding name           | string                |          | Name of the investor |
| funding type           | string                |          | Type of investment (Seed, Round1, Angel investment,...) |
| funding link           | string url            |          | Link for more information about the investment |
| funding value          | string                |          | Value of the investment ($1,500,000) |
| funding time           | string                |          | Date of the investment (YYYY-MM-DD)|
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| history title          | string                |          | Title of events/news related to the project |
| history event_type     | string                |          | Type of event (e.g., Product release, Hack, Launch) |
| history description    | string                |          | Description of the event |
| history time           | string                |          | Time of the event (YYYY-MM-DD) |
| history link           | string url            |          | Link to more information about the event |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| sunset                 | boolean               |          | Project is not active, is not recommended for use, no development  |




# Project Links

| Field                  | Type                  | Required | Description |
|------------------------|-----------------------|----------|-------------|
| links web              | string url            |          | Official project website |
| links github           | string url            |          | Link to the project GitHub repository |
| links block_explorer   | string url            |          | Project or Network Block Explorer |
| links docs             | string url            |          | Link to project documentation |
| links forum            | string url            |          | Link to project forum (ex. Discourse) |
| links whitepaper       | string url            |          | Link to the project whitepaper |
| links changelog        | string url            |          | Link to changelog |
| links snapshot         | string url            |          | Link to Snapshot |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| links blog             | string url            |          | Project Blog (ex. Medium) |
| links twitter          | string url            |          | Project Twitter profile |
| links discord          | string url            |          | Project Discord server |
| links facebook         | string url            |          | Project Facebook page |
| links telegram         | string url            |          | Project Telegram |
| links lens             | string url            |          | Project Lens profile  |
| links farcaster        | string url            |          | Project Farcaster profile |
| links rss_feed         | string url            |          | Link to a information stream related to the project |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| links education        | string url            |          | Links to useful education content about project |




# Technology

| Field                  | Type                  | Required | Description |
|------------------------|-----------------------|----------|-------------|
| technology type        | string                |          | Technology that runs underhood (ex. ZK, ZK-Snarks, PLONK, Monero, Ellipcic curves,...) |
| technology features    | array                 |          | Key aspects of the privacy tech used (ex. Private wallet, P2P Swap, Fluid compliance, ZK Defi, Private bridge,...) |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| blockchain_features encryption | string        |          | Encryption used (ex. Cryptonight, Groth16, ECDH,...) |
| blockchain_features opensource | boolean       |          | Is project opensourced? (Yes/No) |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| project_status live_status | boolean           |          | Is the project currently working? (Yes/No) |
| project_status version | string                |          | Name of the latest version (Ex. Arbitrum Nitro, V2.12, Prototest10,...) |
| project_status testnet | boolean               |          | Does the project have a running testnet? (Yes/No)|
| project_status mainnet | boolean               |          | Is the mainnet running? (Yes/No) |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| client_diversability name | string             |          | Name of the clients using project (ex. Wallet name, Bridge name, Project name,...) |
| client_diversability link | string url         |          | Link to the client's website |




# Privacy

| Field                  | Type                  | Required | Description |
|------------------------|-----------------------|----------|-------------|
| licences               | string                |          | Licenses used (ex. MIT License, GGML,...) |
| privacy_policy defined | boolean               |          | Is there a document defining privacy policies? (Yes/No)|
| privacy_policy link   | string url             |          | Link to the privacy policy document |
| privacy_policy data_usage | string             |          | How is the project using your data? (ex. Selling data, Analytics., Not using,...) |
| tracebility tracked_data | string             |          | What data is the project tracking (ex. Address, Name, Phone, IP,...) |
| default_privacy        | boolean               |          | Is privacy applied by default or must it be turned on? (Yes/No) |
| compliance             | string                |          | Does the project comply with any official blacklists? (ex. OFAC, Hacker_wallet_list, USA,...) |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| storage decentralized | boolean                |          | Is any part of used data stored on decetralized networks? (Yes/No)|
| tracebility kyc      | boolean                |          | Is KYC required for product usage? (Yes/No) |
| tracebility sign_in_type_requirments | string |          | What Sign-in information is required to use the project? (ex. Wallet, Email, Gitcoin Pass,...) |
| blockchain_features identity_integration | string|        | What Identity integration project use? (ex. Gitcoin Pass, Proof of Humanity, Degenscore,...)  |
| blockchain_features p2p | boolean              |          | Is the project Peer to Peer-based or enabling such a feature? (Yes/No) |
| blockchain_features data_masking | string      |          | What type of anonymity mechanism is used? (Mixer, ZK Pool,...)  |
| blockchain_features viewing_key | boolean      |          | Is there a viewing key that can decode your transactions? (Yes/No) |
| blockchain_features dissapearing_tx | boolean  |          | Is there any trace of your transaction on-chain / online? (Yes/No) |
| blockchain_features connected_tx | boolean     |          | Is is possible to identify other transactions when is your address revealed? (Yes/No) |
| blockchain_features frontend_anonymity | string|          | Are you able to use project with TOR / VPN or other anonymity tools? (TOR address, Geo restricted, VPN banned,...) |




# Security

| Field                  | Type                  | Required | Description |
|------------------------|-----------------------|----------|-------------|
| third_party_dependency | string                |          | What third-party technological dependencies project have? (ex. Uniswap hack, USDC stability,...) |
| social_trust           | string                |          | Is there any social dependency (ex. Governance, Board multisig, CEO have 50% tokens,...) |
| technical_spof         | string                |          | Is there any technical single point of failure? (ex. Pool hack, Bridge malfunction, Viewing key leak,...) |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| blockchain_features asset_custody_type | string   |       | Are users assets in any custody, by who? Or is it fully non-custodial? (ex. DAO Multisig, Non-custodial, Centralized bridge,...)  |
| blockchain_features upgradability enabled | boolean |     | Is the project upgradable or immutable? (Yes/No)|
| blockchain_features upgradability type | string |         | Upgradability type (ex. DAO Governance, Admin keys, Random Consensus,...) |
| blockchain_features upgradability admin_keys | string |   | Upgradability condition definition (ex. Multisign of 5, 80% consensus,...) |
|                        |                       |          |              |
|                        |                       |          |              |
|                        |                       |          |              |
| audits name            | string                |          | Name of the project audit (ex. Certik SmartContract check,...) |
| audits company         | string                |          | Company that processed the project audit (ex. Certik, Blocksec,...) |
| audits logo            | string url            |          | Logo of the audit company |
| audits link            | string url            |          | Official link to the audit company website |
| audits time            | string                |          | Date of the audit (YYYY-MM-DD) |



# License

Open Data Commons Open Database License (ODbL)",VRAI
webiny/webiny-js,Application System,Toolkit,2025-05-16T04:00:41Z,2025-04-11T04:16:45Z,0,0,73,0,0,0,0,0,2018-01-09T13:09:44Z,2025-04-07T19:34:39Z,412302,7561,TypeScript,VRAI,622,FAUX,327,"aws,aws-lambda,cloud,cms,graphql,headless,headless-cms,javascript,lambda,lambda-functions,microservice,nodejs,react,serverless,serverless-applications,serverless-architectures,serverless-framework,spa,typescript",327,"Open-source serverless enterprise CMS. Includes a headless CMS, page builder, form builder, and file manager. Easy to customize and expand. Deploys to AWS.",FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,81,"<p align=""center"">
  <img src=""./docs/static/webiny-logo.svg"" width=""250"">
  <br><br>
  <strong>Open-Source Serverless Enterprise CMS</strong>
</p>
<p align=""center"">
  <a href=""https://www.webiny.com"">Website</a> |
  <a href=""https://www.webiny.com/docs/webiny/introduction/"">Documentation</a> |
  <a href=""https://www.webiny.com/slack"">Community Slack</a> |
  <a href=""https://github.com/webiny/webiny-js/discussions"">Forum</a> |
  <a href=""https://twitter.com/WebinyCMS"">Twitter</a> 
</p>

#

<p align=""center"">

[![Prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg)](https://prettier.io)
[![license](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/webiny/webiny-js/blob/master/LICENSE)
[![SemVer](http://img.shields.io/:semver-2.0.0-brightgreen.svg)](http://semver.org)
![](https://img.shields.io/npm/types/scrub-js.svg)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](/docs/CODE_OF_CONDUCT.md)
[![Join our Slack community https://www.webiny.com/slack](https://img.shields.io/badge/Slack-Join%20our%20community!-orange)](https://www.webiny.com/slack)

</p>


https://user-images.githubusercontent.com/2216344/194592342-2a63da40-136c-4190-9776-680d1ac2382f.mp4


Webiny Serverless CMS includes:

1️⃣ **Page Builder** - Drag&drop page editor. Pages are prerendered automatically and cached on CloudFront for lightning-fast delivery. 

2️⃣ **Headless CMS** - Headless CMS with a GraphQL API. Build APIs and content models through a UI. It includes content revisions, localization, and fine-grain permission control.

3️⃣ **File Manager** - Upload files images. Search and organize your assets. It includes a built-in image editor for basic image manipulations.

4️⃣ **Form Builder** - Build forms with a drag&drop editor. Insert forms through Page Builder into your pages. It has webhook support and ReCaptcha integration.


All Webiny apps can be customized easily to fully fit an enterprise publishing workflow and integrate with leading identity providers like OKTA and Cognito.
<br /><br />

## 🏁 Quick installation guide

1. Create a Webiny project:

`npx create-webiny-project my-new-project`

2. Deploy to your AWS cloud:

`yarn webiny deploy`


**Prerequisites**

- Node.js ^20
- yarn ^1.22.21
- AWS account

For the detailed install guide, please see 👉 https://www.webiny.com/docs/get-started/install-webiny

Need help, having trouble installing, find us on our community slack 👉 https://www.webiny.com/slack
<br /><br />

## 📕 Documentation

For complete documentation 👉 https://www.webiny.com/docs
<br /><br />

## 🤝 Community & Support

Community Forum. Best for: help with building, discussion about database best practices 👉 https://www.webiny.com/slack

GitHub Issues. Best for: bugs and errors you encounter using Webbiny 👉 https://github.com/webiny/webiny-js/issues
<br /><br />

## 💪 Contributing

Webiny is all about the community. Please feel free to join in, whether it's fixing bugs, improving our documentation, or simply spreading the word. Please see our [Contributing Guidelines](/docs/CONTRIBUTING.md), which explain project organization, setup, testing, and other steps.
If you need any assistance in contribution, please reach out via our [community Slack](https://www.webiny.com/slack).
<br /><br />

## 📜 License

This project is licensed under the terms of the [MIT license](/LICENSE) except for the following modules, which require a Webiny Enterprise license:

- Multi-tenancy module
- OKTA integration

Contact sales@webiny.com for more information.

Why are those modules paid? It's a way we support the development of the project!
<br /><br />

## 👷‍♀️ When to use Webiny?

Webiny has many features, too many to list to make this readme digestible, so instead of talking about features, here are the common use-cases you can satisfy using Webiny: 

- **Headless CMS** - Programmatically integrate your apps with Webiny's GraphQL Headless CMS.

- **GraphQL API** - You can build a GraphQL API using the [Headless CMS](https://www.webiny.com/serverless-app/headless-cms), but you can [also use the `webiny scaffold` command](https://www.webiny.com/docs/how-to-guides/scaffolding/graphql-api) to create new GraphQL resolvers where you can add your custom business logic.

- **Marketing landing pages and micro-sites** - Using the [Page Builder](https://www.webiny.com/serverless-app/page-builder) marketing teams can quickly build new websites without knowledge of HTML or CSS. 

- **Multi-tenant SaaS applications** - Webiny has a robust multi-tenancy layer with built-in data separation. You can build your own SaaS applications on top and let Webiny handle the API, security, and data storage for you.

- **Full-stack serverless applications** - Besides using Webiny to manage your content needs, you can expand the existing functionality by creating new full-stack serverless applications on top. Follow [this tutorial](https://www.webiny.com/docs/tutorials/create-custom-application/introduction) to build your own full-stack serverless Pinterest clone.

- **Multi-website & multi-language portal** - All Webiny apps are multi-tenant by default, meaning with a single instance of Webiny, you can run hundreds of projects and websites from a single code-base.

- **Dynamic Page** <sup>(coming soon)</sup> - We're working on seamless integration between the Headless CMS and the Page Builder, so you can build and publish dynamic pages without a single line of code or build pipelines required. New content is live instantly and visible to the users.

- **Multi-cloud support** <sup>(coming soon)</sup> - At the moment, Webiny only supports AWS, but we have plans to add support for other cloud vendors such as GCP and Azure. Because Webiny uses cloud-native services to run, not containers, this task is not easy, but we have a plan.

<br /><br />

## ❓ FAQ

**♦ Why serverless?**

We believe serverless is the future of web development. It gives us much more bang for our buck!

**♦ Why open-source?**

Open-source has two main aspects over SaaS: 
1. It's customizable, unlike being locked in a SaaS solution.
2. Your data is stored under your rules, in your data center, with your compliance standard inside your security parameter and delivered through your CDN.

**♦ How is this enterprise?**
1. Webiny is built to be integrated inside enterprise environments. Being open-source is one part of that solution; the other is that Webiny integrates with enterprise IdPs such as OKTA and Cognito.
2. Webiny is architected to sustain heavy usage coming from large volumes of users. 
3. Webiny is built on top of fault-tolerant serverless services.
4. Webiny keeps the data encrypted both in transit and at rest.
5. In the paid edition, enterprises have access to our SLA-based support and consultancy services.

**♦ How fast and scalable is Webiny?**

How about a [load-test](https://www.webiny.com/docs/webiny-overview/performance-benchmark/introduction/) :)

**♦ How much does it cost to run Webiny?**

Webiny comes in 2 database options, DynamoDB + Elasticsearch and DynamoDB only. The latter option, when looking at all the infrastructure pieces Webiny uses to operate, the consumption of the AWS services fully determines the cost. In the DDB + ES option, there is a minimum ~$25/mo charge to AWS for the Elasticsearch cluster as it's not a consumption-based service.

As part of our performance benchmark, we also benchmarked the cost of the DDB + ES, specifically, Headless CMS [read](https://www.webiny.com/docs/webiny-overview/performance-benchmark/headless-cms-read-benchmark#cost-per-10k-requests) and [write](https://www.webiny.com/docs/webiny-overview/performance-benchmark/headless-cms-write-benchmark#cost-per-10k-requests) operations. So that benchmark is a good starting point to determine your cost.

As a rule of thumb, we recommend the DDB option for small and medium-size projects, which should be cheaper when compared to a solution running on VMs or containers.


**♦ Why should my enterprise consider using Webiny?**

Top 5 reasons to do so:
1. **Self-hosted**: Webiny runs inside your own AWS cloud; you keep control over your data and security perimeter.
2. **Open-source**: We released Webiny under the MIT license, so you can customize every aspect of the system to match your needs fully.
3. **Serverless**: Webiny runs on AWS services such as Lambda, S3, and DynamoDB, to offer a highly scalable and fault-tolerant infrastructure.
4. **Cost-savings**: Cut your infrastructure and operations costs by 60% to 80% compared to solutions running on VMs.
5. **Secure**: Webiny follows security best practices by encrypting data both in transit and rest across all services. It integrates with IdPs such as OKTA and Cognito. CodeQL and Dependabot scanning tools ensure code security.

<br />

## Contributors

### 🧡 Thanks goes to these wonderful people!

<a href=""https://github.com/webiny/webiny-js/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=webiny/webiny-js"" />
</a>",VRAI
whchoi98/myeks,Application System,Documentations,2025-05-08T12:10:57Z,2025-01-25T07:29:02Z,0,0,0,0,0,0,0,0,2020-06-26T06:15:15Z,2025-04-07T14:50:40Z,21523,8,Shell,VRAI,16,FAUX,1,,1,,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,FAUX,10,,VRAI
wkspower/wks-platform,Application System,Application System,2025-05-14T16:40:41Z,2025-04-18T15:56:19Z,0,14,0,0,0,0,0,0,2022-08-16T08:52:11Z,2025-03-22T13:54:33Z,18900,39,Java,VRAI,18,FAUX,47,,47,"WKS Platform is a cutting-edge Adaptive Case Management platform built on top of Camunda workflow automation engine. Designed to empower organizations with agile and flexible case management capabilities, WKS Platform enables efficient handling of complex, unstructured processes.",FAUX,VRAI,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,17,"# WKS Platform

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

[On-line documentation](https://docs.wkspower.com/docs/Introduction/)

[Contact Form](https://share-eu1.hsforms.com/1tpt0kdYDS5CbimQTH7xmVA2dcag3)

[Subscribe for news & updates](https://share-eu1.hsforms.com/1gpWZRXQwSoWQNgCeuztetQ2dcag3)

Table of Contents
-----------------
- [Features](#features)
- [Installation](#installation)
- [Diagrams](#diagrams)
- [Screenshots](#screenshots)
- [License](#license)
- [Contact](#contact)

WKS Platform is an open-source Case Management and Process Automation solution that leverages a powerful stack of technologies, including Camunda, MongoDB, Keycloak, Traefik, MinIO, OPA (Open Policy Agent), Form.io, Spring Boot, and React. It provides a comprehensive framework for managing and automating business processes, enabling organizations to streamline their operations, enhance efficiency, and improve decision-making.

WKS Platform is ideal for organizations in various industries, including but not limited to:
- **Financial Services**: Streamline and automate complex financial processes, such as loan approvals, claims management, and risk assessment.
- **Healthcare**: Efficiently manage patient cases, automate healthcare processes, and ensure compliance with regulatory requirements.
- **Insurance**: Automate insurance claims processing, underwriting, policy management, and enhance customer service.
- **Manufacturing**: Streamline production workflows, manage quality control processes, and improve supply chain management.
- **Government**: Automate administrative processes, citizen service requests, and regulatory compliance procedures.
- **Education**: Simplify student enrollment, course registration, and academic workflows.

These are just a few examples of the industry-specific scenarios where WKS Platform can be effectively utilized. Its flexibility and extensibility make it suitable for a wide range of use cases.

WKS Platform is designed as a multi-tenant solution, allowing multiple organizations or departments to use the platform while ensuring data isolation and separation. It provides a secure and customizable environment for each tenant, enabling them to manage their cases, automate processes, and make data-driven decisions within their own dedicated space.

## Features

- **Case Management**: WKS Platform offers a robust case management system that allows users to track and manage cases throughout their lifecycle. It provides features such as case creation, assignment, status tracking, activity logging, and case resolution.

- **Process Automation**: With Camunda at its core, WKS Platform enables the automation of complex business processes. Users can design, model, and execute workflows, define process steps and decision points, and monitor process instances in real-time.

- **Intuitive User Interface**: WKS Platform incorporates a responsive and user-friendly React-based frontend interface. It provides a rich set of features, including task management, case visualization, process monitoring, and reporting, ensuring an intuitive user experience.

- **Dynamic Form Creation**: By leveraging Form.io, WKS Platform allows users to design and create dynamic forms that adapt to specific case requirements. This empowers organizations to collect structured data efficiently, ensuring data consistency and enabling streamlined processes.

- **Data Persistence**: Leveraging MongoDB, WKS Platform ensures reliable and scalable data storage for case-related information. This facilitates efficient retrieval and analysis of data to gain insights and support decision-making.

- **Identity and Access Management**: Integration with Keycloak offers robust identity and access management capabilities, including user authentication, authorization, and role-based access control. This ensures secure access to the platform and its features, protecting sensitive information.

- **Policy Enforcement**: WKS Platform integrates with OPA (Open Policy Agent) to enforce fine-grained policies across the system. Policies can be defined to control access, validate data, enforce business rules, and ensure compliance with organizational regulations.

- **MinIO Integration**: WKS Platform integrates with MinIO, an object storage server, for efficient and scalable storage of files and attachments associated with cases and processes.

- **E-mail to Case**: WKS Platform includes a comprehensive E-mail to Case feature that enables seamless integration between email communication and case management. Users can not only create cases from incoming emails but also receive case-related updates and notifications via email, ensuring efficient and effective communication throughout the case lifecycle. 

- **Robust Backend**: Built on the Spring Boot framework, WKS Platform provides a scalable and high-performance backend infrastructure. It offers reliable API endpoints, data integration capabilities, and supports extensibility through modular design principles.
  
- **Robust API Security with Trust-based Architecture**: WKS Platform prioritizes security and follows the principles of Zero Trust architecture. It ensures safe API communications by implementing secure protocols, encryption, and authentication mechanisms.

- **Traefik Integration**: WKS Platform seamlessly integrates with Traefik, a modern reverse proxy and load balancer, to provide scalable and secure routing of HTTP traffic to the platform's components.

- **Multi-Language Support**: WKS Platform is designed to be a multi-language project, utilizing internationalization (i18n) techniques. This allows for the localization of the platform's interface, making it accessible and usable in different languages. Currently, it supports English and Brazilian Portuguese, with the potential to expand support for additional languages. 

## Installation

[https://www.wkspower.com/docs/instalation-guide/](https://doc.wkspower.com/docs/Installation/Option%201%20Pre-built%20Docker%20Images/)

## Diagrams 

### Architecture overview
<img width=""840"" alt=""image"" src=""https://github.com/wkspower/wks-platform/assets/85225281/323e4811-2a44-4c23-9d38-1f99942dcae5"">


### Case Definition structure
<img width=""1507"" alt=""case-definition-structure"" src=""https://github.com/wkspower/wks-platform/assets/85225281/d478345f-8192-4196-ae53-868151363cf1"">

### Event Hub
![image](https://github.com/wkspower/wks-platform/assets/85225281/1f393c1a-84b7-42d3-971b-0e9a49240d27)


## Screenshots

Form Designer
<img width=""1487"" alt=""image"" src=""https://github.com/wkspower/wks-platform/assets/85225281/9e28708b-0547-4e5f-bee1-5721f63186e7"">

Case kanban
<img width=""1507"" alt=""image"" src=""https://github.com/wkspower/wks-platform/assets/85225281/19ac1611-4328-466b-8574-72a33486edac"">

Task List in a Case
<img width=""1511"" alt=""image"" src=""https://github.com/wkspower/wks-platform/assets/85225281/0c437058-f63e-4d9a-b207-6758aa6a2486"">

Task Form
<img width=""1501"" alt=""image"" src=""https://github.com/wkspower/wks-platform/assets/85225281/55db2e63-64a9-45b8-8d3b-b231f4ac2c31"">

Process diagram in Task Form
<img width=""1488"" alt=""image"" src=""https://github.com/wkspower/wks-platform/assets/85225281/40342c86-451f-40cc-b820-3ae9ee9fc24e"">

Create ad-hoc tasks in cases
<img width=""1512"" alt=""image"" src=""https://github.com/wkspower/wks-platform/assets/85225281/cb98d635-66bd-4877-bcbc-06bac437eb45"">

## License

WKS Platform is released under the [MIT License](LICENSE), allowing users to freely use, modify, and distribute the solution as per the terms of the license.

## Contact

For any questions, feedback, or contributions, please reach out to the project team:

- Email: victor@wkspower.com",FAUX
worldbank/sdg-metadata,Application System,Documentations,2024-11-20T16:06:52Z,2024-04-04T22:06:12Z,0,0,0,0,0,80,0,0,2020-01-18T16:00:09Z,2024-11-20T16:07:29Z,4187048,8,JavaScript,VRAI,7,FAUX,4,,4,SDG Metadata Translation Pilot,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,19,"# SDG Metadata Translation Pilot

Evaluating machine translation of indicator metadata for the Sustainable Development Goals.

[Find more information on this project](https://worldbank.github.io/sdg-metadata/).

## Requirements

Node.js and Ruby 2.x.

## Local installation

To try it out locally, run:

```
make install
make serve
```

## Harvesting the latest English source material

```
git checkout master
git pull origin master
make install
make harvest
git add translations-metadata
git commit -m ""HARVEST: YYYY-MM-DD""
git push origin master
```

Note that - if the `make harvest` gives any warnings or outputs, consult with UNSD to resolve them. You may need to tweak the [list of hardcoded indicator IDs](https://github.com/worldbank/sdg-metadata/blob/master/scripts/harvest-indicators.js#L75).",VRAI
X-lab2017/open-digger,Toolkit,Application System,2025-05-16T09:24:25Z,2024-12-10T11:58:51Z,0,0,0,0,0,4,0,0,2020-08-18T11:03:58Z,2025-03-31T07:25:22Z,53856,322,Jupyter Notebook,VRAI,95,FAUX,24,"data-analysis,github,hacktoberfest,openrank",24,Open source analysis tools,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,47,"# OpenDigger

[![apache2](https://img.shields.io/badge/license-Apache%202-blue)](LICENSE) [![](https://img.shields.io/badge/Data-OpenDigger-2097FF)](https://github.com/X-lab2017/open-digger) [![Node.js CI](https://github.com/X-lab2017/open-digger/actions/workflows/node_ci.yml/badge.svg?branch=master)](https://github.com/X-lab2017/open-digger/actions/workflows/node_ci.yml)

OpenDigger is an open source analysis platform for all open source data initiated by [X-lab](https://x-lab.info), this project aims to combine the wisdom of global developers to jointly analyze and insight into open source related data to help everyone better understand and participate in open source.

## Documentation

Please visit the [official website](https://open-digger.cn/) to find more documentation about OpenDigger.

## Communication

Welcome to join the WeChat group by scanning the QRCode and I will invite you into our WeChat group.

<div align=center>
<img src='./docs/assets/wechat-qrcode.png' width=""250px"">
</div>

## License

We use [Apache-2.0 license](LICENSE) for code part, please make sure abide by the licenses when using the project.",VRAI
XgridInc/xc3,Toolkit,Toolkit,2024-04-19T06:35:38Z,2023-04-19T07:16:08Z,0,0,0,0,0,7,0,0,2023-05-11T15:16:40Z,2024-12-11T04:04:43Z,3143,52,Python,VRAI,48,FAUX,22,"cloud,control,cost,optimize",22,"XC3 is a cloud agnostic and risk free package offering powered by Cloud Custodian that provides resource inventory, tagging compliance, unused or invalid resources cleanup, account maintenance, cost control, backups, monitoring and alerting.",FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,11,"<br>

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://github.com/XgridInc/xc3)
[![Slack](https://slackin.px.dev/badge.svg)](https://app.slack.com/client/T055VHJ0087/C0571UK3SBG)
[![Open AI Reviewer](https://github.com/XgridInc/xc3/actions/workflows/openai-pr-reviewer.yml/badge.svg)](https://github.com/XgridInc/xc3/actions/workflows/openai-pr-reviewer.yml)
[![Code Linter](https://github.com/XgridInc/xc3/actions/workflows/linter.yml/badge.svg)](https://github.com/XgridInc/xc3/actions/workflows/linter.yml)
[![Shellcheck](https://github.com/XgridInc/xc3/actions/workflows/shellcheck.yml/badge.svg)](https://github.com/XgridInc/xc3/actions/workflows/shellcheck.yml)
[![Code Vulnerability](https://github.com/XgridInc/xc3/actions/workflows/checkov.yml/badge.svg)](https://github.com/XgridInc/xc3/actions/workflows/checkov.yml)

<br>

# XC3

Xgrid Cloud Cost Control is a cloud agnostic and risk free package offering powered by Cloud Custodian that provides security enforcement, tagging, unused or invalid resources cleanup, account maintenance, cost control, and backups. It supports managing AWS public cloud environments and provides a visualization of usage of resources in account with support of managing resource utilization on a click. It spins up automation scripts and triggers lambdas to control cost of running resources in aws accounts and maintain state of each resource on which action performed having real-time visibility into who made what changes from where, enables us to detect misconfigurations and non-compliance. It supports rollback plans to prevent risks from materializing. Cloud Cost Control supports conditional policy execution. It generates reports, region vise and maintains state as well.

Check the below video for a quick demo of XC3.

[![XC3 Youtube](https://user-images.githubusercontent.com/114464405/229470468-ab186c9a-c475-40f2-9758-b89a7a3555d9.png)](https://www.youtube.com/watch?v=K4eEcl3wTZ0)

## Features

- One platform to track all your cloud resources be it cloud, multi-cloud, or hybrid infrastructure. It can track GCP, Azure, and AWS resources on a single UI.

- Enforces Tagging compliance that plays a vital role in determining the resources cost and many other aspects as well

- Provides Scheduled monitoring and alerting workflow that helps to track resource utilization and take action immediately.

- Provides cost optimization recommendation workflow without exposing your private information

# XC3 System Architecture Visual Overview

<b>` XC3 has two architecture diagrams, representing its 'Dev' and 'Prod' environments.`</b>

# XC3 Dev Architecture

![XC3 Dev Architecture](https://github.com/XgridInc/xc3/assets/138758061/8bd4a8f3-ee54-44ee-a152-865d7ce6bb2b)

`This diagram illustrates the architecture of the ""dev"" environment for XC3. Below are the key components:`

- EC2 Instance (Public Subnet): Acts as the entry point for the ""dev"" environment.
- Lambda Functions (Private Subnet): Executes serverless tasks within a secure private subnet.
- SQS (Simple Queue Service): Provides queuing capability for asynchronous tasks.
- SES (Simple Email Service): Handles email communications.
- Cost Explorer: Assists in analyzing and managing costs.
- Scheduled CloudWatch Events: Enables automated event triggering.
- S3 (Simple Storage Service): Used for storing state files and other data.
- Additional services (Push Gateway, Grafana, Prometheus, Cloud Custodian) run on the EC2 instance to monitor and manage the environment.

Access to the ""dev"" environment is primarily through the EC2 instance's IP address. SSH is available for administrative purposes.

# XC3 Prod Architecture Diagram

![XC3 Prod Architecture](https://github.com/XgridInc/xc3/assets/122358742/1f9b1c1e-92ca-4b2e-af17-8465214f25e9)

`This diagram illustrates the architecture of the ""prod"" environment for XC3. It includes the following components:`

- Cognito: Manages user authentication and authorization.
- Route 53: Provides DNS routing services for efficient access.
- Elastic Load Balancer: Distributes incoming traffic to ensure high availability.
- EC2 Instance and Lambda Functions (Private Subnet): Similar to the ""dev"" environment but with additional security and scalability measures.
- SQS (Simple Queue Service): Handles queuing tasks.
- SES (Simple Email Service): Manages email communications.
- Cost Explorer: Assists in analyzing and managing costs.
- Scheduled CloudWatch Events: Enables automated event triggering.
- S3 (Simple Storage Service): Used for storing state files and other data.
- EIC Endpoint (Endpoint Isolation and Control): Enhances security and isolation within the ""prod"" environment.

Access to the ""prod"" environment is facilitated through a DNS URL, thanks to Route 53. This architecture prioritizes security, scalability, and high availability to support the production environment for XC3.

# To start using XC3

## Requirements

---

- [Terraform](https://www.terraform.io/downloads.html) 1.0+
- [Python](https://www.python.org/downloads) 3.9
- [AWScli](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html)
- [Cloud Custodian](https://cloudcustodian.io/docs/quickstart/index.html#install-cloud-custodian)
- [Prometheus/Grafana/Pushgateway](https://github.com/Einsteinish/Docker-Compose-Prometheus-and-Grafana.git)
- [checkov](https://github.com/bridgecrewio/checkov) 2.0.574 or later
- [shellcheck](https://github.com/koalaman/shellcheck) 0.7.1 or later

## Pre-requisites

---

1. Clone GitHub repo
   ` git clone https://github.com/XgridInc/xc3.git`
2. An AWS user with specific permission set user access.

   Refer the IAM Permission Set created in `pre_requirement` folder to setup XC3.

3. VPC needs to be present in the master account where you want to set up XC3

4. To store terraform state and to maintain lock, S3 bucket and dynamodb should be available in master account.

5. ACM certificate should be available. It will be associated with loadbalancer and domain.

6. The user has to **enable CostExplorer** by following the below link.

   https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ce-enable.html

   ```
   Note: After enabling CE, it may take up to 24hours for AWS to start capturing your AWS account cost data, hence XC3 may not show the data until CE data is available in AWS account
   ```

# Deployment

1.  Clone the GitHub repository in your local computer to setup XC3 infrastructure.

    ```
    git clone https://github.com/XgridInc/xc3.git
    ```

2.  Go to the directory xc3/ and configure the input.sh file and run the below command

    ```
    cd xc3/

        Note :
            - Configure the input.sh file in directory xc3/

               namespace=""example""
               project=""example""
               region=""eu-west-1""
               allow_traffic=""0.0.0.0/0""
               domain="""" #  [Optional] - If you want to use your own domain then set this variable.
               account_id=""123456789012""
               hosted_zone_id=""Z053166920YP1STI0EK5X""
               owner_email=""admin@example.co""
               creator_email=""admin@example.co""
               ses_email_address=""admin@example.co""
               bucket_name=""terraform-state-example""

            - Before running the below mentioned command:

    bash init.sh
    ```

3.  Wait for few minutes before proceeding further for the application to come online.
    Verify the readiness of the metrics system. Load the Grafana URL in a browser. Live Grafana UI ensures the system is ready to accept and visualize metrics.

    > Verify the readiness of metrics system by accessing Grafana UI: https://xc3.xxx.com/login

    > Verify the readiness of metrics system by accessing Grafana UI: `loadbalancer-dns`. If Hosted zone ID is not provided in `input.tfvars`.

4.  Now setup is complete. If domain is provided in the input.sh then users needs to be added in Cognito pool with requested role (admin/editor/viewer) in respective cognito group. User get random username/password from cognito then you can set password on domain by sign in using random credentials.

5.  SSH into the private instance using EIC Endpoint to check if everything is working fine. Here replace [instance-id] needs to be replaced with ID

    `ssh ubuntu@[instance-id] -i keypair.pem -o ProxyCommand='aws ec2-instance-connect open-tunnel --instance-id %h'`

6.  Go to AWS Systems Manager, select Parameter Store, and create a new parameter named ""/{namespace}/region_names"". Set the value as a dictionary with region IDs as keys and region names as values.

7.  SSH into the private instance using EIC Endpoint to check if everything is working fine. Here replace [instance-id] needs to be replaced with ID

    `ssh ubuntu@[instance-id] -i keypair.pem -o ProxyCommand='aws ec2-instance-connect open-tunnel --instance-id %h'`

8.  Now XC3 will run at 05:00AM UTC every day to generate data and populate Grafana. Few lambdas (Total Account Cost and Project spend) will run twice in a month.

        Note :
            1. If data is not available in Grafana UI then follow the troubleshooting guide at the last section of this page.

# Troubleshooting Guide

case 1: If data is not showing into Grafana UI, there could be several reasons as shown below.

1. If AWS account was created freshly within last 24 hours then, you need to enable CostExplorer by following below link

   https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ce-enable.html

2. If the AWS account was created freshly within the last 24 hours then, it may take up to 24 hours for the AWS team to generate cost information in your account.
   you may see below error in lambda logs in Cloudwatch

   [ERROR] DataUnavailableException: An error occurred (DataUnavailableException) when calling the GetCostAndUsage operation: Data is not available. Please try to adjust the time period. If just enabled Cost Explorer, data might not be ingested yet

3. XC3 Budget Detail/IAM Role/User Workflow lambda may have failed to execute , please check Cloudwatch logs to address the issue.

4. Check if XC3's most expensive services data is missing, and if so, verify the existence of the corresponding SSM parameter in AWS Systems Manager. To address this issue, ensure you follow step 5 of the deployment instructions.

case 2: user not able to change/update/modify default dashboards in Grafana UI

1.  You can't change/update default dashboards.
2.  If you need to make changes, please request for access for Editor/Admin role on

<br clear=""all"">

## Contributor Guide

XC3 is a community-driven project; we welcome your contribution! For code contributions, please read our [contribution guide](./CONTRIBUTING.md).

- File a [GitHub issue](https://github.com/XgridInc/xc3/issues) to report a bug or request a feature.
- Join our [Slack](https://join.slack.com/t/xgrid-group/shared_invite/zt-1uhzlrt6t-Dx_BqfQJKsHhSug1arbbAQ) for live conversations and quick questions.

<br clear=""all"">

## RoadMap

We welcome feedback and suggestions from our community! Please feel free to create an issue or join our discussion forum to share your thoughts.
For project updates, please read our [roadmap guide](./ROADMAP.md).

## License

XC3 is licensed under [Apache License, Version 2.0](./LICENSE).",FAUX
yeo/betterdev.link,Documentations,Documentations,2025-03-04T20:27:34Z,2024-10-14T02:23:52Z,0,0,0,0,0,1,0,0,2017-06-03T14:54:41Z,2025-03-04T20:28:01Z,2414,113,Elixir,VRAI,13,FAUX,9,,9,Links to improve programing skill,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,FAUX,10,"# Better Dev Link

A weekly/daily news letter of resources/articles around the web which is
language/framework agnostic to help readers build a better
understanding of programming in general.

# Submit Content

Create a PR and add a new issue

# Workflow

We use Go itself as a task runner. Since it's so fast, no need to
compile go. Just use `go run`

## build

```
go run cmd/compile.go
```

## run internal server

```
go run cmd/server.go
```

## deploy

```
go run cmd/deploy.go
```

## publish a new issue

```
go run cmd/publish.go issue-number

## Official sendout email
go run cmd/publish.go issue-number --send
```",FAUX
ZUGFeRD/quba-viewer,Application System,Application System,2025-05-15T15:18:54Z,2024-12-05T17:05:25Z,0,0,0,0,0,0,3,0,2021-02-18T08:08:58Z,2025-03-23T03:20:53Z,15686,147,XSLT,VRAI,32,FAUX,69,"e-rechnung,electron,electron-app,electronic-invoices,invoices,invoices-pdf,ubl,viewer,xrechnung,xrechnung-viewer,xslt",69,Quba is a viewer for electronic invoices,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,FAUX,VRAI,FAUX,14,"Quba-Viewer
=============

Quba-Viewer ([homepage](https://quba-viewer.org)) is a cross platform open source application to display 
structured (i.e., XML) and hybrid (Factur-X/ZUGFeRD PDF) electronic invoices.

Documentation-wise there is a doc RE [Architecture, Development, Debugging and testing](doc/development.md) in 
general and some Electron and E-Invoice peculiarities like [interprocess (IPC) communication, XSLT and codelists](doc/electron.md) in particular.

History
=============


<details>
<summary>1.4.3 </summary>
- #100 support ZF1
- #98 Correctly translate bt148 to gross instead of net
- #99 Register shortcut to close app by using cmd+q
- #88 Splash window ""About"" (""Info"" in German) can not be closed on MacOS

</details>

<details>
<summary>1.4.2 01.11.2024</summary>
- #54 print only prints first page
- #83 NaN for VAT amount and rounding amount 
- #82 Drag&Drop does not work

</details>

<details>
<summary>1.4.1 30.10.2024</summary>
- #78 default language to OS language, if de|en|fr
- #68 structured invoice data won't get parsed from zugferd-file
- #23 Does not work on intel macs
- #63 Mention license in about window
- #58 electron 20 to 33
- #53 MSI version

</details>

<details>
<summary>1.4.0 15.01.2024</summary>
- Online validation works again
- Corrected validation now possible with personalized account  
- Be able to visualize Order-X, i.e. CIO #30
- be able to display IDs in whatever language
- Factur-X / ZUGFeRD is now visualized by default and requires no click
- first attempts intel mac
- be able to load same filename multiple times #35
- menu item context for print (/xml and /pdf) and validate 
- get rid off temp files
- changed translation from 3 files to one, translations provided by the app
</details>


<details>
<summary>1.3.0 14.04.2023</summary>
    - support ubl credit notes
</details>

<details>
<summary>1.2.1 13.10.2022</summary>
    - upgrade to Electron 20 enabling e.g. newer Mac versions to run it
</details>

<details>
<summary>1.2.0 26.09.2022</summary>
    - Allow to validate files via Mustangserver
</details>

<details>
<summary>1.1.5 15.08.2022</summary>
    - XML invoice with large PDF (approx. > 1.4 MB) attached can't be opened #16
</details>

<details>
<summary>1.1.4 23.06.2022</summary>
    - No close button for attachments #15
</details>

<details>
<summary>1.1.3 28.04.2022</summary>
    - Accept invoices as command line arguments #12
</details>

<details>
<summary>1.1.2 11.11.2021</summary>
    - Allow drag&drop of files #9
</details>
<details>
<summary>1.1.1 24.09.2021</summary>
    - The window for the example files can now be closed
    - plain pdf without embedded xml not to show button for visualization
</details>
<details>
<summary>1.1.0 20.09.2021</summary>
    - #6 New document shows still old data / invoice
    - Switch to vue.js
    - i18n: Support for EN and FR
    - support FX Referenzprofil XRechnung
</details>
<details>
<summary>1.0 23.08.2021</summary>
    - #5 Codelists are now resolved
    - Support for first FX Extended Element, i.e., Cash Discount in XML 
</details>
<details>
<summary>0.5 27.07.2021</summary>
    - Support for PDF
    - Support for Factur-X/ZUGFeRD
    - Display errors as dialog instead of hiding them 
</details>
<details>
<summary>0.2 10.06.2021</summary>
    - Support for XRechnung (UBL)
    - ""dark"" theming
    - possibility to open multiple files at the same time in the same viewer (tabs)
    - print 
</details> 
<details>
<summary>0.1 2021-03-31</summary>
    - Initial release
    - Support for XRechnung (UN/CEFACT CII)
    - release for Windows on 2021-03-31, for Linux on 2101-04-16 and for Mac on 2021-04-27
</details> 

Pedigree
-------------

![History of Quba](doc/History_of_Quba-02.svg ""Pedigree of Quba"")

The FeRD had published visualization XSLTs for ZF1 (~=UN/CEFACT C13B) as open source, unfortunately the ones for the UN/CEFACT C16B-based
version 2 remains proprietary. The Kosit released XSLT for both CII and UBL of the XRechnung (XR) which has been used for various online viewers
but also for offline viewers like Ultramarinviewer and Open XRechnung Toolbox. Quba uses this work added translations and
at least experimental support for Factur-X/ZUGFeRD profiles higher than EN16931.

Known issues
=============

  * While XRechnung, EN16931 and below should work, not all FX attributes/elements of the Extended Profile have yet been mapped, feel free to [report missing ones](https://github.com/ZUGFeRD/quba-viewer/issues) 
  * conversion of XSLT to sef.json `xslt3 -xsl:ubl-creditnote-xr.xsl -export:ubl-creditnote-xr.sef.json -t` on windows works only in cmd.exe, not powershell",FAUX
